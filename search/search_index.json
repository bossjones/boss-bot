{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Boss-Bot","text":"<p>A powerful Discord bot with advanced AI capabilities and comprehensive development tooling.</p>"},{"location":"#overview","title":"Overview","text":"<p>Boss-Bot is a sophisticated Discord bot that leverages cutting-edge AI models and provides robust development tools. Built with modern Python practices, it combines Discord.py with LangChain, various AI models, and extensive monitoring capabilities.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Advanced AI Integration</li> <li>LangChain integration with multiple providers (Anthropic, OpenAI, Google, Groq)</li> <li>LangGraph for complex AI workflows</li> <li>Embeddings and vector search capabilities</li> <li> <p>Structured AI outputs with advanced prompting</p> </li> <li> <p>Discord Bot Capabilities</p> </li> <li>Modern Discord.py implementation</li> <li>Comprehensive command system</li> <li>File and media handling</li> <li> <p>Customizable bot behaviors</p> </li> <li> <p>Development Tools</p> </li> <li>UV package management for deterministic builds</li> <li>Comprehensive testing suite with pytest</li> <li>Advanced monitoring and logging</li> <li>Docker containerization</li> <li> <p>VSCode devcontainer support</p> </li> <li> <p>Documentation and Quality</p> </li> <li>MkDocs-based documentation</li> <li>Extensive type checking with pyright</li> <li>Code quality tools (ruff, pre-commit)</li> <li>Continuous Integration/Deployment</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/bossjones/boss-bot.git\ncd boss-bot\n\n# Set up environment variables\ncp .env.sample .env\n# Edit .env with your configuration\n\n# Install dependencies using UV\nuv sync\n\n# Install development dependencies\nuv sync --dev\n\n# Run tests\nuv run pytest\n\n# Start the bot\nuv run python -m boss_bot\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>boss-bot/\n\u251c\u2500\u2500 src/boss_bot/          # Main bot package\n\u2502   \u251c\u2500\u2500 bot/              # Core bot functionality\n\u2502   \u251c\u2500\u2500 commands/         # Bot commands\n\u2502   \u251c\u2500\u2500 core/            # Core utilities\n\u2502   \u251c\u2500\u2500 monitoring/      # Monitoring tools\n\u2502   \u2514\u2500\u2500 utils/           # Helper utilities\n\u251c\u2500\u2500 tests/               # Test suite\n\u251c\u2500\u2500 docs/               # Documentation\n\u2514\u2500\u2500 .cursor/rules/      # Cursor IDE automation rules\n</code></pre>"},{"location":"#development","title":"Development","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Discord Bot Token</li> <li>AI API Keys (Anthropic, OpenAI, etc.)</li> <li>Docker (optional)</li> </ul>"},{"location":"#local-development","title":"Local Development","text":"<ol> <li> <p>Set up your development environment:    <pre><code># Install dev dependencies\nuv sync --dev\n\n# Install pre-commit hooks\npre-commit install\n</code></pre></p> </li> <li> <p>Run tests:    <pre><code>uv run pytest\n</code></pre></p> </li> <li> <p>Build documentation:    <pre><code>uv run mkdocs serve\n</code></pre></p> </li> </ol>"},{"location":"#using-docker","title":"Using Docker","text":"<p>```bash</p>"},{"location":"#build-the-container","title":"Build the container","text":"<p>docker compose build</p>"},{"location":"#run-the-bot","title":"Run the bot","text":"<p>docker compose up ```****</p>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#local-development-documentation","title":"Local Development Documentation","text":"<ol> <li>Serve documentation with live reload:    ```bash    # Using mkdocs (recommended)    uv run mkdocs serve</li> </ol> <p># Using pdoc (API documentation)    uv run pdoc --docformat=google --port=8088 src/boss_bot    ```</p> <ol> <li>Build documentation:    ```bash    # Using mkdocs    uv run mkdocs build</li> </ol> <p># Using pdoc    uv run pdoc --docformat=google --output-directory=gh-docs src/boss_bot    ```</p> <ol> <li> <p>Deploy to GitHub Pages:    <code>bash    uv run mkdocs gh-deploy --force --message 'docs(mkdocs): update documentation [skip ci]'</code></p> </li> <li> <p>Visit the full documentation for detailed guides</p> </li> <li>Check CONTRIBUTING.md for contribution guidelines</li> <li>See CHANGELOG.md for version history</li> </ol>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"AI_INTEGRATION/","title":"Boss-Bot AI Integration Guide","text":""},{"location":"AI_INTEGRATION/#overview","title":"\ud83e\udd16 Overview","text":"<p>Boss-Bot now includes a sophisticated AI multi-agent system powered by LangChain and LangGraph for intelligent content analysis, strategy selection, and workflow orchestration. This system enhances the existing Epic 5 strategy pattern with AI capabilities while maintaining 100% backward compatibility.</p>"},{"location":"AI_INTEGRATION/#ai-architecture","title":"\ud83c\udfd7\ufe0f AI Architecture","text":""},{"location":"AI_INTEGRATION/#multi-agent-system-design","title":"Multi-Agent System Design","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Discord Commands                         \u2502\n\u2502  $smart-analyze  \u2502  $smart-download  \u2502    $ai-status        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                LangGraph Workflows                          \u2502\n\u2502              DownloadWorkflow                               \u2502\n\u2502   URL \u2192 Strategy Selection \u2192 Analysis \u2192 Download            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    AI Agents                                \u2502\n\u2502  StrategySelector \u2502 ContentAnalyzer \u2502 SocialMediaAgent      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Model Providers                                \u2502\n\u2502     OpenAI        \u2502   Anthropic     \u2502     Google            \u2502\n\u2502   (GPT-4, 3.5)    \u2502   (Claude)      \u2502   (Gemini)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"AI_INTEGRATION/#ai-agents","title":"\ud83e\udd16 AI Agents","text":""},{"location":"AI_INTEGRATION/#1-strategyselector-agent","title":"1. StrategySelector Agent","text":"<p>Purpose: Intelligent platform detection and optimal strategy selection</p> <p>Capabilities: - URL analysis with confidence scoring (0.0-1.0) - Platform-specific strategy recommendations - User preference integration - Fallback to traditional pattern matching</p> <p>Usage: <pre><code>from boss_bot.ai.agents.strategy_selector import StrategySelector\nfrom boss_bot.ai.agents.context import AgentRequest, AgentContext\n\nselector = StrategySelector(model=llm_model)\nrequest = AgentRequest(\n    context=AgentContext(request_id=\"unique_id\", user_id=\"123\"),\n    action=\"select_strategy\",\n    data={\"url\": \"https://twitter.com/user/status/123456789\"}\n)\n\nresponse = await selector.process_request(request)\nprint(f\"Platform: {response.result['platform']}\")\nprint(f\"Confidence: {response.confidence}\")\nprint(f\"Reasoning: {response.reasoning}\")\n</code></pre></p>"},{"location":"AI_INTEGRATION/#2-contentanalyzer-agent","title":"2. ContentAnalyzer Agent","text":"<p>Purpose: Advanced content quality assessment and metadata enrichment</p> <p>Capabilities: - Content quality scoring (0-10 scale) - Engagement potential prediction - Audience insights and targeting suggestions - Format optimization recommendations</p> <p>Usage: <pre><code>from boss_bot.ai.agents.content_analyzer import ContentAnalyzer\n\nanalyzer = ContentAnalyzer(model=llm_model)\nrequest = AgentRequest(\n    context=AgentContext(request_id=\"unique_id\", user_id=\"123\"),\n    action=\"analyze_content\",\n    data={\n        \"url\": \"https://youtube.com/watch?v=VIDEO_ID\",\n        \"metadata\": {\"title\": \"Amazing Video\", \"duration\": \"5:30\"}\n    }\n)\n\nresponse = await analyzer.process_request(request)\nprint(f\"Quality Score: {response.result['content_quality']}/10\")\nprint(f\"Engagement: {response.result['engagement_prediction']}\")\n</code></pre></p>"},{"location":"AI_INTEGRATION/#3-socialmediaagent","title":"3. SocialMediaAgent","text":"<p>Purpose: Specialized social media content processing and analysis</p> <p>Capabilities: - Sentiment analysis and trend detection - Cross-platform content coordination - Engagement optimization strategies - Content classification and moderation</p>"},{"location":"AI_INTEGRATION/#discord-commands","title":"\ud83d\udd27 Discord Commands","text":""},{"location":"AI_INTEGRATION/#ai-powered-commands","title":"AI-Powered Commands","text":""},{"location":"AI_INTEGRATION/#smart-analyze-url","title":"<code>$smart-analyze &lt;url&gt;</code>","text":"<p>Performs AI-powered content analysis with advanced insights.</p> <p>Example: <pre><code>$smart-analyze https://twitter.com/user/status/123456789\n</code></pre></p> <p>Output: <pre><code>\ud83e\udd16 \ud83d\udcfa AI analyzing Twitter/X content...\n\n\ud83e\udd16 AI Content Analysis for Twitter/X\n\ud83d\udd17 https://twitter.com/user/status/123456789\n\n\ud83d\udcca Quality Score: 8.5/10\n\ud83d\udccb Content Type: social_commentary\n\ud83d\udcc8 Engagement Potential: high\n\ud83d\udc65 Target Audience: tech enthusiasts\n\n\ud83d\udca1 AI Recommendations:\n\u2022 Consider sharing during peak hours for maximum engagement\n\n\ud83c\udfaf Confidence: 90%\n\ud83e\udde0 AI Reasoning: High-quality content with strong engagement signals\n\u26a1 Analysis Time: 250ms\n</code></pre></p>"},{"location":"AI_INTEGRATION/#smart-download-url-upload","title":"<code>$smart-download &lt;url&gt; [upload]</code>","text":"<p>AI-enhanced download with strategy optimization.</p> <p>Example: <pre><code>$smart-download https://youtube.com/watch?v=VIDEO_ID\n</code></pre></p> <p>Output: <pre><code>\ud83e\udd16 AI optimizing download strategy...\n\ud83e\udd16 AI selected YouTube strategy (confidence: 95%)\n\ud83e\udde0 AI Reasoning: Optimal strategy identified based on URL pattern and content type\n\n\ud83d\udca1 AI Recommendations:\n\u2022 quality: 1080p\n\u2022 format: mp4\n\n\ud83d\udcfa Downloading YouTube content: https://youtube.com/watch?v=VIDEO_ID\n\u2705 YouTube download completed!\n</code></pre></p>"},{"location":"AI_INTEGRATION/#ai-status","title":"<code>$ai-status</code>","text":"<p>Shows AI agent status and performance metrics.</p> <p>Example: <pre><code>$ai-status\n</code></pre></p> <p>Output: <pre><code>\ud83e\udd16 AI Agent Status\n\n\u2705 Strategy Selector: Active\n   \u2022 Requests Processed: 15\n   \u2022 Avg Response Time: 123.5ms\n\n\u2705 Content Analyzer: Active\n   \u2022 Requests Processed: 8\n   \u2022 Avg Response Time: 234.7ms\n\n\ud83c\udff3\ufe0f Feature Flags:\n\u2022 AI Strategy Selection: \u2705\n\u2022 AI Content Analysis: \u2705\n\u2022 AI Workflow Orchestration: \u2705\n\n\ud83d\udca1 Enable AI features with environment variables:\n   `AI_STRATEGY_SELECTION_ENABLED=true`\n   `AI_CONTENT_ANALYSIS_ENABLED=true`\n</code></pre></p>"},{"location":"AI_INTEGRATION/#enhanced-traditional-commands","title":"Enhanced Traditional Commands","text":""},{"location":"AI_INTEGRATION/#download-ai-enhanced","title":"<code>$download</code> (AI-Enhanced)","text":"<p>The traditional download command now uses AI strategy selection when enabled.</p> <p>AI Enhancement Indicators: <pre><code>\ud83e\udd16 AI selected Instagram strategy (confidence: 92%)\n\ud83d\ude80 Using experimental API-direct approach for Instagram\n</code></pre></p>"},{"location":"AI_INTEGRATION/#metadata-ai-enhanced","title":"<code>$metadata</code> (AI-Enhanced)","text":"<p>Metadata command enhanced with AI content analysis.</p> <p>AI Enhancement Indicators: <pre><code>\ud83d\udcfa YouTube Content Info (AI Enhanced)\n\ud83e\udd16 AI Insights: High-quality educational content detected\n</code></pre></p>"},{"location":"AI_INTEGRATION/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"AI_INTEGRATION/#environment-variables","title":"Environment Variables","text":"<pre><code># AI Feature Flags\nexport AI_STRATEGY_SELECTION_ENABLED=true   # Enable AI strategy selection\nexport AI_CONTENT_ANALYSIS_ENABLED=true     # Enable AI content analysis\nexport AI_WORKFLOW_ORCHESTRATION_ENABLED=true  # Enable LangGraph workflows\n\n# Model Provider (choose one)\nexport OPENAI_API_KEY=\"your-openai-api-key\"        # GPT-4, GPT-3.5\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"  # Claude models\nexport GOOGLE_API_KEY=\"your-google-api-key\"        # Gemini models\n</code></pre>"},{"location":"AI_INTEGRATION/#model-provider-priority","title":"Model Provider Priority","text":"<ol> <li>OpenAI (if <code>OPENAI_API_KEY</code> set) \u2192 Uses <code>gpt-4o-mini</code></li> <li>Anthropic (if <code>ANTHROPIC_API_KEY</code> set) \u2192 Uses <code>claude-3-haiku-20240307</code></li> <li>Google (if <code>GOOGLE_API_KEY</code> set) \u2192 Uses <code>gemini-1.5-flash</code></li> <li>Fallback \u2192 Traditional non-AI methods</li> </ol>"},{"location":"AI_INTEGRATION/#feature-flag-control","title":"Feature Flag Control","text":"<pre><code>from boss_bot.core.downloads.feature_flags import DownloadFeatureFlags\n\nflags = DownloadFeatureFlags(settings)\n\n# Check AI capabilities\nif flags.ai_strategy_selection_enabled:\n    # Use AI for strategy selection\n    pass\n\nif flags.ai_content_analysis_enabled:\n    # Use AI for content analysis\n    pass\n</code></pre>"},{"location":"AI_INTEGRATION/#langgraph-workflows","title":"\ud83d\udd04 LangGraph Workflows","text":""},{"location":"AI_INTEGRATION/#downloadworkflow","title":"DownloadWorkflow","text":"<p>The <code>DownloadWorkflow</code> orchestrates multi-agent coordination for complex download scenarios.</p> <p>Workflow States: 1. URL Analysis \u2192 StrategySelector Agent 2. Content Analysis \u2192 ContentAnalyzer Agent 3. Strategy Execution \u2192 Download Strategy 4. Validation \u2192 Result Processing 5. Upload \u2192 Discord Integration</p> <p>Usage: <pre><code>from boss_bot.ai.workflows.download_workflow import DownloadWorkflow\n\nworkflow = DownloadWorkflow(\n    strategy_selector=strategy_selector,\n    content_analyzer=content_analyzer\n)\n\nresult = await workflow.execute({\n    \"url\": \"https://twitter.com/user/status/123\",\n    \"user_preferences\": {},\n    \"download_options\": {}\n})\n</code></pre></p>"},{"location":"AI_INTEGRATION/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"AI_INTEGRATION/#ai-test-structure","title":"AI Test Structure","text":"<pre><code>tests/test_ai/\n\u251c\u2500\u2500 test_agents/\n\u2502   \u251c\u2500\u2500 test_base_agent.py           # 17 tests\n\u2502   \u251c\u2500\u2500 test_strategy_selector.py    # 13 tests\n\u2502   \u251c\u2500\u2500 test_content_analyzer.py     # 11 tests\n\u2502   \u2514\u2500\u2500 test_social_media_agent.py   # 12 tests\n\u251c\u2500\u2500 test_workflows/\n\u2502   \u2514\u2500\u2500 test_download_workflow.py    # 20 tests\n\u2514\u2500\u2500 test_integration/\n    \u2514\u2500\u2500 test_discord_ai_integration.py # 12 tests\n</code></pre>"},{"location":"AI_INTEGRATION/#running-ai-tests","title":"Running AI Tests","text":"<pre><code># Run all AI tests\nuv run python -m pytest tests/test_ai/ -v\n\n# Run specific agent tests\nuv run python -m pytest tests/test_ai/test_agents/test_strategy_selector.py -v\n\n# Run Discord integration tests\nuv run python -m pytest tests/test_bot/test_discord_ai_integration.py -v\n</code></pre>"},{"location":"AI_INTEGRATION/#test-patterns","title":"Test Patterns","text":""},{"location":"AI_INTEGRATION/#mock-ai-agent-testing","title":"Mock AI Agent Testing","text":"<pre><code>@pytest.mark.asyncio\nasync def test_ai_strategy_selection(mocker):\n    \"\"\"Test AI strategy selection with mocked response.\"\"\"\n    # Mock AI agent\n    mock_agent = mocker.Mock()\n    mock_response = AgentResponse(\n        success=True,\n        result={\"platform\": \"twitter\", \"confidence\": 0.95},\n        confidence=0.95,\n        reasoning=\"Twitter URL pattern detected\"\n    )\n    mock_agent.process_request = mocker.AsyncMock(return_value=mock_response)\n\n    # Test strategy selection\n    strategy, metadata = await get_ai_enhanced_strategy(url, mock_agent)\n\n    assert strategy is not None\n    assert metadata[\"ai_enhanced\"] is True\n    assert metadata[\"confidence\"] == 0.95\n</code></pre>"},{"location":"AI_INTEGRATION/#feature-flag-testing","title":"Feature Flag Testing","text":"<pre><code>def test_ai_disabled_fallback(ai_disabled_settings):\n    \"\"\"Test fallback when AI is disabled.\"\"\"\n    flags = DownloadFeatureFlags(ai_disabled_settings)\n\n    assert not flags.ai_strategy_selection_enabled\n    assert not flags.ai_content_analysis_enabled\n\n    # Should fall back to traditional methods\n    strategy = get_strategy_for_url(url)  # Traditional method\n    assert strategy is not None\n</code></pre>"},{"location":"AI_INTEGRATION/#performance-monitoring","title":"\ud83d\udcca Performance Monitoring","text":""},{"location":"AI_INTEGRATION/#agent-metrics","title":"Agent Metrics","text":"<p>Each AI agent tracks performance metrics:</p> <pre><code># Access agent performance metrics\nmetrics = strategy_selector.performance_metrics\n\nprint(f\"Request Count: {metrics['request_count']}\")\nprint(f\"Average Response Time: {metrics['average_processing_time_ms']}ms\")\nprint(f\"Success Rate: {metrics['success_rate']:.2%}\")\nprint(f\"Error Rate: {metrics['error_rate']:.2%}\")\n</code></pre>"},{"location":"AI_INTEGRATION/#performance-targets","title":"Performance Targets","text":"<ul> <li>Response Time: &lt; 500ms per AI request</li> <li>Success Rate: &gt; 95% for AI operations</li> <li>Fallback Rate: &lt; 5% (AI failures requiring fallback)</li> <li>Memory Usage: &lt; 100MB additional for AI components</li> </ul>"},{"location":"AI_INTEGRATION/#error-handling","title":"\ud83d\udea8 Error Handling","text":""},{"location":"AI_INTEGRATION/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>try:\n    # Try AI-enhanced processing\n    response = await ai_agent.process_request(request)\n    if response.success:\n        return ai_enhanced_result(response)\nexcept Exception as ai_error:\n    logger.warning(f\"AI processing failed: {ai_error}\")\n\n# Always fall back to traditional methods\nreturn traditional_processing(url)\n</code></pre>"},{"location":"AI_INTEGRATION/#error-types","title":"Error Types","text":"<ol> <li>Model Provider Errors: API key issues, rate limits, service outages</li> <li>Agent Processing Errors: Invalid input, processing failures</li> <li>Workflow Errors: State machine issues, coordination failures</li> <li>Configuration Errors: Missing settings, invalid feature flags</li> </ol>"},{"location":"AI_INTEGRATION/#monitoring","title":"Monitoring","text":"<ul> <li>All AI errors are logged with context</li> <li>Fallback usage is tracked for monitoring</li> <li>Performance metrics include error rates</li> <li>Feature flag status is validated at startup</li> </ul>"},{"location":"AI_INTEGRATION/#langgraph-assistant-management","title":"\ud83c\udfaf LangGraph Assistant Management","text":"<p>Boss-Bot includes a comprehensive CLI system for managing LangGraph Cloud assistants, enabling deployment, synchronization, and lifecycle management of AI assistants.</p>"},{"location":"AI_INTEGRATION/#key-features","title":"Key Features","text":"<ul> <li>Cloud Integration: Direct connection to LangGraph Cloud deployments</li> <li>YAML Configuration: Human-readable assistant definitions with schema validation</li> <li>Bidirectional Sync: Deploy local configs or backup cloud assistants</li> <li>Rich CLI Output: Beautiful tables and progress indicators using Rich</li> <li>Version Control: Track assistant configurations in git</li> </ul>"},{"location":"AI_INTEGRATION/#cli-commands","title":"CLI Commands","text":"<pre><code># Check connectivity\nboss-bot assistants health\n\n# List all assistants\nboss-bot assistants list --graph download_workflow\n\n# Create new assistant config\nboss-bot assistants create-config --name content-analyzer\n\n# Deploy to cloud\nboss-bot assistants sync-to assistants/content-analyzer.yaml\n\n# Backup from cloud\nboss-bot assistants sync-from --output-dir backups/\n</code></pre>"},{"location":"AI_INTEGRATION/#integration-with-ai-system","title":"Integration with AI System","text":"<p>The assistant management system seamlessly integrates with Boss-Bot's AI features:</p> <ol> <li>Dynamic Loading: Deployed assistants are automatically available to Discord commands</li> <li>Workflow Integration: Assistants work within LangGraph workflows for complex tasks</li> <li>Multi-Agent Coordination: Multiple assistants can collaborate on download tasks</li> <li>Hot Reloading: Update assistants without restarting the bot</li> </ol>"},{"location":"AI_INTEGRATION/#configuration_1","title":"Configuration","text":"<pre><code># Required environment variables\nLANGGRAPH_DEPLOYMENT_URL=https://your-deployment.langraph.app\nLANGGRAPH_API_KEY=your-api-key-here\nLANGGRAPH_DEFAULT_GRAPH=download_workflow\n</code></pre> <p>\ud83d\udcda Complete Assistant Management Guide - Detailed documentation with examples</p>"},{"location":"AI_INTEGRATION/#future-enhancements","title":"\ud83d\udd1c Future Enhancements","text":""},{"location":"AI_INTEGRATION/#planned-ai-features","title":"Planned AI Features","text":"<ul> <li>Vision Models: Image and video content analysis</li> <li>Advanced Workflows: Complex multi-agent coordination scenarios</li> <li>User Learning: Personalized recommendations based on usage patterns</li> <li>Content Moderation: AI-powered safety and compliance checking</li> <li>Batch Processing: Intelligent queue optimization and prioritization</li> </ul>"},{"location":"AI_INTEGRATION/#expansion-areas","title":"Expansion Areas","text":"<ul> <li>Additional Agents: Moderation, recommendation, trend analysis</li> <li>Custom Workflows: User-defined multi-step AI processes</li> <li>Advanced Memory: Long-term conversation and preference storage</li> <li>External Integrations: Third-party AI services and APIs</li> </ul>"},{"location":"AI_INTEGRATION/#additional-resources","title":"\ud83d\udcd6 Additional Resources","text":"<ul> <li>LangChain Documentation</li> <li>LangGraph Documentation</li> <li>LangSmith Tracing</li> <li>Project Architecture</li> <li>Testing Guidelines</li> </ul> <p>Status: \u2705 Complete and Production Ready Last Updated: 2025-06-28 Test Coverage: 873+ tests including AI &amp; assistant management (100% passing)</p>"},{"location":"agile-readme/","title":"Cursor Agile Workflow Documentation","text":"<p>This document provides comprehensive documentation for the Agile workflow system integrated with Cursor's AI capabilities. The workflow is designed to maintain project focus and memory and ensure consistent progress through a structured approach to development.</p>"},{"location":"agile-readme/#overview","title":"Overview","text":"<p>The Agile-Cursor workflow combines traditional Agile methodologies with AI-assisted development to create a powerful, efficient development process. It can be utilized in two primary ways:</p> <ol> <li> <p>Rule-Based Implementation (Automatic)</p> </li> <li> <p>Uses <code>.cursor/rules/workflows/workflow-agile-manual</code> and <code>.cursor/templates</code></p> </li> <li>Automatically applies standards to matching files</li> <li>Provides consistent structure enforcement</li> </ol>"},{"location":"agile-readme/#work-item-hierarchy","title":"Work Item Hierarchy","text":"<pre><code>graph TD\n    E[Epic] --&gt; S[Story]\n    S --&gt; T[Task]\n    T --&gt; ST[Subtask]\n\n    style E fill:#f9f,stroke:#333,stroke-width:2px\n    style S fill:#dfd,stroke:#333,stroke-width:2px\n    style T fill:#bbf,stroke:#333,stroke-width:2px\n    style ST fill:#ffd,stroke:#333,stroke-width:2px\n</code></pre> <ol> <li> <p>Epics</p> </li> <li> <p>Large, self-contained features</p> </li> <li>Only one active at a time</li> <li> <p>Example: \"Online Matchmaking System\"</p> </li> <li> <p>Stories</p> </li> <li> <p>Smaller, implementable work units</p> </li> <li>Must belong to an Epic</li> <li> <p>Example: \"User Profile Creation\"</p> </li> <li> <p>Tasks</p> </li> <li> <p>Technical implementation steps</p> </li> <li>Clear completion criteria</li> <li> <p>Example: \"Implement Database Schema\"</p> </li> <li> <p>Subtasks</p> </li> <li>Granular work items</li> <li>Often includes test requirements</li> <li>Example: \"Write Unit Tests\"</li> </ol>"},{"location":"agile-readme/#ai-project-plan-and-memory-structure-the-workflow-will-result-in","title":"AI Project Plan and Memory Structure the Workflow will result in","text":"<pre><code>.ai/\n\u251c\u2500\u2500 prd.md                 # Product Requirements Document\n\u251c\u2500\u2500 arch.md               # Architecture Decision Record\n\u251c\u2500\u2500 epic-1/              # Current Epic directory\n\u2502   \u251c\u2500\u2500 story-1.story.md  # Story files for Epic 1\n\u2502   \u251c\u2500\u2500 story-2.story.md\n\u2502   \u2514\u2500\u2500 story-3.story.md\n\u251c\u2500\u2500 epic-2/              # Future Epic directory\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 epic-3/              # Future Epic directory\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"agile-readme/#workflow-phases","title":"Workflow Phases","text":""},{"location":"agile-readme/#1-initial-planning","title":"1. Initial Planning","text":"<ul> <li>Focus on documentation and planning</li> <li>Only modify <code>.ai/</code>, docs, readme, and rules</li> <li>Required approvals for PRD and then the Architecture</li> </ul>"},{"location":"agile-readme/#2-development-phase","title":"2. Development Phase","text":"<ul> <li>Generates the first or next story and waits on approval</li> <li>Implementation of approved in progress story</li> <li>Task-by-task story execution</li> <li>Continuous testing and validation</li> </ul> <pre><code>graph LR\n    subgraph PLAN Phase\n        A[Project Idea] --&gt; B[PRD Creation]\n        B --&gt; C[Architecture Design]\n        C --&gt; D[Epic Planning]\n        D --&gt; E[Story Definition]\n    end\n\n    subgraph ACT Phase\n        E --&gt; F[Story Implementation]\n        F --&gt; G[Testing &amp; Validation]\n        G --&gt; H[Story Completion]\n    end\n\n    subgraph Iteration\n        H --&gt; |Next Story|E\n        H --&gt; |Epic Complete|I[Epic Completion]\n        I --&gt; |Next Epic|D\n        I --&gt; |Project Complete|J[Release]\n    end\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#dfd,stroke:#333,stroke-width:2px\n    style C fill:#dfd,stroke:#333,stroke-width:2px\n    style D fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#bbf,stroke:#333,stroke-width:2px\n    style F fill:#bbf,stroke:#333,stroke-width:2px\n    style G fill:#bbf,stroke:#333,stroke-width:2px\n    style H fill:#bbf,stroke:#333,stroke-width:2px\n    style I fill:#f9f,stroke:#333,stroke-width:2px\n    style J fill:#f9f,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"agile-readme/#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"agile-readme/#story-implementation-process","title":"Story Implementation Process","text":"<ol> <li> <p>Initialization</p> </li> <li> <p>Verify <code>.ai</code> directory exists</p> </li> <li>Locate approved architecture and current story</li> <li> <p>Ensure story is properly marked as in-progress</p> </li> <li> <p>Development Flow</p> </li> <li> <p>Follow Test-Driven Development (TDD)</p> </li> <li>Update task/subtask status regularly</li> <li>Document all implementation notes</li> <li> <p>Record significant commands used</p> </li> <li> <p>Completion Requirements</p> </li> <li>All tests must pass</li> <li>Documentation must be updated</li> <li>User must approve completion</li> </ol>"},{"location":"agile-readme/#critical-rules","title":"Critical Rules","text":"<p>\ud83d\udea8 Critical Rules:</p> <ul> <li>Never creates first story without PRD and Architecture approval</li> <li>Only one Epic can be in-progress at a time</li> <li>Only one Story can be in-progress at a time</li> <li>Stories must be implemented in PRD-specified order</li> <li>Never implement without story approval from user (marked as in progress on the story file)</li> </ul>"},{"location":"agile-readme/#using-the-workflow","title":"Using the Workflow","text":"<p>The best way post 0.47.x+ of cursor is to use the rules based approach, with either manual, agent selection or always on rules. I prefer manual selection type rule for the workflows, so that they will not be in a context if I do not need it (explanation to follow).</p> <p>If I am starting a brand new project (with our without an existing code template) I have a few options:</p> <ul> <li>Use an external tool to generate the PRD (Such as ChatGPT Canvas or o3 mini Web UI or Google AI Studio)</li> <li>Use the workflow and agent in cursor to generate the PRD   (This comes down to personal preference and consideration of token burn within cursor)</li> </ul> <p>If I am doing this in cursor, I will start a new Agent chat with Claude 3.7 Thinking (or choose a different model if concerned about credit burn) and type something like:</p> <p><code>Lets follow the @workflow-agile-manual to create a PRD for a new project I want to create that will do XYZ, have the following features etc etc. Lets focus on just the MVP feature first will be to deliver X minimally, but lets also plan to have some epics for fast follows or future enhancements such as A B and C.</code></p> <p>As this can be quite lengthy, I will many times craft this prompt in the xnotes folder, and then paste it into the chat, ensuring that the @workflow is still properly added.</p> <p>Note: you can also modify the workflow-agile-manual to be Agent auto-selectable, this work reliably well also - you will just need to ensure the description you give it in the front matter will ensure its used when needed (PRD story and work implementation phases) - or potentially just make it an always rule. When starting out, its fine to make it an always rule, until your project grows to a very significant size, then I suggest turning it off manually, as at that point you might be just going in and making very targeted updates to specific files or features - and do not need the whole workflow as overhead - or you might want to instead select a different workflow (maybe a refactor workflow, a test workflow, an external MCP agent, etc...)</p> <p>The agent should generate a draft prd.md file in a .ai folder.</p> <p>I suggest at this point, you do not approve and jump right in - either in cursor with the agent, or an external tool - engage further with the agent to refine the document, have the agent ask you questions on holes in the document that it might want to know the answer to, ask the agent if it needs any clarifications that will allow for a very jr agent developer to understand and implement the stories, ask the agent if the sequencing of the stories make sense etc...</p> <p>Once you feel its in a good spot - you can mark the file as status: approved.</p> <p>At this point, I would start another chat and with the workflow - the agent will first check for the prd, and then if its approved, will offer to create (if not already existing and approved) the architecture file - and similar a new chat window with the workflow will search for the new first or in progress story.</p> <p>Once a story is in progress and approved by the user - the agent can be told to execute the story. Once a story or part of a story is completed and the story file is updated with progress by the agent, commit often (I use my manual gitpush.mdc manual rule macro). After this, I might start a new chat window with a fresh context and the workflow again loaded. Once a story is complete (status: complete) and tested and pushed, I always will start a new chat window with the workflow, and ask the agent to 'Create the next story draft' - or just ask it what it thinks it should do next, it should recognize what is next story to do from the prd and what story was last marked completed, and generate a draft for the next story, and then stop and ask for my approval before doing any further coding.</p> <p>A more detailed example, up to date repo and video coming soon, but this should give the main ideas...</p> <p>NOTE: Some models (Sonnet 3.7 thinking) have gotten a bit overly aggressive, so the rules might need to be tuned to further ensure the agent does not start updating code until the story is approved.</p>"},{"location":"agile-readme/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Documentation and tips</p> </li> <li> <p>AI will keep PRD and Architecture documents updated - sometimes you will need to tell it to update the prd and arch files as needed.</p> </li> <li>Document all significant decisions</li> <li>Maintain clear implementation notes</li> <li> <p>Have the AI create readme.md files in each src subfolder to help give it direction</p> </li> <li> <p>Testing</p> </li> <li> <p>Have the AI Write tests before implementation - a fun excercise in TDD</p> </li> <li>Maintain high test coverage</li> <li> <p>Verify all tests pass before completion</p> </li> <li> <p>Progress Tracking</p> </li> <li> <p>Have the AI (or you) update story status regularly</p> </li> <li>Record all implementation notes</li> <li> <p>Document command history</p> </li> <li> <p>Context Management</p> </li> <li>Start fresh composer instance per story or after significant recorded progress (recorded in task completion updates)</li> <li>Use appropriate context level</li> <li>Minimize context overhead</li> <li>Consider making a leaner workflow when you are in story execution mode - that does not need all of the templates and overhead of how to create a prd and a architecture. But you will need to consider what other files or parts of other files it might need reference to to retain the plot. This is why currently I still use the full workflow.</li> </ol>"},{"location":"agile-readme/#status-progression","title":"Status Progression","text":"<p>Stories follow a strict status progression:</p> <pre><code>Draft -&gt; In Progress -&gt; Complete\n</code></pre> <p>Epics follow a similar progression:</p> <pre><code>Future -&gt; Current -&gt; Complete\n</code></pre>"},{"location":"agile-readme/#integration-with-cursor-ai","title":"Integration with Cursor AI","text":"<p>The workflow is designed to work seamlessly with Cursor's AI capabilities:</p> <ol> <li> <p>AI-Assisted Planning</p> </li> <li> <p>AI helps create and refine PRD</p> </li> <li>AI suggests architecture improvements</li> <li> <p>AI assists in story breakdown</p> </li> <li> <p>AI-Assisted Implementation</p> </li> <li> <p>AI implements story tasks</p> </li> <li>AI maintains test coverage</li> <li> <p>AI updates documentation</p> </li> <li> <p>AI-Assisted Review</p> </li> <li>AI verifies completion criteria</li> <li>AI suggests improvements</li> <li>AI maintains consistency</li> </ol>"},{"location":"agile-readme/#cost-savings","title":"Cost Savings","text":"<ul> <li>LLMs outside of Cursor, if you have them available, such as ChatGPT, Claude, Gemini, etc. are also great to generate the initial PRD and architecture, and really iterate on them.</li> <li>Within Cursor, currently you can use DeepSeek R1 for example which seems to be free and also decent and udpating PRD and architecture - but I have found it to be a bit less reliable than using Claude to follow the format I want - but much cheaper, if trying to do it all in cursor planning.</li> </ul>"},{"location":"ai/","title":".ai docs","text":""},{"location":"ai/#table-of-contents","title":"Table of Contents","text":"<ul> <li>.ai/architecture/arch.md</li> <li>.ai/defered-stories/defered-story-1.story.md</li> <li>.ai/diagrams/module-fixtures-current.md</li> <li>.ai/diagrams/module-fixtures-recommended.md</li> <li>.ai/diagrams/pytest_fixtures_relationships.mmd</li> <li>.ai/prd/prd.md</li> <li>.ai/stories/epic-1.epic.md</li> <li>.ai/stories/story-1.story.md</li> </ul>"},{"location":"ai/#file-architecture-arch-md","title":".ai/architecture/arch.md","text":""},{"location":"ai/#architecture-for-boss-bot-a-discord-media-download-and-rag-assistant","title":"Architecture for Boss-Bot: A Discord Media Download and RAG Assistant","text":"<p>Status: Approved</p>"},{"location":"ai/#technical-summary","title":"Technical Summary","text":"<p>Boss-Bot is a Discord bot designed to provide reliable media download capabilities from platforms like Twitter and Reddit, with a foundation for future RAG (Retrieval-Augmented Generation) features. The architecture follows a modular, event-driven design using Python 3.12 and discord.py as the core framework. The system implements a robust queue management system for downloads, comprehensive error handling, and a test-driven development approach.</p>"},{"location":"ai/#technology-table","title":"Technology Table","text":"Technology Description Version Python Primary development language, chosen for strong async support and modern features &gt;=3.12 discord.py Discord bot framework providing event handling and API integration &gt;=2.5.2 gallery-dl Media download utility for Twitter, Reddit, and other platforms &gt;=1.29.3 yt-dlp YouTube/video download utility for extended platform support Latest httpx Modern HTTP client for async API interactions Latest pydantic Data validation and settings management &gt;=2.0 pydantic-settings Environment configuration management Latest loguru Advanced logging with structured output Latest pytest Testing framework with async support Latest pytest-asyncio Async test support Latest dpytest Discord.py testing utilities Latest better-exceptions Enhanced exception handling and formatting Latest ruff Fast Python linter and formatter written in Rust Latest uv Modern Python package manager for dependency management Latest"},{"location":"ai/#architectural-diagrams","title":"Architectural Diagrams","text":""},{"location":"ai/#core-system-architecture","title":"Core System Architecture","text":"<pre><code>graph TB\n    DC[Discord Client] --&gt; EH[Event Handler]\n    EH --&gt; CM[Command Manager]\n    CM --&gt; DQ[Download Queue]\n    DQ --&gt; DM[Download Manager]\n    DM --&gt; TD[Twitter Downloader]\n    DM --&gt; RD[Reddit Downloader]\n    DM --&gt; FS[File Storage]\n    FS --&gt; DU[Discord Uploader]\n\n    subgraph Monitoring\n        M[Metrics] --&gt; L[Logging]\n        L --&gt; EX[Error Handler]\n    end\n</code></pre>"},{"location":"ai/#download-flow","title":"Download Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant B as Bot\n    participant Q as Queue\n    participant D as Downloader\n    participant F as FileSystem\n    participant Di as Discord\n\n    U-&gt;&gt;B: $dlt command\n    B-&gt;&gt;Q: Add to queue\n    Q-&gt;&gt;B: Position update\n    B-&gt;&gt;U: Queue status\n    Q-&gt;&gt;D: Process download\n    D-&gt;&gt;F: Save file\n    F-&gt;&gt;Di: Upload file\n    Di-&gt;&gt;U: File link\n</code></pre>"},{"location":"ai/#environment-configuration","title":"Environment Configuration","text":""},{"location":"ai/#settings-management","title":"Settings Management","text":"<pre><code>from pydantic_settings import BaseSettings\nfrom pydantic import SecretStr\n\nclass BotSettings(BaseSettings):\n    \"\"\"Bot configuration settings.\"\"\"\n    # Sensitive data using SecretStr\n    discord_token: SecretStr\n\n    # Basic configuration\n    command_prefix: str = \"$\"\n    max_concurrent_downloads: int = 5\n    max_queue_size: int = 50\n\n    class Config:\n        env_prefix = \"BOSS_BOT_\"\n</code></pre>"},{"location":"ai/#required-environment-variables","title":"Required Environment Variables","text":"<pre><code># Required\nBOSS_BOT_DISCORD_TOKEN=required      # Discord bot token (sensitive)\nBOSS_BOT_COMMAND_PREFIX=$            # Command prefix (optional)\n\n# Optional with defaults\nBOSS_BOT_MAX_CONCURRENT_DOWNLOADS=5\nBOSS_BOT_MAX_QUEUE_SIZE=50\n</code></pre>"},{"location":"ai/#data-models","title":"Data Models","text":""},{"location":"ai/#core-models","title":"Core Models","text":"<pre><code>from datetime import datetime\nfrom typing import Optional, List\nfrom uuid import UUID\nfrom pydantic import BaseModel, HttpUrl\n\nclass DownloadStatus(str, Enum):\n    \"\"\"Status of a download item.\"\"\"\n    QUEUED = \"queued\"\n    DOWNLOADING = \"downloading\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\nclass DownloadPriority(int, Enum):\n    \"\"\"Priority levels for downloads.\"\"\"\n    LOW = 0\n    NORMAL = 1\n    HIGH = 2\n\nclass DownloadItem(BaseModel):\n    \"\"\"Represents a single download request.\"\"\"\n    id: UUID\n    url: HttpUrl\n    status: DownloadStatus\n    priority: DownloadPriority\n    user_id: int\n    guild_id: int\n    channel_id: int\n    created_at: datetime\n    progress: float = 0.0\n    total_size: Optional[int] = None\n    error_message: Optional[str] = None\n\nclass QueueState(BaseModel):\n    \"\"\"Represents the current state of the download queue.\"\"\"\n    items: List[DownloadItem]\n    active_downloads: int\n    total_items: int\n    queue_size: int\n\n    @property\n    def is_full(self) -&gt; bool:\n        return self.total_items &gt;= self.queue_size\n\nclass ErrorResponse(BaseModel):\n    \"\"\"Standardized error response.\"\"\"\n    error_code: str\n    message: str\n    details: Optional[str] = None\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n</code></pre>"},{"location":"ai/#project-structure","title":"Project Structure","text":"<pre><code>boss-bot/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 boss_bot/\n\u2502   \u2502   \u251c\u2500\u2500 bot/           # Discord bot core\n\u2502   \u2502   \u251c\u2500\u2500 commands/      # Command implementations\n\u2502   \u2502   \u251c\u2500\u2500 core/          # Core functionality\n\u2502   \u2502   \u251c\u2500\u2500 downloaders/   # Download implementations\n\u2502   \u2502   \u251c\u2500\u2500 schemas/       # Data models\n\u2502   \u2502   \u2514\u2500\u2500 utils/         # Utilities\n\u251c\u2500\u2500 tests/                 # Test suite\n\u251c\u2500\u2500 docs/                  # Documentation\n\u2514\u2500\u2500 scripts/              # Utility scripts\n</code></pre>"},{"location":"ai/#infrastructure","title":"Infrastructure","text":""},{"location":"ai/#development-environment","title":"Development Environment","text":"<ul> <li>Local development using Python virtual environments</li> <li>UV for dependency management</li> <li>Pre-commit hooks for code quality</li> <li>pytest for testing infrastructure</li> </ul>"},{"location":"ai/#runtime-environment","title":"Runtime Environment","text":"<ul> <li>Local machine (macOS) or Linux server</li> <li>Python 3.12+ runtime</li> <li>Persistent storage for temporary files</li> <li>Discord API integration</li> <li>Logging and monitoring setup</li> </ul>"},{"location":"ai/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Basic system requirements for local/server deployment:</li> <li>CPU: 1-2 cores</li> <li>Memory: 512MB minimum</li> <li>Storage: 5GB minimum for temporary files</li> <li>Network: Stable internet connection</li> </ul>"},{"location":"ai/#deployment-plan","title":"Deployment Plan","text":""},{"location":"ai/#phase-1-mvp-setup","title":"Phase 1: MVP Setup","text":"<ol> <li>Initialize Python project with UV</li> <li>Set up testing infrastructure with GitHub Actions CI/CD</li> <li>Implement basic Discord bot framework</li> <li>Add core download functionality</li> <li>Implement queue management</li> <li>Set up local deployment via <code>uv run bossbotctl</code></li> </ol>"},{"location":"ai/#phase-2-enhanced-features","title":"Phase 2: Enhanced Features","text":"<ol> <li>Add progress tracking</li> <li>Implement file management</li> <li>Add error handling</li> <li>Set up monitoring</li> <li>Add user management</li> <li>Implement bot admin restart command</li> </ol>"},{"location":"ai/#phase-3-security-enhancements-nice-to-have","title":"Phase 3: Security Enhancements (Nice to Have)","text":"<ol> <li>Implement API key rotation strategy</li> <li>Add detailed rate limiting per command</li> <li>Enhance file storage security</li> <li>Implement granular user permissions</li> <li>Set up audit logging</li> <li>Add security monitoring integration</li> </ol>"},{"location":"ai/#phase-4-future-rag-integration","title":"Phase 4: Future RAG Integration","text":"<ol> <li>Set up vector store</li> <li>Implement document processing</li> <li>Add RAG functionality</li> <li>Enhance command set</li> </ol>"},{"location":"ai/#error-handling-resilience","title":"Error Handling &amp; Resilience","text":"<p>The system leverages built-in error handling and resilience features from gallery-dl and yt-dlp, complemented by our own error management:</p>"},{"location":"ai/#platform-specific-rate-limiting","title":"Platform-Specific Rate Limiting","text":"<ul> <li>Utilizes gallery-dl and yt-dlp's built-in rate limit handling</li> <li>Automatic backoff and retry mechanisms per platform</li> <li>Configurable delay between requests</li> <li>Platform-specific quota management</li> </ul>"},{"location":"ai/#download-retry-strategy","title":"Download Retry Strategy","text":"<ul> <li>Leverages gallery-dl and yt-dlp's retry mechanisms</li> <li>Automatic retry on transient failures</li> <li>Exponential backoff for repeated failures</li> <li>Maximum retry attempts configurable per platform</li> </ul>"},{"location":"ai/#network-resilience","title":"Network Resilience","text":"<ul> <li>Timeout handling through gallery-dl and yt-dlp</li> <li>Automatic connection recovery</li> <li>Partial download resume capability</li> <li>Bandwidth throttling support</li> </ul>"},{"location":"ai/#failure-recovery","title":"Failure Recovery","text":"<ul> <li>Automatic cleanup of failed downloads</li> <li>Session recovery after bot restart</li> <li>Queue state preservation</li> <li>Incomplete download handling</li> </ul> <pre><code>class DownloadRetryConfig:\n    \"\"\"Configuration for download retry behavior.\"\"\"\n    MAX_RETRIES = 3\n    INITIAL_DELAY = 1.0  # seconds\n    MAX_DELAY = 30.0  # seconds\n    BACKOFF_FACTOR = 2.0\n\n    @classmethod\n    def get_retry_delay(cls, attempt: int) -&gt; float:\n        \"\"\"Calculate delay for retry attempt.\"\"\"\n        delay = cls.INITIAL_DELAY * (cls.BACKOFF_FACTOR ** (attempt - 1))\n        return min(delay, cls.MAX_DELAY)\n\nclass DownloadManager:\n    \"\"\"Manages download operations with resilience.\"\"\"\n\n    async def download_with_retry(\n        self,\n        url: str,\n        download_id: str\n    ) -&gt; Path:\n        \"\"\"Attempt download with retry logic.\"\"\"\n        for attempt in range(1, DownloadRetryConfig.MAX_RETRIES + 1):\n            try:\n                return await self._perform_download(url, download_id)\n            except Exception as e:\n                if attempt == DownloadRetryConfig.MAX_RETRIES:\n                    raise DownloadError(f\"Max retries exceeded: {str(e)}\")\n\n                delay = DownloadRetryConfig.get_retry_delay(attempt)\n                logger.warning(\n                    f\"Download attempt {attempt} failed, retrying in {delay}s\",\n                    error=str(e),\n                    download_id=download_id\n                )\n                await asyncio.sleep(delay)\n</code></pre>"},{"location":"ai/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>The project uses GitHub Actions for continuous integration and deployment: - Automated testing on pull requests - Code quality checks using ruff - Documentation generation and validation - Dependency management with UV</p>"},{"location":"ai/#bot-management","title":"Bot Management","text":"<ul> <li>Bot runs locally via <code>uv run bossbotctl</code></li> <li>Admin-only bot restart command (planned feature)</li> <li>No containerization or complex infrastructure required</li> <li>Simple start/stop/restart procedures through command line</li> </ul>"},{"location":"ai/#testing-strategy","title":"Testing Strategy","text":""},{"location":"ai/#coverage-requirements","title":"Coverage Requirements","text":"<pre><code>COVERAGE_TARGETS = {\n    \"core_download\": 0.30,  # 30% coverage\n    \"command_parsing\": 0.30,\n    \"discord_events\": 0.30,\n    \"file_management\": 0.30\n}\n</code></pre>"},{"location":"ai/#test-infrastructure","title":"Test Infrastructure","text":"<p>The testing infrastructure is built on pytest with async support and specialized Discord testing utilities:</p> <pre><code># conftest.py\nimport pytest\nimport discord.ext.test as dpytest\nfrom discord.ext import commands\nfrom pathlib import Path\n\ndef pytest_configure(config):\n    \"\"\"Configure pytest with custom markers.\"\"\"\n    config.addinivalue_line(\n        \"markers\",\n        \"integration: mark test as integration test\"\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"slow: mark test as slow running\"\n    )\n\n@pytest.fixture\nasync def bot():\n    \"\"\"Create a bot instance for testing.\"\"\"\n    intents = discord.Intents.default()\n    intents.message_content = True\n    bot = commands.Bot(command_prefix=\"$\", intents=intents)\n    await bot._async_setup_hook()\n    dpytest.configure(bot)\n    yield bot\n    await dpytest.empty_queue()\n\n@pytest.fixture\ndef mock_gallery_dl(mocker):\n    \"\"\"Mock gallery-dl functionality using pytest-mock.\"\"\"\n    return mocker.patch(\"gallery_dl.download\")\n\n@pytest.fixture\ndef mock_yt_dlp(mocker):\n    \"\"\"Mock yt-dlp functionality using pytest-mock.\"\"\"\n    return mocker.patch(\"yt_dlp.YoutubeDL\")\n\n@pytest.fixture\ndef sample_download_data():\n    \"\"\"Provide sample download data for tests.\"\"\"\n    return {\n        \"twitter_url\": \"https://twitter.com/user/status/123\",\n        \"reddit_url\": \"https://reddit.com/r/sub/comments/123\",\n        \"expected_output\": \"video.mp4\"\n    }\n\n@pytest.fixture\ndef download_dir(tmpdir):\n    \"\"\"Create a temporary download directory.\"\"\"\n    downloads = tmpdir.mkdir(\"downloads\")\n    return Path(str(downloads))\n\n@pytest.fixture\ndef mock_discord_message(mocker):\n    \"\"\"Create a mock Discord message.\"\"\"\n    message = mocker.MagicMock()\n    message.author.id = 123456789\n    message.guild.id = 987654321\n    message.channel.id = 456789123\n    return message\n\n@pytest.fixture\ndef mock_download_manager(mocker, download_dir):\n    \"\"\"Create a mock download manager.\"\"\"\n    manager = mocker.MagicMock()\n    manager.download_dir = download_dir\n    return manager\n</code></pre>"},{"location":"ai/#integration-test-strategy","title":"Integration Test Strategy","text":"<p>Integration tests focus on key user flows and system interactions:</p> <pre><code>@pytest.mark.asyncio\nclass TestDownloadFlow:\n    \"\"\"Test complete download flows.\"\"\"\n\n    @pytest.mark.integration\n    async def test_twitter_download_flow(\n        self,\n        bot,\n        mock_gallery_dl,\n        sample_download_data,\n        download_dir,\n        mock_discord_message\n    ):\n        \"\"\"Test complete Twitter download flow.\"\"\"\n        # Arrange\n        url = sample_download_data[\"twitter_url\"]\n        output_file = download_dir / sample_download_data[\"expected_output\"]\n        mock_gallery_dl.return_value = str(output_file)\n\n        # Act\n        await dpytest.message(f\"$dlt {url}\")\n\n        # Assert\n        assert dpytest.verify().message().contains(\"Download started\")\n        mock_gallery_dl.assert_called_once_with(\n            url,\n            mocker.ANY  # Config dict\n        )\n        assert dpytest.verify().message().contains(\"Download complete\")\n\n    @pytest.mark.integration\n    async def test_queue_management(\n        self,\n        bot,\n        mock_gallery_dl,\n        download_dir,\n        mock_discord_message\n    ):\n        \"\"\"Test queue handling with multiple downloads.\"\"\"\n        # Arrange\n        urls = [f\"https://twitter.com/user/status/{i}\" for i in range(3)]\n        mock_gallery_dl.side_effect = [\n            str(download_dir / f\"video_{i}.mp4\") for i in range(3)\n        ]\n\n        # Act &amp; Assert\n        for i, url in enumerate(urls, 1):\n            await dpytest.message(f\"$dlt {url}\")\n            assert dpytest.verify().message().contains(f\"Position in queue: {i}\")\n\n    @pytest.mark.integration\n    async def test_error_handling(\n        self,\n        bot,\n        mock_gallery_dl,\n        mock_discord_message\n    ):\n        \"\"\"Test error handling in download flow.\"\"\"\n        # Arrange\n        mock_gallery_dl.side_effect = Exception(\"Download failed\")\n\n        # Act\n        await dpytest.message(\"$dlt https://twitter.com/user/status/123\")\n\n        # Assert\n        assert dpytest.verify().message().contains(\"Error occurred\")\n\n    def test_download_retry(\n        self,\n        mocker,\n        mock_download_manager,\n        download_dir\n    ):\n        \"\"\"Test download retry mechanism.\"\"\"\n        # Arrange\n        url = \"https://twitter.com/user/status/123\"\n        mock_download_manager.download.side_effect = [\n            Exception(\"First attempt failed\"),\n            Exception(\"Second attempt failed\"),\n            str(download_dir / \"success.mp4\")\n        ]\n\n        # Act\n        result = mock_download_manager.download_with_retry(url)\n\n        # Assert\n        assert mock_download_manager.download.call_count == 3\n        assert str(result).endswith(\"success.mp4\")\n</code></pre>"},{"location":"ai/#external-service-mocking","title":"External Service Mocking","text":"<p>Guidelines for mocking external services using pytest-mock:</p> <ol> <li> <p>Gallery-dl Mocking <pre><code>def test_gallery_dl_download(mocker, download_dir):\n    \"\"\"Example of gallery-dl mocking.\"\"\"\n    # Arrange\n    mock_gallery_dl = mocker.patch(\"gallery_dl.download\")\n    mock_gallery_dl.return_value = {\n        \"filename\": str(download_dir / \"video.mp4\"),\n        \"filesize\": 1024,\n        \"url\": \"https://example.com/video.mp4\"\n    }\n\n    # Act &amp; Assert\n    result = mock_gallery_dl(\"https://twitter.com/user/status/123\")\n    assert Path(result[\"filename\"]).parent == download_dir\n</code></pre></p> </li> <li> <p>YT-DLP Mocking <pre><code>def test_yt_dlp_download(mocker, download_dir):\n    \"\"\"Example of yt-dlp mocking.\"\"\"\n    # Arrange\n    mock_yt_dlp = mocker.patch(\"yt_dlp.YoutubeDL\")\n    mock_instance = mocker.MagicMock()\n    mock_instance.extract_info.return_value = {\n        \"title\": \"Test Video\",\n        \"duration\": 60,\n        \"url\": str(download_dir / \"video.mp4\")\n    }\n    mock_yt_dlp.return_value = mock_instance\n\n    # Act &amp; Assert\n    with mock_yt_dlp() as ydl:\n        result = ydl.extract_info(\"https://youtube.com/watch?v=123\")\n        assert Path(result[\"url\"]).parent == download_dir\n</code></pre></p> </li> </ol>"},{"location":"ai/#test-data-management","title":"Test Data Management","text":"<ol> <li>Test Data Storage <pre><code>@pytest.fixture\ndef test_data_dir(tmpdir):\n    \"\"\"Set up test data directory structure.\"\"\"\n    base_dir = Path(str(tmpdir))\n\n    # Create directory structure\n    samples = base_dir / \"samples\"\n    samples.mkdir()\n    for platform in [\"twitter\", \"reddit\", \"youtube\"]:\n        (samples / platform).mkdir()\n\n    # Create mock responses directory\n    responses = base_dir / \"responses\"\n    responses.mkdir()\n\n    return base_dir\n\n@pytest.fixture\ndef cleanup_downloads():\n    \"\"\"Clean up downloads after tests.\"\"\"\n    yield\n    # Cleanup happens automatically with tmpdir\n</code></pre></li> </ol>"},{"location":"ai/#ci-integration","title":"CI Integration","text":"<p>GitHub Actions workflow for automated testing:</p> <pre><code>name: Test Suite\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.12\"]\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install uv\n      run: |\n        curl -LsSf https://astral.sh/uv/install.sh | sh\n\n    - name: Install dependencies\n      run: |\n        uv pip install -r requirements.txt\n        uv pip install -r requirements-dev.txt\n\n    - name: Run tests\n      run: |\n        pytest tests/ --cov=boss_bot --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v4\n      with:\n        file: ./coverage.xml\n        fail_ci_if_error: true\n</code></pre>"},{"location":"ai/#error-handling-and-logging","title":"Error Handling and Logging","text":""},{"location":"ai/#logging-configuration","title":"Logging Configuration","text":"<pre><code>import sys\nfrom loguru import logger\nimport better_exceptions\n\n# Enable better exception formatting\nbetter_exceptions.hook()\n\n# Configure Loguru\nlogger.remove()  # Remove default handler\nlogger.configure(\n    handlers=[\n        {\n            \"sink\": sys.stdout,\n            \"format\": \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - &lt;level&gt;{message}&lt;/level&gt;\",\n            \"level\": \"INFO\",\n            \"colorize\": True,\n            \"backtrace\": True,\n            \"diagnose\": True,\n            \"enqueue\": True,\n        }\n    ]\n)\n\n# Error logging handler\nlogger.add(\n    sys.stderr,\n    format=\"{time} | {level} | {message}\",\n    filter=lambda record: record[\"level\"].name == \"ERROR\",\n    level=\"ERROR\",\n    backtrace=True,\n    diagnose=True,\n)\n</code></pre>"},{"location":"ai/#error-handling-strategy","title":"Error Handling Strategy","text":"<ol> <li>All errors are caught and logged with context</li> <li>User-facing errors are displayed in Discord embeds</li> <li>Stack traces are formatted using better-exceptions</li> <li>Sensitive information is never included in error messages</li> </ol> <p>Example error handler: <pre><code>from discord.ext import commands\nfrom loguru import logger\n\nclass ErrorHandler(commands.Cog):\n    \"\"\"Handles all command errors.\"\"\"\n\n    @commands.Cog.listener()\n    async def on_command_error(self, ctx: commands.Context, error: Exception):\n        \"\"\"Global error handler for all commands.\"\"\"\n        # Log the error with context\n        logger.error(\n            \"Command error occurred\",\n            command=ctx.command.name if ctx.command else \"Unknown\",\n            user_id=ctx.author.id,\n            guild_id=ctx.guild.id if ctx.guild else None,\n            error=str(error)\n        )\n\n        # Create user-facing error message\n        embed = discord.Embed(\n            title=\"Error Occurred\",\n            description=f\"```python\\n{better_exceptions.format_exception(error)}\\n```\",\n            color=discord.Color.red()\n        )\n\n        # Add error context if available\n        if isinstance(error, commands.CommandError):\n            embed.add_field(\n                name=\"Error Type\",\n                value=error.__class__.__name__,\n                inline=False\n            )\n\n        await ctx.send(embed=embed)\n</code></pre></p>"},{"location":"ai/#logging-best-practices","title":"Logging Best Practices","text":"<ol> <li> <p>Use structured logging with context:    <pre><code>logger.info(\n    \"Download started\",\n    url=url,\n    user_id=user_id,\n    guild_id=guild_id\n)\n</code></pre></p> </li> <li> <p>Log all state transitions:    <pre><code>logger.debug(\n    \"Download state changed\",\n    download_id=download.id,\n    old_state=old_state,\n    new_state=new_state\n)\n</code></pre></p> </li> <li> <p>Never log sensitive information:    <pre><code># Good\nlogger.info(\"Bot connected\", application_id=bot.application.id)\n\n# Bad - never log tokens or sensitive data\nlogger.info(\"Bot connected with token\", token=token)  # Don't do this!\n</code></pre></p> </li> </ol>"},{"location":"ai/#change-log","title":"Change Log","text":"Version Date Author Changes 0.1.0 2024-04-17 @bossjones Initial architecture draft 0.1.1 2024-04-17 @bossjones Added testing strategy and error handling"},{"location":"ai/#development-guidelines","title":"Development Guidelines","text":""},{"location":"ai/#code-style-guidelines","title":"Code Style Guidelines","text":"<pre><code># Formatting and Linting\n- All code is formatted using ruff\n- Maximum line length: 88 characters (ruff default)\n- Use type hints for all functions and variables\n- Follow Google docstring format\n- Use absolute imports (configured in VSCode settings)\n\n# VSCode Configuration\n- Format on save enabled\n- Ruff as default formatter\n- Pylance as language server\n- Semantic highlighting enabled\n- Inlay hints for:\n  * Variable types\n  * Function return types\n  * Pytest parameters\n\n# Error Checking\n- Pylint configuration:\n  * Enable: F,E,E1101 (errors and type checking)\n  * Disable: C0111,E0401,C,W,E1205 (style and import warnings)\n  * Max line length: 120\n  * Plugins: pylint_pydantic, pylint_per_file_ignores\n\n# Type Checking\n- Use Pylance/Pyright for type checking\n- Strict type checking enabled\n- Report missing imports as errors\n- Report import cycles as errors\n</code></pre>"},{"location":"ai/#git-workflow","title":"Git Workflow","text":"<pre><code># Branching Strategy\n- Main branch: `main` (protected, represents production-ready code)\n- Development branches: Use feature branches based off `main`.\n- Branch Naming Conventions:\n  - `feature/&lt;short-description&gt;`: For new features\n  - `bugfix/&lt;short-description&gt;`: For bug fixes\n  - `hotfix/&lt;short-description&gt;`: For critical production fixes (branch off `main` or release tags)\n  - `chore/&lt;short-description&gt;`: For maintenance tasks\n\n# Commit Messages (Conventional Commits)\n- Use commitizen for structured commit messages: `&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;`\n- Types: `feat`, `fix`, `docs`, `style`, `refactor`, `test`, `chore`\n- Example: `feat(download): add Twitter video support`\n</code></pre>"},{"location":"ai/#versioning-scheme","title":"Versioning Scheme","text":"<ul> <li>Semantic Versioning (SemVer) 2.0.0 is used.</li> <li>Version numbers are managed automatically using <code>commitizen</code> based on conventional commit messages.</li> </ul>"},{"location":"ai/#release-process","title":"Release Process","text":"<ul> <li>Releases are automated via GitHub Actions (see <code>.github/workflows/release.yml</code>).</li> <li><code>commitizen</code> bumps the version number and generates the changelog based on commits since the last tag.</li> <li>GitHub CLI (<code>gh</code>) is used to create GitHub releases.</li> <li>New versions are tagged in Git (e.g., <code>v1.2.3</code>).</li> </ul>"},{"location":"ai/#feature-flag-strategy","title":"Feature Flag Strategy","text":"<ul> <li>Feature flags are managed using environment variables.</li> <li>The <code>pydantic-settings</code> library is used to load and validate these environment variables, allowing features to be toggled on or off without code changes.</li> <li>Example: An environment variable like <code>BOSS_BOT_ENABLE_EXPERIMENTAL_DOWNLOADER=true</code> could enable a new feature.</li> </ul>"},{"location":"ai/#file-management","title":"File Management","text":""},{"location":"ai/#storage-structure","title":"Storage Structure","text":"<pre><code>/tmp/boss-bot/\n\u251c\u2500\u2500 downloads/                    # Organized downloads\n\u2502   \u251c\u2500\u2500 {guild_id}/              # Per-guild storage\n\u2502   \u2502   \u251c\u2500\u2500 {yyyy-mm-dd}/        # Date-based organization\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 {download_id}/   # Individual download\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 metadata.json # Download metadata\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 content/     # Downloaded files\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 .cleanup         # Cleanup marker\n\u2502   \u2502   \u2514\u2500\u2500 .stats              # Guild statistics\n\u2502   \u2514\u2500\u2500 .maintenance            # Maintenance logs\n\u251c\u2500\u2500 temp/                       # Temporary storage\n\u2502   \u2514\u2500\u2500 {download_id}/         # In-progress downloads\n\u2514\u2500\u2500 .locks/                    # Lock files\n</code></pre>"},{"location":"ai/#file-naming-conventions","title":"File Naming Conventions","text":"<pre><code>class FileNaming:\n    \"\"\"File naming conventions.\"\"\"\n    DOWNLOAD_FILE = \"{timestamp}_{original_name}\"  # e.g., 20240417_123456_video.mp4\n    TEMP_FILE = \"{download_id}_{timestamp}_temp\"   # e.g., abc123_20240417_123456_temp\n    LOG_FILE = \"boss-bot_{date}.log\"              # e.g., boss-bot_20240417.log\n    METADATA_FILE = \"metadata.json\"               # Standard name for all metadata files\n</code></pre>"},{"location":"ai/#storage-policies","title":"Storage Policies","text":"<pre><code>class StoragePolicy:\n    \"\"\"Storage management policies.\"\"\"\n    # Size Limits\n    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB (Discord limit)\n    MAX_TEMP_STORAGE = 1024 * 1024 * 1024  # 1GB\n    MAX_GUILD_DAILY = 500 * 1024 * 1024  # 500MB per guild per day\n\n    # Retention Periods (in seconds)\n    TEMP_FILE_RETENTION = 3600  # 1 hour\n    SUCCESSFUL_DOWNLOAD = 86400  # 24 hours\n    FAILED_DOWNLOAD = 21600  # 6 hours\n\n    # Cleanup Intervals\n    TEMP_SCAN_INTERVAL = 300  # 5 minutes\n    MAIN_SCAN_INTERVAL = 3600  # 1 hour\n</code></pre>"},{"location":"ai/#file-operations","title":"File Operations","text":"<pre><code>from pathlib import Path\nfrom typing import Optional\nimport aiofiles\nimport json\n\nasync def save_download(\n    content: bytes,\n    guild_id: int,\n    download_id: str,\n    original_name: str\n) -&gt; Path:\n    \"\"\"Save downloaded content with metadata.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"{timestamp}_{original_name}\"\n\n    # Create directory structure\n    download_dir = Path(f\"/tmp/boss-bot/downloads/{guild_id}/{timestamp[:8]}/{download_id}\")\n    download_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save content\n    file_path = download_dir / \"content\" / filename\n    async with aiofiles.open(file_path, \"wb\") as f:\n        await f.write(content)\n\n    # Save metadata\n    metadata = {\n        \"original_name\": original_name,\n        \"timestamp\": timestamp,\n        \"size\": len(content),\n        \"download_id\": download_id\n    }\n    async with aiofiles.open(download_dir / \"metadata.json\", \"w\") as f:\n        await f.write(json.dumps(metadata))\n\n    return file_path\n</code></pre>"},{"location":"ai/#cleanup-strategy","title":"Cleanup Strategy","text":"<pre><code>class CleanupManager:\n    \"\"\"Manages file cleanup operations.\"\"\"\n\n    async def cleanup_temp_files(self):\n        \"\"\"Clean temporary files older than retention period.\"\"\"\n        temp_dir = Path(\"/tmp/boss-bot/temp\")\n        current_time = time.time()\n\n        async for entry in aiofiles.os.scandir(temp_dir):\n            if entry.is_file():\n                stats = await aiofiles.os.stat(entry.path)\n                age = current_time - stats.st_mtime\n                if age &gt; StoragePolicy.TEMP_FILE_RETENTION:\n                    await aiofiles.os.remove(entry.path)\n\n    async def cleanup_downloads(self):\n        \"\"\"Clean old downloads based on retention policy.\"\"\"\n        downloads_dir = Path(\"/tmp/boss-bot/downloads\")\n        # Implementation details...\n</code></pre>"},{"location":"ai/#command-system","title":"Command System","text":""},{"location":"ai/#command-structure","title":"Command Structure","text":"Command Description Permissions Rate Limit Cooldown <code>/download &lt;url&gt;</code> Download media from supported platforms <code>send_messages</code> 5/minute 10s <code>/queue list</code> Show current download queue <code>send_messages</code> 10/minute 5s <code>/queue clear</code> Clear download queue <code>manage_messages</code> 2/minute 30s <code>/settings view</code> View guild settings <code>send_messages</code> 10/minute 5s <code>/settings update</code> Update guild settings <code>manage_guild</code> 5/minute 15s <code>/stats</code> View download statistics <code>send_messages</code> 10/minute 5s"},{"location":"ai/#command-implementation","title":"Command Implementation","text":"<pre><code>from discord import app_commands\nfrom discord.ext import commands\nfrom typing import Optional\n\nclass DownloadCog(commands.Cog):\n    \"\"\"Download command implementation.\"\"\"\n\n    def __init__(self, bot):\n        self.bot = bot\n        self._cd = commands.CooldownMapping.from_cooldown(\n            5, 60, commands.BucketType.user\n        )\n\n    @app_commands.command(name=\"download\")\n    @app_commands.describe(url=\"URL to download from\")\n    async def download(\n        self,\n        interaction: discord.Interaction,\n        url: str,\n        format: Optional[str] = None\n    ):\n        \"\"\"Download media from supported platforms.\"\"\"\n        # Check cooldown\n        bucket = self._cd.get_bucket(interaction)\n        retry_after = bucket.update_rate_limit()\n        if retry_after:\n            raise commands.CommandOnCooldown(bucket, retry_after)\n\n        # Validate URL\n        if not self.is_supported_url(url):\n            await interaction.response.send_message(\n                \"\u274c URL not supported\",\n                ephemeral=True\n            )\n            return\n\n        # Queue download\n        download_id = await self.queue_download(interaction.guild_id, url, format)\n\n        await interaction.response.send_message(\n            f\"\u2705 Download queued! ID: `{download_id}`\"\n        )\n</code></pre>"},{"location":"ai/#permission-system","title":"Permission System","text":"<pre><code>from enum import Enum\nfrom typing import Set\n\nclass BotPermission(Enum):\n    \"\"\"Bot-specific permissions.\"\"\"\n    DOWNLOAD = \"download\"\n    MANAGE_QUEUE = \"manage_queue\"\n    MANAGE_SETTINGS = \"manage_settings\"\n    VIEW_STATS = \"view_stats\"\n\nclass PermissionManager:\n    \"\"\"Manages bot permissions.\"\"\"\n\n    def __init__(self):\n        self.role_permissions: Dict[int, Set[BotPermission]] = {}\n\n    async def has_permission(\n        self,\n        member: discord.Member,\n        permission: BotPermission\n    ) -&gt; bool:\n        \"\"\"Check if member has required permission.\"\"\"\n        # Bot owner always has permission\n        if await self.bot.is_owner(member):\n            return True\n\n        # Check Discord permissions\n        if permission == BotPermission.MANAGE_SETTINGS:\n            return member.guild_permissions.manage_guild\n\n        if permission == BotPermission.MANAGE_QUEUE:\n            return member.guild_permissions.manage_messages\n\n        # Check role-based permissions\n        for role in member.roles:\n            if permission in self.role_permissions.get(role.id, set()):\n                return True\n\n        return False\n</code></pre>"},{"location":"ai/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from datetime import datetime, timedelta\nfrom collections import defaultdict\n\nclass RateLimiter:\n    \"\"\"Rate limit implementation.\"\"\"\n\n    def __init__(self):\n        self.limits = defaultdict(list)\n\n    async def check_rate_limit(\n        self,\n        key: str,\n        max_requests: int,\n        time_window: int\n    ) -&gt; tuple[bool, Optional[float]]:\n        \"\"\"\n        Check if request is rate limited.\n\n        Args:\n            key: Unique identifier (e.g., user_id or guild_id)\n            max_requests: Maximum requests allowed\n            time_window: Time window in seconds\n\n        Returns:\n            Tuple of (is_limited, retry_after)\n        \"\"\"\n        now = datetime.utcnow()\n        window_start = now - timedelta(seconds=time_window)\n\n        # Clean old entries\n        self.limits[key] = [\n            ts for ts in self.limits[key]\n            if ts &gt; window_start\n        ]\n\n        # Check limit\n        if len(self.limits[key]) &gt;= max_requests:\n            retry_after = (\n                self.limits[key][0] - window_start\n            ).total_seconds()\n            return True, retry_after\n\n        # Add new timestamp\n        self.limits[key].append(now)\n        return False, None\n</code></pre>"},{"location":"ai/#queue-management","title":"Queue Management","text":""},{"location":"ai/#queue-structure","title":"Queue Structure","text":"<pre><code>from dataclasses import dataclass\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional, Dict, Any\n\nclass DownloadStatus(Enum):\n    \"\"\"Download status states.\"\"\"\n    QUEUED = \"queued\"\n    PROCESSING = \"processing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n@dataclass\nclass QueueItem:\n    \"\"\"Represents a queued download.\"\"\"\n    download_id: str\n    guild_id: int\n    channel_id: int\n    user_id: int\n    url: str\n    status: DownloadStatus\n    created_at: datetime\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error: Optional[str] = None\n    metadata: Dict[str, Any] = None\n</code></pre>"},{"location":"ai/#queue-manager","title":"Queue Manager","text":"<pre><code>from asyncio import Queue, Lock\nfrom collections import defaultdict\n\nclass QueueManager:\n    \"\"\"Manages download queues per guild.\"\"\"\n\n    def __init__(self, max_concurrent_downloads: int = 3):\n        self.max_concurrent = max_concurrent_downloads\n        self.queues: Dict[int, Queue[QueueItem]] = defaultdict(Queue)\n        self.active_downloads: Dict[int, set] = defaultdict(set)\n        self.locks: Dict[int, Lock] = defaultdict(Lock)\n\n    async def add_to_queue(\n        self,\n        guild_id: int,\n        channel_id: int,\n        user_id: int,\n        url: str\n    ) -&gt; QueueItem:\n        \"\"\"Add a new download to the queue.\"\"\"\n        item = QueueItem(\n            download_id=self.generate_id(),\n            guild_id=guild_id,\n            channel_id=channel_id,\n            user_id=user_id,\n            url=url,\n            status=DownloadStatus.QUEUED,\n            created_at=datetime.utcnow()\n        )\n\n        await self.queues[guild_id].put(item)\n        return item\n\n    async def process_queue(self, guild_id: int):\n        \"\"\"Process queued downloads for a guild.\"\"\"\n        async with self.locks[guild_id]:\n            if len(self.active_downloads[guild_id]) &gt;= self.max_concurrent:\n                return\n\n            while not self.queues[guild_id].empty():\n                if len(self.active_downloads[guild_id]) &gt;= self.max_concurrent:\n                    break\n\n                item = await self.queues[guild_id].get()\n                self.active_downloads[guild_id].add(item.download_id)\n\n                try:\n                    item.status = DownloadStatus.PROCESSING\n                    item.started_at = datetime.utcnow()\n\n                    # Process download\n                    await self.process_download(item)\n\n                    item.status = DownloadStatus.COMPLETED\n\n                except Exception as e:\n                    item.status = DownloadStatus.FAILED\n                    item.error = str(e)\n\n                finally:\n                    item.completed_at = datetime.utcnow()\n                    self.active_downloads[guild_id].remove(item.download_id)\n                    self.queues[guild_id].task_done()\n</code></pre>"},{"location":"ai/#queue-storage","title":"Queue Storage","text":"<p>The queue state is persisted in Redis to maintain queue order and state across bot restarts:</p> <pre><code>import aioredis\nfrom typing import List\n\nclass QueueStorage:\n    \"\"\"Persistent queue storage using Redis.\"\"\"\n\n    def __init__(self, redis_url: str):\n        self.redis = aioredis.from_url(redis_url)\n\n    async def save_queue_item(self, item: QueueItem):\n        \"\"\"Save queue item to Redis.\"\"\"\n        key = f\"queue:{item.guild_id}:{item.download_id}\"\n        await self.redis.hmset(key, item.__dict__)\n        await self.redis.zadd(\n            f\"queue:{item.guild_id}\",\n            {item.download_id: item.created_at.timestamp()}\n        )\n\n    async def get_queue_items(\n        self,\n        guild_id: int,\n        status: Optional[DownloadStatus] = None\n    ) -&gt; List[QueueItem]:\n        \"\"\"Get all queue items for a guild.\"\"\"\n        items = []\n        download_ids = await self.redis.zrange(f\"queue:{guild_id}\", 0, -1)\n\n        for download_id in download_ids:\n            key = f\"queue:{guild_id}:{download_id}\"\n            data = await self.redis.hgetall(key)\n\n            if not data:\n                continue\n\n            item = QueueItem(**data)\n            if status is None or item.status == status:\n                items.append(item)\n\n        return items\n</code></pre>"},{"location":"ai/#bot-initialization-and-core-services","title":"Bot Initialization and Core Services","text":""},{"location":"ai/#bot-initialization-sequence","title":"Bot Initialization Sequence","text":"<pre><code>import discord\nfrom discord.ext import commands\nfrom typing import Optional\n\nclass BossBot(commands.Bot):\n    def __init__(self):\n        intents = discord.Intents.default()\n        intents.message_content = True\n\n        super().__init__(\n            command_prefix=\"$\",\n            intents=intents,\n            description=\"Boss-Bot: A Discord Media Download Assistant\"\n        )\n\n        # Initialize services\n        self.queue_manager = QueueManager()\n        self.download_manager = DownloadManager()\n\n    async def setup_hook(self):\n        \"\"\"Initialize services and load extensions\"\"\"\n        # Load command extensions\n        await self.load_extension(\"boss_bot.cogs.downloads\")\n        await self.load_extension(\"boss_bot.cogs.queue\")\n\n    async def on_ready(self):\n        \"\"\"Called when bot is ready and connected\"\"\"\n        print(f'Logged in as {self.user} (ID: {self.user.id})')\n</code></pre>"},{"location":"ai/#core-services","title":"Core Services","text":"<ol> <li>Discord Connection Service</li> <li> <p>Utilizes discord.py v2.5.2's built-in features:</p> <ul> <li>Automatic reconnection handling</li> <li>Exponential backoff for reconnection attempts</li> <li>Shard management (if needed)</li> <li>Rate limit handling</li> </ul> </li> <li> <p>Download Manager Service</p> </li> <li>Leverages existing tools:<ul> <li>gallery-dl: Image/media downloads with built-in retry mechanism</li> <li>yt-dlp: Video downloads with built-in retry mechanism</li> </ul> </li> <li> <p>Responsibilities:</p> <ul> <li>Platform detection and routing</li> <li>Download progress tracking</li> <li>Error handling and reporting</li> </ul> </li> <li> <p>Queue Management Service <pre><code>from dataclasses import dataclass\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Dict, Optional\nimport asyncio\n\nclass DownloadStatus(Enum):\n    QUEUED = \"queued\"\n    DOWNLOADING = \"downloading\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n@dataclass\nclass QueueItem:\n    id: str\n    url: str\n    user_id: int\n    channel_id: int\n    status: DownloadStatus\n    created_at: datetime\n    error: Optional[str] = None\n\nclass QueueManager:\n    def __init__(self, max_concurrent: int = 3):\n        self.queue: asyncio.Queue[QueueItem] = asyncio.Queue()\n        self.active_downloads: Dict[str, QueueItem] = {}\n        self.max_concurrent = max_concurrent\n\n    async def add_download(self, item: QueueItem) -&gt; None:\n        await self.queue.put(item)\n\n    async def get_next_download(self) -&gt; Optional[QueueItem]:\n        if len(self.active_downloads) &gt;= self.max_concurrent:\n            return None\n        try:\n            return await self.queue.get_nowait()\n        except asyncio.QueueEmpty:\n            return None\n</code></pre></p> </li> <li> <p>Command System</p> </li> <li>Implements discord.py's commands extension</li> <li>Supports both slash commands and prefix commands</li> <li>Includes permission handling and rate limiting</li> <li>Command categories:<ul> <li>Download commands</li> <li>Queue management</li> <li>Status and information</li> </ul> </li> </ol>"},{"location":"ai/#error-handling-strategy_1","title":"Error Handling Strategy","text":"<pre><code>@bot.event\nasync def on_error(event: str, *args, **kwargs):\n    \"\"\"Global error handler for all events\"\"\"\n    logger.error(f\"Error in {event}\", exc_info=True)\n\n@bot.tree.error\nasync def on_app_command_error(\n    interaction: discord.Interaction,\n    error: app_commands.AppCommandError\n):\n    \"\"\"Handle errors in application (slash) commands\"\"\"\n    if isinstance(error, app_commands.CommandOnCooldown):\n        await interaction.response.send_message(\n            f\"This command is on cooldown. Try again in {error.retry_after:.2f}s\",\n            ephemeral=True\n        )\n    else:\n        logger.error(\"Command error\", exc_info=error)\n</code></pre>"},{"location":"ai/#supported-platforms-and-limitations","title":"Supported Platforms and Limitations","text":"<ol> <li>Media Sources</li> <li>Twitter/X (via gallery-dl)</li> <li>Instagram (via gallery-dl)</li> <li>Reddit (via gallery-dl)</li> <li>YouTube (via yt-dlp)</li> <li> <p>TikTok (via yt-dlp)</p> </li> <li> <p>Download Capabilities</p> </li> <li>Single media downloads</li> <li>Playlist/album downloads</li> <li>Thread/post downloads</li> <li> <p>Bulk download support</p> </li> <li> <p>Limitations</p> </li> <li>Discord file size limit: 25MB (or 100MB with nitro)</li> <li>Maximum concurrent downloads: 3</li> <li>Rate limits per platform</li> <li>Queue size limit: 50 items</li> </ol>"},{"location":"ai/#data-management","title":"Data Management","text":""},{"location":"ai/#redis-configuration","title":"Redis Configuration","text":"<ul> <li>Redis 6.2.10 via docker-compose</li> <li>Port: 7600</li> <li>Persistent storage via Docker volume: boss_redis_data</li> <li>Health checks enabled</li> <li>No authentication in development (controlled access)</li> </ul>"},{"location":"ai/#data-retention-policies","title":"Data Retention Policies","text":""},{"location":"ai/#downloaded-media","title":"Downloaded Media","text":"<ul> <li>Location: /tmp/boss-bot/downloads/{guild_id}/{yyyy-mm-dd}/</li> <li>Retention Period: 24 hours from download completion</li> <li>Cleanup: Automatic removal after retention period</li> <li>Storage Strategy: Date-based organization per guild</li> <li>Exception: Failed downloads removed after 6 hours</li> </ul>"},{"location":"ai/#queue-data-redis","title":"Queue Data (Redis)","text":"<ul> <li>Active Queue: Real-time queue state</li> <li>Queue History:</li> <li>Completed downloads: 24 hours</li> <li>Failed downloads: 48 hours for debugging</li> <li>Cancelled downloads: 12 hours</li> <li>Queue Metrics: 7 days rolling window</li> </ul>"},{"location":"ai/#logging","title":"Logging","text":"<ul> <li>Primary Output: stdout via Loguru</li> <li>Log Format: Structured JSON</li> <li>Log Levels: INFO and above for stdout, ERROR for stderr</li> <li>Retention: Managed by system log rotation</li> <li>No persistent log storage needed</li> </ul>"},{"location":"ai/#data-storage-structure","title":"Data Storage Structure","text":"<pre><code>/tmp/boss-bot/\n\u251c\u2500\u2500 downloads/                    # Temporary media storage\n\u2502   \u251c\u2500\u2500 {guild_id}/              # Per-guild organization\n\u2502   \u2502   \u251c\u2500\u2500 {yyyy-mm-dd}/        # Date-based structure\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 {download_id}/   # Individual downloads\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 metadata.json # Download metadata\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 content/     # Media files\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 .cleanup         # Cleanup marker\n\u2502   \u2514\u2500\u2500 .maintenance            # Maintenance logs\n\u2514\u2500\u2500 temp/                       # In-progress downloads\n    \u2514\u2500\u2500 {download_id}/         # Temporary processing\n</code></pre>"},{"location":"ai/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<p>Monitoring and observability are crucial for maintaining the health and performance of the system. These features can be feature-flagged for future implementation.</p>"},{"location":"ai/#monitoring-metrics-definition","title":"Monitoring Metrics Definition","text":"<ul> <li>Request Rate: Number of requests per second</li> <li>Error Rate: Percentage of failed requests</li> <li>Resource Utilization: CPU, memory, and disk usage</li> <li>Queue Length: Number of items in the download queue</li> </ul>"},{"location":"ai/#alerting-thresholds","title":"Alerting Thresholds","text":"<ul> <li>High Error Rate: Alert if error rate exceeds 5%</li> <li>High CPU Usage: Alert if CPU usage exceeds 80%</li> <li>Long Queue Length: Alert if queue length exceeds 50 items</li> </ul>"},{"location":"ai/#logging-levels-and-retention","title":"Logging Levels and Retention","text":"<ul> <li>INFO: General operational information</li> <li>WARN: Non-critical issues that require attention</li> <li>ERROR: Critical issues that need immediate action</li> <li>Retention: Logs retained for 30 days</li> </ul>"},{"location":"ai/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li>Response Time: Track average and peak response times</li> <li>Throughput: Measure data processed per second</li> <li>Latency: Monitor delays in processing requests</li> </ul>"},{"location":"ai/#health-check-endpoints","title":"Health Check Endpoints","text":"<ul> <li>/health: Basic health check for the application</li> <li>/metrics: Expose application metrics for monitoring tools</li> </ul>"},{"location":"ai/#dashboard-requirements","title":"Dashboard Requirements","text":"<ul> <li>Visualization: Graphs for request rates, error rates, and resource utilization</li> <li>Alerts: Real-time alerts for threshold breaches</li> <li>Historical Data: Access to historical performance data</li> </ul>"},{"location":"ai/#performance-scaling-future-considerations","title":"Performance &amp; Scaling (Future Considerations)","text":"<p>Current State: Basic queue management defined. Performance and scaling are considered nice-to-have features for future iterations.</p> <p>Needed Additions: - [ ] Define performance benchmarks for key operations (e.g., download speed, queue processing time). - [ ] Establish scaling thresholds based on metrics like queue length, resource utilization, or request rate. - [ ] Set clear resource utilization limits (CPU, memory, network) for the bot process. - [ ] Design and implement a caching strategy for frequently accessed data or downloaded content (potentially using Redis or local cache). - [ ] Develop guidelines for optimizing queue processing, potentially including priority adjustments, worker scaling, or task distribution.</p>"},{"location":"ai/#documentation-requirements","title":"Documentation Requirements","text":"<p>Current State: Basic structure provided</p> <p>Needed Additions: - [ ] API documentation standards - [ ] Logging standards - [ ] Contribution guidelines - [ ] Development setup guide - [ ] Deployment guide - [ ] Troubleshooting guide</p>"},{"location":"ai/#github-workflow-for-documentation-deployment","title":"GitHub Workflow for Documentation Deployment","text":"<pre><code>name: docs-mkdocs-gh-deploy\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - \"docs/**\"\n      - \"mkdocs.yml\"\n      - \".github/workflows/docs.yml\"\n      - \"Makefile.ci\"\n  # Allow manual triggering\n  workflow_dispatch:\n    inputs:\n      debug_enabled:\n        description: Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)\n        required: false\n        default: \"false\"\nconcurrency:\n  cancel-in-progress: true\n  group: publish-workflow\n\npermissions:\n  contents: write\n\njobs:\n  deploy:\n    name: Mkdocs ${{ matrix.os }} / Python ${{ matrix.python-version }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]\n        python-version: [\"3.12\"]\n    runs-on: ${{ matrix.os }}\n    env:\n      GH_PAGER: cat\n      UV_NO_PROMPT: 1\n      UV_PROJECT_ENVIRONMENT: true\n      SKIP: commitizen-branch # Skip commitizen checks for gh-pages\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          token: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n\n      - name: Set shell path\n        id: set-shell\n        shell: bash\n        run: |\n          if [[ \"$RUNNER_OS\" == \"macOS\" ]]; then\n            echo \"SHELL_PATH=/opt/homebrew/bin/zsh\" &gt;&gt; $GITHUB_ENV\n          else\n            echo \"SHELL_PATH=/bin/bash\" &gt;&gt; $GITHUB_ENV\n          fi\n          echo \"Shell path set to: $SHELL_PATH\"\n\n      - name: Install UV\n        uses: astral-sh/setup-uv@v5\n        with:\n          enable-cache: true\n          version: \"0.6.11\"\n\n      - name: Setup Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Setup debug tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' &amp;&amp; github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n\n      - name: Install additional macos dependencies zsh etc\n        run: |\n          sudo apt-get update\n          sudo apt-get install zsh -y\n\n      - name: Sync dependencies with UV\n        run: uv sync --dev\n\n      - name: Display environment information\n        run: |\n          make -f Makefile.ci env-info\n\n      - name: Install dependencies\n        run: |\n          make -f Makefile.ci ci-install\n\n      - name: build mkdocs documentation site\n        run: |\n          uv run mkdocs build\n\n      - name: Deploy mkdocs\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          touch .nojekyll\n          uv run mkdocs gh-deploy --force --message 'docs(mkdocs): update documentation [skip ci]'\n</code></pre>"},{"location":"ai/#file-defered-stories-defered-story-1-story-md","title":".ai/defered-stories/defered-story-1.story.md","text":"<p> <p> <p></p>"},{"location":"ai/#file-diagrams-module-fixtures-current-md","title":".ai/diagrams/module-fixtures-current.md","text":""},{"location":"ai/#pytest-fixtures-current-state-class-diagram","title":"Pytest Fixtures Current State - Class Diagram","text":""},{"location":"ai/#overview","title":"Overview","text":"<p>Shows the current state of pytest fixtures in the project, highlighting duplications and potential conflicts.</p>"},{"location":"ai/#source","title":"Source","text":"<ul> <li>tests/conftest.py</li> <li>tests/test_bot/test_help.py</li> <li>tests/test_bot/test_download_cog.py</li> <li>tests/test_downloaders/test_base.py</li> <li>tests/test_core/test_queue_manager.py</li> <li>tests/test_bot/test_cogs/test_downloads.py</li> </ul>"},{"location":"ai/#diagram","title":"Diagram","text":"<pre><code>classDiagram\n    class Fixture {\n        &lt;&gt;\n        +name: str\n        +scope: str\n        +file: str\n    }\n\n    class bot {\n        +scope: function\n        +file: conftest.py\n        +description: \"Test bot instance with mocked Discord.py methods\"\n    }\n\n    class bot_duplicate {\n        +scope: function\n        +file: test_download_cog.py\n        +description: \"Create a bot instance for testing\"\n    }\n\n    class mock_bot {\n        +scope: function\n        +file: test_downloads.py\n        +description: \"Create a mocked bot instance\"\n    }\n\n    class ctx {\n        +scope: function\n        +file: conftest.py\n        +description: \"Mock Discord context for testing\"\n    }\n\n    class queue_manager {\n        +scope: function\n        +file: conftest.py\n        +description: \"Test queue manager instance\"\n    }\n\n    class queue_manager_duplicate {\n        +scope: function\n        +file: test_queue_manager.py\n        +description: \"Create a queue manager instance for testing\"\n    }\n\n    class download_manager {\n        +scope: function\n        +file: conftest.py\n        +description: \"Test download manager instance\"\n    }\n\n    class download_manager_duplicate {\n        +scope: function\n        +file: test_base.py\n        +description: \"Create a download manager instance for testing\"\n    }\n\n    class help_command {\n        +scope: function\n        +file: test_help.py\n        +description: \"Help command instance for testing\"\n    }\n\n    class cog {\n        +scope: function\n        +file: test_downloads.py\n        +description: \"Downloads cog instance for testing\"\n    }\n\n    Fixture &lt;|-- bot\n    Fixture &lt;|-- bot_duplicate\n    Fixture &lt;|-- mock_bot\n    Fixture &lt;|-- ctx\n    Fixture &lt;|-- queue_manager\n    Fixture &lt;|-- queue_manager_duplicate\n    Fixture &lt;|-- download_manager\n    Fixture &lt;|-- download_manager_duplicate\n    Fixture &lt;|-- help_command\n    Fixture &lt;|-- cog\n\n    bot &lt;-- help_command : depends on\n    bot &lt;-- ctx : depends on\n    mock_bot &lt;-- cog : depends on\n    bot .. bot_duplicate : conflicts with\n    bot .. mock_bot : similar to\n    queue_manager .. queue_manager_duplicate : conflicts with\n    download_manager .. download_manager_duplicate : conflicts with"},{"location":"ai/#notes","title":"Notes","text":"<ol>\n<li>Red flags identified:</li>\n<li><code>bot</code> fixture is duplicated in test_download_cog.py</li>\n<li><code>queue_manager</code> fixture is duplicated in test_queue_manager.py</li>\n<li><code>download_manager</code> fixture is duplicated in test_base.py</li>\n<li><code>mock_bot</code> is similar to <code>bot</code> but with a different name</li>\n<li>Potential issues:</li>\n<li>Inconsistent fixture behavior across tests</li>\n<li>Risk of fixture state bleeding between tests</li>\n<li>Maintenance overhead from duplicate code</li>\n<li>Unclear which fixture version should be used</li>\n</ol>"},{"location":"ai/#file-diagrams-module-fixtures-recommended-md","title":".ai/diagrams/module-fixtures-recommended.md","text":""},{"location":"ai/#pytest-fixtures-recommended-state-class-diagram","title":"Pytest Fixtures Recommended State - Class Diagram","text":""},{"location":"ai/#overview_1","title":"Overview","text":"<p>Shows the recommended state of pytest fixtures after consolidation and proper organization.</p>"},{"location":"ai/#source_1","title":"Source","text":"<p>Based on analysis of current fixtures in:\n- tests/conftest.py\n- tests/test_bot/test_help.py\n- tests/test_bot/test_download_cog.py\n- tests/test_downloaders/test_base.py\n- tests/test_core/test_queue_manager.py\n- tests/test_bot/test_cogs/test_downloads.py</p>"},{"location":"ai/#diagram_1","title":"Diagram","text":"<pre><code>classDiagram\n    class Fixture {\n        &lt;&gt;\n        +name: str\n        +scope: str\n        +file: str\n    }\n\n    class bot {\n        +scope: function\n        +file: conftest.py\n        +description: \"Test bot instance with mocked Discord.py methods\"\n        +is_mock: bool\n        +configure_mock()\n    }\n\n    class ctx {\n        +scope: function\n        +file: conftest.py\n        +description: \"Mock Discord context for testing\"\n        +bot: Bot\n    }\n\n    class queue_manager {\n        +scope: function\n        +file: conftest.py\n        +description: \"Test queue manager instance\"\n        +reset_state()\n    }\n\n    class download_manager {\n        +scope: function\n        +file: conftest.py\n        +description: \"Test download manager instance\"\n        +reset_state()\n    }\n\n    class help_command {\n        +scope: function\n        +file: conftest.py\n        +description: \"Help command instance for testing\"\n        +bot: Bot\n    }\n\n    class cog {\n        +scope: function\n        +file: conftest.py\n        +description: \"Downloads cog instance for testing\"\n        +bot: Bot\n        +download_manager: DownloadManager\n    }\n\n    class mock_env_vars {\n        +scope: function\n        +file: conftest.py\n        +description: \"Mock environment variables\"\n    }\n\n    class mock_settings {\n        +scope: function\n        +file: conftest.py\n        +description: \"Standardized test settings\"\n    }\n\n    Fixture &lt;|-- bot\n    Fixture &lt;|-- ctx\n    Fixture &lt;|-- queue_manager\n    Fixture &lt;|-- download_manager\n    Fixture &lt;|-- help_command\n    Fixture &lt;|-- cog\n    Fixture &lt;|-- mock_env_vars\n    Fixture &lt;|-- mock_settings\n\n    bot &lt;-- ctx : uses\n    bot &lt;-- help_command : uses\n    bot &lt;-- cog : uses\n    download_manager &lt;-- cog : uses\n    mock_settings &lt;-- bot : configures\n    mock_env_vars &lt;-- mock_settings : uses\n\n    note for bot \"Consolidated bot fixture with mock configuration\"\n    note for queue_manager \"Single source of truth in conftest.py\"\n    note for download_manager \"Single source of truth in conftest.py\""},{"location":"ai/#notes_1","title":"Notes","text":"<ol>\n<li>Key improvements:</li>\n<li>All fixtures consolidated in conftest.py</li>\n<li>Clear dependency hierarchy</li>\n<li>Consistent mocking strategy</li>\n<li>State management with reset methods</li>\n<li>\n<p>Proper scope isolation</p>\n</li>\n<li>\n<p>Implementation recommendations:</p>\n</li>\n<li>Move all fixture definitions to conftest.py</li>\n<li>Add reset_state() methods to stateful fixtures</li>\n<li>Use bot.is_mock flag to control mock behavior</li>\n<li>Ensure proper cleanup between tests</li>\n<li>\n<p>Document fixture dependencies clearly</p>\n</li>\n<li>\n<p>Migration steps:</p>\n</li>\n<li>Delete duplicate fixtures from individual test files</li>\n<li>Update tests to use consolidated fixtures</li>\n<li>Add proper teardown/cleanup in conftest.py</li>\n<li>Add type hints and docstrings to all fixtures</li>\n</ol>"},{"location":"ai/#file-diagrams-pytest_fixtures_relationships-mmd","title":".ai/diagrams/pytest_fixtures_relationships.mmd","text":"<p>title: Pytest Fixtures Relationship Diagram\ndescription: Shows the relationships, dependencies, and potential conflicts between pytest fixtures in the boss-bot project</p>\n\n<p>erDiagram\n    mock_env_vars_main ||--|| settings : \"provides\"\n    mock_env_vars_main ||--|| mock_env_vars_core : \"duplicates\"\n    mock_env_vars_main {\n        string DISCORD_TOKEN\n        string DISCORD_CLIENT_ID\n        string scope \"session\"\n        string file \"conftest.py\"\n    }</p>\n<pre><code>mock_env_vars_core {\n    string DISCORD_TOKEN\n    string DISCORD_CLIENT_ID\n    string scope \"function\"\n    string file \"test_core/conftest.py\"\n}\n\nsettings ||--|| bot : \"configures\"\nsettings ||--|| queue_manager : \"configures\"\nsettings ||--|| download_manager : \"configures\"\nsettings {\n    string discord_token\n    string discord_client_id\n    string scope \"function\"\n    string file \"conftest.py\"\n}\n\nbot ||--|| ctx : \"uses\"\nbot {\n    string instance \"BossBot\"\n    string scope \"function\"\n    string file \"conftest.py\"\n    string duplicate_1 \"test_download_cog.py\"\n    string duplicate_2 \"test_queue_cog.py\"\n}\n\nmock_bot ||--|| cog : \"provides\"\nmock_bot {\n    string instance \"Mock\"\n    string scope \"function\"\n    string file \"test_downloads.py\"\n}\n\nqueue_manager {\n    int max_queue_size\n    string scope \"function\"\n    string file \"conftest.py\"\n}\n\ndownload_manager {\n    int max_concurrent\n    string scope \"function\"\n    string file \"conftest.py\"\n}\n\nctx {\n    string instance \"Mock\"\n    string scope \"function\"\n    string file \"conftest.py\"\n}\n\ncog {\n    string instance \"DownloadCog\"\n    string scope \"function\"\n    string file \"test_downloads.py\"\n}\n\nmock_env ||--|| mock_settings : \"provides\"\nmock_env {\n    string env_vars \"dict\"\n    string scope \"function\"\n    string file \"test_env.py\"\n}\n</code></pre>\n<p>%% Notes about the diagram\n%% mock_env_vars_main: Main environment fixture with session scope\n%% mock_env_vars_core: Duplicate fixture with different scope\n%% bot: Multiple implementations across test files</p>"},{"location":"ai/#file-prd-prd-md","title":".ai/prd/prd.md","text":""},{"location":"ai/#title-prd-for-boss-bot-a-discord-media-download-and-rag-assistant","title":"Title: PRD for Boss-Bot: A Discord Media Download and RAG Assistant","text":"<p>\nBoss-Bot is designed to enhance Discord server productivity through media downloads and RAG features. The initial focus is on reliable media downloads from platforms like Twitter and Reddit, with a foundation for future AI capabilities.\n</p>\n<p>\n- Product Owner: @bossjones\n- Technical Lead: @bossjones\n- Engineering Manager: @bossjones\n</p>\n<p>\n- Uptime: &gt;99%\n- Response Time: &lt;2s for command acknowledgment\n- Download Queue Processing: &lt;5min per item\n- Test Coverage (MVP):\n  * Core Download: 30%\n  * Command Parsing: 25%\n  * Discord Events: 20%\n  * File Management: 20%\n</p>\n<p>\n- Python 3.11+ required (3.12 recommended)\n- Maximum module size: 120 lines\n- Discord file size limit: 50MB\n- Maximum concurrent downloads: 5\n- Queue size limit: 50 items\n</p>\n<p>\n- Users have basic Discord command knowledge\n- Stable internet connection available\n- Sufficient storage space for temporary files\n- Discord API remains stable\n- Gallery-dl continues to support target platforms\n</p>\n<p>\n- Discord API changes could break functionality\n- Target platforms may change their APIs\n- Rate limiting could affect user experience\n- Storage management could become complex\n- Network issues could interrupt downloads\n</p>\n<p>\n- Discord.py for bot framework\n- Gallery-dl for media downloads\n- UV for package management\n- Pytest for testing infrastructure\n- Ruff for code quality\n</p>\n<p>1.0.0</p>"},{"location":"ai/#status-approved","title":"Status: Approved","text":""},{"location":"ai/#approval","title":"Approval","text":"Role\nName\nDate\nStatus\n\n\n\n\nProduct Owner\n@bossjones\n2024-04-17\n\u2705 Approved\n\n\nTechnical Lead\n@bossjones\n2024-04-17\n\u2705 Approved\n\n\nEngineering Manager\n@bossjones\n2024-04-17\n\u2705 Approved"},{"location":"ai/#approval-notes","title":"Approval Notes","text":"<p>PRD has been reviewed and approved. The document provides:\n- Comprehensive technical specifications and requirements\n- Clear project structure and implementation timeline\n- Detailed test strategy with coverage targets\n- Well-defined user experience and interface design\n- Thorough error handling and monitoring strategy</p>"},{"location":"ai/#version-history","title":"Version History","text":"Version\nDate\nAuthor\nChanges\n\n\n\n\n1.0.0\n2024-04-17\n@bossjones\nInitial PRD approved"},{"location":"ai/#intro","title":"Intro","text":"<p>Boss-Bot is a Discord bot designed to enhance server productivity by providing robust media download capabilities and future RAG (Retrieval-Augmented Generation) features. The initial MVP focuses on reliable media downloads from popular platforms like Twitter and Reddit, with a strong foundation for future AI-powered features. The bot emphasizes test-driven development, clean code practices, and modular design to ensure maintainability and extensibility.</p>"},{"location":"ai/#development-environment_1","title":"Development Environment","text":""},{"location":"ai/#setup-requirements","title":"Setup Requirements","text":"<ul>\n<li>Python 3.11 or higher (3.12 recommended)</li>\n<li>UV package manager</li>\n<li>Git</li>\n</ul>"},{"location":"ai/#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/bossjones/boss-bot.git\ncd boss-bot\n\n# Install dependencies\nuv sync --dev\n\n# Set up environment variables\ncp env.sample .env\n# Edit .env with your Discord credentials\n\n# Run tests\nuv run pytest\n\n# Start the bot\nuv run python -m boss_bot\n</code></pre>"},{"location":"ai/#key-configuration-files","title":"Key Configuration Files","text":""},{"location":"ai/#1-package-management-pyprojecttoml","title":"1. Package Management (pyproject.toml)","text":"<pre><code>[project]\nname = \"boss-bot\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"discord-py&gt;=2.5.2\",\n    \"gallery-dl&gt;=1.29.3\",\n    \"loguru&gt;=0.7.3\",\n    \"pydantic-settings&gt;=2.8.1\",\n    # ... other dependencies\n]\n</code></pre>"},{"location":"ai/#2-code-quality-tools","title":"2. Code Quality Tools","text":"<ul>\n<li>Ruff: Primary linter and formatter\n  <pre><code># .pre-commit-config.yaml\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  hooks:\n    - id: ruff\n      args: [--fix, --exit-non-zero-on-fix]\n    - id: ruff-format\n</code></pre></li>\n</ul>"},{"location":"ai/#3-testing-framework","title":"3. Testing Framework","text":"<ul>\n<li>pytest with extensions:</li>\n<li>pytest-asyncio for async testing</li>\n<li>pytest-recording for HTTP mocking</li>\n<li>pytest-cov for coverage reporting</li>\n<li>pytest-retry for flaky tests</li>\n<li>pytest-skipuntil for handling flaky Discord API tests</li>\n</ul>"},{"location":"ai/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared test configuration and fixtures\n\u251c\u2500\u2500 fixtures/               # Test data and mock responses\n\u2502   \u251c\u2500\u2500 tweet_data.py      # Twitter API mock responses\n\u2502   \u251c\u2500\u2500 reddit_data.py     # Reddit API mock responses\n\u2502   \u251c\u2500\u2500 discord_data.py    # Discord API mock responses\n\u2502   \u2514\u2500\u2500 cassettes/         # VCR cassettes for HTTP mocking\n\u251c\u2500\u2500 unit/                  # Unit tests directory\n\u251c\u2500\u2500 integration/          # Integration tests\n\u2514\u2500\u2500 eval/                # Future RAG evaluation tests\n</code></pre>"},{"location":"ai/#handling-flaky-tests","title":"Handling Flaky Tests","text":"<ul>\n<li>Use pytest-skipuntil for Discord API tests that may be unreliable</li>\n<li>Store HTTP interactions in VCR cassettes under tests/fixtures/cassettes/</li>\n<li>Use dpytest for reliable Discord command testing</li>\n<li>Example:\n  <pre><code>@pytest.mark.skipuntil(\"2024-06-01\")  # Skip until Discord API is stable\nasync def test_discord_feature():\n    # Test implementation\n</code></pre></li>\n</ul>"},{"location":"ai/#test-fixtures","title":"Test Fixtures","text":"<ul>\n<li>Add new fixtures in tests/fixtures/</li>\n<li>Use VCR for HTTP mocking:\n  <pre><code>@pytest.fixture(scope=\"module\")\ndef vcr_config():\n    \"\"\"VCR configuration for HTTP mocking.\"\"\"\n    return {\n        \"filter_headers\": [\"authorization\"],\n        \"filter_post_data_parameters\": [\"token\", \"api_key\"]\n    }\n</code></pre></li>\n</ul>"},{"location":"ai/#4-documentation","title":"4. Documentation","text":"<ul>\n<li>MkDocs with extensions:\n  <pre><code># mkdocs.yml configuration\nplugins:\n  - mkdocstrings\n  - coverage\n  - git-revision-date-localized\n  - macros\n</code></pre></li>\n</ul>"},{"location":"ai/#development-workflow","title":"Development Workflow","text":"<ol>\n<li>Create feature branch</li>\n<li>Write tests first (TDD)</li>\n<li>Implement feature</li>\n<li>Run pre-commit hooks</li>\n<li>Submit PR</li>\n</ol>"},{"location":"ai/#cicd-pipeline_1","title":"CI/CD Pipeline","text":"<p>GitHub Actions workflow includes:\n- Linting with Ruff\n- Testing with pytest\n- Documentation generation\n- Coverage reporting</p>"},{"location":"ai/#environment-variables","title":"Environment Variables","text":"<p>Required for development:\n<pre><code>DISCORD_TOKEN=required                # Discord bot token\nDISCORD_CLIENT_ID=required           # Discord application client ID\nDISCORD_SERVER_ID=required           # Target Discord server ID\nDISCORD_ADMIN_USER_ID=required       # Admin user ID\n</code></pre></p>"},{"location":"ai/#goals","title":"Goals","text":"<ul>\n<li>Create a reliable and efficient Discord bot for media downloads with &gt;99% uptime</li>\n<li>Implement MVP test coverage targets:</li>\n<li>Core Download: 30%</li>\n<li>Command Parsing: 25%</li>\n<li>Discord Events: 20% (limited by dpytest capabilities)</li>\n<li>File Management: 20%</li>\n<li>Ensure all modules follow clean code practices and stay under 120 lines</li>\n<li>Build a foundation for future RAG capabilities</li>\n<li>Provide clear progress tracking and queue management for downloads</li>\n<li>Maintain clear documentation and type hints for junior developer onboarding</li>\n</ul>"},{"location":"ai/#test-driven-development-guidelines","title":"Test-Driven Development Guidelines","text":""},{"location":"ai/#core-principles","title":"Core Principles","text":"<ol>\n<li>Write failing test first</li>\n<li>Keep tests focused and small</li>\n<li>Use descriptive test names that explain the behavior</li>\n<li>Follow the Arrange-Act-Assert pattern</li>\n<li>No implementation without a failing test</li>\n</ol>"},{"location":"ai/#example-test-patterns","title":"Example Test Patterns","text":""},{"location":"ai/#command-testing","title":"Command Testing","text":"<pre><code>async def test_download_command_validates_url():\n    \"\"\"Test that download command properly validates URLs.\"\"\"\n    # Arrange\n    bot = await create_test_bot()\n    invalid_url = \"not-a-url\"\n\n    # Act\n    await send_command(bot, f\"$dlt {invalid_url}\")\n\n    # Assert\n    assert_bot_replied_with(bot, \"Invalid URL\")\n\nasync def test_download_command_handles_valid_url():\n    \"\"\"Test that download command processes valid URLs.\"\"\"\n    # Arrange\n    bot = await create_test_bot()\n    valid_url = \"https://twitter.com/user/status/123\"\n\n    # Act\n    await send_command(bot, f\"$dlt {valid_url}\")\n\n    # Assert\n    assert_download_started(bot, valid_url)\n</code></pre>"},{"location":"ai/#event-testing","title":"Event Testing","text":"<pre><code>async def test_bot_handles_disconnect():\n    \"\"\"Test that bot properly handles disconnection events.\"\"\"\n    # Arrange\n    bot = await create_test_bot()\n    await ensure_bot_connected(bot)\n\n    # Act\n    await simulate_disconnect(bot)\n\n    # Assert\n    assert_reconnection_attempted(bot)\n    assert_event_logged(bot, \"disconnect\")\n</code></pre>"},{"location":"ai/#best-practices","title":"Best Practices","text":"<ol>\n<li>Test Isolation</li>\n<li>Each test should be independent</li>\n<li>Clean up resources after each test</li>\n<li>\n<p>Use fresh fixtures for each test</p>\n</li>\n<li>\n<p>Meaningful Names</p>\n</li>\n<li>Test names should describe behavior</li>\n<li>Use consistent naming patterns</li>\n<li>\n<p>Include success and failure cases</p>\n</li>\n<li>\n<p>Test Organization</p>\n</li>\n<li>Group related tests in classes</li>\n<li>Use descriptive test class names</li>\n<li>\n<p>Keep test files focused and small</p>\n</li>\n<li>\n<p>Mocking and Fixtures</p>\n</li>\n<li>Mock external dependencies</li>\n<li>Use fixtures for common setup</li>\n<li>Keep mocks simple and focused</li>\n</ol>\n<p>"},{"location":"ai/#features-and-requirements","title":"Features and Requirements","text":""},{"location":"ai/#functional-requirements","title":"Functional Requirements","text":"<ul>\n<li>Discord bot integration using discord.py</li>\n<li>Media download commands ($dlt, $dlr) with progress tracking</li>\n<li>Queue management system for multiple download requests</li>\n<li>Temporary file storage and cleanup mechanism</li>\n<li>Error handling and user feedback system</li>\n<li>Command help and documentation</li>\n</ul>"},{"location":"ai/#non-functional-requirements","title":"Non-functional Requirements","text":"<ul>\n<li>Response time &lt;2s for command acknowledgment</li>\n<li>Download queue processing time &lt;5min per item</li>\n<li>Maximum module size of 120 lines</li>\n<li>Type hints for all functions and classes</li>\n<li>Comprehensive docstrings following Google style</li>\n<li>Test coverage targets (MVP):</li>\n<li>Core Download: 30%</li>\n<li>Command Parsing: 25%</li>\n<li>Discord Events: 20% (limited by dpytest capabilities)</li>\n<li>File Management: 20%</li>\n<li>Higher coverage targets will be set post-MVP</li>\n<li>Adherence to DRY (Don't Repeat Yourself) and YAGNI (You Aren't Gonna Need It) principles</li>\n<li>Performance testing requirements:</li>\n<li>Load testing for concurrent downloads (minimum 10 simultaneous)</li>\n<li>Memory usage monitoring (max 500MB under load)</li>\n<li>CPU usage monitoring (max 50% under load)</li>\n<li>Network bandwidth monitoring and throttling capabilities</li>\n<li>Code quality requirements:</li>\n<li>All code must pass ruff linting and formatting</li>\n<li>Zero tolerance for unhandled exceptions</li>\n<li>Comprehensive error handling with better-exceptions</li>\n<li>All HTTP interactions must be tested with respx mocking</li>\n<li>Flaky tests must be identified and managed with pytest-ignore-flaky</li>\n<li>Critical tests must use pytest-retry for reliability</li>\n</ul>"},{"location":"ai/#user-experience-requirements","title":"User Experience Requirements","text":"<ul>\n<li>Clear progress indicators for downloads</li>\n<li>Intuitive command syntax</li>\n<li>Helpful error messages</li>\n<li>Command usage examples</li>\n<li>Queue status visibility</li>\n</ul>"},{"location":"ai/#integration-requirements","title":"Integration Requirements","text":"<ul>\n<li>Discord API integration</li>\n<li>Twitter API integration via gallery_dl</li>\n<li>Reddit API integration via gallery_dl</li>\n<li>File system management</li>\n<li>Future: Vector store integration\n</li>\n</ul>\n<p>"},{"location":"ai/#data-models_1","title":"Data Models","text":"<p>All data models will be implemented using Pydantic for validation and serialization. Models are organized by domain and include comprehensive type hints and validation rules.</p>"},{"location":"ai/#core-models_1","title":"Core Models","text":""},{"location":"ai/#botconfig","title":"BotConfig","text":"<pre><code>class BotConfig(BaseSettings):\n    \"\"\"Bot configuration settings.\"\"\"\n    token: SecretStr\n    command_prefix: str = \"$\"\n    max_concurrent_downloads: int = 5\n    max_queue_size: int = 50\n    temp_file_retention_hours: int = 24\n    max_file_size_mb: int = 50\n    log_level: str = \"INFO\"\n\n    class Config:\n        env_prefix = \"BOSS_BOT_\"\n</code></pre>"},{"location":"ai/#downloaditem","title":"DownloadItem","text":"<pre><code>class DownloadStatus(str, Enum):\n    \"\"\"Status of a download item.\"\"\"\n    QUEUED = \"queued\"\n    DOWNLOADING = \"downloading\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\nclass DownloadPriority(int, Enum):\n    \"\"\"Priority levels for downloads.\"\"\"\n    LOW = 0\n    NORMAL = 1\n    HIGH = 2\n\nclass DownloadItem(BaseModel):\n    \"\"\"Represents a single download request.\"\"\"\n    id: UUID\n    url: HttpUrl\n    status: DownloadStatus\n    priority: DownloadPriority = DownloadPriority.NORMAL\n    user_id: int\n    guild_id: int\n    channel_id: int\n    created_at: datetime\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    progress: float = 0.0\n    total_size: Optional[int] = None\n    current_size: Optional[int] = None\n    attempt_count: int = 0\n    max_attempts: int = 3\n    error_message: Optional[str] = None\n\n    class Config:\n        json_encoders = {\n            datetime: lambda v: v.isoformat(),\n            UUID: lambda v: str(v)\n        }\n</code></pre>"},{"location":"ai/#queuestate","title":"QueueState","text":"<pre><code>class QueueState(BaseModel):\n    \"\"\"Represents the current state of the download queue.\"\"\"\n    items: List[DownloadItem]\n    active_downloads: int\n    total_items: int\n    queue_size: int\n\n    @property\n    def is_full(self) -&gt; bool:\n        return self.total_items &gt;= self.queue_size\n</code></pre>"},{"location":"ai/#downloadprogress","title":"DownloadProgress","text":"<pre><code>class ProgressUpdate(BaseModel):\n    \"\"\"Progress update for a download.\"\"\"\n    item_id: UUID\n    bytes_downloaded: int\n    total_bytes: Optional[int]\n    speed_bps: float\n    eta_seconds: Optional[float]\n    status_message: str\n</code></pre>"},{"location":"ai/#user-management-models","title":"User Management Models","text":""},{"location":"ai/#userpermissions","title":"UserPermissions","text":"<pre><code>class PermissionLevel(str, Enum):\n    \"\"\"User permission levels.\"\"\"\n    NORMAL = \"normal\"\n    PREMIUM = \"premium\"\n    ADMIN = \"admin\"\n\nclass UserSettings(BaseModel):\n    \"\"\"User-specific settings and permissions.\"\"\"\n    user_id: int\n    permission_level: PermissionLevel = PermissionLevel.NORMAL\n    max_concurrent_downloads: int = 2\n    max_file_size_mb: int = 50\n    total_downloads: int = 0\n    created_at: datetime\n    last_download: Optional[datetime] = None\n</code></pre>"},{"location":"ai/#file-management-models","title":"File Management Models","text":""},{"location":"ai/#downloadedfile","title":"DownloadedFile","text":"<pre><code>class DownloadedFile(BaseModel):\n    \"\"\"Represents a downloaded file in temporary storage.\"\"\"\n    id: UUID\n    download_item_id: UUID\n    filename: str\n    file_path: Path\n    size_bytes: int\n    mime_type: str\n    created_at: datetime\n    expires_at: datetime\n    checksum: str\n\n    @property\n    def is_expired(self) -&gt; bool:\n        return datetime.now() &gt; self.expires_at\n</code></pre>"},{"location":"ai/#error-models","title":"Error Models","text":""},{"location":"ai/#downloaderror","title":"DownloadError","text":"<pre><code>class ErrorType(str, Enum):\n    \"\"\"Types of download errors.\"\"\"\n    NETWORK = \"network\"\n    RATE_LIMIT = \"rate_limit\"\n    FILE_TOO_LARGE = \"file_too_large\"\n    INVALID_URL = \"invalid_url\"\n    PERMISSION_DENIED = \"permission_denied\"\n    UNKNOWN = \"unknown\"\n\nclass DownloadError(BaseModel):\n    \"\"\"Detailed error information for failed downloads.\"\"\"\n    item_id: UUID\n    error_type: ErrorType\n    message: str\n    timestamp: datetime\n    retry_after: Optional[float] = None\n    is_permanent: bool = False\n</code></pre>"},{"location":"ai/#metrics-models","title":"Metrics Models","text":""},{"location":"ai/#downloadmetrics","title":"DownloadMetrics","text":"<pre><code>class DownloadMetrics(BaseModel):\n    \"\"\"Metrics for download performance monitoring.\"\"\"\n    total_downloads: int = 0\n    failed_downloads: int = 0\n    average_speed_bps: float = 0.0\n    total_bytes_downloaded: int = 0\n    queue_wait_time_seconds: float = 0.0\n    active_downloads: int = 0\n    success_rate: float = 100.0\n</code></pre>"},{"location":"ai/#prometheus-metrics-structure","title":"Prometheus Metrics Structure","text":"<p>The following metrics will be collected and exposed via Prometheus:</p>\n<pre><code>from prometheus_client import Counter, Gauge, Histogram\n\n# Core Download Metrics\nDOWNLOAD_REQUESTS = Counter(\n    'download_requests_total',\n    'Total download requests',\n    ['platform', 'status']  # platform: twitter/reddit, status: success/failed\n)\n\nDOWNLOAD_DURATION = Histogram(\n    'download_duration_seconds',\n    'Time spent downloading',\n    ['platform'],\n    buckets=[1, 5, 10, 30, 60, 120, 300]  # 1s to 5min buckets\n)\n\nQUEUE_SIZE = Gauge(\n    'download_queue_size',\n    'Current size of download queue'\n)\n\n# Performance Metrics\nCOMMAND_LATENCY = Histogram(\n    'command_latency_seconds',\n    'Command processing time',\n    ['command'],\n    buckets=[0.1, 0.5, 1, 2, 5]  # 100ms to 5s buckets\n)\n\nDISCORD_API_LATENCY = Gauge(\n    'discord_api_latency_seconds',\n    'Discord API latency'\n)\n\n# Error Metrics\nERROR_COUNT = Counter(\n    'error_total',\n    'Total errors by type',\n    ['error_type', 'platform']\n)\n\n# Resource Usage Metrics\nMEMORY_USAGE = Gauge(\n    'memory_usage_bytes',\n    'Memory usage in bytes'\n)\n\nCPU_USAGE = Gauge(\n    'cpu_usage_percent',\n    'CPU usage percentage'\n)\n\n# File Management Metrics\nFILE_SIZE = Histogram(\n    'download_file_size_bytes',\n    'Size of downloaded files',\n    ['platform'],\n    buckets=[1024*1024*x for x in [1, 5, 10, 25, 50]]  # 1MB to 50MB buckets\n)\n\nTEMP_FILES = Gauge(\n    'temp_files_count',\n    'Number of files in temporary storage'\n)\n</code></pre>\n<p>These metrics provide comprehensive monitoring of:\n- Download performance and success rates\n- Command response times\n- Resource utilization\n- Error tracking\n- File management statistics</p>\n<p>Metrics will be exposed on <code>/metrics</code> endpoint for Prometheus scraping.</p>\n<p>These models provide a strong foundation for type safety and data validation throughout the application. Each model includes:\n- Comprehensive type hints\n- Default values where appropriate\n- Validation rules\n- JSON serialization support\n- Clear documentation\n- Enum-based status and type fields</p>\n<p>The models are designed to be:\n- Immutable where possible\n- Self-validating\n- Easy to serialize/deserialize\n- Well-documented\n- Extensible for future features\n</p>\n<p>"},{"location":"ai/#file-management-specifications","title":"File Management Specifications","text":""},{"location":"ai/#storage-architecture","title":"Storage Architecture","text":""},{"location":"ai/#directory-structure","title":"Directory Structure","text":"<pre><code>/tmp/boss-bot/\n\u251c\u2500\u2500 downloads/\n\u2502   \u251c\u2500\u2500 {guild_id}/\n\u2502   \u2502   \u251c\u2500\u2500 {yyyy-mm-dd}/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 {download_id}/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 metadata.json\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 content/\n\u2502   \u2502   \u2502   \u2502       \u2514\u2500\u2500 {filename}\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 .cleanup\n\u2502   \u2502   \u2514\u2500\u2500 .stats\n\u2502   \u2514\u2500\u2500 .maintenance\n\u251c\u2500\u2500 temp/\n\u2502   \u2514\u2500\u2500 {download_id}/\n\u2514\u2500\u2500 .locks/\n</code></pre>"},{"location":"ai/#storage-policies_1","title":"Storage Policies","text":""},{"location":"ai/#temporary-storage","title":"Temporary Storage","text":"<ul>\n<li>Location: All downloads initially go to <code>/tmp/boss-bot/temp/{download_id}/</code></li>\n<li>Retention: Files in temp are deleted after:</li>\n<li>Successful upload to Discord (immediate)</li>\n<li>Failed download (30 minutes)</li>\n<li>Abandoned download (1 hour)</li>\n<li>Size Limits:</li>\n<li>Individual file: 50MB (Discord limit)</li>\n<li>Total temp storage: 1GB</li>\n<li>Per guild daily quota: 500MB</li>\n</ul>"},{"location":"ai/#organized-storage","title":"Organized Storage","text":"<ul>\n<li>Structure: Downloads are organized by guild and date</li>\n<li>Metadata: Each download includes a metadata.json file containing:</li>\n<li>Original URL</li>\n<li>Download timestamp</li>\n<li>User information</li>\n<li>File checksums</li>\n<li>Processing history</li>\n<li>Retention Periods:</li>\n<li>Successful downloads: 24 hours</li>\n<li>Failed downloads with retry potential: 6 hours</li>\n<li>Premium guild downloads: 72 hours</li>\n</ul>"},{"location":"ai/#cleanup-mechanisms","title":"Cleanup Mechanisms","text":""},{"location":"ai/#scheduled-cleanup","title":"Scheduled Cleanup","text":"<pre><code>class CleanupSchedule:\n    \"\"\"Cleanup schedule configuration.\"\"\"\n    TEMP_SCAN_INTERVAL: int = 300  # 5 minutes\n    MAIN_SCAN_INTERVAL: int = 3600  # 1 hour\n    DEEP_SCAN_INTERVAL: int = 86400  # 24 hours\n</code></pre>"},{"location":"ai/#cleanup-rules","title":"Cleanup Rules","text":"<ol>\n<li>Temporary Files:</li>\n<li>Run every 5 minutes</li>\n<li>Remove files older than their retention period</li>\n<li>Remove files from completed/failed downloads</li>\n<li>\n<p>Clean partial downloads older than 1 hour</p>\n</li>\n<li>\n<p>Main Storage:</p>\n</li>\n<li>Run every hour</li>\n<li>Remove expired files based on retention policy</li>\n<li>Clean empty directories</li>\n<li>\n<p>Update storage statistics</p>\n</li>\n<li>\n<p>Deep Cleanup:</p>\n</li>\n<li>Run daily during low-usage hours</li>\n<li>Perform integrity checks</li>\n<li>Remove orphaned files</li>\n<li>Compress and archive logs</li>\n<li>Generate storage reports</li>\n</ol>"},{"location":"ai/#file-operations_1","title":"File Operations","text":""},{"location":"ai/#download-process","title":"Download Process","text":"<ol>\n<li>\n<p>Initial Download:\n   <pre><code>async def download_flow(url: str, guild_id: int) -&gt; Path:\n    temp_path = get_temp_path(download_id)\n    try:\n        await download_to_temp(url, temp_path)\n        await validate_download(temp_path)\n        final_path = organize_download(temp_path, guild_id)\n        return final_path\n    except Exception as e:\n        await cleanup_failed_download(temp_path)\n        raise\n</code></pre></p>\n</li>\n<li>\n<p>Validation Checks:</p>\n</li>\n<li>File size limits</li>\n<li>MIME type verification</li>\n<li>Malware scanning</li>\n<li>\n<p>File integrity checks</p>\n</li>\n<li>\n<p>Organization Process:</p>\n</li>\n<li>Create guild/date directories</li>\n<li>Generate metadata</li>\n<li>Move from temp to organized storage</li>\n<li>Update storage statistics</li>\n</ol>"},{"location":"ai/#error-handling","title":"Error Handling","text":""},{"location":"ai/#storage-errors","title":"Storage Errors","text":"<pre><code>class StorageError(Enum):\n    DISK_FULL = \"Insufficient disk space\"\n    QUOTA_EXCEEDED = \"Guild quota exceeded\"\n    INVALID_FILE = \"File validation failed\"\n    CLEANUP_ERROR = \"Cleanup process failed\"\n</code></pre>"},{"location":"ai/#recovery-procedures","title":"Recovery Procedures","text":"<ol>\n<li>Disk Space Issues:</li>\n<li>Trigger emergency cleanup</li>\n<li>Notify administrators</li>\n<li>\n<p>Temporarily reject new downloads</p>\n</li>\n<li>\n<p>Quota Exceeded:</p>\n</li>\n<li>Notify guild administrators</li>\n<li>Provide cleanup recommendations</li>\n<li>\n<p>Offer premium upgrade options</p>\n</li>\n<li>\n<p>Validation Failures:</p>\n</li>\n<li>Log detailed error information</li>\n<li>Notify user with specific reason</li>\n<li>Clean up invalid files immediately</li>\n</ol>"},{"location":"ai/#monitoring-and-metrics","title":"Monitoring and Metrics","text":""},{"location":"ai/#storage-metrics","title":"Storage Metrics","text":"<pre><code>class StorageMetrics(BaseModel):\n    \"\"\"Storage monitoring metrics.\"\"\"\n    total_space_used: int\n    temp_space_used: int\n    downloads_per_guild: Dict[int, int]\n    cleanup_stats: CleanupStats\n    error_counts: Dict[StorageError, int]\n</code></pre>"},{"location":"ai/#alerts","title":"Alerts","text":"<ul>\n<li>Disk space usage &gt; 80%</li>\n<li>Cleanup job failures</li>\n<li>High error rates</li>\n<li>Quota approaching limits</li>\n<li>Suspicious file patterns</li>\n</ul>"},{"location":"ai/#file-types-and-processing","title":"File Types and Processing","text":""},{"location":"ai/#supported-file-types","title":"Supported File Types","text":"<pre><code>class SupportedTypes(BaseModel):\n    \"\"\"Supported file types and their processors.\"\"\"\n    IMAGES: List[str] = [\"jpg\", \"png\", \"gif\", \"webp\"]\n    VIDEOS: List[str] = [\"mp4\", \"webm\", \"mov\"]\n    AUDIO: List[str] = [\"mp3\", \"wav\", \"ogg\"]\n    MAX_SIZES: Dict[str, int] = {\n        \"image\": 5_242_880,  # 5MB\n        \"video\": 52_428_800,  # 50MB\n        \"audio\": 10_485_760  # 10MB\n    }\n</code></pre>"},{"location":"ai/#processing-rules","title":"Processing Rules","text":"<ol>\n<li>Images:</li>\n<li>Convert to Discord-optimal formats</li>\n<li>Resize if exceeding limits</li>\n<li>\n<p>Strip metadata</p>\n</li>\n<li>\n<p>Videos:</p>\n</li>\n<li>Transcode to Discord-compatible format</li>\n<li>Adjust bitrate if needed</li>\n<li>\n<p>Generate thumbnail</p>\n</li>\n<li>\n<p>Audio:</p>\n</li>\n<li>Convert to Discord-supported format</li>\n<li>Adjust quality if size exceeds limits</li>\n</ol>\n<p>This comprehensive file management system ensures:\n- Efficient use of storage space\n- Reliable cleanup of temporary files\n- Clear organization of downloads\n- Robust error handling\n- Detailed monitoring and metrics\n- Type-safe file processing\n</p>\n<p>"},{"location":"ai/#user-experience-specifications","title":"User Experience Specifications","text":""},{"location":"ai/#command-interface","title":"Command Interface","text":""},{"location":"ai/#command-structure_1","title":"Command Structure","text":"<pre><code>class CommandFormat:\n    \"\"\"Standard command format specifications.\"\"\"\n    PREFIX: str = \"$\"\n    COMMANDS: Dict[str, str] = {\n        \"dlt\": \"Download from Twitter\",\n        \"dlr\": \"Download from Reddit\",\n        \"dlq\": \"Show queue status\",\n        \"dlc\": \"Cancel download\",\n        \"dls\": \"Show settings\",\n        \"dlh\": \"Show help\"\n    }\n</code></pre>"},{"location":"ai/#progress-updates","title":"Progress Updates","text":"<pre><code>class ProgressFormat:\n    \"\"\"Progress message formatting.\"\"\"\n    TEMPLATE: str = \"\"\"\n    Downloading: {filename}\n    Progress: {bar} {percentage}%\n    Speed: {speed}/s\n    ETA: {eta}\n    Status: {status}\n    \"\"\"\n    UPDATE_INTERVAL: int = 5  # seconds\n    BAR_LENGTH: int = 20\n</code></pre>\n<p>Example Progress Message:\n<pre><code>Downloading: funny_cat_video.mp4\nProgress: [====================] 100%\nSpeed: 1.2 MB/s\nETA: Complete\nStatus: Processing for Discord upload\n</code></pre></p>"},{"location":"ai/#user-interactions","title":"User Interactions","text":""},{"location":"ai/#command-flow","title":"Command Flow","text":"<ol>\n<li>\n<p>Download Initiation:\n   <pre><code>User: $dlt https://twitter.com/user/status/123\nBot: Starting download...\n     Queue position: 2\n     Estimated start: 2 minutes\n</code></pre></p>\n</li>\n<li>\n<p>Progress Updates:\n   <pre><code>Bot: [Progress message updates every 5 seconds]\n     Updates merge into single message\n     Uses reactions for user controls\n</code></pre></p>\n</li>\n<li>\n<p>Completion/Error:\n   <pre><code>Bot: \u2705 Download complete!\n     File: funny_cat_video.mp4\n     Size: 2.3MB\n     Time: 45s\n</code></pre></p>\n</li>\n</ol>"},{"location":"ai/#interactive-elements","title":"Interactive Elements","text":"<ol>\n<li>Progress Control Reactions:</li>\n<li>\u23f8\ufe0f Pause download</li>\n<li>\u25b6\ufe0f Resume download</li>\n<li>\u23f9\ufe0f Cancel download</li>\n<li>\u2139\ufe0f Show details</li>\n<li>\n<p>\ud83d\udd04 Retry failed download</p>\n</li>\n<li>\n<p>Queue Management:\n   <pre><code>User: $dlq\nBot: Current Queue Status:\n     1. video1.mp4 [===&gt;    ] 35%\n     2. image.jpg [WAITING]\n     3. video2.mp4 [WAITING]\n\n     Your position: 2\n     Estimated wait: 3 minutes\n</code></pre></p>\n</li>\n<li>\n<p>Settings Management:\n   <pre><code>User: $dls\nBot: Your Settings:\n     Max concurrent downloads: 2\n     Notification preference: Mentions\n     Default priority: Normal\n     Total downloads today: 5/10\n</code></pre></p>\n</li>\n</ol>"},{"location":"ai/#notification-system","title":"Notification System","text":""},{"location":"ai/#update-types","title":"Update Types","text":"<pre><code>class NotificationPreference(Enum):\n    \"\"\"User notification preferences.\"\"\"\n    NONE = \"none\"  # No updates\n    MINIMAL = \"minimal\"  # Start/finish only\n    NORMAL = \"normal\"  # Regular progress\n    VERBOSE = \"verbose\"  # Detailed updates\n</code></pre>"},{"location":"ai/#notification-rules","title":"Notification Rules","text":"<ol>\n<li>Queue Position Updates:</li>\n<li>When moving up in queue</li>\n<li>When about to start</li>\n<li>\n<p>On significant delays</p>\n</li>\n<li>\n<p>Download Progress:</p>\n</li>\n<li>Start of download</li>\n<li>Regular progress updates</li>\n<li>Completion/failure</li>\n<li>\n<p>Processing status</p>\n</li>\n<li>\n<p>Error Notifications:</p>\n</li>\n<li>Clear error description</li>\n<li>Recommended actions</li>\n<li>Retry instructions</li>\n<li>Support information</li>\n</ol>"},{"location":"ai/#user-settings","title":"User Settings","text":""},{"location":"ai/#configurable-options","title":"Configurable Options","text":"<pre><code>class UserPreferences(BaseModel):\n    \"\"\"User-configurable settings.\"\"\"\n    notification_level: NotificationPreference\n    progress_bar_style: str\n    default_priority: DownloadPriority\n    mention_on_complete: bool\n    auto_retry: bool\n    max_retries: int = 3\n</code></pre>"},{"location":"ai/#default-values","title":"Default Values","text":"<pre><code>DEFAULT_PREFERENCES = UserPreferences(\n    notification_level=NotificationPreference.NORMAL,\n    progress_bar_style=\"standard\",\n    default_priority=DownloadPriority.NORMAL,\n    mention_on_complete=True,\n    auto_retry=True\n)\n</code></pre>"},{"location":"ai/#help-system","title":"Help System","text":""},{"location":"ai/#command-help","title":"Command Help","text":"<pre><code>class HelpFormat:\n    \"\"\"Help message formatting.\"\"\"\n    TEMPLATE: str = \"\"\"\n    {command}: {description}\n    Usage: {usage}\n    Examples:\n    {examples}\n    Notes:\n    {notes}\n    \"\"\"\n</code></pre>\n<p>Example Help Message:\n<pre><code>$dlt: Download from Twitter\nUsage: $dlt &lt;url&gt; [priority]\n\nExamples:\n  $dlt https://twitter.com/user/status/123\n  $dlt https://twitter.com/user/status/123 high\n\nNotes:\n- Supports single tweets and threads\n- Max file size: 50MB\n- Supported formats: Images, Videos\n</code></pre></p>"},{"location":"ai/#error-messages","title":"Error Messages","text":""},{"location":"ai/#user-friendly-errors","title":"User-Friendly Errors","text":"<pre><code>class ErrorMessages:\n    \"\"\"User-friendly error messages.\"\"\"\n    TEMPLATES = {\n        ErrorType.NETWORK: (\n            \"\ud83d\udd0c Connection issue! I couldn't reach {platform}.\\n\"\n            \"I'll retry {retry_count} more times.\\n\"\n            \"Try again in {retry_after} seconds.\"\n        ),\n        ErrorType.RATE_LIMIT: (\n            \"\u23f3 We're being rate limited by {platform}.\\n\"\n            \"Please wait {retry_after} seconds.\"\n        ),\n        ErrorType.FILE_TOO_LARGE: (\n            \"\ud83d\udce6 File is too large ({size}MB)!\\n\"\n            \"Maximum size: {max_size}MB\\n\"\n            \"Try requesting a smaller version.\"\n        )\n    }\n</code></pre>\n<p>The user experience design focuses on:\n- Clear and intuitive commands\n- Real-time progress feedback\n- Interactive controls\n- Customizable notifications\n- Helpful error messages\n- Comprehensive help system</p>\n<p>Would you like me to:\n1. Add more command examples?\n2. Expand the notification system?\n3. Add more interactive features?\n4. Detail the help system further?\n</p>\n<p>"},{"location":"ai/#epic-list","title":"Epic List","text":""},{"location":"ai/#epic-1-core-bot-infrastructure","title":"Epic-1: Core Bot Infrastructure","text":"<ul>\n<li>Discord bot setup and configuration</li>\n<li>Command handling framework</li>\n<li>Error handling and logging</li>\n<li>Testing infrastructure</li>\n</ul>"},{"location":"ai/#epic-2-media-download-system","title":"Epic-2: Media Download System","text":"<ul>\n<li>Download queue implementation</li>\n<li>Progress tracking</li>\n<li>File management</li>\n<li>Platform-specific downloaders</li>\n</ul>"},{"location":"ai/#epic-3-future-rag-enhancement-beyond-current-prd","title":"Epic-3: Future RAG Enhancement (Beyond Current PRD)","text":"<ul>\n<li>LangChain and LangGraph integration</li>\n<li>Redis vector store setup</li>\n<li>Extended command set</li>\n<li>CLI interface\n</li>\n</ul>\n<p>"},{"location":"ai/#epic-1-story-list","title":"Epic 1: Story List","text":"<ul>\n<li>Story 1: Project Initialization and Environment Setup\n  Status: ''\n  Requirements:</li>\n<li>Initialize Python project with uv</li>\n<li>Create project structure following the defined layout</li>\n<li>Set up pyproject.toml with initial dependencies</li>\n<li>Configure ruff for linting and formatting</li>\n<li>Create initial README.md with setup instructions</li>\n<li>Set up pre-commit hooks for code quality\n  Acceptance Criteria:</li>\n<li>Project can be cloned and installed with uv</li>\n<li>Ruff runs successfully on empty project</li>\n<li>\n<p>README contains clear setup steps\n  Dependencies: None</p>\n</li>\n<li>\n<p>Story 2: Test Infrastructure Setup\n  Status: ''\n  Requirements:</p>\n</li>\n<li>Set up pytest with all testing dependencies</li>\n<li>Create basic test configuration in conftest.py</li>\n<li>Set up coverage reporting with coverage[toml]</li>\n<li>Configure tox-uv for test automation</li>\n<li>Create test helper utilities and fixtures</li>\n<li>Add example tests to validate setup\n  Acceptance Criteria:</li>\n<li>All test dependencies installed and configured</li>\n<li>Example tests run successfully</li>\n<li>\n<p>Coverage reports generate correctly\n  Dependencies: Story 1</p>\n</li>\n<li>\n<p>Story 3: Logging and Monitoring Setup\n  Status: ''\n  Requirements:</p>\n</li>\n<li>Implement logging system with loguru</li>\n<li>Configure better-exceptions for error handling</li>\n<li>Set up basic performance monitoring</li>\n<li>Create logging configuration file</li>\n<li>Add log rotation and management\n  Acceptance Criteria:</li>\n<li>Logs are properly formatted and stored</li>\n<li>Better-exceptions shows detailed error traces</li>\n<li>\n<p>Basic metrics are collected\n  Dependencies: Story 1</p>\n</li>\n<li>\n<p>Story 4: Basic Discord Bot Setup\n  Status: ''\n  Requirements:</p>\n</li>\n<li>Create Discord application and bot user</li>\n<li>Implement basic bot client with required intents</li>\n<li>Set up environment configuration with pydantic-settings</li>\n<li>Create connection and basic event handling</li>\n<li>Add health check command\n  Acceptance Criteria:</li>\n<li>Bot successfully connects to Discord</li>\n<li>Basic events (ready, disconnect) are handled</li>\n<li>Health check command responds</li>\n<li>Bot recovers gracefully from disconnections</li>\n<li>\n<p>Connection state changes are properly logged\n  Dependencies: Story 1, Story 3</p>\n</li>\n<li>\n<p>Story 4a: Discord Connection Setup\n  Status: ''\n  Requirements:</p>\n</li>\n<li>Create Discord application and bot user</li>\n<li>Implement basic bot client with required intents</li>\n<li>Set up environment configuration with pydantic-settings</li>\n<li>Implement connection handling and basic event loop</li>\n<li>Add health check endpoint</li>\n<li>Implement graceful disconnection recovery</li>\n<li>Add connection state logging\n  Acceptance Criteria:</li>\n<li>Bot successfully connects to Discord</li>\n<li>Bot responds to health check command</li>\n<li>Bot handles connection state changes</li>\n<li>Environment variables are properly loaded</li>\n<li>Bot automatically recovers from disconnections</li>\n<li>\n<p>All connection state changes are logged\n  Dependencies: Story 1, Story 3</p>\n</li>\n<li>\n<p>Story 4b: Command Framework Base\n  Status: ''\n  Requirements:</p>\n</li>\n<li>Set up command registration system</li>\n<li>Implement basic error handling for commands</li>\n<li>Add help command structure</li>\n<li>Create command testing utilities</li>\n<li>(Nice to Have) Implement command cooldowns</li>\n<li>(Nice to Have) Implement rate limiting\n  Acceptance Criteria:</li>\n<li>Commands can be registered and respond</li>\n<li>Error handling works for basic cases</li>\n<li>Help command shows available commands</li>\n<li>Command tests pass with dpytest</li>\n<li>\n<p>(Nice to Have) Rate limiting prevents command spam\n  Dependencies: Story 4a</p>\n</li>\n<li>\n<p>Story 4c: Event Handler Implementation\n  Status: ''\n  Requirements:</p>\n</li>\n<li>Add core event handlers (ready, disconnect, etc.)</li>\n<li>Implement reconnection logic</li>\n<li>Add basic event logging</li>\n<li>Create event testing framework</li>\n<li>Implement error recovery for events\n  Acceptance Criteria:</li>\n<li>All core events are handled and logged</li>\n<li>Bot recovers from disconnections</li>\n<li>Event tests pass with dpytest</li>\n<li>Error recovery works as expected</li>\n<li>\n<p>Event handling coverage meets 20% target\n  Dependencies: Story 4b</p>\n</li>\n<li>\n<p>Story 5: Queue System Foundation\n  Status: ''\n  Requirements:</p>\n</li>\n<li>Design queue data structures</li>\n<li>Implement basic queue manager</li>\n<li>Add queue persistence</li>\n<li>Create queue status command</li>\n<li>Implement queue tests\n  Acceptance Criteria:</li>\n<li>Queue can add and remove items</li>\n<li>Queue state persists across restarts</li>\n<li>Queue status is queryable</li>\n<li>Queue tests pass\n  Dependencies: Story 5, Story 6</li>\n</ul>\n<p>Each story includes clear dependencies, making it easier for junior developers to understand the progression. Stories are broken down into smaller, manageable tasks with clear acceptance criteria. \ud83c\udfd7\ufe0f\n</p>\n<p>"},{"location":"ai/#technology-stack","title":"Technology Stack","text":"Technology\nDescription\n\n\n\n\nPython 3.12\nPrimary development language\n\n\nuv\nPackage management and dependency resolution\n\n\ndiscord.py\nDiscord bot framework\n\n\npytest\nTesting framework with powerful fixture support and assertion introspection\n\n\ndpytest\nDiscord.py testing utilities\n\n\ngallery-dl\nReddit, instagram, twitter, other social media media download utility\n\n\nyt-dlp\nyoutube/video download utility\n\n\nhttpx\nFully featured HTTP client for Python 3, with sync and async APIs, and HTTP/1.1 and HTTP/2 support\n\n\npydantic\nData validation\n\n\npydantic-settings\nConfiguration management\n\n\nloguru\nLogging utility\n\n\naiofiles\nAsynchronous file I/O operations using asyncio\n\n\nbetter-exceptions\nEnhanced exception handling with more informative error messages"},{"location":"ai/#testing-dependencies","title":"Testing Dependencies","text":"Technology\nDescription\n\n\n\n\npytest-mock\nThin-wrapper around the unittest.mock package for easier mock creation\n\n\nrespx\nModern, elegant HTTP mocking for Python tests\n\n\npytest-recording\nRecord and replay test interactions for reliable testing\n\n\npytest-retry\nRetry flaky tests to improve reliability\n\n\npytest-skip-slow\nSkip slow tests for faster development cycles\n\n\npytest-ignore-flaky\nManage and track flaky tests separately\n\n\ntox-uv\nTox plugin for UV package manager integration\n\n\nruff\nFast Python linter and code formatter written in Rust\n\n\ncoverage[toml]\nCode coverage measurement with TOML configuration support"},{"location":"ai/#future-dependencies","title":"Future Dependencies","text":"Technology\nDescription\n\n\n\n\nLangChain\nRAG framework\n\n\nLangGraph\nRAG workflow management\n\n\nOpenAI\nEmbeddings and LLM via LangChain\n\n\nRedis\nVector store\n\n\nTyper\nCLI interface\n\n\n\n\n\n\n\n<p>"},{"location":"ai/#project-structure_1","title":"Project Structure","text":"<p><pre><code>boss-bot/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 boss_bot/\n\u2502   \u2502   \u251c\u2500\u2500 bot/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 client.py          # Discord client setup\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 events.py          # Event handlers\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 cogs/             # Discord cogs directory\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 download_cog.py    # Media download commands\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 queue_cog.py       # Queue management commands\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 rag_cog.py         # RAG-related commands\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 admin_cog.py       # Admin/utility commands\n\u2502   \u2502   \u251c\u2500\u2500 cli/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app.py            # Typer CLI application\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 commands/         # CLI command modules\n\u2502   \u2502   \u251c\u2500\u2500 commands/             # Shared command logic\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 download.py       # Download command business logic\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 queue.py          # Queue management logic\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 rag.py           # RAG-related logic\n\u2502   \u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 config.py        # Configuration management\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 queue.py         # Queue implementation\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 storage.py       # File storage management\n\u2502   \u2502   \u251c\u2500\u2500 downloaders/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base.py          # Base downloader class\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 twitter.py       # Twitter downloader\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 reddit.py        # Reddit downloader\n\u2502   \u2502   \u251c\u2500\u2500 llm/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 chains/         # LangChain components\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 graphs/         # LangGraph workflows\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 embeddings.py   # Embedding configurations\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 models.py       # LLM configurations\n\u2502   \u2502   \u251c\u2500\u2500 rag/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 indexer.py      # Document indexing\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 retriever.py    # Vector store retrieval\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 store.py        # Redis vector store management\n\u2502   \u2502   \u251c\u2500\u2500 schemas/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 download.py     # Download-related models\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 queue.py        # Queue-related models\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 rag.py          # RAG-related models\n\u2502   \u2502   \u2514\u2500\u2500 utils/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 logging.py      # Logging configuration\n\u2502   \u2502       \u2514\u2500\u2500 metrics.py      # Performance monitoring\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u251c\u2500\u2500 test_bot/\n\u2502   \u251c\u2500\u2500 test_cli/\n\u2502   \u251c\u2500\u2500 test_commands/\n\u2502   \u251c\u2500\u2500 test_downloaders/\n\u2502   \u251c\u2500\u2500 test_llm/\n\u2502   \u2514\u2500\u2500 test_rag/\n\u251c\u2500\u2500 docs/                       # Documentation directory\n\u251c\u2500\u2500 scripts/                    # Utility scripts\n\u251c\u2500\u2500 .devcontainer/             # Development container config\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 README.md\n</code></pre>\n</p>"},{"location":"ai/#data-models_2","title":"Data Models","text":""},{"location":"ai/#download-item-schema","title":"Download Item Schema","text":"<p>"},{"location":"ai/#implementation-strategy-and-timeline","title":"Implementation Strategy and Timeline","text":"<p>Our implementation follows a strict TDD-first approach with two main phases:</p>"},{"location":"ai/#phase-1-mvp-discord-core","title":"Phase 1 (MVP - Discord Core)","text":"<ul>\n<li>Test infrastructure setup</li>\n<li>Basic Discord bot framework</li>\n<li>Download command implementation</li>\n<li>Queue management system</li>\n</ul>"},{"location":"ai/#phase-2-mvp-download-features","title":"Phase 2 (MVP - Download Features)","text":"<ul>\n<li>Enhanced download capabilities</li>\n<li>Progress tracking</li>\n<li>File management</li>\n<li>Error handling</li>\n</ul>"},{"location":"ai/#development-methodology","title":"Development Methodology","text":"<p>For each phase, we follow these steps:\n1. Write comprehensive test suites first\n2. Define clear acceptance criteria\n3. Implement minimum code to pass tests\n4. Refactor while maintaining coverage\n5. Document all components</p>"},{"location":"ai/#implementation-timeline","title":"Implementation Timeline","text":"Task\nStatus\nDeadline\n\n\n\n\nTest Infrastructure Setup\nTo Do\n2024-05-15\n\n\nBasic Discord Integration\nTo Do\n2024-05-22\n\n\nDownload Commands (Twitter)\nTo Do\n2024-05-29\n\n\nDownload Commands (Reddit)\nTo Do\n2024-06-05\n\n\nQueue Management\nTo Do\n2024-06-12\n\n\nProgress Tracking\nTo Do\n2024-06-19"},{"location":"ai/#testing-requirements","title":"Testing Requirements","text":"<ul>\n<li>Test coverage must be &gt;= 90%</li>\n<li>All error cases must be covered</li>\n<li>Performance tests must be included</li>\n<li>Integration tests required for each feature</li>\n<li>External services must be mocked appropriately</li>\n</ul>"},{"location":"ai/#quality-gates","title":"Quality Gates","text":"<p>Each task must pass these gates before being considered complete:\n1. All tests passing (unit, integration, performance)\n2. Code review completed\n3. Documentation updated\n4. Test coverage meets minimum threshold\n5. Performance metrics within acceptable ranges\n6. All error handling scenarios tested\n7. Clean code principles verified\n8. Type hints and docstrings complete\n</p>\n<p>"},{"location":"ai/#discord-integration-and-download-system","title":"Discord Integration and Download System","text":""},{"location":"ai/#1-discord-bot-configuration","title":"1. Discord Bot Configuration","text":""},{"location":"ai/#required-intents","title":"Required Intents","text":"<ul>\n<li>message_content</li>\n<li>guilds</li>\n<li>members</li>\n<li>messages</li>\n<li>reactions</li>\n</ul>"},{"location":"ai/#command-structure_2","title":"Command Structure","text":"Command\nDescription\n\n\n\n\n$dlt \nTwitter/video downloads\n\n\n$dlr \nReddit downloads\n\n\n$dlq\nShow queue status\n\n\n$dlc\nCancel download"},{"location":"ai/#permission-model","title":"Permission Model","text":"<ul>\n<li>Role-based access control</li>\n<li>Download size limits per role</li>\n<li>Queue priority handling</li>\n</ul>"},{"location":"ai/#2-download-system-architecture","title":"2. Download System Architecture","text":""},{"location":"ai/#download-manager","title":"Download Manager","text":"<ul>\n<li>Async download queue implementation</li>\n<li>Real-time progress tracking</li>\n<li>Rate limiting and throttling</li>\n<li>Concurrent download limits (max 5)</li>\n</ul>"},{"location":"ai/#file-management_1","title":"File Management","text":"<ul>\n<li>Temporary storage handling with cleanup</li>\n<li>Automatic file cleanup after successful upload</li>\n<li>File size validation (max 50MB - Discord limit)</li>\n<li>Format verification and validation</li>\n</ul>"},{"location":"ai/#3-error-handling-and-recovery","title":"3. Error Handling and Recovery","text":"<ul>\n<li>Network failure recovery with automatic retries</li>\n<li>Invalid URL handling with user feedback</li>\n<li>File system error management</li>\n<li>Rate limit handling with exponential backoff</li>\n<li>Queue overflow management with priority system</li>\n</ul>"},{"location":"ai/#4-performance-requirements","title":"4. Performance Requirements","text":"<ul>\n<li>Download initiation response: &lt; 1 second</li>\n<li>Queue status updates: Every 5 seconds</li>\n<li>Maximum queue size: 50 items</li>\n<li>Concurrent downloads: 5 max</li>\n<li>File size limits: 50MB (Discord limit)</li>\n<li>Memory usage: &lt; 500MB under load</li>\n<li>CPU usage: &lt; 50% under load</li>\n</ul>"},{"location":"ai/#5-integration-points","title":"5. Integration Points","text":"<ul>\n<li>Discord.py event system</li>\n<li>Gallery-dl integration for media downloads</li>\n<li>File system for temporary storage</li>\n<li>Discord CDN for file uploads</li>\n<li>Rate limiting systems\n</li>\n</ul>\n<p>"},{"location":"ai/#test-driven-development-strategy","title":"Test-Driven Development Strategy","text":""},{"location":"ai/#1-core-test-infrastructure","title":"1. Core Test Infrastructure","text":""},{"location":"ai/#test-fixtures_1","title":"Test Fixtures","text":"<pre><code>@pytest.fixture\nasync def test_bot():\n    \"\"\"Core bot fixture for Discord command testing.\"\"\"\n    intents = discord.Intents.default()\n    intents.message_content = True\n    bot = commands.Bot(command_prefix=\"$\", intents=intents)\n    await bot._async_setup_hook()\n    dpytest.configure(bot)\n    yield bot\n    await dpytest.empty_queue()\n\n@pytest.fixture\ndef mock_downloader():\n    \"\"\"Mock downloader for testing download functionality.\"\"\"\n    with patch('bot.download.Downloader') as mock:\n        yield mock\n</code></pre>"},{"location":"ai/#2-command-testing-strategy","title":"2. Command Testing Strategy","text":""},{"location":"ai/#download-command-tests","title":"Download Command Tests","text":"<pre><code>@pytest.mark.asyncio\nclass TestDownloadCommands:\n    async def test_valid_twitter_url(self, test_bot, mock_downloader):\n        url = \"https://twitter.com/user/status/123\"\n        await dpytest.message(f\"$dlt {url}\")\n        assert dpytest.verify().message().contains()\n        mock_downloader.download.assert_called_once_with(url)\n\n    async def test_invalid_url(self, test_bot):\n        url = \"not_a_url\"\n        await dpytest.message(f\"$dlt {url}\")\n        assert dpytest.verify().message().contains(\"Invalid URL\")\n\n    async def test_queue_full(self, test_bot, mock_downloader):\n        mock_downloader.queue_size.return_value = 50\n        url = \"https://twitter.com/user/status/123\"\n        await dpytest.message(f\"$dlt {url}\")\n        assert dpytest.verify().message().contains(\"Queue full\")\n</code></pre>"},{"location":"ai/#3-integration-testing","title":"3. Integration Testing","text":"<pre><code>@pytest.mark.integration\nclass TestDownloadFlow:\n    async def test_download_to_completion(self, test_bot, mock_downloader):\n        # Given\n        url = \"https://twitter.com/user/status/123\"\n        mock_downloader.download.return_value = \"file.mp4\"\n\n        # When\n        await dpytest.message(f\"$dlt {url}\")\n\n        # Then\n        assert dpytest.verify().message().contains(\"Started\")\n        await asyncio.sleep(1)\n        assert dpytest.verify().message().contains(\"Complete\")\n        assert os.path.exists(\"file.mp4\")\n</code></pre>"},{"location":"ai/#4-error-case-testing","title":"4. Error Case Testing","text":"<pre><code>@pytest.mark.asyncio\nclass TestErrorHandling:\n    async def test_network_failure(self, test_bot, mock_downloader):\n        mock_downloader.download.side_effect = NetworkError\n        url = \"https://twitter.com/user/status/123\"\n        await dpytest.message(f\"$dlt {url}\")\n        assert dpytest.verify().message().contains(\"Network error\")\n\n    async def test_rate_limit(self, test_bot, mock_downloader):\n        mock_downloader.download.side_effect = RateLimitError\n        url = \"https://twitter.com/user/status/123\"\n        await dpytest.message(f\"$dlt {url}\")\n        assert dpytest.verify().message().contains(\"Rate limited\")\n</code></pre>"},{"location":"ai/#5-performance-testing","title":"5. Performance Testing","text":"<pre><code>@pytest.mark.performance\nclass TestPerformance:\n    async def test_response_time(self, test_bot):\n        start = time.time()\n        await dpytest.message(\"$dlt url\")\n        response_time = time.time() - start\n        assert response_time &lt; 1.0\n\n    async def test_concurrent_downloads(self, test_bot):\n        urls = [f\"url{i}\" for i in range(10)]\n        tasks = [dpytest.message(f\"$dlt {url}\") for url in urls]\n        await asyncio.gather(*tasks)\n        assert mock_downloader.active_downloads &lt;= 5\n</code></pre>"},{"location":"ai/#6-test-coverage-requirements","title":"6. Test Coverage Requirements","text":"<ul>\n<li>Unit Tests:</li>\n<li>All command handlers</li>\n<li>Queue management</li>\n<li>File operations</li>\n<li>Error handlers</li>\n<li>\n<p>Permission checks</p>\n</li>\n<li>\n<p>Integration Tests:</p>\n</li>\n<li>End-to-end download flows</li>\n<li>Discord message handling</li>\n<li>File upload process</li>\n<li>\n<p>Queue management system</p>\n</li>\n<li>\n<p>Performance Tests:</p>\n</li>\n<li>Response times</li>\n<li>Concurrent operations</li>\n<li>Memory usage</li>\n<li>CPU utilization</li>\n<li>Network bandwidth</li>\n</ul>"},{"location":"ai/#7-testing-best-practices","title":"7. Testing Best Practices","text":"<ul>\n<li>Use pytest markers for test categorization</li>\n<li>Implement proper cleanup in fixtures</li>\n<li>Mock external dependencies</li>\n<li>Use parameterized tests for edge cases</li>\n<li>Maintain test isolation</li>\n<li>Follow AAA pattern (Arrange-Act-Assert)</li>\n<li>Document test purposes and scenarios</li>\n<li>Regular test execution in CI/CD pipeline</li>\n</ul>"},{"location":"ai/#test-infrastructure_1","title":"Test Infrastructure","text":""},{"location":"ai/#directory-structure_1","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared test configuration and fixtures\n\u251c\u2500\u2500 fixtures/               # Test data and mock responses\n\u2502   \u251c\u2500\u2500 tweet_data.py      # Twitter API mock responses\n\u2502   \u251c\u2500\u2500 reddit_data.py     # Reddit API mock responses\n\u2502   \u251c\u2500\u2500 discord_data.py    # Discord API mock responses\n\u2502   \u2514\u2500\u2500 cassettes/         # VCR cassettes for HTTP mocking\n\u251c\u2500\u2500 unit/                  # Unit tests directory\n\u251c\u2500\u2500 integration/          # Integration tests\n\u2514\u2500\u2500 eval/                # Future RAG evaluation tests\n</code></pre>"},{"location":"ai/#test-data-management_1","title":"Test Data Management","text":"<pre><code># Example test fixture in tests/fixtures/tweet_data.py\n@pytest.fixture\ndef sample_tweet_data():\n    \"\"\"Sample tweet data for testing.\"\"\"\n    return {\n        \"url\": \"https://twitter.com/user/status/123\",\n        \"media\": [\"video.mp4\"],\n        \"text\": \"Sample tweet\"\n    }\n\n# Example VCR configuration in conftest.py\n@pytest.fixture(scope=\"module\")\ndef vcr_config():\n    \"\"\"VCR configuration for HTTP interaction recording.\"\"\"\n    return {\n        \"filter_headers\": [\"authorization\"],\n        \"filter_post_data_parameters\": [\"token\", \"api_key\"],\n        \"match_on\": [\"method\", \"scheme\", \"host\", \"port\", \"path\", \"query\"]\n    }\n</code></pre>"},{"location":"ai/#coverage-requirements-mvp","title":"Coverage Requirements (MVP)","text":"Component\nTarget Coverage\n\n\n\n\nCore Download\n30%\n\n\nCommand Parsing\n25%\n\n\nDiscord Events\n20%\n\n\nFile Management\n20%\n\n\n\n<p>Note: These are minimum targets for MVP. Higher coverage will be required for critical paths.\n</p>\n<p>"},{"location":"ai/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"ai/#error-handling-mvp","title":"Error Handling (MVP)","text":""},{"location":"ai/#core-error-types","title":"Core Error Types","text":"<pre><code>class BotErrorType(str, Enum):\n    \"\"\"Core error types for MVP.\"\"\"\n    DOWNLOAD_FAILED = \"download_failed\"\n    INVALID_URL = \"invalid_url\"\n    QUEUE_FULL = \"queue_full\"\n    FILE_TOO_LARGE = \"file_too_large\"\n    DISCORD_ERROR = \"discord_error\"\n</code></pre>"},{"location":"ai/#logging-configuration_1","title":"Logging Configuration","text":"<p>The application uses Loguru for structured logging with the following configuration:</p>\n<pre><code>import sys\nfrom loguru import logger\n\n# Remove default handler\nlogger.remove()\n\n# Configure Loguru\nlogger.configure(\n    handlers=[\n        {\n            \"sink\": sys.stdout,\n            \"format\": \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - &lt;level&gt;{message}&lt;/level&gt;\",\n            \"level\": \"INFO\",\n            \"colorize\": True,\n            \"backtrace\": True,\n            \"diagnose\": True,\n            \"enqueue\": True,\n        }\n    ]\n)\n\n# Add better exception handling\nlogger.add(\n    sys.stderr,\n    format=\"{time} | {level} | {message}\",\n    filter=lambda record: record[\"level\"].name == \"ERROR\",\n    level=\"ERROR\",\n    backtrace=True,\n    diagnose=True,\n)\n\n# Example logging usage\nlogger.info(\"Bot starting up...\")\nlogger.debug(\"Processing download request: {url}\", url=\"https://example.com\")\nlogger.warning(\"Rate limit approaching: {limit}\", limit=10)\nlogger.error(\"Download failed: {error}\", error=\"Network timeout\")\n</code></pre>"},{"location":"ai/#log-levels","title":"Log Levels","text":"<ul>\n<li>DEBUG: Detailed information for debugging</li>\n<li>INFO: General operational events</li>\n<li>WARNING: Unexpected but handled situations</li>\n<li>ERROR: Errors that need attention</li>\n<li>CRITICAL: System-level failures</li>\n</ul>"},{"location":"ai/#logging-guidelines","title":"Logging Guidelines","text":"<ol>\n<li>\n<p>Use structured logging with context:\n   <pre><code>logger.info(\"Download started\", url=url, user_id=user_id)\n</code></pre></p>\n</li>\n<li>\n<p>Include relevant context in errors:\n   <pre><code>try:\n    await download_file(url)\nexcept Exception as e:\n    logger.exception(f\"Download failed: {str(e)}\", url=url)\n</code></pre></p>\n</li>\n<li>\n<p>Use appropriate log levels:</p>\n</li>\n<li>DEBUG: Detailed flow information</li>\n<li>INFO: Normal operations</li>\n<li>WARNING: Potential issues</li>\n<li>ERROR: Operation failures</li>\n<li>\n<p>CRITICAL: System failures</p>\n</li>\n<li>\n<p>Log sensitive information appropriately:\n   <pre><code>logger.info(\n    \"Processing user request\",\n    user_id=user.id,  # OK to log\n    # Never log tokens or passwords\n)\n</code></pre></p>\n</li>\n</ol>"},{"location":"ai/#performance-metrics-mvp","title":"Performance Metrics (MVP)","text":""},{"location":"ai/#key-metrics","title":"Key Metrics","text":"<ol>\n<li>\n<p>Essential Metrics:\n   <pre><code>class CoreMetrics:\n    \"\"\"Essential performance metrics.\"\"\"\n    download_count: int = 0\n    failed_downloads: int = 0\n    queue_size: int = 0\n    active_downloads: int = 0\n</code></pre></p>\n</li>\n<li>\n<p>Basic Monitoring:</p>\n</li>\n<li>Queue length tracking</li>\n<li>Download success/failure rates</li>\n<li>Basic response time logging</li>\n</ol>\n<p>Nice to Have (Post-MVP):\n- Detailed performance analytics\n- Real-time monitoring dashboard\n- Resource usage tracking\n- Per-guild metrics</p>"},{"location":"ai/#api-documentation-mvp","title":"API Documentation (MVP)","text":""},{"location":"ai/#documentation-approach","title":"Documentation Approach","text":"<ol>\n<li>Code Documentation:</li>\n<li>Google-style docstrings</li>\n<li>Type hints for all functions</li>\n<li>\n<p>Basic README with setup instructions</p>\n</li>\n<li>\n<p>Command Documentation:</p>\n</li>\n<li>Help command with usage examples</li>\n<li>Basic command parameter descriptions</li>\n<li>Error message documentation</li>\n</ol>\n<p>Example Docstring Format:\n<pre><code>def download_media(url: str, user_id: int) -&gt; DownloadResult:\n    \"\"\"Download media from supported platforms.\n\n    Args:\n        url: The URL to download from (Twitter or Reddit)\n        user_id: Discord user ID requesting the download\n\n    Returns:\n        DownloadResult containing file path and metadata\n\n    Raises:\n        DownloadError: If download fails or URL is invalid\n        QueueFullError: If download queue is full\n    \"\"\"\n</code></pre></p>\n<p>Nice to Have (Post-MVP):\n- Auto-generated API documentation\n- Interactive command examples\n- Detailed troubleshooting guides\n- API versioning documentation</p>"},{"location":"ai/#deployment-strategy-mvp","title":"Deployment Strategy (MVP)","text":""},{"location":"ai/#initial-deployment","title":"Initial Deployment","text":"<ol>\n<li>Requirements:</li>\n<li>Python 3.11+</li>\n<li>Discord Bot Token</li>\n<li>Basic environment variables</li>\n<li>\n<p>Storage directory setup</p>\n</li>\n<li>\n<p>Deployment Steps:\n   <pre><code># Setup\nuv venv\nsource .venv/bin/activate\nuv sync --dev\n\n# Run\nuv run python -m boss_bot\n</code></pre></p>\n</li>\n<li>\n<p>Environment Variables:\n   <pre><code># Required for MVP\nDISCORD_TOKEN=required                # Discord bot authentication token\nDISCORD_CLIENT_ID=required           # Discord application client ID\nDISCORD_SERVER_ID=required           # Target Discord server ID\nDISCORD_ADMIN_USER_ID=required       # Admin user ID for bot management\nDISCORD_ADMIN_USER_INVITED=optional  # Track if admin user is invited\n\n# Debug and Development (Optional for MVP)\nBETTER_EXCEPTIONS=1                  # Enhanced exception formatting\nDEBUG_AIDER=0                        # Additional debug information\nLOCAL_TEST_DEBUG=0                   # Local testing mode\nLOCAL_TEST_ENABLE_EVALS=0           # Enable evaluation tests\nPYTHON_DEBUG=0                       # Python debug mode\nPYTHONASYNCIODEBUG=0                # AsyncIO debug mode\n\n# Monitoring and Error Tracking (Optional for MVP)\nENABLE_SENTRY=0                      # Enable Sentry error tracking\nSENTRY_DSN=optional                  # Sentry DSN if enabled\n\n# Future Features (Post-MVP)\n## AI Integration\nENABLE_AI=0                          # Enable AI features\nANTHROPIC_API_KEY=optional          # Anthropic API access\nCOHERE_API_KEY=optional             # Cohere API access\nOPENAI_API_KEY=optional             # OpenAI API access\n\n## LangChain Integration\nLANGCHAIN_API_KEY=optional          # LangChain API access\nLANGCHAIN_DEBUG_LOGS=0              # LangChain debug logging\nLANGCHAIN_ENDPOINT=optional         # LangChain API endpoint\nLANGCHAIN_HUB_API_KEY=optional      # LangChain Hub access\nLANGCHAIN_HUB_API_URL=optional      # LangChain Hub URL\nLANGCHAIN_PROJECT=optional          # LangChain project name\nLANGCHAIN_TRACING_V2=0             # LangChain tracing\n\n## Vector Store\nENABLE_REDIS=0                      # Enable Redis vector store\nPINECONE_API_KEY=optional          # Pinecone API access\nPINECONE_ENV=optional              # Pinecone environment\nPINECONE_INDEX=optional            # Pinecone index name\n\n## Additional Services\nFIRECRAWL_API_KEY=optional         # Firecrawl API access\nTAVILY_API_KEY=optional            # Tavily API access\nUNSTRUCTURED_API_KEY=optional      # Unstructured API access\nUNSTRUCTURED_API_URL=optional      # Unstructured API endpoint\n</code></pre></p>\n</li>\n</ol>\n<p>Nice to Have (Post-MVP):\n- Docker deployment\n- CI/CD pipeline\n- Multiple environment support\n- Automated backups\n- Rolling updates\n</p>\n<p>"},{"location":"ai/#core-technical-decisions","title":"Core Technical Decisions","text":"<ol>\n<li>Test-Driven Development\n\nImplement strict TDD practices with comprehensive test coverage\n\n\n<li>Ensures code reliability</li>\n<li>Facilitates junior developer onboarding</li>\n<li>Prevents regression issues</li>\n<li>\n<p>Makes refactoring safer\n</p>\n</li>\n<li>\n<p>Queue Management\n\nImplement asynchronous download queue with priority system\n\n\n\n<li>Prevents server overload</li>\n<li>Provides fair resource allocation</li>\n<li>Enables premium user prioritization</li>\n<li>\n<p>Allows for graceful error handling\n</p>\n</li>\n<li>\n<p>Storage Architecture\n\nImplement hierarchical storage with temporary and organized sections\n\n\n\n<li>Enables efficient cleanup</li>\n<li>Provides clear organization</li>\n<li>Supports quota management</li>\n<li>\n<p>Facilitates error recovery\n</p>\n</li>\n<li>\n<p>Error Handling\n\nImplement comprehensive error handling with retry mechanisms\n\n\n\n<li>Improves user experience</li>\n<li>Handles network instability</li>\n<li>Provides clear error messages</li>\n<li>\n<p>Supports automatic recovery\n</p>\n</li>\n<li>\n<p>Monitoring System\n\nImplement Prometheus metrics with detailed logging\n\n\n\n<li>Enables performance tracking</li>\n<li>Facilitates debugging</li>\n<li>Provides usage analytics</li>\n<li>Supports capacity planning\n\n</li>\n\n<p>"},{"location":"ai/#core-acceptance-criteria","title":"Core Acceptance Criteria","text":"<ol>\n<li>Download Functionality</li>\n<li>Successfully downloads media from Twitter and Reddit</li>\n<li>Provides real-time progress updates</li>\n<li>Handles errors gracefully with clear messages</li>\n<li>\n<p>Respects Discord file size limits</p>\n</li>\n<li>\n<p>Queue Management</p>\n</li>\n<li>Maintains ordered download queue</li>\n<li>Shows accurate queue position and ETA</li>\n<li>Allows download cancellation</li>\n<li>\n<p>Handles concurrent downloads properly</p>\n</li>\n<li>\n<p>User Experience</p>\n</li>\n<li>Commands respond within 2 seconds</li>\n<li>Progress updates are clear and accurate</li>\n<li>Error messages are user-friendly</li>\n<li>\n<p>Help documentation is comprehensive</p>\n</li>\n<li>\n<p>System Stability</p>\n</li>\n<li>Maintains 99% uptime</li>\n<li>Recovers from errors automatically</li>\n<li>Handles Discord API outages gracefully</li>\n<li>Manages resources efficiently\n</li>\n</ol>\n<p>"},{"location":"ai/#future-enhancements","title":"Future Enhancements","text":"<ol>\n<li>RAG Integration</li>\n<li>LangChain and LangGraph integration</li>\n<li>Vector store setup with Redis</li>\n<li>Enhanced command set for AI features</li>\n<li>\n<p>Document indexing and retrieval</p>\n</li>\n<li>\n<p>Premium Features</p>\n</li>\n<li>Priority queue access</li>\n<li>Higher concurrent download limits</li>\n<li>Extended file retention</li>\n<li>\n<p>Custom download preferences</p>\n</li>\n<li>\n<p>Advanced Monitoring</p>\n</li>\n<li>Real-time dashboard</li>\n<li>Advanced analytics</li>\n<li>Performance optimization</li>\n<li>\n<p>Capacity planning tools</p>\n</li>\n<li>\n<p>Platform Extensions</p>\n</li>\n<li>Support for additional media platforms</li>\n<li>Custom media processing options</li>\n<li>Batch download capabilities</li>\n<li>Format conversion options\n</li>\n</ol>\n<p>"},{"location":"ai/#known-technical-debt","title":"Known Technical Debt","text":"<ol>\n<li>Testing Coverage</li>\n<li>Initial MVP coverage targets are minimal</li>\n<li>Some Discord events can't be fully tested</li>\n<li>Integration tests need expansion</li>\n<li>\n<p>Performance testing framework needed</p>\n</li>\n<li>\n<p>Error Handling</p>\n</li>\n<li>Basic retry mechanism needs enhancement</li>\n<li>More granular error categorization needed</li>\n<li>Better rate limit handling required</li>\n<li>\n<p>Extended logging for debugging</p>\n</li>\n<li>\n<p>Documentation</p>\n</li>\n<li>API documentation needs expansion</li>\n<li>More code examples needed</li>\n<li>Troubleshooting guide required</li>\n<li>Architecture documentation needed\n</li>\n</ol>"},{"location":"ai/#file-stories-epic-1-epic-md","title":".ai/stories/epic-1.epic.md","text":""},{"location":"ai/#epic-1-core-bot-infrastructure_1","title":"Epic-1: Core Bot Infrastructure","text":""},{"location":"ai/#status","title":"Status","text":"<p>In Progress</p>"},{"location":"ai/#description","title":"Description","text":"<p>This epic focuses on establishing the foundational infrastructure for the Boss-Bot Discord media download assistant, following the requirements specified in the PRD. It includes setting up the development environment, implementing the core bot framework, establishing error handling and logging systems, and creating a comprehensive testing infrastructure.</p>"},{"location":"ai/#goals_1","title":"Goals","text":"<ul>\n<li>Establish a solid development foundation with proper tooling and standards</li>\n<li>Implement core Discord bot functionality with proper event handling</li>\n<li>Set up comprehensive testing infrastructure with PRD-specified coverage targets</li>\n<li>Implement robust error handling and logging systems using loguru and better-exceptions</li>\n<li>Establish secure file management and storage infrastructure</li>\n<li>Configure comprehensive monitoring and metrics collection</li>\n</ul>"},{"location":"ai/#success-criteria","title":"Success Criteria","text":"<ol>\n<li>Development environment is fully configured and documented</li>\n<li>Python 3.12+ environment setup</li>\n<li>UV package management configured</li>\n<li>\u2705 Ruff linting and formatting setup via pre-commit</li>\n<li>\u2705 Pre-commit hooks installed and configured</li>\n<li>\n<p>CI/CD pipeline operational</p>\n</li>\n<li>\n<p>Discord bot connects and responds to basic commands</p>\n</li>\n<li>Command response time &lt;2s</li>\n<li>Proper error handling with better-exceptions</li>\n<li>Clear user feedback for all operations</li>\n<li>Type hints and docstrings for all code</li>\n<li>Maximum concurrent downloads: 5</li>\n<li>Maximum queue size: 50 items</li>\n<li>\n<p>Maximum file size: 50MB</p>\n</li>\n<li>\n<p>Test infrastructure achieves PRD coverage targets:</p>\n</li>\n<li>Core Download: 30%</li>\n<li>Command Parsing: 25%</li>\n<li>Discord Events: 20%</li>\n<li>File Management: 20%</li>\n<li>VCR/cassette setup for HTTP mocking</li>\n<li>\n<p>Comprehensive test patterns and examples</p>\n</li>\n<li>\n<p>Error handling and logging provide clear visibility</p>\n</li>\n<li>Loguru configured for structured logging</li>\n<li>Better-exceptions integration</li>\n<li>Clear error messages for users</li>\n<li>Proper error categorization and handling</li>\n<li>Log rotation policies implemented</li>\n<li>\n<p>Monitoring dashboard setup</p>\n</li>\n<li>\n<p>File management system meets requirements</p>\n</li>\n<li>Temporary storage structure implemented</li>\n<li>Cleanup policies enforced</li>\n<li>\u2705 Storage quotas configured and tested (85% coverage)<ul>\n<li>Byte and megabyte reporting</li>\n<li>File size limits (50MB)</li>\n<li>Concurrent download limits (5)</li>\n<li>Usage percentage tracking</li>\n</ul>\n</li>\n<li>\u2705 File validation implemented (57% coverage)<ul>\n<li>File type validation</li>\n<li>Name sanitization</li>\n<li>Security checks</li>\n</ul>\n</li>\n<li>\n<p>Storage usage monitoring active</p>\n</li>\n<li>\n<p>Security measures properly implemented</p>\n</li>\n<li>Environment variable security</li>\n<li>Discord permission scopes</li>\n<li>Rate limiting</li>\n<li>File validation</li>\n<li>Secure storage practices</li>\n</ol>"},{"location":"ai/#stories","title":"Stories","text":"<ol>\n<li>Project Initialization and Environment Setup</li>\n<li>Status: In Progress</li>\n<li>\u2705 File validation implemented with security checks</li>\n<li>\u2705 Storage quota management implemented with limits</li>\n<li>Set up project structure following PRD layout</li>\n<li>Configure UV for package management</li>\n<li>Set up Ruff with PRD-specified rules</li>\n<li>Configure pre-commit hooks</li>\n<li>Create comprehensive README</li>\n<li>Ensure all modules respect 120-line limit</li>\n<li>Set up CI/CD pipeline</li>\n<li>\n<p>Configure development workflow</p>\n</li>\n<li>\n<p>Test Infrastructure Setup</p>\n</li>\n<li>Status: Not Started</li>\n<li>Configure pytest with coverage targets from PRD</li>\n<li>Set up test fixtures and utilities</li>\n<li>Implement test categorization</li>\n<li>Configure coverage reporting</li>\n<li>Set up dpytest for Discord testing</li>\n<li>Configure VCR for HTTP mocking</li>\n<li>Create test patterns and examples</li>\n<li>\n<p>Set up test automation in CI</p>\n</li>\n<li>\n<p>Logging and Monitoring Setup</p>\n</li>\n<li>Status: Not Started</li>\n<li>Implement loguru with structured logging</li>\n<li>Configure better-exceptions</li>\n<li>Set up basic performance monitoring</li>\n<li>Implement response time tracking</li>\n<li>Configure log rotation and management</li>\n<li>Set up monitoring dashboard</li>\n<li>Configure metrics collection</li>\n<li>Implement storage usage tracking</li>\n<li>\n<p>Set up resource monitoring (CPU, Memory)</p>\n</li>\n<li>\n<p>Basic Discord Bot Setup</p>\n</li>\n<li>Status: Not Started</li>\n<li>Create Discord application and bot</li>\n<li>Implement bot client with required intents</li>\n<li>Set up environment configuration</li>\n<li>Create connection handling</li>\n<li>Implement command framework</li>\n<li>Add health check command</li>\n<li>Ensure &lt;2s response time</li>\n<li>Configure rate limiting</li>\n<li>\n<p>Set up permission scopes</p>\n</li>\n<li>\n<p>Security and Permission Setup</p>\n</li>\n<li>Status: Not Started</li>\n<li>Implement Discord permission scopes</li>\n<li>Set up environment variable security</li>\n<li>Configure rate limiting</li>\n<li>Implement file validation</li>\n<li>Set up secure storage practices</li>\n<li>Configure access controls</li>\n<li>Implement security monitoring</li>\n<li>\n<p>Set up security logging</p>\n</li>\n<li>\n<p>File Management Infrastructure</p>\n</li>\n<li>Status: Not Started</li>\n<li>Implement temporary storage structure</li>\n<li>Set up file cleanup policies</li>\n<li>Configure storage quotas</li>\n<li>Implement file organization strategy</li>\n<li>Set up monitoring for storage usage</li>\n<li>Configure backup policies</li>\n<li>Implement file validation</li>\n<li>Set up cleanup automation</li>\n</ol>"},{"location":"ai/#technical-requirements","title":"Technical Requirements","text":"<ul>\n<li>Python 3.12+</li>\n<li>UV for package management</li>\n<li>Ruff for code quality</li>\n<li>Pytest for testing</li>\n<li>Discord.py v2.5.2+</li>\n<li>Loguru for logging</li>\n<li>Better-exceptions for error handling</li>\n<li>Gallery-dl for media downloads</li>\n<li>Maximum concurrent downloads: 5</li>\n<li>Maximum queue size: 50 items</li>\n<li>Maximum file size: 50MB</li>\n<li>Storage cleanup policies</li>\n<li>Rate limiting configuration</li>\n<li>Environment variable security</li>\n</ul>"},{"location":"ai/#monitoring-requirements","title":"Monitoring Requirements","text":"<ul>\n<li>Storage usage tracking</li>\n<li>Queue length monitoring</li>\n<li>Download speed metrics</li>\n<li>Error rate tracking</li>\n<li>Response time monitoring</li>\n<li>Resource usage monitoring (CPU, Memory)</li>\n<li>Security event monitoring</li>\n<li>API latency tracking</li>\n<li>File system metrics</li>\n<li>Queue performance metrics</li>\n</ul>"},{"location":"ai/#dependencies","title":"Dependencies","text":"<ul>\n<li>None (This is the first epic)</li>\n</ul>"},{"location":"ai/#risks","title":"Risks","text":"<ul>\n<li>Discord API changes could affect implementation</li>\n<li>Test coverage targets may be challenging for Discord events</li>\n<li>Integration testing complexity with Discord API</li>\n<li>Response time requirements may be challenging under load</li>\n<li>Storage management complexity</li>\n<li>Security configuration complexity</li>\n<li>Rate limiting edge cases</li>\n<li>File system performance under load</li>\n</ul>"},{"location":"ai/#notes_2","title":"Notes","text":"<ul>\n<li>Follow TDD practices strictly</li>\n<li>Maintain clear documentation</li>\n<li>Ensure all code has proper type hints and docstrings</li>\n<li>Keep modules under 120 lines</li>\n<li>Use async/await patterns consistently</li>\n<li>Implement proper error handling with better-exceptions</li>\n<li>Use loguru for structured logging</li>\n<li>Follow security best practices</li>\n<li>Maintain comprehensive monitoring</li>\n<li>Regular security reviews</li>\n</ul>"},{"location":"ai/#timeline","title":"Timeline","text":"<p>Estimated completion: 2024-05-22</p>"},{"location":"ai/#progress-tracking","title":"Progress Tracking","text":"<ul>\n<li>[\ud83d\udea7] Story 1: Project Initialization</li>\n<li>\u2705 File validation (57% coverage)</li>\n<li>\u2705 Storage quotas (96% coverage)</li>\n<li>\u2705 Storage directory structure (100% coverage)</li>\n<li>\u2705 Development environment (Ruff + pre-commit)</li>\n<li>\ud83d\udea7 Project structure</li>\n<li>\u274c CI/CD pipeline</li>\n<li> Story 2: Test Infrastructure</li>\n<li> Story 3: Logging Setup</li>\n<li> Story 4: Discord Bot Setup</li>\n<li> Story 5: Security Setup</li>\n<li> Story 6: File Management</li>\n</ul>"},{"location":"ai/#related-documents","title":"Related Documents","text":"<ul>\n<li>PRD: .ai/prd.md</li>\n<li>Architecture: .ai/arch.md</li>\n<li>Current Story: .ai/story-1.story.md</li>\n</ul>"},{"location":"ai/#implementation-status","title":"Implementation Status","text":""},{"location":"ai/#completed-features","title":"Completed Features","text":"<ul>\n<li>\u2705 Development environment configuration</li>\n<li>Ruff linting and formatting via pre-commit</li>\n<li>Comprehensive pre-commit hook setup</li>\n<li>Multiple git hooks (pre-commit, commit-msg, pre-push)</li>\n<li>Project validation and security scanning</li>\n<li>\u2705 Storage quota management system</li>\n<li>Implemented 50MB file size limit</li>\n<li>Added 5 concurrent downloads limit</li>\n<li>Created comprehensive test suite (96% coverage)</li>\n<li>Added detailed status reporting with byte/MB metrics</li>\n<li>\u2705 File validation system</li>\n<li>Added file type validation</li>\n<li>Implemented filename sanitization</li>\n<li>Added path traversal detection</li>\n<li>Created test suite (57% coverage)</li>\n<li>\u2705 Temporary storage structure</li>\n<li>Created downloads directory hierarchy</li>\n<li>Implemented idempotent directory creation</li>\n<li>Added file preservation logic</li>\n<li>Created comprehensive test suite (100% coverage)</li>\n</ul>"},{"location":"ai/#in-progress","title":"In Progress","text":"<ul>\n<li>\ud83d\udea7 Project structure setup</li>\n<li>\ud83d\udea7 Documentation setup</li>\n<li>\u274c CI/CD pipeline</li>\n</ul>"},{"location":"ai/#deferred-to-phase-2","title":"Deferred to Phase 2","text":"<ul>\n<li>File cleanup policies</li>\n<li>Enhanced error handling</li>\n<li>Monitoring dashboards</li>\n<li>Backup configuration</li>\n<li>Storage monitoring</li>\n</ul>"},{"location":"ai/#deferred-to-phase-3","title":"Deferred to Phase 3","text":"<ul>\n<li>Storage security configuration</li>\n<li>Rate limiting</li>\n</ul>"},{"location":"ai/#next-steps","title":"Next Steps","text":"<ol>\n<li>Implement core download functionality</li>\n<li>Set up basic queue management</li>\n<li>Add essential error handling</li>\n<li>Implement basic metrics/monitoring</li>\n</ol>"},{"location":"ai/#file-stories-story-1-story-md","title":".ai/stories/story-1.story.md","text":"<p>\n<p>"},{"location":"ai/#epic-1-story-1","title":"Epic-1 - Story-1","text":""},{"location":"ai/#project-initialization-and-environment-setup","title":"Project Initialization and Environment Setup","text":"<p>As a developer\nI want to set up the initial project structure and development environment with security and monitoring foundations\nso that we have a solid, secure, and observable foundation for building the Boss-Bot Discord media download assistant\n</p>\n<p>"},{"location":"ai/#status_1","title":"Status","text":"<p>In Progress\n</p>\n<p>\n- Status\n- Context\n- Estimation\n- Tasks\n- Deferred Tasks\n- Constraints\n- Data Models / Schema\n- Structure\n- Diagrams\n- Dev Notes\n- Chat Command Log\n- Implementation Evidence\n</p>\n<p>"},{"location":"ai/#context","title":"Context","text":"<p>This is the first story of Epic-1 (Core Bot Infrastructure) which sets up the foundational project structure and development environment. This story is critical as it establishes:</p>\n<ul>\n<li>Basic project structure following the defined layout \u2705</li>\n<li>Development environment configuration \ud83d\udea7</li>\n<li>Code quality tools and standards \u2705</li>\n<li>Initial test infrastructure \ud83d\udea7</li>\n<li>Documentation foundation \u274c</li>\n<li>Security baseline \ud83d\udea7</li>\n<li>Monitoring setup \u2705</li>\n<li>CI/CD pipeline foundation \u274c</li>\n</ul>\n<p>\nKey technical decisions from the PRD and architecture documents:\n- Python 3.12 as the primary development language \u2705\n- UV for package management \u2705\n- Ruff for code quality \u2705\n- Pytest for testing infrastructure \u2705\n- Comprehensive test coverage targets for MVP \u2705 (Current: 56.16%, exceeding MVP targets)\n- Loguru for logging \u2705\n- Better-exceptions for error handling \u2705\n- Security-first approach with proper environment variable handling \ud83d\udea7\n- Monitoring and metrics collection from the start \u2705\n\n</p>\n<p>"},{"location":"ai/#estimation","title":"Estimation","text":"<p>Story Points: 5 (5 days human development = 50 minutes AI development)\nIncreased from 3 to 5 points due to additional security, monitoring, and CI/CD requirements.\n</p>\n<p>"},{"location":"ai/#tasks","title":"Tasks","text":"<p>\n1. - [\ud83d\udea7] Initialize Python Project\n   1. - [\u2705] Create project structure following PRD layout\n   2. - [\u2705] Set up pyproject.toml with initial dependencies\n   3. - [\u2705] Configure UV for package management\n      * \u2705 UV v0.6.13 installed and configured\n      * \u2705 Dependencies properly managed in pyproject.toml\n      * \u2705 Dev dependencies correctly configured\n      * \u2705 UV workspace setup complete\n   4. - [\u2705] Create initial README.md with setup instructions\n   5. - [\u2705] Set up secure environment variable handling\n      * \u2705 Implemented comprehensive pydantic-settings configuration\n      * \u2705 Added secure secret handling with SecretStr\n      * \u2705 Added validation for all environment variables\n      * \u2705 Configured .env and secrets directory support\n      * \u2705 Added type safety and validation for all settings\n   6. - [\ud83d\udea7] Configure dependency security scanning\n   7. - [\ud83d\udea7] Set up initial health checks\n      * \u2705 Basic health check implementation (80% coverage)\n      * \u274c Periodic health check failing\n      * \ud83d\udea7 Component health checks need refinement\n   8. - [\u2705] Configure storage directory structure\n</p>\n<p>\n2. - [\ud83d\udea7] Configure Development Environment\n   1. - [\u2705] Set up Ruff for linting and formatting\n      * \u2705 Basic configuration in pyproject.toml\n      * \u2705 Integrated with pre-commit hooks\n      * \u2705 Configured with two hooks: ruff (linting) and ruff-format (formatting)\n      * \u2705 Set to run before each commit with --fix and --exit-non-zero-on-fix\n      * \u2705 Properly ordered before other formatting tools\n   2. - [\u2705] Configure pre-commit hooks\n      * \u2705 Added validate-pyproject for pyproject.toml validation\n      * \u2705 Added gitleaks for secret scanning\n      * \u2705 Added ruff and ruff-format hooks\n      * \u2705 Added additional code quality hooks\n      * \u2705 Configured to run on pre-commit, commit-msg, and pre-push\n   3. - [\u2705] Set up VSCode settings\n   4. - [\u2705] Create .env.sample with required variables\n      * \u2705 Added all required environment variables\n      * \u2705 Added descriptive comments and sections\n      * \u2705 Included default values from env.py\n      * \u2705 Added placeholders for sensitive values\n   5. - [\ud83d\udea7] Set up development secrets management\n   6. - [\ud83d\udea7] Configure development security checks\n   7. - [\u2705] Set up detailed VSCode configuration\n   8. - [\u274c] Configure dependency review automation\n</p>\n<p>\n3. - [\ud83d\udea7] Set up Test Infrastructure\n   1. - [\u2705] Configure pytest with required plugins\n   2. - [\u2705] Set up test directory structure\n   3. - [\u2705] Create initial test fixtures\n   4. - [\u2705] Configure coverage reporting\n   5. - [\u2705] Set up VCR for HTTP mocking\n   6. - [\ud83d\udea7] Configure test security scanning\n   7. - [\u2705] Set up async test support\n   8. - [\u2705] Configure parallel testing\n   9. - [\ud83d\udea7] Set up Discord.py testing utilities\n      * \u274c Bot test environment validation failing\n      * \u274c Mock configuration issues in bot tests\n      * \ud83d\udea7 Help command tests need fixes\n</p>\n<p>\n4. - [\u274c] Initialize Documentation\n   1. - [\u274c] Set up MkDocs with required extensions\n   2. - [\u274c] Create initial documentation structure\n   3. - [\u274c] Document setup process\n   4. - [\u274c] Add development guidelines\n   5. - [\u274c] Add security guidelines\n   6. - [\u274c] Document monitoring setup\n   7. - [\u274c] Create troubleshooting guide\n   8. - [\u274c] Create code style guide\n   9. - [\u274c] Create testing guide\n   10. - [\u274c] Create storage management guide\n</p>\n<p>\n5. - [\u274c] Set up CI/CD Pipeline\n   1. - [\u274c] Configure GitHub Actions workflow\n   2. - [\u274c] Set up dependency scanning\n   3. - [\u274c] Configure automated testing\n   4. - [\u274c] Set up code quality checks\n   5. - [\u274c] Configure security scanning\n   6. - [\u274c] Set up documentation building\n   7. - [\u274c] Configure automated deployments\n   8. - [\u274c] Set up CodeQL analysis\n   9. - [\u274c] Configure dependency review\n   10. - [\u274c] Set up release drafting\n</p>\n<p>\n6. - [\ud83d\udea7] Configure Monitoring Foundation\n   1. - [\u2705] Set up loguru configuration\n   2. - [\u2705] Configure better-exceptions\n   3. - [\u2705] Set up basic metrics collection\n      * \u2705 Core metrics implemented\n      * \u274c Histogram label issues need fixing\n   4. - [\u2705] Configure log rotation\n   5. - [\u2705] Set up monitoring dashboard structure\n   6. - [\u2705] Configure resource usage monitoring\n   7. - [\u2705] Set up security event logging\n   8. - [\ud83d\udea7] Set up health check endpoints\n   9. - [\u2705] Set up storage monitoring\n   10. - [\u2705] Set up performance profiling\n</p>\n<p>\n7. - [\u2705] Initialize Storage Structure\n   1. - [\u2705] Set up temporary storage directory structure\n      * \u2705 Created main downloads directory\n      * \u2705 Created temp storage directory\n      * \u2705 Created completed downloads directory\n      * \u2705 Created failed downloads directory\n      * \u2705 Added comprehensive tests (100% coverage)\n      * \u2705 Implemented idempotent creation\n      * \u2705 Added file preservation checks\n   2. - [\u2705] Add file validation checks to QuotaManager\n      * \u2705 File type validation - Implemented in FileValidator with ALLOWED_EXTENSIONS\n      * \u2705 File name sanitization - Implemented with sanitize_filename method\n      * \u2705 Basic security checks - Implemented path traversal detection and forbidden character validation\n      * \u2705 Test coverage: 57% for validation.py\n   3. - [ ] Configure cleanup policies (Deferred to Phase 2)\n   4. - [\u2705] Set up storage quota management\n      * \u2705 Basic quota tracking with byte and megabyte reporting\n      * \u2705 File size limits (50MB per file)\n      * \u2705 Concurrent download limits (5 max)\n      * \u2705 Quota status reporting with usage percentage\n      * \u2705 Test coverage: 96% for quotas.py\n   5. - [ ] Configure backup locations (Deferred to Phase 2)\n   6. - [ ] Set up storage monitoring (Deferred to Phase 2)\n   7. - [ ] Configure storage security (Deferred to Phase 3)\n\n</p>\n<p>"},{"location":"ai/#deferred-tasks","title":"Deferred Tasks","text":"<p>The following tasks have been deferred to future phases:</p>\n<ol>\n<li>Storage Management (Task Group 7)</li>\n<li>Configure cleanup policies (Phase 2)</li>\n<li>Configure backup locations (Phase 2)</li>\n<li>Set up storage monitoring (Phase 2)</li>\n<li>Configure storage security (Phase 3)</li>\n</ol>\n<p>\nRationale for Deferral:\n- These features belong to later phases per phased development plan\n- Not critical for MVP functionality\n- Current focus is on core bot infrastructure and basic file validation\n\n</p>\n<p>"},{"location":"ai/#constraints","title":"Constraints","text":"<ul>\n<li>Python 3.12+ required</li>\n<li>Maximum module size: 120 lines</li>\n<li>Test coverage targets (MVP):</li>\n<li>Core Download: 30%</li>\n<li>Command Parsing: 25%</li>\n<li>Discord Events: 20%</li>\n<li>File Management: 20%</li>\n<li>Maximum concurrent downloads: 5</li>\n<li>Maximum queue size: 50 items</li>\n<li>Maximum file size: 50MB</li>\n<li>Secure environment variable handling required</li>\n<li>Monitoring metrics must be collected from start\n</li>\n</ul>\n<p>"},{"location":"ai/#data-models-schema","title":"Data Models / Schema","text":"<p><pre><code># pyproject.toml structure\n[project]\nname = \"boss-bot\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"discord-py&gt;=2.5.2\",\n    \"gallery-dl&gt;=1.29.3\",\n    \"loguru&gt;=0.7.3\",\n    \"pydantic-settings&gt;=2.8.1\",\n    \"better-exceptions&gt;=0.3.3\",\n    \"prometheus-client&gt;=0.17.1\",\n    \"pytest-recording&gt;=0.13.0\",\n    \"pytest-cov&gt;=4.1.0\",\n    \"mkdocs-material&gt;=9.4.0\",\n    \"pytest-asyncio&gt;=0.23.0\",\n    \"pytest-mock&gt;=3.12.0\",\n    \"pytest-timeout&gt;=2.2.0\",\n    \"pytest-xdist&gt;=3.5.0\",\n    \"respx&gt;=0.20.2\",\n    \"dpytest&gt;=0.7.0\",\n]\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\naddopts = \"\"\"\n    --cov=boss_bot\n    --cov-report=xml\n    --cov-report=term-missing\n    --asyncio-mode=auto\n    --numprocesses=auto\n    --dist=loadfile\n\"\"\"\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py312\"\nselect = [\n    \"E\",   # pycodestyle errors\n    \"W\",   # pycodestyle warnings\n    \"F\",   # pyflakes\n    \"I\",   # isort\n    \"C\",   # flake8-comprehensions\n    \"B\",   # flake8-bugbear\n]\n</code></pre>\n</p>\n<p>"},{"location":"ai/#structure","title":"Structure","text":"<p>Following the project structure from the PRD:</p>\n<p><pre><code>boss-bot/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u251c\u2500\u2500 ci.yml\n\u2502       \u251c\u2500\u2500 dependency-review.yml\n\u2502       \u251c\u2500\u2500 codeql-analysis.yml\n\u2502       \u251c\u2500\u2500 security-audit.yml\n\u2502       \u251c\u2500\u2500 release-drafter.yml\n\u2502       \u2514\u2500\u2500 security.yml\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 boss_bot/\n\u2502   \u2502   \u251c\u2500\u2500 bot/\n\u2502   \u2502   \u251c\u2500\u2500 commands/\n\u2502   \u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u251c\u2500\u2500 downloaders/\n\u2502   \u2502   \u251c\u2500\u2500 schemas/\n\u2502   \u2502   \u251c\u2500\u2500 monitoring/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 metrics.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 health.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 logging.py\n\u2502   \u2502   \u251c\u2500\u2500 storage/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cleanup.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 quotas.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 validation.py\n\u2502   \u2502   \u2514\u2500\u2500 utils/\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u251c\u2500\u2500 test_bot/\n\u2502   \u251c\u2500\u2500 test_commands/\n\u2502   \u251c\u2500\u2500 test_downloaders/\n\u2502   \u251c\u2500\u2500 test_storage/\n\u2502   \u2514\u2500\u2500 cassettes/\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 development/\n\u2502   \u2502   \u251c\u2500\u2500 code_style.md\n\u2502   \u2502   \u251c\u2500\u2500 testing_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 security_practices.md\n\u2502   \u2502   \u251c\u2500\u2500 monitoring_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 storage_management.md\n\u2502   \u2502   \u2514\u2500\u2500 deployment_guide.md\n\u2502   \u251c\u2500\u2500 setup.md\n\u2502   \u251c\u2500\u2500 security.md\n\u2502   \u251c\u2500\u2500 monitoring.md\n\u2502   \u2514\u2500\u2500 troubleshooting.md\n\u251c\u2500\u2500 scripts/\n\u251c\u2500\u2500 .vscode/\n\u2502   \u251c\u2500\u2500 settings.json\n\u2502   \u251c\u2500\u2500 launch.json\n\u2502   \u2514\u2500\u2500 extensions.json\n\u251c\u2500\u2500 .env.sample\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 README.md\n</code></pre>\n</p>\n<p>"},{"location":"ai/#diagrams","title":"Diagrams","text":"<p><pre><code>graph TD\n    A[Initialize Project] --&gt; B[Configure Dev Environment]\n    B --&gt; C[Set up Testing]\n    C --&gt; D[Initialize Docs]\n    D --&gt; E[Set up CI/CD]\n    E --&gt; F[Configure Monitoring]\n\n    B --&gt; B1[Ruff]\n    B --&gt; B2[Pre-commit]\n    B --&gt; B3[VSCode]\n    B --&gt; B4[Security]\n\n    C --&gt; C1[Pytest]\n    C --&gt; C2[Coverage]\n    C --&gt; C3[Fixtures]\n    C --&gt; C4[VCR]\n\n    D --&gt; D1[MkDocs]\n    D --&gt; D2[Guidelines]\n    D --&gt; D3[Security Docs]\n\n    E --&gt; E1[GitHub Actions]\n    E --&gt; E2[Security Scans]\n    E --&gt; E3[Automated Tests]\n\n    F --&gt; F1[Loguru]\n    F --&gt; F2[Metrics]\n    F --&gt; F3[Monitoring]\n</code></pre>\n</p>\n<p>"},{"location":"ai/#dev-notes","title":"Dev Notes","text":"<ul>\n<li>Ensure all dependencies are pinned to specific versions for reproducibility</li>\n<li>Configure Ruff to enforce type hints and docstrings</li>\n<li>Set up pre-commit hooks to run before each commit</li>\n<li>Create comprehensive test fixtures for Discord bot testing</li>\n<li>Document all setup steps clearly for other developers</li>\n<li>Implement security best practices from the start</li>\n<li>Set up monitoring and metrics collection early</li>\n<li>Set up CI/CD pipeline includes security checks</li>\n<li>Configure proper secret management</li>\n<li>Set up automated dependency updates with security checks</li>\n<li>\u2705 Implemented file validation with comprehensive tests (coverage: 57% for validation.py)</li>\n<li>Added support for common media file types</li>\n<li>Implemented secure filename sanitization</li>\n<li>Added path traversal detection</li>\n<li>Created thorough test suite with edge cases</li>\n<li>\u2705 Implemented storage quota management (coverage: 96% for quotas.py)</li>\n<li>Added quota tracking with byte/MB reporting</li>\n<li>Implemented file size and concurrent download limits</li>\n<li>Created comprehensive test suite</li>\n<li>Added detailed status reporting</li>\n<li>\u2705 Implemented storage directory structure (100% test coverage)</li>\n<li>Created required directory hierarchy</li>\n<li>Added idempotent creation</li>\n<li>Ensured file preservation</li>\n<li>Added comprehensive test suite\n</li>\n</ul>\n<p>"},{"location":"ai/#chat-command-log","title":"Chat Command Log","text":"<p>No commands executed yet - initial story creation.\n</p>\n<p>"},{"location":"ai/#implementation-evidence","title":"Implementation Evidence","text":"<p>"},{"location":"ai/#test-coverage-status","title":"Test Coverage Status","text":"<ol>\n<li>Overall Coverage: 56.16% (Exceeding MVP targets)</li>\n<li>Key Component Coverage:</li>\n<li>Storage/Quotas: 96% \u2705</li>\n<li>Storage/Validation: 57% \u2705</li>\n<li>Core/Environment: 94% \u2705</li>\n<li>Core/Queue: 94% \u2705</li>\n<li>Monitoring/Health: 80% \u2705</li>\n<li>Monitoring/Logging: 100% \u2705</li>\n<li>Monitoring/Metrics: 100% \u2705</li>\n<li>Bot/Help: 85% \u2705</li>\n<li>Bot/Client: 32% \u2705 (Meets MVP target)</li>\n<li>Bot/Cogs: ~30% \u2705 (Meets MVP target)\n</li>\n</ol>\n<p>"},{"location":"ai/#test-results-summary","title":"Test Results Summary","text":"<ul>\n<li>Total Tests: 123</li>\n<li>Passed: 78 \u2705</li>\n<li>Failed: 12 \u274c</li>\n<li>Errors: 33 \u274c</li>\n<li>Key Issues:</li>\n<li>Discord environment settings validation errors</li>\n<li>Metrics histogram label issues</li>\n<li>Health check periodic testing</li>\n<li>Bot help command formatting</li>\n<li>Mock configuration issues in bot tests\n</li>\n</ul>\n<p>"},{"location":"ai/#next-priority-tasks","title":"Next Priority Tasks","text":"<ol>\n<li>Fix environment validation errors in bot tests</li>\n<li>Address metrics histogram label issues</li>\n<li>Fix health check periodic testing</li>\n<li>Resolve bot help command formatting</li>\n<li>Fix mock configuration in bot tests\n</li>\n</ol>\n<p>"},{"location":"ai/#environment-configuration_1","title":"Environment Configuration","text":"<ol>\n<li>Environment Settings:</li>\n<li>Location: src/boss_bot/core/env.py</li>\n<li>\n<p>Key Features:</p>\n<ul>\n<li>Comprehensive pydantic-settings implementation</li>\n<li>Secure secret handling with SecretStr</li>\n<li>Validation for all environment variables</li>\n<li>Support for .env and secrets directory</li>\n<li>Type safety and validation</li>\n<li>Environment-specific configuration</li>\n</ul>\n</li>\n<li>\n<p>Package Management:</p>\n</li>\n<li>Location: pyproject.toml, uv.lock</li>\n<li>\n<p>Features:</p>\n<ul>\n<li>UV v0.6.13 configuration</li>\n<li>Properly managed dependencies</li>\n<li>Dev dependencies setup</li>\n<li>Workspace configuration</li>\n<li>Version pinning</li>\n</ul>\n</li>\n<li>\n<p>Environment Templates:</p>\n</li>\n<li>Location: .env.sample</li>\n<li>Features:<ul>\n<li>Complete environment variable listing</li>\n<li>Organized sections with comments</li>\n<li>Default values from env.py</li>\n<li>Secure placeholders for API keys</li>\n<li>Development-focused defaults\n</li>\n</ul>\n</li>\n</ol>\n<p>"},{"location":"ai/#storage-management-implementation","title":"Storage Management Implementation","text":"<ol>\n<li>Storage Quota System:</li>\n<li>Location: src/boss_bot/storage/quotas.py</li>\n<li>Test Coverage: 96%</li>\n<li>\n<p>Key Features:</p>\n<ul>\n<li>File size limits (50MB)</li>\n<li>Concurrent download tracking</li>\n<li>Usage reporting</li>\n<li>Comprehensive test suite</li>\n</ul>\n</li>\n<li>\n<p>File Validation:</p>\n</li>\n<li>Location: src/boss_bot/storage/validation.py</li>\n<li>Test Coverage: 57%</li>\n<li>\n<p>Key Features:</p>\n<ul>\n<li>File type validation</li>\n<li>Name sanitization</li>\n<li>Security checks</li>\n<li>Path traversal prevention</li>\n</ul>\n</li>\n<li>\n<p>Storage Structure:</p>\n</li>\n<li>Location: src/boss_bot/storage/</li>\n<li>Test Coverage: 100% for directory management</li>\n<li>Features:<ul>\n<li>Organized directory hierarchy</li>\n<li>Idempotent creation</li>\n<li>File preservation</li>\n<li>Comprehensive tests\n</li>\n</ul>\n</li>\n</ol>\n<p>"},{"location":"ai/#development-environment_2","title":"Development Environment","text":"<ol>\n<li>Pre-commit Configuration:</li>\n<li>Location: .pre-commit-config.yaml</li>\n<li>\n<p>Key Features:</p>\n<ul>\n<li>Ruff integration with two hooks:</li>\n<li>ruff: Linting with --fix and --exit-non-zero-on-fix</li>\n<li>ruff-format: Formatting with proper configuration</li>\n<li>Comprehensive hook setup for code quality</li>\n<li>Multiple git hooks configured (pre-commit, commit-msg, pre-push)</li>\n<li>Proper hook ordering for optimal formatting</li>\n<li>Validation hooks for project configuration</li>\n</ul>\n</li>\n<li>\n<p>Environment Settings:</p>\n</li>\n<li>Location: src/boss_bot/core/env.py</li>\n<li>Key Features:<ul>\n<li>Comprehensive pydantic-settings implementation</li>\n<li>Secure secret handling with SecretStr</li>\n<li>Validation for all environment variables</li>\n<li>Support for .env and secrets directory</li>\n<li>Type safety and validation</li>\n<li>Environment-specific configuration\n\n</li>\n</ul>\n</li>\n</ol>\n<p></p>"},{"location":"cli/","title":"BossBot CLI (bossctl) Documentation","text":"<p>The BossBot CLI tool <code>bossctl</code> provides a comprehensive command-line interface for managing and using the BossBot Discord bot functionality. It supports media downloads from various platforms using the experimental strategy pattern with API-direct and CLI-based modes.</p>"},{"location":"cli/#installation-usage","title":"Installation &amp; Usage","text":"<pre><code># Run any bossctl command\nbossctl &lt;command&gt; [options]\n\n# Or using uv (recommended during development)\nuv run python -m boss_bot.cli.main &lt;command&gt; [options]\n</code></pre>"},{"location":"cli/#main-commands","title":"Main Commands","text":""},{"location":"cli/#version-information-commands","title":"Version &amp; Information Commands","text":""},{"location":"cli/#version","title":"<code>version</code>","text":"<p>Display the current version of boss_bot.</p> <pre><code>bossctl version\n</code></pre> <p>Output: <code>boss_bot version: X.X.X</code></p>"},{"location":"cli/#deps","title":"<code>deps</code>","text":"<p>Show dependency versions for all major components.</p> <pre><code>bossctl deps\n</code></pre> <p>Output: Displays versions for: - boss_bot - langchain packages (core, community, openai, text_splitters) - chromadb - langsmith - pydantic packages - ruff</p>"},{"location":"cli/#about","title":"<code>about</code>","text":"<p>Show basic information about BossBot CLI.</p> <pre><code>bossctl about\n</code></pre>"},{"location":"cli/#configuration-commands","title":"Configuration Commands","text":""},{"location":"cli/#config","title":"<code>config</code>","text":"<p>Show comprehensive BossSettings configuration and environment variables status.</p> <pre><code>bossctl config\n</code></pre> <p>Features: - Displays all configuration fields with their values - Masks sensitive information (tokens, secrets) - Shows environment variable status (set/not set) - Checks core settings, feature flags, download settings, and monitoring options</p> <p>Environment Variables Checked: - Core: <code>DISCORD_TOKEN</code>, <code>OPENAI_API_KEY</code>, <code>LANGCHAIN_API_KEY</code>, <code>PREFIX</code>, <code>DEBUG</code>, <code>LOG_LEVEL</code>, <code>ENVIRONMENT</code> - Features: <code>ENABLE_AI</code>, <code>ENABLE_REDIS</code>, <code>ENABLE_SENTRY</code> - Downloads: <code>MAX_QUEUE_SIZE</code>, <code>MAX_CONCURRENT_DOWNLOADS</code>, <code>STORAGE_ROOT</code>, <code>MAX_FILE_SIZE_MB</code> - Strategy Flags: <code>TWITTER_USE_API_CLIENT</code>, <code>REDDIT_USE_API_CLIENT</code>, <code>INSTAGRAM_USE_API_CLIENT</code>, <code>YOUTUBE_USE_API_CLIENT</code>, <code>DOWNLOAD_API_FALLBACK_TO_CLI</code> - Monitoring: <code>ENABLE_METRICS</code>, <code>METRICS_PORT</code>, <code>ENABLE_HEALTH_CHECK</code>, <code>HEALTH_CHECK_PORT</code></p>"},{"location":"cli/#setup-config","title":"<code>setup-config</code>","text":"<p>Create a dummy gallery-dl configuration file at <code>~/.gallery-dl.conf</code> with comprehensive platform settings.</p> <pre><code>bossctl setup-config [options]\n</code></pre> <p>Options: - <code>--force</code>: Skip confirmation prompt and create config file - <code>--dry-run</code>: Show what would be changed without making modifications</p> <p>Features: - Automatic Backup: Creates timestamped backups of existing config files (e.g., <code>.gallery-dl.conf.backup_20241215_143021</code>) - User Confirmation: Prompts before replacing existing files with backup preview - Comprehensive Config: Includes settings for 20+ platforms (Twitter, Instagram, Reddit, YouTube, etc.) - Placeholder Values: Uses <code>&lt;CHANGEME&gt;</code> and <code>&lt;REDACT&gt;</code> for credentials that need user input - Safety: Error handling with automatic cleanup on failures</p> <p>Dry-Run Features: - Diff Visualization: Shows unified diff with color coding when existing config is found   - \ud83d\udfe2 Green: Lines that would be added   - \ud83d\udd34 Red: Lines that would be removed   - \ud83d\udd35 Blue: File headers and context - New File Preview: Shows syntax-highlighted preview of config that would be created   - Line numbers and JSON syntax highlighting   - File size and statistics   - Platform summary - No Modifications: Guarantees no files are created or modified in dry-run mode</p> <p>Examples: <pre><code># Interactive mode - prompts for confirmation\nbossctl setup-config\n\n# See what would change without modifying files\nbossctl setup-config --dry-run\n\n# Force creation without prompts\nbossctl setup-config --force\n\n# Preview changes to existing config\nbossctl setup-config --dry-run  # Shows diff if config exists\n</code></pre></p> <p>Generated Config Includes: - Platform Extractors: Twitter, Instagram, Reddit, YouTube, DeviantArt, Pixiv, Tumblr, Pinterest, and 15+ more - Download Settings: File handling, retries, timeouts, user agents - Output Configuration: Progress indicators, logging, file naming - Authentication Placeholders: Cookie settings, API keys, usernames/passwords marked for user input</p> <p>Next Steps After Setup: 1. Edit <code>~/.gallery-dl.conf</code> to replace <code>&lt;CHANGEME&gt;</code> values with actual credentials 2. Test configuration with <code>bossctl check-config</code> 3. View configuration with <code>bossctl show-configs</code></p>"},{"location":"cli/#show-configs","title":"<code>show-configs</code>","text":"<p>Display gallery-dl and yt-dlp configuration files with sensitive data masked.</p> <pre><code>bossctl show-configs\n</code></pre> <p>Features: - Locates and displays configuration files from common locations - Masks sensitive configuration values (passwords, tokens, keys) - Shows JSON and text-based configurations - Provides help for configuration file locations</p>"},{"location":"cli/#check-config","title":"<code>check-config</code>","text":"<p>Check and validate gallery-dl configuration using gallery-dl's config.load and GalleryDLConfig validation.</p> <pre><code>bossctl check-config\n</code></pre> <p>Features: - Uses gallery-dl's internal config loading mechanism - Validates configuration with GalleryDLConfig schema - Shows configuration sections and platform-specific settings - Reports configuration file locations and status - Provides detailed error messages for invalid configurations</p>"},{"location":"cli/#doctor","title":"<code>doctor</code>","text":"<p>Run health checks to verify repository requirements and configurations.</p> <pre><code>bossctl doctor\n</code></pre> <p>Health Checks: - Gallery-dl configuration validation - Configuration file accessibility - JSON syntax validation - Required sections verification</p>"},{"location":"cli/#bot-management","title":"Bot Management","text":""},{"location":"cli/#go","title":"<code>go</code>","text":"<p>Start the Discord bot.</p> <pre><code>bossctl go\n</code></pre> <p>Description: Launches the BossBot Discord bot using the configured settings.</p>"},{"location":"cli/#show","title":"<code>show</code>","text":"<p>Show basic bot information.</p> <pre><code>bossctl show\n</code></pre>"},{"location":"cli/#generic-download-command","title":"Generic Download Command","text":""},{"location":"cli/#fetch","title":"<code>fetch</code>","text":"<p>Download media from URLs using appropriate API client (gallery-dl or yt-dlp).</p> <pre><code>bossctl fetch &lt;url1&gt; [url2 ...] [options]\n</code></pre> <p>Arguments: - <code>urls</code> (required): One or more URLs to download</p> <p>Options: - <code>--output</code>, <code>-o</code>: Output directory for downloads (default: <code>./downloads</code>) - <code>--verbose</code>, <code>-v</code>: Enable verbose output - <code>--dry-run</code>: Show what would be downloaded without actually downloading</p> <p>Examples: <pre><code># Download from multiple URLs\nbossctl fetch https://twitter.com/user/status/123 https://youtube.com/watch?v=abc\n\n# Custom output directory\nbossctl fetch https://reddit.com/r/pics/comments/abc123/ -o ./my-downloads\n\n# Dry run to see what would be downloaded\nbossctl fetch https://instagram.com/p/ABC123/ --dry-run\n\n# Verbose output for debugging\nbossctl fetch https://youtu.be/VIDEO_ID --verbose\n</code></pre></p> <p>Features: - Automatic tool selection based on URL patterns - Uses yt-dlp for video platforms (YouTube, Twitch, Vimeo, TikTok) - Uses gallery-dl for gallery platforms (Twitter, Instagram, Reddit, Imgur) - Async download processing - Download success/failure reporting</p>"},{"location":"cli/#download-commands","title":"Download Commands","text":"<p>The download commands use the experimental strategy pattern with feature flag support for API-direct or CLI modes.</p>"},{"location":"cli/#platform-specific-commands","title":"Platform-Specific Commands","text":""},{"location":"cli/#download-twitter","title":"<code>download twitter</code>","text":"<p>Download Twitter/X content with comprehensive options.</p> <pre><code>bossctl download twitter &lt;url&gt; [options]\n</code></pre> <p>Arguments: - <code>url</code> (required): Twitter/X URL to download</p> <p>Options: - <code>--output-dir</code>, <code>-o</code>: Directory to save downloads - <code>--async</code>: Use async download mode - <code>--metadata-only</code>, <code>-m</code>: Extract metadata only, don't download files - <code>--verbose</code>, <code>-v</code>: Show verbose output</p> <p>Supported URL Formats: - <code>https://twitter.com/username/status/123456789</code> - <code>https://x.com/username/status/123456789</code> - <code>https://twitter.com/username</code> - <code>https://x.com/username</code></p> <p>Examples: <pre><code># Download a specific tweet\nbossctl download twitter https://twitter.com/username/status/123456789\n\n# Download to custom directory\nbossctl download twitter https://x.com/username/status/123456789 --output-dir ./downloads\n\n# Extract metadata only\nbossctl download twitter https://twitter.com/username --metadata-only\n\n# Verbose output with async mode\nbossctl download twitter https://x.com/username --async --verbose\n</code></pre></p>"},{"location":"cli/#download-reddit","title":"<code>download reddit</code>","text":"<p>Download Reddit content with config and authentication support.</p> <pre><code>bossctl download reddit &lt;url&gt; [options]\n</code></pre> <p>Arguments: - <code>url</code> (required): Reddit URL to download</p> <p>Options: - <code>--output-dir</code>, <code>-o</code>: Directory to save downloads - <code>--async</code>: Use async download mode - <code>--metadata-only</code>, <code>-m</code>: Extract metadata only, don't download files - <code>--verbose</code>, <code>-v</code>: Show verbose output - <code>--config</code>: Custom gallery-dl config file - <code>--cookies</code>: Browser cookies file</p> <p>Supported URL Formats: - <code>https://reddit.com/r/subreddit/comments/abc123/title/</code> - <code>https://www.reddit.com/r/subreddit/comments/abc123/title/</code> - <code>https://old.reddit.com/r/subreddit/comments/abc123/title/</code></p> <p>Examples: <pre><code># Download Reddit post\nbossctl download reddit https://reddit.com/r/pics/comments/abc123/title/\n\n# Use custom config and cookies\nbossctl download reddit &lt;url&gt; --output-dir ./downloads --cookies cookies.txt\n\n# Metadata only with custom config\nbossctl download reddit &lt;url&gt; --metadata-only --config reddit-config.json\n</code></pre></p>"},{"location":"cli/#download-instagram","title":"<code>download instagram</code>","text":"<p>Download Instagram content with experimental features.</p> <pre><code>bossctl download instagram &lt;url&gt; [options]\n</code></pre> <p>Arguments: - <code>url</code> (required): Instagram URL to download</p> <p>Options: - <code>--output-dir</code>, <code>-o</code>: Directory to save downloads - <code>--async</code>: Use async download mode - <code>--metadata-only</code>, <code>-m</code>: Extract metadata only, don't download files - <code>--verbose</code>, <code>-v</code>: Show verbose output - <code>--cookies-browser</code>: Browser to extract cookies from (default: Firefox) - <code>--user-agent</code>: Custom user agent string (default: Wget/1.21.1)</p> <p>Supported URL Formats: - <code>https://instagram.com/p/ABC123/</code> - <code>https://www.instagram.com/p/ABC123/</code> - <code>https://instagram.com/username/</code> - <code>https://www.instagram.com/username/</code></p> <p>Examples: <pre><code># Download Instagram post\nbossctl download instagram https://instagram.com/p/ABC123/\n\n# Use Chrome cookies and custom user agent\nbossctl download instagram &lt;url&gt; --cookies-browser Chrome --user-agent \"Custom Agent 1.0\"\n\n# Download profile with Firefox cookies\nbossctl download instagram https://instagram.com/username/ --output-dir ./downloads\n</code></pre></p>"},{"location":"cli/#download-youtube","title":"<code>download youtube</code>","text":"<p>Download YouTube content with quality and format control.</p> <pre><code>bossctl download youtube &lt;url&gt; [options]\n</code></pre> <p>Arguments: - <code>url</code> (required): YouTube URL to download</p> <p>Options: - <code>--output-dir</code>, <code>-o</code>: Directory to save downloads - <code>--async</code>: Use async download mode - <code>--metadata-only</code>, <code>-m</code>: Extract metadata only, don't download files - <code>--verbose</code>, <code>-v</code>: Show verbose output - <code>--quality</code>, <code>-q</code>: Video quality (e.g., 720p, 1080p, best) (default: best) - <code>--audio-only</code>, <code>-a</code>: Download audio only</p> <p>Supported URL Formats: - <code>https://youtube.com/watch?v=VIDEO_ID</code> - <code>https://www.youtube.com/watch?v=VIDEO_ID</code> - <code>https://youtu.be/VIDEO_ID</code> - <code>https://youtube.com/playlist?list=PLAYLIST_ID</code></p> <p>Examples: <pre><code># Download video with best quality\nbossctl download youtube https://youtube.com/watch?v=VIDEO_ID\n\n# Download with specific quality\nbossctl download youtube https://youtu.be/VIDEO_ID --quality 720p\n\n# Download audio only\nbossctl download youtube &lt;url&gt; --audio-only --output-dir ./downloads\n\n# Extract metadata only\nbossctl download youtube &lt;url&gt; --metadata-only\n</code></pre></p>"},{"location":"cli/#download-information-commands","title":"Download Information Commands","text":""},{"location":"cli/#download-info","title":"<code>download info</code>","text":"<p>Show comprehensive information about download capabilities.</p> <pre><code>bossctl download info\n</code></pre> <p>Output includes: - Supported platforms and their capabilities - Available commands - Strategy features (API-Direct, CLI Mode, Auto-Fallback) - Usage examples</p>"},{"location":"cli/#download-strategies","title":"<code>download strategies</code>","text":"<p>Show current download strategy configuration.</p> <pre><code>bossctl download strategies\n</code></pre> <p>Features: - Shows API-direct vs CLI mode status for each platform - Displays auto-fallback configuration - Provides tips for enabling experimental features</p>"},{"location":"cli/#strategy-pattern-features","title":"Strategy Pattern Features","text":""},{"location":"cli/#modes","title":"Modes","text":"<ul> <li>\ud83d\ude80 API-Direct Mode: Experimental direct API integration for faster performance</li> <li>\ud83d\udda5\ufe0f CLI Mode: Stable subprocess-based approach (default)</li> <li>\ud83d\udd04 Auto-Fallback: API failures automatically fallback to CLI when enabled</li> </ul>"},{"location":"cli/#feature-flags","title":"Feature Flags","text":"<p>Control strategy behavior using environment variables:</p> <pre><code># Enable API-direct mode per platform\nexport TWITTER_USE_API_CLIENT=true\nexport REDDIT_USE_API_CLIENT=true\nexport INSTAGRAM_USE_API_CLIENT=true\nexport YOUTUBE_USE_API_CLIENT=true\n\n# Control auto-fallback\nexport DOWNLOAD_API_FALLBACK_TO_CLI=true\n</code></pre>"},{"location":"cli/#status-indicators","title":"Status Indicators","text":"<p>Commands show the current strategy mode: - \ud83d\ude80 API-Direct Mode: Using experimental direct API integration - \ud83d\udda5\ufe0f CLI Mode: Using stable subprocess-based approach - \ud83d\udd04 Auto-Fallback: API failures automatically fallback to CLI</p>"},{"location":"cli/#output-error-handling","title":"Output &amp; Error Handling","text":""},{"location":"cli/#success-output","title":"Success Output","text":"<ul> <li>\u2705 Download completion status</li> <li>\ud83d\udcc4 List of downloaded files</li> <li>\ud83d\ude80/\ud83d\udda5\ufe0f Method used (API/CLI)</li> <li>Metadata display (when using <code>--verbose</code> or <code>--metadata-only</code>)</li> </ul>"},{"location":"cli/#error-handling","title":"Error Handling","text":"<ul> <li>\u274c Clear error messages</li> <li>\ud83d\udd04 Automatic fallback when enabled</li> <li>Detailed tracebacks with <code>--verbose</code></li> <li>URL validation with helpful format examples</li> </ul>"},{"location":"cli/#progress-indicators","title":"Progress Indicators","text":"<ul> <li>Spinner progress bars during downloads</li> <li>Real-time status updates</li> <li>Download summary with success/failure counts</li> </ul>"},{"location":"cli/#environment-setup","title":"Environment Setup","text":""},{"location":"cli/#required-environment-variables","title":"Required Environment Variables","text":"<pre><code># Core Discord bot functionality\nDISCORD_TOKEN=your_discord_token\n\n# Optional but recommended\nOPENAI_API_KEY=your_openai_key\nLANGCHAIN_API_KEY=your_langchain_key\n</code></pre>"},{"location":"cli/#download-configuration","title":"Download Configuration","text":"<pre><code># Download directory (default: .downloads/)\nBOSS_BOT_DOWNLOAD_DIR=\"./downloads\"\n\n# Queue and concurrency limits\nMAX_QUEUE_SIZE=10\nMAX_CONCURRENT_DOWNLOADS=3\nMAX_FILE_SIZE_MB=100\n</code></pre>"},{"location":"cli/#debug-and-development","title":"Debug and Development","text":"<pre><code># Enable debug mode\nDEBUG=true\nLOG_LEVEL=DEBUG\nENVIRONMENT=development\n\n# Enable verbose logging\nVERBOSE=true\n</code></pre>"},{"location":"cli/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"cli/#quick-downloads","title":"Quick Downloads","text":"<pre><code># Single URL download\nbossctl fetch https://twitter.com/user/status/123\n\n# Multiple URLs\nbossctl fetch url1 url2 url3 --output ./downloads\n</code></pre>"},{"location":"cli/#platform-specific-downloads","title":"Platform-Specific Downloads","text":"<pre><code># Twitter with metadata\nbossctl download twitter &lt;url&gt; --metadata-only --verbose\n\n# Reddit with authentication\nbossctl download reddit &lt;url&gt; --cookies cookies.txt --config config.json\n\n# YouTube with quality control\nbossctl download youtube &lt;url&gt; --quality 720p --audio-only\n</code></pre>"},{"location":"cli/#configuration-management","title":"Configuration Management","text":"<pre><code># Create initial gallery-dl configuration\nbossctl setup-config\n\n# Preview configuration changes\nbossctl setup-config --dry-run\n\n# Check current configuration\nbossctl config\n\n# Show download tool configs\nbossctl show-configs\n\n# Validate gallery-dl configuration\nbossctl check-config\n\n# Run health checks\nbossctl doctor\n\n# Check strategy status\nbossctl download strategies\n</code></pre>"},{"location":"cli/#assistant-management-commands","title":"Assistant Management Commands","text":"<p>Boss-Bot includes comprehensive LangGraph assistant management capabilities. These commands allow you to create, deploy, sync, and manage AI assistants.</p>"},{"location":"cli/#prerequisites","title":"Prerequisites","text":"<p>Set these environment variables in your <code>.env</code> file: <pre><code>LANGGRAPH_DEPLOYMENT_URL=https://your-deployment.langraph.app\nLANGGRAPH_API_KEY=your-api-key-here\n</code></pre></p>"},{"location":"cli/#assistants-list","title":"<code>assistants list</code>","text":"<p>List all assistants from your LangGraph Cloud deployment.</p> <pre><code>boss-bot assistants list [OPTIONS]\n</code></pre> <p>Options: - <code>--limit INTEGER</code>: Maximum assistants to display (default: 100) - <code>--graph TEXT</code>: Filter by graph name - <code>--format [table|json]</code>: Output format (default: table)</p> <p>Example: <pre><code># List all assistants\nboss-bot assistants list\n\n# Filter by graph\nboss-bot assistants list --graph download_workflow --limit 10\n</code></pre></p>"},{"location":"cli/#assistants-health","title":"<code>assistants health</code>","text":"<p>Check LangGraph Cloud connectivity and authentication.</p> <pre><code>boss-bot assistants health\n</code></pre> <p>Output: - \u2705 \"LangGraph Cloud connection is healthy\" - \u274c Error message with troubleshooting steps</p>"},{"location":"cli/#assistants-create-config","title":"<code>assistants create-config</code>","text":"<p>Generate a new assistant configuration YAML file.</p> <pre><code>boss-bot assistants create-config [OPTIONS]\n</code></pre> <p>Options: - <code>--name TEXT</code>: Assistant name (required) - <code>--graph TEXT</code>: Graph name (default: from environment) - <code>--output PATH</code>: Output file path (default: <code>assistants/{name}.yaml</code>) - <code>--version TEXT</code>: Assistant version (default: \"1.0.0\")</p> <p>Example: <pre><code># Create basic configuration\nboss-bot assistants create-config --name content-analyzer\n\n# Custom output path\nboss-bot assistants create-config --name my-assistant --output configs/assistant.yaml\n</code></pre></p>"},{"location":"cli/#assistants-sync-from","title":"<code>assistants sync-from</code>","text":"<p>Download assistant configurations from LangGraph Cloud to local YAML files.</p> <pre><code>boss-bot assistants sync-from [OPTIONS]\n</code></pre> <p>Options: - <code>--output-dir PATH</code>: Directory for YAML files (default: <code>assistants/</code>) - <code>--graph TEXT</code>: Filter by graph name - <code>--overwrite</code>: Overwrite existing files</p> <p>Example: <pre><code># Sync all assistants\nboss-bot assistants sync-from\n\n# Sync specific graph\nboss-bot assistants sync-from --graph download_workflow --overwrite\n</code></pre></p>"},{"location":"cli/#assistants-sync-to","title":"<code>assistants sync-to</code>","text":"<p>Deploy local YAML configurations to LangGraph Cloud.</p> <pre><code>boss-bot assistants sync-to CONFIG_PATH [OPTIONS]\n</code></pre> <p>Arguments: - <code>CONFIG_PATH</code>: Path to YAML file or directory</p> <p>Options: - <code>--dry-run</code>: Preview changes without deploying - <code>--update</code>: Update existing assistants - <code>--force</code>: Skip confirmation prompts</p> <p>Example: <pre><code># Deploy single assistant\nboss-bot assistants sync-to assistants/content-analyzer.yaml\n\n# Deploy directory with preview\nboss-bot assistants sync-to assistants/ --dry-run\n</code></pre></p>"},{"location":"cli/#assistants-graphs","title":"<code>assistants graphs</code>","text":"<p>List available graphs from your LangGraph deployment.</p> <pre><code>boss-bot assistants graphs [OPTIONS]\n</code></pre> <p>Options: - <code>--format [table|json|list]</code>: Output format (default: table)</p> <p>Example: <pre><code># Show graphs table\nboss-bot assistants graphs\n\n# Simple list\nboss-bot assistants graphs --format list\n</code></pre></p> <p>\ud83d\udcda Complete Assistant Management Guide - Detailed documentation with YAML schemas, workflows, and troubleshooting</p>"},{"location":"cli/#bot-management_1","title":"Bot Management","text":"<pre><code># Start the bot\nbossctl go\n\n# Check version and dependencies\nbossctl version\nbossctl deps\n</code></pre>"},{"location":"cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/#common-issues","title":"Common Issues","text":"<ol> <li>URL Validation Errors: Ensure URLs match supported formats shown in error messages</li> <li>Download Failures: Use <code>--verbose</code> for detailed error information</li> <li>Authentication Issues: Check cookie files and configuration for private content</li> <li>Strategy Issues: Verify environment variables for API-direct mode</li> </ol>"},{"location":"cli/#debug-commands","title":"Debug Commands","text":"<pre><code># Verbose output for debugging\nbossctl download &lt;platform&gt; &lt;url&gt; --verbose\n\n# Dry run to test without downloading\nbossctl fetch &lt;url&gt; --dry-run\n\n# Preview config changes\nbossctl setup-config --dry-run\n\n# Check configuration status\nbossctl config\nbossctl show-configs\nbossctl check-config\nbossctl doctor\n</code></pre>"},{"location":"cli/#getting-help","title":"Getting Help","text":"<pre><code># General help\nbossctl --help\n\n# Command-specific help\nbossctl download --help\nbossctl download twitter --help\n</code></pre>"},{"location":"codebase_structure/","title":"Boss-Bot Codebase Structure","text":"<p>This document provides a comprehensive overview of the boss-bot project structure, including all directories, major components, and architectural patterns.</p> <p>Generated on: 2024-05-27</p>"},{"location":"codebase_structure/#project-overview","title":"\ud83d\udcc1 Project Overview","text":"<p>The boss-bot project follows modern Python development practices with comprehensive tooling and documentation:</p> <pre><code>.\n\u251c\u2500\u2500 \ud83d\udccb Core Documentation\n\u2502   \u251c\u2500\u2500 CHANGELOG.md                     # Automated changelog via towncrier\n\u2502   \u251c\u2500\u2500 CLAUDE.md                        # Claude Code AI assistant instructions\n\u2502   \u251c\u2500\u2500 CONTRIBUTING.md                  # Contributor guidelines and workflows\n\u2502   \u251c\u2500\u2500 EXPERIMENTAL.md                  # Experimental features architecture guide\n\u2502   \u251c\u2500\u2500 MIGRATION.md                     # Project structure migration documentation\n\u2502   \u251c\u2500\u2500 README.md                        # Main project documentation\n\u2502   \u2514\u2500\u2500 VCR_SETUP_SUMMARY.md            # VCR testing implementation guide\n\u2502\n\u251c\u2500\u2500 \ud83d\udc33 Container &amp; Deployment\n\u2502   \u251c\u2500\u2500 Dockerfile                       # Multi-stage Python container build\n\u2502   \u2514\u2500\u2500 docker-compose.yml              # Development environment orchestration\n\u2502\n\u251c\u2500\u2500 \u2699\ufe0f Build &amp; Configuration\n\u2502   \u251c\u2500\u2500 Justfile                         # Main build system entry point\n\u2502   \u251c\u2500\u2500 justfiles/                       # Modular build recipes (20+ files)\n\u2502   \u251c\u2500\u2500 pyproject.toml                   # Python project configuration\n\u2502   \u251c\u2500\u2500 pyrightconfig.json              # Type checking configuration\n\u2502   \u251c\u2500\u2500 taplo.toml                       # TOML formatting configuration\n\u2502   \u2514\u2500\u2500 sample.env                       # Environment variable template\n\u2502\n\u251c\u2500\u2500 \ud83d\udcca Testing &amp; Quality Assurance\n\u2502   \u251c\u2500\u2500 tests/                           # Comprehensive test suite (65% coverage)\n\u2502   \u251c\u2500\u2500 codecov.yml                      # Code coverage reporting configuration\n\u2502   \u251c\u2500\u2500 cov_annotate/                    # Line-by-line coverage annotations (100+ files)\n\u2502   \u251c\u2500\u2500 detect_pytest_live_logging.sh   # Security check for test logging\n\u2502   \u2514\u2500\u2500 junit/                           # JUnit XML test results\n\u2502\n\u251c\u2500\u2500 \ud83d\udcda Documentation System\n\u2502   \u251c\u2500\u2500 docs/                            # MkDocs documentation site\n\u2502   \u251c\u2500\u2500 docs_templates/                  # Jinja2 templates for auto-generated docs\n\u2502   \u251c\u2500\u2500 mkdocs.yml                       # Documentation site configuration\n\u2502   \u2514\u2500\u2500 mkdocs_macro_plugin.py          # Custom macros for dynamic content\n\u2502\n\u251c\u2500\u2500 \ud83e\udde0 AI &amp; Development Intelligence\n\u2502   \u251c\u2500\u2500 ai_docs/                         # AI-assisted development documentation\n\u2502   \u2502   \u251c\u2500\u2500 audit-cursor-rules/          # Cursor IDE rule analysis and optimization\n\u2502   \u2502   \u2514\u2500\u2500 plans/                       # Implementation plans and documentation\n\u2502   \u2502       \u251c\u2500\u2500 incorporate_check_instagram.md # Original validation integration request\n\u2502   \u2502       \u2514\u2500\u2500 claude_check_instagram.md # Completed implementation plan (\u2705 NEW)\n\u2502   \u2514\u2500\u2500 hack/                            # Advanced development configurations (100+ files)\n\u2502\n\u251c\u2500\u2500 \ud83d\udee0\ufe0f Development Environment\n\u2502   \u251c\u2500\u2500 boss-bot.code-workspace          # VS Code workspace configuration\n\u2502   \u251c\u2500\u2500 configure_claude_ignore.sh       # Claude Code ignore pattern setup\n\u2502   \u2514\u2500\u2500 .claude/                         # Claude Code custom slash commands\n\u2502\n\u251c\u2500\u2500 \ud83d\udcc4 Legal &amp; Licensing\n\u2502   \u251c\u2500\u2500 LICENSE                          # Main license file\n\u2502   \u2514\u2500\u2500 LICENSE.txt                      # Additional licensing information\n\u2502\n\u2514\u2500\u2500 \ud83d\udce6 Source Code &amp; Scripts\n    \u251c\u2500\u2500 src/                             # Main application source code\n    \u2514\u2500\u2500 scripts/                         # Development automation scripts (30+ utilities)\n</code></pre>"},{"location":"codebase_structure/#key-file-details","title":"Key File Details:","text":""},{"location":"codebase_structure/#core-documentation","title":"\ud83d\udccb Core Documentation","text":"<ul> <li><code>CLAUDE.md</code> - Comprehensive instructions for Claude Code AI assistant including build commands, architecture patterns, testing guidelines, and project context</li> <li><code>EXPERIMENTAL.md</code> - 1,300+ line architecture document detailing API-direct download strategies, feature flags, and implementation roadmap for 4 platforms</li> <li><code>VCR_SETUP_SUMMARY.md</code> - Complete guide for VCR (Video Cassette Recorder) testing setup with security-first approach for API interaction testing</li> <li><code>MIGRATION.md</code> - Detailed project structure migration plan addressing current issues and proposing scalable AI-ready architecture</li> </ul>"},{"location":"codebase_structure/#ai-development-intelligence","title":"\ud83e\udde0 AI &amp; Development Intelligence","text":"<ul> <li><code>ai_docs/audit-cursor-rules/</code> - Production environment analysis of 29 cursor rules with distribution metrics and optimization recommendations</li> <li><code>mkdocs_macro_plugin.py</code> - Custom MkDocs macros for dynamic documentation generation with Jinja2 templating support</li> </ul>"},{"location":"codebase_structure/#development-environment","title":"\ud83d\udee0\ufe0f Development Environment","text":"<ul> <li><code>boss-bot.code-workspace</code> - VS Code workspace with pytest integration, YAML formatting rules, and JSON schema validation</li> <li><code>configure_claude_ignore.sh</code> - Script that synchronizes .gitignore patterns with Claude Code's ignore settings for consistent file filtering</li> <li><code>detect_pytest_live_logging.sh</code> - Security validation script ensuring live logging is disabled in commits to prevent secret leakage</li> </ul>"},{"location":"codebase_structure/#testing-quality-infrastructure","title":"\ud83d\udcca Testing &amp; Quality Infrastructure","text":"<ul> <li><code>cov_annotate/</code> - Contains 100+ <code>.cover</code> files providing line-by-line coverage annotations for every source file, enabling detailed coverage analysis</li> <li><code>codecov.yml</code> - Advanced code coverage configuration with thresholds, ignore patterns, and integration settings</li> </ul>"},{"location":"codebase_structure/#documentation-templates","title":"\ud83d\udcda Documentation Templates","text":"<ul> <li><code>docs_templates/</code> - Jinja2 templates for automated documentation generation:</li> <li><code>person.jinja</code> - Staff/contributor profile template with photo, contact, and timezone info</li> <li><code>project.jinja</code> - Project documentation template</li> <li><code>service.jinja</code> - Service documentation template</li> </ul>"},{"location":"codebase_structure/#configuration-excellence","title":"\u2699\ufe0f Configuration Excellence","text":"<ul> <li><code>pyproject.toml</code> - Comprehensive Python project configuration with build system, dependencies, testing, linting, and development tool settings</li> <li><code>pyrightconfig.json</code> - Type checking configuration optimized for Discord bot development with async patterns</li> <li><code>taplo.toml</code> - TOML file formatting and validation configuration</li> </ul> <p>This structure demonstrates a mature, production-ready project with enterprise-grade development practices, comprehensive testing infrastructure, AI-assisted development workflows, and automated quality assurance systems.</p>"},{"location":"codebase_structure/#core-architecture","title":"\ud83c\udfd7\ufe0f Core Architecture","text":""},{"location":"codebase_structure/#source-code-srcboss_bot","title":"Source Code (<code>src/boss_bot/</code>)","text":"<p>The main application code follows a modular architecture with clear separation of concerns:</p> <pre><code>src/boss_bot/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 __main__.py\n\u251c\u2500\u2500 __version__.py\n\u251c\u2500\u2500 main_bot.py                    # Legacy entry point\n\u251c\u2500\u2500 cli.py                         # CLI interface\n\u251c\u2500\u2500 ai/                           # \ud83e\udd16 AI Components\n\u251c\u2500\u2500 api/                          # \ud83c\udf10 REST API (Future)\n\u251c\u2500\u2500 bot/                          # \ud83e\udd16 Discord Bot Core\n\u251c\u2500\u2500 cli/                          # \ud83d\udda5\ufe0f CLI Components\n\u251c\u2500\u2500 commands/                     # Legacy commands\n\u251c\u2500\u2500 core/                         # \ud83c\udfd7\ufe0f Core Business Logic\n\u251c\u2500\u2500 downloaders/                  # Legacy downloaders\n\u251c\u2500\u2500 global_cogs/                  # Global Discord cogs\n\u251c\u2500\u2500 integrations/                 # \ud83d\udd0c External Integrations\n\u251c\u2500\u2500 monitoring/                   # \ud83d\udcca Monitoring &amp; Observability\n\u251c\u2500\u2500 schemas/                      # \ud83d\udcc4 Data Schemas\n\u251c\u2500\u2500 storage/                      # \ud83d\udcbe Storage &amp; Data Management\n\u2514\u2500\u2500 utils/                        # \ud83d\udd27 Shared Utilities\n</code></pre>"},{"location":"codebase_structure/#key-component-details","title":"Key Component Details","text":""},{"location":"codebase_structure/#discord-bot-bot","title":"Discord Bot (<code>bot/</code>)","text":"<pre><code>bot/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 bot_help.py                   # Custom help system\n\u251c\u2500\u2500 client.py                     # Main BossBot class\n\u251c\u2500\u2500 cogs/                         # Command modules\n\u2502   \u251c\u2500\u2500 downloads.py              # Download commands (\u2705 Strategy + Validation)\n\u2502   \u251c\u2500\u2500 queue.py                  # Queue management\n\u2502   \u2514\u2500\u2500 task_queue.py             # Task queue operations\n\u251c\u2500\u2500 events/                       # Event handlers\n\u2514\u2500\u2500 middleware/                   # Bot middleware\n</code></pre>"},{"location":"codebase_structure/#core-business-logic-core","title":"Core Business Logic (<code>core/</code>)","text":"<pre><code>core/\n\u251c\u2500\u2500 core_queue.py                 # Legacy queue (deprecated)\n\u251c\u2500\u2500 env.py                        # Environment configuration\n\u251c\u2500\u2500 downloads/                    # Download system\n\u2502   \u251c\u2500\u2500 clients/                  # API-direct clients\n\u2502   \u2502   \u251c\u2500\u2500 aio_gallery_dl.py     # Async gallery-dl wrapper\n\u2502   \u2502   \u251c\u2500\u2500 aio_yt_dlp.py         # Async yt-dlp wrapper\n\u2502   \u2502   \u251c\u2500\u2500 aio_gallery_dl_utils.py # Gallery-dl utilities\n\u2502   \u2502   \u2514\u2500\u2500 config/               # Client configurations\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 gallery_dl_config.py # Gallery-dl configuration model\n\u2502   \u2502       \u2514\u2500\u2500 gallery_dl_validator.py # Configuration validation (\u2705 NEW)\n\u2502   \u251c\u2500\u2500 feature_flags.py          # Feature flag management\n\u2502   \u251c\u2500\u2500 handlers/                 # Platform-specific handlers\n\u2502   \u2502   \u251c\u2500\u2500 base_handler.py       # Abstract base handler\n\u2502   \u2502   \u251c\u2500\u2500 instagram_handler.py  # Instagram downloads\n\u2502   \u2502   \u251c\u2500\u2500 reddit_handler.py     # Reddit downloads\n\u2502   \u2502   \u251c\u2500\u2500 twitter_handler.py    # Twitter/X downloads\n\u2502   \u2502   \u2514\u2500\u2500 youtube_handler.py    # YouTube downloads\n\u2502   \u251c\u2500\u2500 manager.py                # Download manager\n\u2502   \u2514\u2500\u2500 strategies/               # Strategy pattern implementation\n\u2502       \u251c\u2500\u2500 base_strategy.py      # Strategy interface\n\u2502       \u251c\u2500\u2500 instagram_strategy.py # Instagram strategy (\u2705 Complete + Validation)\n\u2502       \u251c\u2500\u2500 reddit_strategy.py    # Reddit strategy (\u2705 Complete)\n\u2502       \u251c\u2500\u2500 twitter_strategy.py   # Twitter strategy (\u2705 Complete)\n\u2502       \u2514\u2500\u2500 youtube_strategy.py   # YouTube strategy (\u2705 Complete)\n\u251c\u2500\u2500 queue/                        # Queue management\n\u2502   \u2514\u2500\u2500 manager.py                # Queue manager\n\u2514\u2500\u2500 services/                     # Core services\n</code></pre>"},{"location":"codebase_structure/#cli-interface-cli","title":"CLI Interface (<code>cli/</code>)","text":"<pre><code>cli/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 main.py                       # Main CLI entry point\n\u251c\u2500\u2500 commands/                     # CLI subcommands\n\u2502   \u2514\u2500\u2500 download.py               # Download commands (\u2705 + Config Validation)\n\u251c\u2500\u2500 config/                       # CLI configuration\n\u2514\u2500\u2500 utils/                        # CLI utilities\n</code></pre>"},{"location":"codebase_structure/#ai-components-ai","title":"AI Components (<code>ai/</code>)","text":"<pre><code>ai/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 agents/                       # LangGraph agents\n\u251c\u2500\u2500 chains/                       # LangChain chains\n\u251c\u2500\u2500 memory/                       # Conversation memory\n\u251c\u2500\u2500 prompts/                      # Prompt templates\n\u2514\u2500\u2500 tools/                        # LangChain tools\n</code></pre>"},{"location":"codebase_structure/#storage-system-storage","title":"Storage System (<code>storage/</code>)","text":"<pre><code>storage/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 backends/                     # Storage backends\n\u251c\u2500\u2500 cleanup/                      # Cleanup operations\n\u251c\u2500\u2500 managers/                     # Storage managers\n\u2502   \u251c\u2500\u2500 quota_manager.py          # Quota management\n\u2502   \u2514\u2500\u2500 validation_manager.py     # File validation\n\u251c\u2500\u2500 migrations/                   # Database migrations\n\u251c\u2500\u2500 models/                       # Data models\n\u251c\u2500\u2500 quotas/                       # Quota system\n\u251c\u2500\u2500 quotas_manager.py             # Legacy quota manager\n\u251c\u2500\u2500 validation_manager.py         # Legacy validation\n\u2514\u2500\u2500 validations/                  # Validation rules\n</code></pre>"},{"location":"codebase_structure/#monitoring-monitoring","title":"Monitoring (<code>monitoring/</code>)","text":"<pre><code>monitoring/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 health/                       # Health check system\n\u2502   \u251c\u2500\u2500 checker.py                # Health check manager\n\u2502   \u2514\u2500\u2500 checks/                   # Individual checks\n\u251c\u2500\u2500 health.py                     # Legacy health\n\u251c\u2500\u2500 health_check.py               # Health check implementation\n\u251c\u2500\u2500 logging/                      # Logging configuration\n\u2502   \u2514\u2500\u2500 logging_config.py         # Logging setup\n\u251c\u2500\u2500 metrics/                      # Metrics collection\n\u2502   \u251c\u2500\u2500 collector.py              # Metrics collector\n\u2502   \u2514\u2500\u2500 exporters/                # Metrics exporters\n\u2514\u2500\u2500 metrics.py                    # Legacy metrics\n</code></pre>"},{"location":"codebase_structure/#testing-structure-tests","title":"\ud83e\uddea Testing Structure (<code>tests/</code>)","text":"<p>Comprehensive test suite with excellent coverage:</p> <pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 conftest.py                   # Shared test fixtures\n\u251c\u2500\u2500 example_vcr_test.py           # VCR testing example\n\u251c\u2500\u2500 cassettes/                    # VCR cassettes for HTTP testing\n\u251c\u2500\u2500 fixtures/                     # Test data files\n\u251c\u2500\u2500 test_bot/                     # Discord bot tests\n\u2502   \u251c\u2500\u2500 conftest.py               # Bot-specific fixtures\n\u2502   \u251c\u2500\u2500 test_client.py            # Bot client tests\n\u2502   \u251c\u2500\u2500 test_cogs/                # Cog testing\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads.py     # Download cog tests\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads_reddit.py\n\u2502   \u2502   \u2514\u2500\u2500 test_downloads_twitter.py\n\u2502   \u251c\u2500\u2500 test_core.py              # Core functionality tests\n\u2502   \u251c\u2500\u2500 test_download_cog.py      # Download cog integration\n\u2502   \u251c\u2500\u2500 test_help.py              # Help system tests\n\u2502   \u251c\u2500\u2500 test_queue.py             # Queue tests\n\u2502   \u2514\u2500\u2500 test_queue_cog.py         # Queue cog tests\n\u251c\u2500\u2500 test_cli/                     # CLI testing\n\u2502   \u2514\u2500\u2500 test_commands/            # CLI command tests\n\u251c\u2500\u2500 test_core/                    # Core logic testing\n\u2502   \u251c\u2500\u2500 test_downloads/           # Download system tests\n\u2502   \u2502   \u251c\u2500\u2500 test_clients/         # API client tests\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_gallery_dl_validator.py # Validation tests (\u2705 NEW)\n\u2502   \u2502   \u251c\u2500\u2500 test_handlers/        # Handler tests\n\u2502   \u2502   \u2514\u2500\u2500 test_strategies/      # Strategy tests (\u2705 All platforms)\n\u2502   \u2502       \u2514\u2500\u2500 test_instagram_strategy_validation.py # Strategy validation tests (\u2705 NEW)\n\u2502   \u251c\u2500\u2500 test_env.py               # Environment tests\n\u2502   \u251c\u2500\u2500 test_project_structure.py # Structure validation\n\u2502   \u2514\u2500\u2500 test_queue_manager.py     # Queue manager tests\n\u251c\u2500\u2500 test_downloaders/             # Legacy downloader tests\n\u251c\u2500\u2500 test_monitoring/              # Monitoring tests\n\u2514\u2500\u2500 test_storage/                 # Storage tests\n</code></pre>"},{"location":"codebase_structure/#documentation-docs","title":"\ud83d\udcda Documentation (<code>docs/</code>)","text":"<p>Comprehensive documentation system:</p> <pre><code>docs/\n\u251c\u2500\u2500 README.md                     # Documentation index\n\u251c\u2500\u2500 agile-readme.md               # Agile development guide\n\u251c\u2500\u2500 ai.md                         # AI integration docs\n\u251c\u2500\u2500 contributors/                 # Contributor guides\n\u2502   \u2514\u2500\u2500 dpytest_example.md        # Discord bot testing guide\n\u251c\u2500\u2500 css/                          # Documentation styling\n\u251c\u2500\u2500 cursor-rules-reference.md     # Development rules\n\u251c\u2500\u2500 development/                  # Development guides\n\u251c\u2500\u2500 environment.md                # Environment setup\n\u251c\u2500\u2500 images/                       # Documentation images\n\u251c\u2500\u2500 img/                          # Additional images\n\u251c\u2500\u2500 vcr.md                        # VCR testing guide\n\u251c\u2500\u2500 versioning.md                 # Version management\n\u2514\u2500\u2500 workflow-rules.md             # Workflow guidelines\n</code></pre>"},{"location":"codebase_structure/#development-tools","title":"\ud83d\udd27 Development Tools","text":""},{"location":"codebase_structure/#build-system-justfiles","title":"Build System (<code>justfiles/</code>)","text":"<p>Modular Justfile-based build system:</p> <pre><code>justfiles/\n\u251c\u2500\u2500 audit.just                   # Security auditing\n\u251c\u2500\u2500 changelog.just               # Changelog management\n\u251c\u2500\u2500 check.just                   # Code quality checks\n\u251c\u2500\u2500 clean.just                   # Cleanup operations\n\u251c\u2500\u2500 common.just                  # Common functions\n\u251c\u2500\u2500 convert.just                 # File conversions\n\u251c\u2500\u2500 cz.just                      # Conventional commits\n\u251c\u2500\u2500 doc.just                     # Documentation\n\u251c\u2500\u2500 firecrawl.just               # Web scraping\n\u251c\u2500\u2500 format.just                  # Code formatting\n\u251c\u2500\u2500 install.just                 # Installation\n\u251c\u2500\u2500 monkeytype.just              # Type inference\n\u251c\u2500\u2500 package.just                 # Package management\n\u251c\u2500\u2500 release.just                 # Release management\n\u251c\u2500\u2500 security.just                # Security scanning\n\u251c\u2500\u2500 taplo.just                   # TOML formatting\n\u251c\u2500\u2500 test.just                    # Testing\n\u251c\u2500\u2500 towncrier.just               # News fragments\n\u251c\u2500\u2500 uv.just                      # UV package manager\n\u251c\u2500\u2500 validate.just                # Validation\n\u2514\u2500\u2500 variables.just               # Build variables\n</code></pre>"},{"location":"codebase_structure/#scripts-scripts","title":"Scripts (<code>scripts/</code>)","text":"<p>Development and automation scripts organized by functionality:</p> <pre><code>scripts/\n\u251c\u2500\u2500 \ud83d\udd0d Code Quality &amp; Analysis\n\u2502   \u251c\u2500\u2500 audit_cursor_rules_headers.py    # Validates YAML frontmatter in cursor rules\n\u2502   \u251c\u2500\u2500 blame.py                         # Git blame analysis and code attribution\n\u2502   \u251c\u2500\u2500 check_rule_lines.py              # Validates cursor rule formatting\n\u2502   \u251c\u2500\u2500 mock_patch_checker.py            # Checks proper mock/patch usage in tests\n\u2502   \u2514\u2500\u2500 validate_frontmatter.py          # YAML frontmatter validation\n\u2502\n\u251c\u2500\u2500 \ud83e\udd16 AI &amp; Content Analysis\n\u2502   \u251c\u2500\u2500 bboxes.py                        # Object detection with Google Gemini AI\n\u2502   \u251c\u2500\u2500 token_counter.py                 # Counts tokens for AI model context windows\n\u2502   \u2514\u2500\u2500 q_a.json                         # Q&amp;A data for AI training\n\u2502\n\u251c\u2500\u2500 \ud83d\udcda Documentation\n\u2502   \u251c\u2500\u2500 docs/\n\u2502   \u2502   \u2514\u2500\u2500 gen_ref_pages.py             # Auto-generates API reference pages\n\u2502   \u251c\u2500\u2500 serve_docs.py                    # MkDocs server with port conflict resolution\n\u2502   \u251c\u2500\u2500 download_readthedocs.sh          # Downloads documentation from ReadTheDocs\n\u2502   \u251c\u2500\u2500 update-docs.sh                   # Updates documentation build\n\u2502   \u251c\u2500\u2500 jekyll_build.sh                  # Jekyll documentation builder\n\u2502   \u2514\u2500\u2500 jekyll_run.sh                    # Jekyll development server\n\u2502\n\u251c\u2500\u2500 \ud83d\ude80 CI/CD &amp; Release Management\n\u2502   \u251c\u2500\u2500 ci/\n\u2502   \u2502   \u251c\u2500\u2500 cz-prepare-release.sh        # Prepares release with Commitizen\n\u2502   \u2502   \u251c\u2500\u2500 cz-release.sh                # Creates GitHub releases\n\u2502   \u2502   \u251c\u2500\u2500 increase_version_number.py   # Automated version bumping\n\u2502   \u2502   \u251c\u2500\u2500 prepare-release.sh           # Release preparation automation\n\u2502   \u2502   \u251c\u2500\u2500 release-manually.sh          # Manual release process\n\u2502   \u2502   \u2514\u2500\u2500 release.sh                   # Main release automation\n\u2502   \u251c\u2500\u2500 publish-pypi                     # PyPI package publishing\n\u2502   \u2514\u2500\u2500 init-changelog.sh               # Initializes changelog format\n\u2502\n\u251c\u2500\u2500 \ud83d\udee0\ufe0f Development Tools\n\u2502   \u251c\u2500\u2500 createstubs.sh                   # Generates type stubs with pyright\n\u2502   \u251c\u2500\u2500 cursor-logs.sh                   # Cursor IDE log analysis\n\u2502   \u251c\u2500\u2500 migration_health_check.py        # Verifies system integrity after migrations\n\u2502   \u251c\u2500\u2500 open-browser.py                  # Cross-platform browser launcher\n\u2502   \u251c\u2500\u2500 retry                            # Command retry utility\n\u2502   \u251c\u2500\u2500 unittest-local                   # Local unit test runner\n\u2502   \u2514\u2500\u2500 manhole-shell                    # Debug shell for running processes\n\u2502\n\u251c\u2500\u2500 \ud83d\udce6 Package &amp; Environment Management\n\u2502   \u251c\u2500\u2500 uv-workspace-init-package.sh     # UV workspace package initialization\n\u2502   \u2514\u2500\u2500 update_changelog.py             # Automated changelog updates\n\u2502\n\u251c\u2500\u2500 \ud83c\udfac Media Processing\n\u2502   \u251c\u2500\u2500 compress-discord.sh              # Video compression for Discord uploads\n\u2502   \u2514\u2500\u2500 Dockerfile.jekyll               # Jekyll documentation container\n\u2502\n\u2514\u2500\u2500 \ud83d\udcca Project Management\n    \u2514\u2500\u2500 update_changelog.py             # Maintains project changelog\n</code></pre>"},{"location":"codebase_structure/#key-script-details","title":"Key Script Details:","text":"<p>\ud83d\udd0d Code Quality Scripts: - <code>audit_cursor_rules_headers.py</code> - Ensures all cursor rules have proper YAML frontmatter with required fields (description, globs, alwaysApply) - <code>mock_patch_checker.py</code> - AST-based analysis tool from Yelp/Tron that finds incomplete mocked objects in tests - <code>blame.py</code> - Advanced git blame analysis for code attribution and ownership tracking</p> <p>\ud83e\udd16 AI Integration Scripts: - <code>bboxes.py</code> - Uses Google Gemini AI for object detection and bounding box analysis in images - <code>token_counter.py</code> - Counts tokens for various AI models (GPT-4, Claude, etc.) to manage context windows - Supports multiple model encodings: <code>o200k_base</code>, <code>cl100k_base</code>, <code>p50k_base</code></p> <p>\ud83d\udcda Documentation Automation: - <code>gen_ref_pages.py</code> - Auto-generates MkDocs API reference pages from source code (adapted from Hikari project) - <code>serve_docs.py</code> - Smart MkDocs server that handles port conflicts and process management - Jekyll integration for alternative documentation builds</p> <p>\ud83d\ude80 Release Management: - Full Commitizen integration for semantic versioning and changelog generation - Automated GitHub release creation with release notes - PyPI publishing automation with proper version management - Multi-stage release pipeline with health checks</p> <p>\ud83d\udee0\ufe0f Development Utilities: - <code>createstubs.sh</code> - Generates type stubs using pyright for better IDE support - <code>migration_health_check.py</code> - Runs comprehensive system integrity checks after code migrations - <code>retry</code> - Robust command retry utility for flaky operations - <code>manhole-shell</code> - Debug shell for introspecting running processes</p> <p>\ud83d\udce6 Package Management: - UV workspace integration for modern Python package management - Automated dependency updates and conflict resolution - Container-based build environments</p> <p>\ud83c\udfac Media Processing: - <code>compress-discord.sh</code> - Video compression pipeline optimized for Discord's file size limits - Handles various video formats and compression settings</p>"},{"location":"codebase_structure/#development-configuration-hack","title":"Development Configuration (<code>hack/</code>)","text":"<p>Advanced development configurations and rules:</p> <pre><code>hack/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 drafts/                      # Draft configurations\n\u2502   \u251c\u2500\u2500 cursor_rules/            # Cursor IDE rules (100+ files)\n\u2502   \u251c\u2500\u2500 cursor_rules_v2/         # Next-generation rules\n\u2502   \u2514\u2500\u2500 disabled/                # Disabled configurations\n\u251c\u2500\u2500 ide-configs/                 # IDE configurations\n\u2502   \u2514\u2500\u2500 vscode/                  # VS Code settings\n\u251c\u2500\u2500 jsonschema/                  # JSON schemas\n\u2514\u2500\u2500 schemas/                     # Configuration schemas\n</code></pre>"},{"location":"codebase_structure/#configuration-files","title":"\u2699\ufe0f Configuration Files","text":""},{"location":"codebase_structure/#core-configuration","title":"Core Configuration","text":"<ul> <li><code>pyproject.toml</code> - Python project configuration</li> <li><code>pyrightconfig.json</code> - Type checking configuration</li> <li><code>taplo.toml</code> - TOML formatting configuration</li> <li><code>mkdocs.yml</code> - Documentation site configuration</li> <li><code>codecov.yml</code> - Code coverage configuration</li> <li><code>docker-compose.yml</code> - Container orchestration</li> <li><code>Dockerfile</code> - Container build instructions</li> </ul>"},{"location":"codebase_structure/#package-management","title":"Package Management","text":"<ul> <li><code>uv.lock</code> - Lock file for uv package manager</li> <li><code>sample.env</code> - Environment variable template</li> </ul>"},{"location":"codebase_structure/#documentation-templates_1","title":"Documentation Templates","text":"<ul> <li><code>docs_templates/</code> - Jinja2 templates for documentation generation</li> </ul>"},{"location":"codebase_structure/#key-features-status","title":"\ud83c\udfaf Key Features &amp; Status","text":""},{"location":"codebase_structure/#implemented-features","title":"\u2705 Implemented Features","text":"<ul> <li>Discord Bot Core - Full discord.py bot with cogs</li> <li>Download System - 4 platform support (Twitter, Reddit, YouTube, Instagram)</li> <li>Strategy Pattern - CLI/API switching with feature flags</li> <li>Configuration Validation - Instagram gallery-dl config validation (\u2705 NEW)</li> <li>Queue Management - Async download queue with priority</li> <li>CLI Interface - Typer-based command-line interface with config commands</li> <li>Monitoring - Health checks, metrics, logging</li> <li>Storage System - File management with quotas and validation</li> <li>Testing - Comprehensive test suite with 65% coverage</li> <li>Documentation - MkDocs-based documentation system</li> </ul>"},{"location":"codebase_structure/#in-development","title":"\ud83d\udd04 In Development","text":"<ul> <li>AI Integration - LangChain/LangGraph components</li> <li>REST API - FastAPI-based web interface</li> <li>Advanced Monitoring - Prometheus metrics export</li> </ul>"},{"location":"codebase_structure/#architecture-patterns","title":"\ud83d\udccb Architecture Patterns","text":"<ol> <li>Strategy Pattern - Download implementations (CLI vs API)</li> <li>Cog Pattern - Discord command organization</li> <li>Factory Pattern - Handler creation and management</li> <li>Observer Pattern - Event handling and monitoring</li> <li>Repository Pattern - Data access abstraction</li> <li>Command Pattern - CLI command structure</li> </ol>"},{"location":"codebase_structure/#quick-start-locations","title":"\ud83d\ude80 Quick Start Locations","text":"<ul> <li>Main Bot Entry: <code>src/boss_bot/bot/client.py</code></li> <li>Download Commands: <code>src/boss_bot/bot/cogs/downloads.py</code></li> <li>Strategy Implementation: <code>src/boss_bot/core/downloads/strategies/</code></li> <li>Configuration: <code>src/boss_bot/core/env.py</code></li> <li>Testing Examples: <code>tests/test_bot/test_cogs/</code></li> <li>CLI Commands: <code>src/boss_bot/cli/commands/</code></li> </ul>"},{"location":"codebase_structure/#project-statistics","title":"\ud83d\udcca Project Statistics","text":"<ul> <li>Total Files: 555+ (including new validation files)</li> <li>Total Directories: 112+</li> <li>Test Coverage: 65%</li> <li>Platform Support: 4 (Twitter, Reddit, YouTube, Instagram)</li> <li>Configuration Validation: Instagram (with extensible framework)</li> <li>Test Cases: 328+ passing, 9 skipped</li> <li>Lines of Code: ~15,500+ (estimated)</li> <li>CLI Commands: 15+ (including 3 new config validation commands)</li> <li>Discord Commands: 10+ (including 2 new config validation commands)</li> </ul> <p>This structure demonstrates a well-organized, production-ready Discord bot with modern Python practices, comprehensive testing, and extensible architecture.</p>"},{"location":"cursor-rules-reference/","title":"Cursor Rules Reference","text":"<p>This document provides a comprehensive reference for all Cursor rules in the project. Each rule is designed to govern specific aspects of development, ensuring consistency and quality across the codebase.</p>"},{"location":"cursor-rules-reference/#core-rules","title":"Core Rules","text":""},{"location":"cursor-rules-reference/#rule-generation","title":"Rule Generation","text":"<p>This rule is essential for maintaining consistency and quality in rule creation across the codebase. Apply this rule when: - Creating new rules - Modifying existing rules - Implementing behavior patterns that need to be remembered - Requesting future behavior changes</p>"},{"location":"cursor-rules-reference/#diagram-generation","title":"Diagram Generation","text":"<p>Governs the generation and management of Mermaid diagrams from stories and tests. Apply this rule when: - Analyzing story files in .ai/stories/ - Analyzing Python test files - Generating or updating diagrams in .ai/diagrams/ - Reviewing relationships between business logic and code dependencies</p>"},{"location":"cursor-rules-reference/#project-status-tracking","title":"Project Status Tracking","text":"<p>Governs project status tracking and reporting. Apply this rule when: - Requesting status checks - Evaluating progress against story files - Planning new implementation tasks - Verifying project structure</p>"},{"location":"cursor-rules-reference/#development-environment","title":"Development Environment","text":"<p>Governs development environment standards. Apply this rule when: - Setting up development environments - Discussing IDE configurations - Handling environment variables - Discussing deployment environments</p>"},{"location":"cursor-rules-reference/#epic-story-management","title":"Epic Story Management","text":"<p>Governs the management of epics and stories. Apply this rule when: - Creating or updating stories - Managing epic status and progress - Transitioning between stories - Validating story completeness</p>"},{"location":"cursor-rules-reference/#phased-development","title":"Phased Development","text":"<p>Governs the phased development approach. Apply this rule when: - Creating new features or modules - Modifying existing code - Reviewing implementation details - Planning development tasks</p>"},{"location":"cursor-rules-reference/#phased-implementation","title":"Phased Implementation","text":"<p>Governs feature implementation based on phase status. Apply this rule when: - Implementing new features from stories - Modifying existing features - Adding functionality to existing components - Reviewing implementation tasks</p>"},{"location":"cursor-rules-reference/#prd-section-navigation","title":"PRD Section Navigation","text":"<p>Governs efficient navigation of PRD documents. Apply this rule when: - Searching for specific PRD sections - Reading or updating PRD content - Analyzing PRD sections for completeness - Referencing PRD content during development</p>"},{"location":"cursor-rules-reference/#security-monitoring","title":"Security Monitoring","text":"<p>Governs security and monitoring standards. Apply this rule when: - Implementing security measures - Setting up monitoring - Handling rate limits - Implementing error handling</p>"},{"location":"cursor-rules-reference/#secure-environment-testing","title":"Secure Environment Testing","text":"<p>Governs secure environment variable handling in tests. Apply this rule when: - Writing tests that involve environment variables - Handling sensitive data in tests - Implementing settings validation tests - Working with secret management</p>"},{"location":"cursor-rules-reference/#testing-rules","title":"Testing Rules","text":""},{"location":"cursor-rules-reference/#pytest-mock-auto","title":"Pytest Mock Auto","text":"<p>Automatically enforces pytest-mock standards in test files. This rule: - Prevents usage of unittest.mock - Enforces pytest-mock's mocker fixture - Ensures proper mock cleanup - Maintains consistent mocking patterns</p>"},{"location":"cursor-rules-reference/#pytest-mock-agent","title":"Pytest Mock Agent","text":"<p>Governs the use of mocking in Python tests. Apply this rule when: - Writing new test files - Modifying existing tests with mocks - Reviewing test code with mocks - Converting tests to use pytest-mock</p>"},{"location":"cursor-rules-reference/#global-rules","title":"Global Rules","text":""},{"location":"cursor-rules-reference/#emoji-communication","title":"Emoji Communication","text":"<p>Governs the use of emojis in communication. This rule is always applied and ensures: - Purposeful emoji usage to enhance meaning - Professional yet engaging communication - Consistent emoji placement and frequency - Contextually appropriate emoji choices</p>"},{"location":"cursor-rules-reference/#python-rules","title":"Python Rules","text":""},{"location":"cursor-rules-reference/#python-tdd","title":"Python TDD","text":"<p>Governs Test-Driven Development workflow. Apply this rule when: - Implementing new features - Fixing bugs - Refactoring code - Working on story implementations</p>"},{"location":"cursor-rules-reference/#pytest-fixture-naming","title":"Pytest Fixture Naming","text":"<p>Governs naming conventions for pytest fixtures to avoid collisions with built-ins, modules, and other Python entities. Apply this rule when: - Generating code in the tests///* directory - Ensuring fixture names are descriptive and scoped - Avoiding naming collisions with Python built-ins and modules</p>"},{"location":"cursor-rules-reference/#tool-rules","title":"Tool Rules","text":""},{"location":"cursor-rules-reference/#git-commit-and-push","title":"Git Commit and Push","text":"<p>Governs git commit and push operations. Apply this rule when: - Committing changes - Pushing changes - Following git commit conventions - Updating work in git</p>"},{"location":"cursor-rules-reference/#langgraph-documentation","title":"LangGraph Documentation","text":"<p>Governs handling of LangGraph documentation queries. Apply this rule when: - Asking about LangGraph functionality - Seeking clarification on LangGraph features - Implementing LangGraph components - Reviewing LangGraph documentation</p>"},{"location":"cursor-rules-reference/#uv-package-manager","title":"UV Package Manager","text":"<p>Governs UV package manager usage. Apply this rule when: - Installing or managing Python packages - Setting up Python environments - Running Python code or tests - Modifying dependency files</p>"},{"location":"cursor-rules-reference/#typescript-rules","title":"TypeScript Rules","text":""},{"location":"cursor-rules-reference/#typescript-best-practices","title":"TypeScript Best Practices","text":"<p>Governs TypeScript development practices. Apply this rule when: - Planning TypeScript features/components - Modifying TypeScript code - Reviewing/fixing TypeScript bugs - Making architectural decisions - Creating/refactoring TypeScript files</p>"},{"location":"cursor-rules-reference/#ui-rules","title":"UI Rules","text":""},{"location":"cursor-rules-reference/#ui-best-practices","title":"UI Best Practices","text":"<p>Governs UI development practices. Apply this rule when: - Implementing user interfaces - Designing UI components - Ensuring accessibility standards - Managing UI state</p>"},{"location":"cursor-rules-reference/#workflow-rules","title":"Workflow Rules","text":""},{"location":"cursor-rules-reference/#architecture-workflow","title":"Architecture Workflow","text":"<p>Governs architectural decisions and workflows. Apply this rule when: - Making architectural decisions - Designing system components - Planning system integrations - Evaluating technical choices</p>"},{"location":"cursor-rules-reference/#development-workflow","title":"Development Workflow","text":"<p>Governs development workflows and practices. Apply this rule when: - Following development processes - Setting up development tasks - Managing development cycles - Coordinating development efforts</p>"},{"location":"cursor-rules-reference/#project-management-workflow","title":"Project Management Workflow","text":"<p>Governs project management practices. Apply this rule when: - Managing project timelines - Coordinating team efforts - Tracking project progress - Planning project milestones</p>"},{"location":"cursor-rules-reference/#rule-types","title":"Rule Types","text":"<p>The project uses several types of rules: - Agent Rules (<code>*-agent.mdc</code>): Rules that are applied by the AI when specific conditions are met - Auto Rules (<code>*-auto.mdc</code>): Rules that automatically apply to files matching specific patterns - Manual Rules (<code>*-manual.mdc</code>): Rules that must be explicitly invoked - Always Rules (<code>*-always.mdc</code>): Rules that are always applied in every conversation</p>"},{"location":"downloaders/","title":"Download System: gallery-dl and yt-dlp Integration","text":"<p>Boss-Bot supports three distinct ways of invoking gallery-dl and yt-dlp for media downloads. This document explains each approach, when to use them, and how they're implemented.</p>"},{"location":"downloaders/#overview","title":"Overview","text":"<p>The download system uses a strategy pattern that allows switching between different invocation methods based on feature flags. This provides flexibility, backward compatibility, and graceful fallback mechanisms.</p>"},{"location":"downloaders/#supported-invocation-methods","title":"Supported Invocation Methods","text":"<ol> <li>Direct API Usage (Experimental) - Direct Python API integration</li> <li>CLI/Subprocess Calls (Stable) - Traditional command-line invocation</li> <li>Strategy Pattern (Hybrid) - Feature flag-controlled switching with fallback</li> </ol>"},{"location":"downloaders/#1-direct-api-usage-experimental","title":"1. Direct API Usage (Experimental)","text":"<p>When to use: When you need fine-grained control, better error handling, and async integration.</p> <p>Status: Experimental - enabled via feature flags per platform.</p>"},{"location":"downloaders/#gallery-dl-api-integration","title":"Gallery-dl API Integration","text":"<p>Location: <code>src/boss_bot/core/downloads/clients/aio_gallery_dl.py</code></p> <pre><code>import gallery_dl\nfrom gallery_dl import config as gdl_config, extractor, job\n\nclass AioGalleryDl:\n    async def extract_metadata(self, url: str):\n        def _extract_metadata_sync():\n            # Load configuration\n            gdl_config.clear()\n            gdl_config.load(files=config_files)\n\n            # Find and create extractor\n            extr = extractor.find(url)\n\n            # Extract metadata\n            for msg in extr:\n                if msg[0] == \"url\":\n                    yield msg[1]  # URL info\n\n        return await loop.run_in_executor(self._executor, _extract_metadata_sync)\n</code></pre> <p>Benefits: - Direct access to Python objects and data structures - Better error handling and exception propagation - No subprocess overhead - Async/await integration with proper thread pool execution</p>"},{"location":"downloaders/#yt-dlp-api-integration","title":"yt-dlp API Integration","text":"<p>Location: <code>src/boss_bot/core/downloads/clients/aio_yt_dlp.py</code></p> <pre><code>import yt_dlp\n\nclass AioYtDlp:\n    def _setup_yt_dlp(self):\n        options = {\n            \"format\": \"best[height&lt;=720]\",\n            \"writeinfojson\": True,\n            \"writedescription\": True,\n            \"writethumbnail\": True,\n            \"noplaylist\": True,\n            \"retries\": 3,\n        }\n        self._yt_dlp = yt_dlp.YoutubeDL(options)\n\n    async def extract_info(self, url: str, download: bool = True):\n        result = await loop.run_in_executor(None, self._yt_dlp.extract_info, url, download)\n        return result\n</code></pre> <p>Benefits: - Programmatic access to all yt-dlp features - Real-time progress callbacks - Direct metadata access without JSON parsing - Better resource management</p>"},{"location":"downloaders/#2-clisubprocess-calls-stable","title":"2. CLI/Subprocess Calls (Stable)","text":"<p>When to use: When you need stability, compatibility, or are working with platforms not yet migrated to API mode.</p> <p>Status: Stable and battle-tested - the default for most operations.</p>"},{"location":"downloaders/#gallery-dl-cli-pattern","title":"Gallery-dl CLI Pattern","text":"<p>Location: <code>src/boss_bot/core/downloads/handlers/twitter_handler.py</code></p> <pre><code>def _build_gallery_dl_command(self, url: str, **options):\n    cmd = [\n        \"gallery-dl\",\n        \"--no-mtime\",           # Don't set file modification time\n        \"-v\",                   # Verbose output\n        \"--write-info-json\",    # Write metadata to JSON file\n        \"--write-metadata\",     # Write metadata to file\n        url,\n    ]\n    return cmd\n\n# Sync execution\nresult = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n\n# Async execution\nprocess = await asyncio.create_subprocess_exec(*cmd, stdout=PIPE, stderr=PIPE)\nstdout, stderr = await process.communicate()\n</code></pre>"},{"location":"downloaders/#yt-dlp-cli-pattern","title":"yt-dlp CLI Pattern","text":"<p>Location: <code>src/boss_bot/core/downloads/handlers/youtube_handler.py</code></p> <pre><code>def _build_yt_dlp_command(self, url: str, **options):\n    cmd = [\"yt-dlp\"]\n\n    # Quality selection\n    quality = options.get(\"quality\", \"720p\")\n    if quality == \"720p\":\n        cmd.extend([\"--format\", \"best[height&lt;=720]\"])\n\n    # Metadata options\n    cmd.extend([\n        \"--write-info-json\",     # Write video metadata to JSON\n        \"--write-description\",   # Write video description\n        \"--write-thumbnail\",     # Download thumbnail\n    ])\n\n    cmd.append(url)\n    return cmd\n</code></pre> <p>Benefits: - Proven stability and reliability - Full access to all CLI features - Easy debugging with command-line tools - Isolated process execution</p>"},{"location":"downloaders/#3-strategy-pattern-hybrid-approach","title":"3. Strategy Pattern (Hybrid Approach)","text":"<p>When to use: When you want to gradually migrate from CLI to API while maintaining fallback capabilities.</p> <p>Status: Production-ready with comprehensive feature flag control.</p>"},{"location":"downloaders/#implementation","title":"Implementation","text":"<p>Location: <code>src/boss_bot/core/downloads/strategies/twitter_strategy.py</code></p> <pre><code>class TwitterDownloadStrategy:\n    async def download(self, url: str, **kwargs):\n        # Feature flag: choose implementation\n        if self.feature_flags.use_api_twitter:\n            try:\n                return await self._download_via_api(url, **kwargs)\n            except Exception as e:\n                if self.feature_flags.api_fallback_to_cli:\n                    logger.warning(f\"API download failed, falling back to CLI: {e}\")\n                    return await self._download_via_cli(url, **kwargs)\n                raise\n        else:\n            return await self._download_via_cli(url, **kwargs)\n\n    async def _download_via_api(self, url: str, **kwargs):\n        async with self.api_client as client:\n            results = []\n            async for item in client.download(url, **kwargs):\n                results.append(item)\n            return self._convert_api_response_to_metadata(results[0], url)\n\n    async def _download_via_cli(self, url: str, **kwargs):\n        loop = asyncio.get_event_loop()\n        download_result = await loop.run_in_executor(None, self.cli_handler.download, url, **kwargs)\n        return self._convert_download_result_to_metadata(download_result, url)\n</code></pre>"},{"location":"downloaders/#feature-flag-configuration","title":"Feature Flag Configuration","text":"<p>Control which invocation method is used via environment variables:</p> <pre><code># Enable API-direct mode per platform\nexport TWITTER_USE_API_CLIENT=true          # Twitter/X API mode\nexport REDDIT_USE_API_CLIENT=true           # Reddit API mode\nexport INSTAGRAM_USE_API_CLIENT=true        # Instagram API mode\nexport YOUTUBE_USE_API_CLIENT=true          # YouTube API mode\n\n# Fallback control\nexport DOWNLOAD_API_FALLBACK_TO_CLI=true    # Auto-fallback on API errors\n</code></pre>"},{"location":"downloaders/#platform-support-status","title":"Platform Support Status","text":"Platform CLI Support API Support Strategy Pattern Twitter/X \u2705 Stable \u2705 Experimental \u2705 Complete Reddit \u2705 Stable \u2705 Experimental \u2705 Complete Instagram \u2705 Stable \u2705 Experimental \u2705 Complete YouTube \u2705 Stable \u2705 Experimental \u2705 Complete"},{"location":"downloaders/#base-handler-pattern","title":"Base Handler Pattern","text":"<p>Location: <code>src/boss_bot/core/downloads/handlers/base_handler.py</code></p> <p>All download handlers inherit from a base class that provides shared subprocess utilities:</p> <pre><code>class BaseDownloadHandler:\n    def _run_command(self, cmd: list[str], cwd: Path | None = None) -&gt; DownloadResult:\n        try:\n            result = subprocess.run(\n                cmd,\n                cwd=cwd or self.download_dir,\n                capture_output=True,\n                text=True,\n                timeout=300,  # 5 minute timeout\n            )\n\n            return DownloadResult(\n                success=result.returncode == 0,\n                stdout=result.stdout,\n                stderr=result.stderr,\n                return_code=result.returncode,\n                error=result.stderr if result.returncode != 0 else None,\n            )\n        except subprocess.TimeoutExpired as e:\n            return DownloadResult(success=False, error=f\"Command timed out: {' '.join(cmd)}\")\n\n    async def _arun_command(self, cmd: list[str], cwd: Path | None = None) -&gt; DownloadResult:\n        # Async version of command execution\n        # ... implementation details\n</code></pre>"},{"location":"downloaders/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"downloaders/#gallery-dl-configuration","title":"Gallery-dl Configuration","text":"<pre><code>from gallery_dl import config as gdl_config\n\n# Load configuration files\ngdl_config.clear()\ngdl_config.load(files=config_files)\n\n# Merge instance config\nif self.config:\n    from gallery_dl import util\n    util.combine_dict(gdl_config._config, self.config)\n</code></pre>"},{"location":"downloaders/#yt-dlp-configuration","title":"yt-dlp Configuration","text":"<pre><code>options = {\n    \"format\": \"best[height&lt;=720]\",\n    \"writeinfojson\": True,\n    \"writedescription\": True,\n    \"writethumbnail\": True,\n    \"noplaylist\": True,\n    \"retries\": 3,\n    \"outtmpl\": str(output_dir / \"%(uploader)s/%(title)s.%(ext)s\")\n}\nself._yt_dlp = yt_dlp.YoutubeDL(options)\n</code></pre>"},{"location":"downloaders/#error-handling-and-fallback","title":"Error Handling and Fallback","text":"<p>The strategy pattern includes comprehensive error handling:</p> <ol> <li>API First: Attempts API-based download when enabled</li> <li>Automatic Fallback: Falls back to CLI on API errors (if enabled)</li> <li>Error Propagation: Provides detailed error information for debugging</li> <li>Timeout Handling: Prevents hanging operations with configurable timeouts</li> </ol>"},{"location":"downloaders/#usage-in-discord-bot","title":"Usage in Discord Bot","text":"<p>Discord commands automatically use the strategy pattern:</p> <pre><code># Discord commands (uses strategy pattern automatically)\n$download https://twitter.com/user/status/123456789\n$info https://reddit.com/r/pics/comments/abc123/title/\n\n# Shows current strategy configuration\n$strategies\n</code></pre>"},{"location":"downloaders/#usage-in-cli","title":"Usage in CLI","text":"<p>CLI commands also leverage the strategy pattern:</p> <pre><code># Uses configured strategy for each platform\nbossctl download twitter https://twitter.com/user/status/123456789\nbossctl download reddit https://reddit.com/r/pics/comments/abc123/title/\n\n# Show current strategy configuration\nbossctl download strategies\n</code></pre>"},{"location":"downloaders/#download-system-architecture","title":"Download System Architecture","text":"<p>The following diagram illustrates how to invoke each downloader and the complete flow from user interaction to final output:</p> <pre><code>graph TD\n    A[Discord User] --&gt;|download URL| B[Discord Bot]\n    C[CLI User] --&gt;|bossctl download| D[CLI Commands]\n\n    B --&gt; E[DownloadCog.download]\n    E --&gt; F{URL Supported?}\n    F --&gt;|Yes| G[Get Strategy for URL]\n    F --&gt;|No| H[Fallback to Queue System]\n\n    D --&gt; I[CLI Platform Commands]\n    I --&gt; J[Platform Selection]\n    J --&gt; K[Get Strategy for Platform]\n\n    G --&gt; L[Strategy Pattern Router]\n    K --&gt; L\n    L --&gt; M{Platform Type?}\n    M --&gt;|Twitter| N[TwitterDownloadStrategy]\n    M --&gt;|Reddit| O[RedditDownloadStrategy]\n    M --&gt;|Instagram| P[InstagramDownloadStrategy]\n    M --&gt;|YouTube| Q[YouTubeDownloadStrategy]\n\n    N --&gt; R{API Client Enabled?}\n    O --&gt; S{API Client Enabled?}\n    P --&gt; T{API Client Enabled?}\n    Q --&gt; U{API Client Enabled?}\n\n    R --&gt;|true| V[API-Direct Mode]\n    S --&gt;|true| W[API-Direct Mode]\n    T --&gt;|true| X[API-Direct Mode]\n    U --&gt;|true| Y[API-Direct Mode]\n\n    R --&gt;|false| Z[CLI Mode]\n    S --&gt;|false| AA[CLI Mode]\n    T --&gt;|false| BB[CLI Mode]\n    U --&gt;|false| CC[CLI Mode]\n\n    V --&gt; DD[AsyncGalleryDL Client]\n    W --&gt; DD\n    X --&gt; DD\n    Y --&gt; EE[AsyncYtDlp Client]\n\n    Z --&gt; FF[TwitterHandler subprocess]\n    AA --&gt; GG[RedditHandler subprocess]\n    BB --&gt; HH[InstagramHandler subprocess]\n    CC --&gt; II[YouTubeHandler subprocess]\n\n    DD --&gt; JJ[gallery-dl Python API]\n    EE --&gt; KK[yt-dlp Python API]\n    JJ --&gt; LL{Success?}\n    KK --&gt; LL\n\n    FF --&gt; MM[gallery-dl CLI]\n    GG --&gt; MM\n    HH --&gt; MM\n    II --&gt; NN[yt-dlp CLI]\n    MM --&gt; OO{Success?}\n    NN --&gt; OO\n\n    LL --&gt;|Error| PP{Fallback Enabled?}\n    PP --&gt;|true| QQ[Fallback to CLI Mode]\n    PP --&gt;|false| RR[Return API Error]\n    QQ --&gt; MM\n    QQ --&gt; NN\n\n    LL --&gt;|Success| SS[Convert API Response]\n    OO --&gt;|Success| TT[Convert CLI Result]\n\n    SS --&gt; UU[Return MediaMetadata]\n    TT --&gt; UU\n    RR --&gt; VV[Return Error]\n    OO --&gt;|Error| VV\n\n    H --&gt; WW[QueueManager.add_to_queue]\n    WW --&gt; XX[Create QueueItem]\n    XX --&gt; YY[Add to Download Queue]\n    YY --&gt; ZZ[DownloadManager processes queue]\n\n    UU --&gt; AAA{Invocation Method?}\n    AAA --&gt;|Discord| BBB[Send Discord Message]\n    AAA --&gt;|CLI| CCC[Rich Console Output]\n    VV --&gt; AAA\n\n    DDD[Environment Variables] --&gt; EEE[FeatureFlags]\n    EEE --&gt; R\n    EEE --&gt; S\n    EEE --&gt; T\n    EEE --&gt; U\n    EEE --&gt; PP\n\n    classDef apiMode fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef cliMode fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n    classDef strategy fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px\n    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px\n    classDef queue fill:#fff3e0,stroke:#ef6c00,stroke-width:2px\n\n    class V,W,X,Y,DD,EE,JJ,KK,SS apiMode\n    class Z,AA,BB,CC,FF,GG,HH,II,MM,NN,TT cliMode\n    class N,O,P,Q,L strategy\n    class RR,VV error\n    class H,WW,XX,YY,ZZ queue\n</code></pre>"},{"location":"downloaders/#architecture-key-features","title":"Architecture Key Features","text":"<ul> <li>Dual Entry Points: Both Discord commands and CLI commands lead to the same strategy pattern</li> <li>Platform-Specific Strategies: Each platform (Twitter, Reddit, Instagram, YouTube) has its own strategy implementation</li> <li>Feature Flag Control: Environment variables determine whether to use API-direct or CLI modes</li> <li>Automatic Fallback: API failures can automatically fall back to CLI when <code>DOWNLOAD_API_FALLBACK_TO_CLI=true</code></li> <li>Queue System Fallback: Unsupported URLs fall back to the legacy queue-based download system</li> <li>Unified Output: Both Discord and CLI provide structured output using the same MediaMetadata format</li> </ul>"},{"location":"downloaders/#best-practices","title":"Best Practices","text":"<ol> <li>Start with CLI: Use CLI mode for production until API mode is well-tested</li> <li>Enable Fallback: Always enable <code>DOWNLOAD_API_FALLBACK_TO_CLI=true</code> during transition</li> <li>Monitor Logs: Watch for fallback events in logs to identify API issues</li> <li>Platform-Specific: Enable API mode per platform based on testing results</li> <li>Gradual Migration: Roll out API mode gradually, starting with less critical platforms</li> </ol>"},{"location":"downloaders/#troubleshooting","title":"Troubleshooting","text":""},{"location":"downloaders/#common-issues","title":"Common Issues","text":"<ol> <li>API Import Errors: Ensure gallery-dl and yt-dlp are installed with API dependencies</li> <li>Timeout Issues: Adjust timeout values in base handler if downloads fail</li> <li>Configuration Conflicts: Check that API and CLI configurations don't conflict</li> <li>Feature Flag Issues: Verify environment variables are properly set</li> </ol>"},{"location":"downloaders/#debug-commands","title":"Debug Commands","text":"<pre><code># Check current strategy status\nbossctl download strategies\n\n# Test specific platform with verbose output\nbossctl download twitter &lt;url&gt; --verbose\n\n# Force CLI mode temporarily\nTWITTER_USE_API_CLIENT=false bossctl download twitter &lt;url&gt;\n</code></pre>"},{"location":"downloaders/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Progress Callbacks: Real-time download progress via API mode</li> <li>Batch Operations: Optimized bulk downloads using API integration</li> <li>Advanced Configuration: Per-URL configuration overrides</li> <li>Performance Metrics: Comparative performance tracking between modes</li> </ul>"},{"location":"downloaders_config/","title":"Download Client Configuration","text":""},{"location":"downloaders_config/#asyncgallerydl-configuration-attributes","title":"AsyncGalleryDL Configuration Attributes","text":"<p>The <code>AsyncGalleryDL</code> class in <code>src/boss_bot/core/downloads/clients/aio_gallery_dl.py</code> uses three different configuration attributes that serve distinct purposes:</p>"},{"location":"downloaders_config/#selfconfig-instance-configuration","title":"<code>self.config</code> (Instance Configuration)","text":"<p>Type: <code>dict[str, Any]</code> Purpose: User-provided configuration that starts as overrides, then becomes the final merged configuration</p> <p>Key Characteristics: - Source: Initially passed to <code>__init__()</code> constructor or set via initialization parameters - Content: Lifecycle: Raw user config \u2192 enhanced with cookies/dirs \u2192 updated to final merged config - Initialization: Set during object creation and enhanced with cookies/download settings - Priority: Highest priority during merging - overrides all file-based configuration sources - Final State: Updated to contain the complete merged configuration after <code>_load_configuration()</code> - Mutability: Modified during initialization, then updated with final merged result</p> <p>Lifecycle Process: <pre><code># 1. Set in __init__\nself.config = config or {}\n\n# 2. Enhanced with runtime settings\nif cookies_file:\n    self.config.setdefault(\"extractor\", {})[\"cookies\"] = str(cookies_file)\nelif cookies_from_browser:\n    self.config.setdefault(\"extractor\", {})[\"cookies-from-browser\"] = cookies_from_browser\n\nif download_dir:\n    self.config.setdefault(\"extractor\", {})[\"base-directory\"] = str(download_dir)\n\n# Additional kwargs merged in\nif kwargs:\n    self.config.update(kwargs)\n\n# 3. After _load_configuration() - UPDATED with final merged result\nself.config = self._gdl_config.copy()  # Now contains file config + original overrides\n</code></pre></p>"},{"location":"downloaders_config/#_gdl_config-native-gallery-dl-configuration","title":"<code>_gdl_config</code> (Native Gallery-DL Configuration)","text":"<p>Type: <code>dict[str, Any]</code> Purpose: Stores the raw configuration dictionary loaded directly from gallery-dl's native configuration system</p> <p>Key Characteristics: - Source: Loaded using <code>gallery_dl.config.load()</code> - gallery-dl's built-in configuration loader - Content: Final merged configuration - combination of file config + <code>self.config</code> overrides - Loading Process:   1. Uses <code>gallery_dl.config.clear()</code> to ensure clean state   2. Loads from gallery-dl config file locations (e.g., <code>~/.gallery-dl.conf</code>)   3. Crucially: Merges with <code>self.config</code> using <code>gallery_dl.util.combine_dict()</code> for final result - Thread Safety: Copied and used in thread pool operations for isolation - Priority: Final effective configuration used for all gallery-dl operations</p> <p>Actual Loading and Merging: <pre><code># Load from file\ngdl_config.load(files=config_files)\nloaded_config = gdl_config._config.copy() if gdl_config._config else {}\n\n# Merge with instance config (self.config has highest priority)\nif self.config:\n    from gallery_dl import util\n    util.combine_dict(loaded_config, self.config)  # self.config overrides file config\n\nreturn loaded_config  # This becomes _gdl_config\n</code></pre></p> <p>Fallback Behavior: <pre><code># If gallery-dl import fails or config loading fails\ndefault_config = GalleryDLConfig()\nif self.config:\n    default_config = default_config.merge_with(self.config)\nreturn default_config.to_dict()\n</code></pre></p>"},{"location":"downloaders_config/#_gallery_dl_config-pydantic-validation-wrapper","title":"<code>_gallery_dl_config</code> (Pydantic Validation Wrapper)","text":"<p>Type: <code>GalleryDLConfig | None</code> Purpose: Provides validation and type safety through Pydantic model wrapper</p> <p>Key Characteristics: - Source: Created from <code>_gdl_config</code> using <code>GalleryDLConfig.from_dict()</code> - Content: Validated Pydantic model with type checking and defaults - Validation: Ensures configuration structure is valid and provides safe defaults - Usage: Used for validation/compatibility checks and as fallback - Creation: Built after <code>_gdl_config</code> is successfully loaded</p> <p>Creation Process: <pre><code>try:\n    self._gallery_dl_config = (\n        GalleryDLConfig.from_dict(self._gdl_config) if self._gdl_config else GalleryDLConfig()\n    )\nexcept Exception as e:\n    logger.warning(f\"Could not validate config with GalleryDLConfig: {e}\")\n    self._gallery_dl_config = GalleryDLConfig()\n</code></pre></p>"},{"location":"downloaders_config/#configuration-flow-and-hierarchy","title":"Configuration Flow and Hierarchy","text":""},{"location":"downloaders_config/#configuration-merging-flow","title":"Configuration Merging Flow","text":"<p>The configuration system works in layers, with <code>self.config</code> serving dual roles:</p> <pre><code>1. gallery-dl config files (base configuration)\n   \u2193\n2. self.config (overrides - highest priority during merge)\n   \u2193\n3. _gdl_config (final merged result: file + self.config)\n   \u2193\n4. self.config \u2190 _gdl_config.copy() (updated to final state)\n   \u2193\n5. _gallery_dl_config (validation wrapper around _gdl_config)\n</code></pre> <p>Key Change: After configuration loading, <code>self.config</code> is updated to contain the final merged configuration, making it equivalent to <code>_gdl_config</code>.</p>"},{"location":"downloaders_config/#retrieval-priority-in-_get_effective_config","title":"Retrieval Priority in <code>_get_effective_config()</code>","text":"<p>When the system needs to get the effective configuration, it follows this fallback order:</p> <ol> <li><code>_gdl_config</code> (Primary Choice)</li> <li>Final merged configuration (file config + original <code>self.config</code>)</li> <li>Most compatible with gallery-dl operations</li> <li> <p>Used when gallery-dl is available and config loaded successfully</p> </li> <li> <p><code>_gallery_dl_config.to_dict()</code> (Secondary Fallback)</p> </li> <li>Pydantic-validated configuration converted to dictionary</li> <li> <p>Used when <code>_gdl_config</code> is not available but validation succeeded</p> </li> <li> <p><code>self.config</code> (Final Fallback)</p> </li> <li>Note: After successful loading, <code>self.config</code> equals <code>_gdl_config</code></li> <li>Used only when both other configs failed to load</li> <li>In practice, this means all three sources now contain the same data after initialization</li> </ol> <pre><code>def _get_effective_config(self) -&gt; dict[str, Any]:\n    \"\"\"Get the effective configuration dictionary.\"\"\"\n    # Return the gallery-dl native config if available\n    if self._gdl_config:\n        return self._gdl_config\n    # Fallback to GalleryDLConfig if available\n    elif self._gallery_dl_config:\n        return self._gallery_dl_config.to_dict()\n    # Final fallback to instance config\n    return self.config\n</code></pre>"},{"location":"downloaders_config/#when-each-configuration-is-used","title":"When Each Configuration is Used","text":""},{"location":"downloaders_config/#selfconfig-usage","title":"<code>self.config</code> Usage","text":"<ul> <li>Initialization: Set during object creation with user preferences</li> <li>Enhancement: Modified to add cookies, download directory, and kwargs</li> <li>Override Source: Highest priority values that override any file-based configuration</li> <li>Merging Input: Original values used as input to <code>util.combine_dict()</code> for final configuration creation</li> <li>Final State: Updated to contain the complete merged configuration after loading</li> <li>Runtime Access: Now equivalent to <code>_gdl_config</code> and can be used directly for configuration queries</li> </ul>"},{"location":"downloaders_config/#_gdl_config-usage","title":"<code>_gdl_config</code> Usage","text":"<ul> <li>Primary: All gallery-dl operations (metadata extraction, downloads)</li> <li>Thread Operations: Copied for thread-safe gallery-dl API calls</li> <li>Final Configuration: Contains merged result of file config + <code>self.config</code> overrides</li> <li>Runtime Merging: Combined with operation-specific options before each API call</li> </ul>"},{"location":"downloaders_config/#_gallery_dl_config-usage","title":"<code>_gallery_dl_config</code> Usage","text":"<ul> <li>Validation: Ensures configuration structure is valid</li> <li>Fallback: When native gallery-dl config is unavailable</li> <li>Type Safety: Provides Pydantic model benefits for development</li> <li>Development: Better IDE support and type hints during development</li> </ul>"},{"location":"downloaders_config/#configuration-loading-flow","title":"Configuration Loading Flow","text":"<pre><code>graph TD\n    A[AsyncGalleryDL.__init__] --&gt; B[Set self.config from params]\n    B --&gt; C[Enhance self.config with cookies/dirs]\n    C --&gt; D[AsyncGalleryDL.__aenter__]\n    D --&gt; E[_load_configuration]\n    E --&gt; F{gallery-dl available?}\n    F --&gt;|Yes| G[gallery_dl.config.load from files]\n    F --&gt;|No| H[Use GalleryDLConfig default]\n    G --&gt; I[loaded_config = file config]\n    I --&gt; J[util.combine_dict with self.config]\n    J --&gt; K[Store merged result in _gdl_config]\n    H --&gt; L[default.merge_with self.config]\n    L --&gt; M[Store fallback in _gdl_config]\n    K --&gt; N[self.config = _gdl_config.copy SYNC]\n    M --&gt; N\n    N --&gt; O[Create GalleryDLConfig from _gdl_config]\n    O --&gt; P[Store in _gallery_dl_config]\n    P --&gt; Q[Configuration Ready - All Synchronized]\n\n    style N fill:#90EE90\n    style Q fill:#FFD700\n</code></pre>"},{"location":"downloaders_config/#benefits-of-triple-configuration-approach","title":"Benefits of Triple Configuration Approach","text":"<ol> <li>User Control: <code>self.config</code> provides highest-priority user overrides for any setting</li> <li>Native Compatibility: <code>_gdl_config</code> ensures maximum compatibility with gallery-dl's native behavior</li> <li>Type Safety: <code>_gallery_dl_config</code> provides validation and type safety through Pydantic</li> <li>Layered Merging: Configuration properly merges from files \u2192 user overrides \u2192 final result</li> <li>Fallback Safety: Multiple fallback layers prevent configuration failures</li> <li>Thread Safety: Configuration is properly isolated in thread pool operations</li> <li>Development Experience: Pydantic model provides better IDE support and type hints</li> <li>Runtime Flexibility: <code>self.config</code> can be enhanced with cookies, directories, and options during initialization</li> </ol>"},{"location":"downloaders_config/#key-insight-configuration-synchronization","title":"Key Insight: Configuration Synchronization","text":"<p>After Recent Changes: The configuration system now maintains synchronization between all three attributes:</p> <ul> <li><code>self.config</code> starts with HIGHEST priority during configuration loading (overrides file config)</li> <li><code>_gdl_config</code> contains the MERGED result (file config + original <code>self.config</code>)</li> <li><code>self.config</code> gets UPDATED to match <code>_gdl_config</code> after successful loading</li> <li>Result: <code>self.config</code> and <code>_gdl_config</code> are equivalent after initialization</li> </ul>"},{"location":"downloaders_config/#benefits-of-this-approach","title":"Benefits of This Approach:","text":"<ol> <li>Consistency: <code>self.config</code> always reflects the current effective configuration</li> <li>Backward Compatibility: Code that accesses <code>self.config</code> gets the correct merged configuration</li> <li>Transparency: No confusion about which configuration source to use</li> <li>Debugging: <code>self.config</code> can be inspected to see the final effective configuration</li> </ol> <p>This synchronized approach balances user control, gallery-dl compatibility, and modern Python development practices with robust error handling and configuration transparency.</p>"},{"location":"environment/","title":"Environment","text":""},{"location":"environment/#macros-plugin-environment","title":"Macros Plugin Environment","text":""},{"location":"environment/#general-list","title":"General List","text":"<p>All available variables and filters within the macros plugin:</p> Variable Type Content extra dict social = [{'icon': 'fontawesome/brands/github', 'link': 'https://github.com/bossjones/boss-bot'}] config MkDocsConfig {'config_file_path': '/home/runner/work/boss-bot/boss-bot/mkdocs.yml', 'site_name': 'Boss Bot', 'nav': None, 'pages': None, 'exclude_docs': None, 'draft_docs': None, 'not_in_nav': None, 'site_url': 'https://bossjones.github.io/boss-bot/docs/', 'site_description': 'AI-powered code generation tooling for efficient software development', 'site_author': 'Malcolm Jones', 'theme': Theme(name='material', dirs=['/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/material/templates', '/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/mkdocs/templates'], static_templates={'sitemap.xml', '404.html'}, name='material', locale=Locale('en'), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'content.code.copy', 'content.code.select', 'content.tooltips', 'navigation.footer', 'search.share', 'navigation.indexes', 'navigation.sections'], font={'code': 'JetBrains Mono'}, icon={'repo': 'fontawesome/brands/github', 'edit': 'material/pencil', 'view': 'material/eye', 'admonition': {'note': 'octicons/tag-16', 'abstract': 'octicons/checklist-16', 'info': 'octicons/info-16', 'tip': 'octicons/squirrel-16', 'success': 'octicons/check-16', 'question': 'octicons/question-16', 'warning': 'octicons/alert-16', 'failure': 'octicons/x-circle-16', 'danger': 'octicons/zap-16', 'bug': 'octicons/bug-16', 'example': 'octicons/beaker-16', 'quote': 'octicons/quote-16'}}, favicon='images/logo3.png', logo='images/logo3.png', palette=[{'media': '(prefers-color-scheme: light)', 'scheme': 'default', 'primary': 'deep purple', 'accent': 'deep purple', 'toggle': {'icon': 'material/lightbulb', 'name': 'Switch to dark mode'}}, {'media': '(prefers-color-scheme: dark)', 'scheme': 'slate', 'primary': 'deep purple', 'accent': 'deep purple', 'toggle': {'icon': 'material/lightbulb-outline', 'name': 'Switch to light mode'}}]), 'docs_dir': '/home/runner/work/boss-bot/boss-bot/docs', 'site_dir': '/home/runner/work/boss-bot/boss-bot/site', 'copyright': 'Copyright \u00a9 2023 - 2025 Malcolm Jones', 'google_analytics': None, 'dev_addr': _IpAddressValue(host='127.0.0.1', port=8000), 'use_directory_urls': True, 'repo_url': 'https://github.com/bossjones/boss-bot', 'repo_name': 'bossjones/boss-bot', 'edit_uri_template': None, 'edit_uri': 'edit/main/docs/', 'extra_css': ['assets/_mkdocstrings.css', 'css/neoteroi.cards.css'], 'extra_javascript': [], 'extra_templates': [], 'markdown_extensions': ['toc', 'tables', 'fenced_code', 'abbr', 'admonition', 'attr_list', 'def_list', 'footnotes', 'meta', 'md_in_html', 'pymdownx.snippets', 'pymdownx.caret', 'pymdownx.details', 'pymdownx.keys', 'pymdownx.mark', 'pymdownx.superfences', 'pymdownx.tilde', 'pymdownx.emoji', 'pymdownx.betterem', 'pymdownx.tabbed', 'pymdownx.tasklist', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.magiclink', , ], 'mdx_configs': {'toc': {'permalink': True}, 'pymdownx.superfences': {'custom_fences': [{'name': 'mermaid', 'class': 'mermaid', 'format': functools.partial(, custom=True)}]}, 'pymdownx.emoji': {'emoji_index': , 'emoji_generator': }, 'pymdownx.betterem': {'smart_enable': 'all'}, 'pymdownx.tabbed': {'alternate_style': True}, 'pymdownx.tasklist': {'custom_checkbox': True}}, 'strict': False, 'remote_branch': 'gh-pages', 'remote_name': 'origin', 'extra': {'social': [{'icon': 'fontawesome/brands/github', 'link': 'https://github.com/bossjones/boss-bot'}]}, 'plugins': {'autorefs': , 'awesome-pages': , 'material/meta': , 'git-revision-date-localized': , 'git-authors': , 'macros': , 'mermaid2': , 'gen-files': , 'mkdocstrings': , 'material/blog': , 'material/tags': , 'material/search': , 'coverage': }, 'hooks': {}, 'watch': ['/home/runner/work/boss-bot/boss-bot/mkdocs_macro_plugin.py', '/home/runner/work/boss-bot/boss-bot/docs_templates', '/home/runner/work/boss-bot/boss-bot/src/boss_bot', '/home/runner/work/boss-bot/boss-bot/README.md', '/home/runner/work/boss-bot/boss-bot/CHANGELOG.md', '/home/runner/work/boss-bot/boss-bot/scripts/docs'], 'validation': {'nav': {'omitted_files': 20, 'not_found': 30, 'absolute_links': 20}, 'links': {'not_found': 30, 'absolute_links': 20, 'unrecognized_links': 20, 'anchors': 20}}} environment dict system = 'Linux', system_version = '6.11.0-1018-azure', python_version = '3.12.11', mkdocs_version = '1.6.1', macros_plugin_version = '1.3.7', jinja2_version = '3.1.6' plugin LegacyConfig {'module_name': 'mkdocs_macro_plugin', 'modules': [], 'render_by_default': True, 'force_render_paths': '', 'include_dir': '', 'include_yaml': [], 'j2_block_start_string': '', 'j2_block_end_string': '', 'j2_variable_start_string': '', 'j2_variable_end_string': '', 'j2_comment_start_string': '', 'j2_comment_end_string': '', 'on_undefined': 'keep', 'on_error_fail': False, 'verbose': True} git dict status = True, date [datetime], short_commit = '932afc2', commit = '932afc2b459f4a13157b2efeaf681ec0da0673f1', tag = 'v0.13.0', short_tag = 'v0.13.0', author = 'Malcolm Jones', author_email = 'bossjones@theblacktonystark.com', committer = 'Malcolm Jones', committer_email = 'bossjones@theblacktonystark.com', date_ISO = 'Tue Aug 12 19:41:12 2025 -0400', message = 'bump: version 0.12.0 \u2192 0.13.0', raw = 'commit 932afc2b459f4a13157b2efeaf681ec0da0673f1\\nAuthor: Malcolm Jones \\nDate:   Tue Aug 12 19:41:12 2025 -0400\\n\\n    bump: version 0.12.0 \u2192 0.13.0', root_dir = '/home/runner/work/boss-bot/boss-bot' social SuperList [{'icon': 'fontawesome/brands/github', 'link': 'https://github.com/bossjones/boss-bot'}] macros SuperDict context [function], macros_info [function], now [function], fix_url [function], include_ai_markdown [function], include_ai_files_with_headers [function], include_file [function], doc_env [function], render_with_page_template [function], list_contributors [function] filters dict pretty [function], relative_url [function] filters_builtin dict abs [builtin_function_or_method], attr [function], batch [function], capitalize [function], center [function], count [builtin_function_or_method], d [function], default [function], dictsort [function], e [function], escape [function], filesizeformat [function], first [function], float [function], forceescape [function], format [function], groupby [function], indent [function], int [function], join [function], last [function], length [builtin_function_or_method], list [function], lower [function], items [function], map [function], min [function], max [function], pprint [function], random [function], reject [function], rejectattr [function], replace [function], reverse [function], round [function], safe [function], select [function], selectattr [function], slice [function], sort [function], string [function], striptags [function], sum [function], title [function], trim [function], truncate [function], unique [function], upper [function], urlencode [function], urlize [function], wordcount [function], wordwrap [function], xmlattr [function], tojson [function] navigation Navigation files Files page Page Page(title='Environment', url='/boss-bot/docs/environment/')"},{"location":"environment/#config-information","title":"Config Information","text":"<p>Standard MkDocs configuration information. Do not try to modify.</p> <p>e.g. <code>{{ config.docs_dir }}</code></p> <p>See also the MkDocs documentation on the config object.</p> Variable Type Content config_file_path str '/home/runner/work/boss-bot/boss-bot/mkdocs.yml' site_name str 'Boss Bot' nav NoneType None pages NoneType None exclude_docs NoneType None draft_docs NoneType None not_in_nav NoneType None site_url str 'https://bossjones.github.io/boss-bot/docs/' site_description str 'AI-powered code generation tooling for efficient software development' site_author str 'Malcolm Jones' theme Theme Theme(name='material', dirs=['/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/material/templates', '/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/mkdocs/templates'], static_templates={'sitemap.xml', '404.html'}, name='material', locale=Locale('en'), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'content.code.copy', 'content.code.select', 'content.tooltips', 'navigation.footer', 'search.share', 'navigation.indexes', 'navigation.sections'], font={'code': 'JetBrains Mono'}, icon={'repo': 'fontawesome/brands/github', 'edit': 'material/pencil', 'view': 'material/eye', 'admonition': {'note': 'octicons/tag-16', 'abstract': 'octicons/checklist-16', 'info': 'octicons/info-16', 'tip': 'octicons/squirrel-16', 'success': 'octicons/check-16', 'question': 'octicons/question-16', 'warning': 'octicons/alert-16', 'failure': 'octicons/x-circle-16', 'danger': 'octicons/zap-16', 'bug': 'octicons/bug-16', 'example': 'octicons/beaker-16', 'quote': 'octicons/quote-16'}}, favicon='images/logo3.png', logo='images/logo3.png', palette=[{'media': '(prefers-color-scheme: light)', 'scheme': 'default', 'primary': 'deep purple', 'accent': 'deep purple', 'toggle': {'icon': 'material/lightbulb', 'name': 'Switch to dark mode'}}, {'media': '(prefers-color-scheme: dark)', 'scheme': 'slate', 'primary': 'deep purple', 'accent': 'deep purple', 'toggle': {'icon': 'material/lightbulb-outline', 'name': 'Switch to light mode'}}]) docs_dir str '/home/runner/work/boss-bot/boss-bot/docs' site_dir str '/home/runner/work/boss-bot/boss-bot/site' copyright str 'Copyright \u00a9 2023 - 2025 Malcolm Jones' google_analytics NoneType None dev_addr _IpAddressValue _IpAddressValue(host='127.0.0.1', port=8000) use_directory_urls bool True repo_url str 'https://github.com/bossjones/boss-bot' repo_name str 'bossjones/boss-bot' edit_uri_template NoneType None edit_uri str 'edit/main/docs/' extra_css list ['assets/_mkdocstrings.css', 'css/neoteroi.cards.css'] extra_javascript list [] extra_templates list [] markdown_extensions list ['toc', 'tables', 'fenced_code', 'abbr', 'admonition', 'attr_list', 'def_list', 'footnotes', 'meta', 'md_in_html', 'pymdownx.snippets', 'pymdownx.caret', 'pymdownx.details', 'pymdownx.keys', 'pymdownx.mark', 'pymdownx.superfences', 'pymdownx.tilde', 'pymdownx.emoji', 'pymdownx.betterem', 'pymdownx.tabbed', 'pymdownx.tasklist', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.magiclink', , ] mdx_configs dict toc [dict], pymdownx.superfences [dict], pymdownx.emoji [dict], pymdownx.betterem [dict], pymdownx.tabbed [dict], pymdownx.tasklist [dict] strict bool False remote_branch str 'gh-pages' remote_name str 'origin' extra LegacyConfig {'social': [{'icon': 'fontawesome/brands/github', 'link': 'https://github.com/bossjones/boss-bot'}]} plugins PluginCollection autorefs [AutorefsPlugin], awesome-pages [AwesomePagesPlugin], material/meta [MetaPlugin], git-revision-date-localized [GitRevisionDateLocalizedPlugin], git-authors [GitAuthorsPlugin], macros [MacrosPlugin], mermaid2 [MarkdownMermaidPlugin], gen-files [GenFilesPlugin], mkdocstrings [MkdocstringsPlugin], material/blog [BlogPlugin], material/tags [TagsPlugin], material/search [SearchPlugin], coverage [MkDocsCoveragePlugin] hooks dict watch list ['/home/runner/work/boss-bot/boss-bot/mkdocs_macro_plugin.py', '/home/runner/work/boss-bot/boss-bot/docs_templates', '/home/runner/work/boss-bot/boss-bot/src/boss_bot', '/home/runner/work/boss-bot/boss-bot/README.md', '/home/runner/work/boss-bot/boss-bot/CHANGELOG.md', '/home/runner/work/boss-bot/boss-bot/scripts/docs'] validation Validation {'nav': {'omitted_files': 20, 'not_found': 30, 'absolute_links': 20}, 'links': {'not_found': 30, 'absolute_links': 20, 'unrecognized_links': 20, 'anchors': 20}}"},{"location":"environment/#macros","title":"Macros","text":"<p>These macros have been defined programmatically for this environment (module or pluglets). </p>"},{"location":"environment/#typeerror-isinstance-arg-2-must-be-a-type-a-tuple-of-types-or-a-union","title":"TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union","text":"<p>Traceback (most recent call last):   File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/mkdocs_macros/context.py\", line 355, in pretty     rows = [(\"%s\" % var, \"%s\" % var_type,              <sub>~</sub><sub>~</sub><sub>~</sub><sub>^</sub>~   File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/super_collections/init.py\", line 168, in str     return self.to_hjson()            <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>   File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/super_collections/init.py\", line 162, in to_hjson     python_dict = json.loads(self.to_json())                              <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^   File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/super_collections/init.py\", line 152, in to_json     return json.dumps(self, cls=CustomEncoder)            <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^   File \"/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/json/init.py\", line 238, in dumps     **kw).encode(obj)           <sup>^</sup><sup>^</sup><sup>^</sup>^^   File \"/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/json/encoder.py\", line 200, in encode     chunks = self.iterencode(o, one_shot=True)              <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^   File \"/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/json/encoder.py\", line 258, in iterencode     return _iterencode(o, 0)            <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^   File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/super_collections/__init_.py\", line 33, in default     if isinstance(obj, datetime):        <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^ TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union</p>"},{"location":"environment/#git-information","title":"Git Information","text":"<p>Information available on the last commit and the git repository containing the documentation project:</p> <p>e.g. <code>{{ git.message }}</code></p> Variable Type Content status bool True date datetime datetime.datetime(2025, 8, 12, 19, 41, 12, tzinfo=tzoffset(None, -14400)) short_commit str '932afc2' commit str '932afc2b459f4a13157b2efeaf681ec0da0673f1' tag str 'v0.13.0' short_tag str 'v0.13.0' author str 'Malcolm Jones' author_email str 'bossjones@theblacktonystark.com' committer str 'Malcolm Jones' committer_email str 'bossjones@theblacktonystark.com' date_ISO str 'Tue Aug 12 19:41:12 2025 -0400' message str 'bump: version 0.12.0 \u2192 0.13.0' raw str 'commit 932afc2b459f4a13157b2efeaf681ec0da0673f1\\nAuthor: Malcolm Jones \\nDate:   Tue Aug 12 19:41:12 2025 -0400\\n\\n    bump: version 0.12.0 \u2192 0.13.0' root_dir str '/home/runner/work/boss-bot/boss-bot'"},{"location":"environment/#page-attributes","title":"Page Attributes","text":"<p>Provided by MkDocs. These attributes change for every page (the attributes shown are for this page).</p> <p>e.g. <code>{{ page.title }}</code></p> <p>See also the MkDocs documentation on the page object.</p> Variable Type Content file File src_uri = 'environment.md', src_dir = '/home/runner/work/boss-bot/boss-bot/docs', dest_dir = '/home/runner/work/boss-bot/boss-bot/site', use_directory_urls = True, inclusion [InclusionLevel], name = 'environment', dest_uri = 'environment/index.html', abs_src_path = '/home/runner/work/boss-bot/boss-bot/docs/environment.md', page [Page], url = 'environment/' children NoneType None previous_page Page Page(title='Download Client Configuration', url='/boss-bot/docs/downloaders_config/') next_page Page Page(title=[blank], url='/boss-bot/docs/langgraph-assistants/') _Page__active bool False update_date str '2025-08-13' canonical_url str 'https://bossjones.github.io/boss-bot/docs/environment/' abs_url str '/boss-bot/docs/environment/' edit_url str 'https://github.com/bossjones/boss-bot/edit/main/docs/environment.md' markdown str \"# Environment\\n\\n{{ macros_info() }}\\n\\n\\n## Updates\\n{% for page in navigation.pages %}\\n1. {{ page.title }} ({{ page.update_date }})\\n{% endfor %}\\n\\n\\n## Mkdocs.yal file (portion)\\n\\n```\\n{{ include_file('mkdocs.yml', 0, 5)}}\\n```\\n\\n## List env object\\n\\n```\\n{{ doc_env() | pprint }}\\n```\\n\" _title_from_render NoneType None content NoneType None toc list [] meta dict foo = 'Hello world', bar [dict], bingo = 'Hello', git_revision_date_localized = 'August 12, 2025 23:41:12', git_revision_date_localized_hash = '932afc2b459f4a13157b2efeaf681ec0da0673f1', git_revision_date_localized_tag = 'v0.13.0', git_revision_date_localized_raw_date = 'August 12, 2025', git_revision_date_localized_raw_datetime = 'August 12, 2025 23:41:12', git_revision_date_localized_raw_iso_date = '2025-08-12', git_revision_date_localized_raw_iso_datetime = '2025-08-12 23:41:12', git_revision_date_localized_raw_timeago = '', git_revision_date_localized_raw_custom = '12. August 2025', git_site_revision_date_localized_hash = '932afc2b459f4a13157b2efeaf681ec0da0673f1', git_site_revision_date_localized_tag = 'v0.13.0', git_site_revision_date_localized = 'August 12, 2025 23:41:12', git_site_revision_date_localized_raw_date = 'August 12, 2025', git_site_revision_date_localized_raw_datetime = 'August 12, 2025 23:41:12', git_site_revision_date_localized_raw_iso_date = '2025-08-12', git_site_revision_date_localized_raw_iso_datetime = '2025-08-12 23:41:12', git_site_revision_date_localized_raw_timeago = '', git_site_revision_date_localized_raw_custom = '12. August 2025', git_creation_date_localized_hash = '', git_creation_date_localized_tag = '', git_creation_date_localized = 'August 12, 2025 23:41:12', git_creation_date_localized_raw_date = 'August 12, 2025', git_creation_date_localized_raw_datetime = 'August 12, 2025 23:41:12', git_creation_date_localized_raw_iso_date = '2025-08-12', git_creation_date_localized_raw_iso_datetime = '2025-08-12 23:41:12', git_creation_date_localized_raw_timeago = '', git_creation_date_localized_raw_custom = '12. August 2025' <p>To have all titles of all pages, use:</p> <pre><code>{% for page in navigation.pages %}\n- {{ page.title }}\n{% endfor %}\n</code></pre>"},{"location":"environment/#plugin-filters","title":"Plugin Filters","text":"<p>These filters are provided as a standard by the macros plugin.</p> Variable Type Content pretty function (var_list, var, var_type, content, rows, header, e) <p>Default Mkdocs-Macro: Prettify a dictionary or object          (used for environment documentation, or debugging).</p> relative_url function (path) <p>Default Mkdocs-Macro:         convert the path of any page according to MkDoc's internal logic,         into a URL relative to the current page         (implements the <code>normalize_url()</code> function from <code>mkdocs.util</code>).         Typically used to manage custom navigation:         <code>{{ page.url | relative_url }}</code>.</p>"},{"location":"environment/#builtin-jinja2-filters","title":"Builtin Jinja2 Filters","text":"<p>These filters are provided by Jinja2 as a standard.</p> <p>See also the Jinja2 documentation on builtin filters.</p> Variable Type Content abs builtin_function_or_method <p>Return the absolute value of the argument.</p> attr function (environment, obj, name) <p>Get an attribute of an object. <code>foo|attr(\"bar\")</code> works like     <code>foo.bar</code>, but returns undefined instead of falling back to <code>foo[\"bar\"]</code>     if the attribute doesn't exist.</p> batch function (value, linecount, fill_with, tmp, item) <p>A filter that batches items. It works pretty much like <code>slice</code>     just the other way round. It returns a list of lists with the     given number of items. If you provide a second parameter this     is used to fill up missing items. See this example.</p> capitalize function (s) <p>Capitalize a value. The first character will be uppercase, all others     lowercase.</p> center function (value, width) <p>Centers the value in a field of a given width.</p> count builtin_function_or_method <p>Return the number of items in a container.</p> d function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> default function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> dictsort function (value, case_sensitive, by, reverse, sort_func) <p>Sort a dict and yield (key, value) pairs. Python dicts may not     be in the order you want to display them in, so sort them first.</p> e function (s) <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in     the string with HTML-safe sequences. Use this if you need to display     text that might contain such characters in HTML.</p> escape function (s) <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in     the string with HTML-safe sequences. Use this if you need to display     text that might contain such characters in HTML.</p> filesizeformat function (value, binary, bytes, base, prefixes, i, prefix, unit) <p>Format the value like a 'human-readable' file size (i.e. 13 kB,     4.1 MB, 102 Bytes, etc).  Per default decimal prefixes are used (Mega,     Giga, etc.), if the second parameter is set to <code>True</code> the binary     prefixes are used (Mebi, Gibi).</p> first function (args, kwargs, b) <p>Return the first item of a sequence.</p> float function (value, default) <p>Convert the value into a floating point number. If the     conversion doesn't work it will return <code>0.0</code>. You can     override this default using the first parameter.</p> forceescape function (value) <p>Enforce HTML escaping.  This will probably double escape variables.</p> format function (value, args, kwargs) <p>Apply the given values to a <code>printf-style</code>_ format string, like     <code>string % values</code>.</p> groupby function (args, kwargs, b) <p>Group a sequence of objects by an attribute using Python's     :func:<code>itertools.groupby</code>. The attribute can use dot notation for     nested access, like <code>\"address.city\"</code>. Unlike Python's <code>groupby</code>,     the values are sorted first so only one group is returned for each     unique value.</p> indent function (s, width, first, blank, newline, rv, lines) <p>Return a copy of the string with each line indented by 4 spaces. The     first line and blank lines are not indented by default.</p> int function (value, default, base) <p>Convert the value into an integer. If the     conversion doesn't work it will return <code>0</code>. You can     override this default using the first parameter. You     can also override the default base (10) in the second     parameter, which handles input with prefixes such as     0b, 0o and 0x for bases 2, 8 and 16 respectively.     The base is ignored for decimal numbers and non-string values.</p> join function (args, kwargs, b) <p>Return a string which is the concatenation of the strings in the     sequence. The separator between elements is an empty string per     default, you can define it with the optional parameter.</p> last function (environment, seq) <p>Return the last item of a sequence.</p> length builtin_function_or_method <p>Return the number of items in a container.</p> list function (args, kwargs, b) <p>Convert the value into a list.  If it was a string the returned list     will be a list of characters.</p> lower function (s) <p>Convert a value to lowercase.</p> items function (value) <p>Return an iterator over the <code>(key, value)</code> items of a mapping.</p> map function (args, kwargs, b) <p>Applies a filter on a sequence of objects or looks up an attribute.     This is useful when dealing with lists of objects but you are really     only interested in a certain value of it.</p> min function (environment, value, case_sensitive, attribute) <p>Return the smallest item from the sequence.</p> max function (environment, value, case_sensitive, attribute) <p>Return the largest item from the sequence.</p> pprint function (value) <p>Pretty print a variable. Useful for debugging.</p> random function (context, seq) <p>Return a random item from the sequence.</p> reject function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and rejecting the objects with the test succeeding.</p> rejectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and rejecting the objects with the test     succeeding.</p> replace function (eval_ctx, s, old, new, count) <p>Return a copy of the value with all occurrences of a substring     replaced with a new one. The first argument is the substring     that should be replaced, the second is the replacement string.     If the optional third argument <code>count</code> is given, only the first     <code>count</code> occurrences are replaced.</p> reverse function (value, rv, e) <p>Reverse the object or return an iterator that iterates over it the other     way round.</p> round function (value, precision, method, func) <p>Round the number to a given precision. The first     parameter specifies the precision (default is <code>0</code>), the     second the rounding method.</p> safe function (value) <p>Mark the value as safe which means that in an environment with automatic     escaping enabled this variable will not be escaped.</p> select function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and only selecting the objects with the test succeeding.</p> selectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and only selecting the objects with the     test succeeding.</p> slice function (args, kwargs, b) <p>Slice an iterator and return a list of lists containing     those items. Useful if you want to create a div containing     three ul tags that represent columns.</p> sort function (environment, value, reverse, case_sensitive, attribute, key_func) <p>Sort an iterable using Python's :func:<code>sorted</code>.</p> string function (s) <p>Convert an object to a string if it isn't already. This preserves     a :class:<code>Markup</code> string rather than converting it back to a basic     string, so it will still be marked as safe and won't be escaped     again.</p> striptags function (value) <p>Strip SGML/XML tags and replace adjacent whitespace by one space.</p> sum function (args, kwargs, b) <p>Returns the sum of a sequence of numbers plus the value of parameter     'start' (which defaults to 0).  When the sequence is empty it returns     start.</p> title function (s, item) <p>Return a titlecased version of the value. I.e. words will start with     uppercase letters, all remaining characters are lowercase.</p> trim function (value, chars) <p>Strip leading and trailing characters, by default whitespace.</p> truncate function (env, s, length, killwords, end, leeway, result) <p>Return a truncated copy of the string. The length is specified     with the first parameter which defaults to <code>255</code>. If the second     parameter is <code>true</code> the filter will cut the text at length. Otherwise     it will discard the last word. If the text was in fact     truncated it will append an ellipsis sign (<code>\"...\"</code>). If you want a     different ellipsis sign than <code>\"...\"</code> you can specify it using the     third parameter. Strings that only exceed the length by the tolerance     margin given in the fourth parameter will not be truncated.</p> unique function (args, kwargs, b) <p>Returns a list of unique items from the given iterable.</p> upper function (s) <p>Convert a value to uppercase.</p> urlencode function (value, items) <p>Quote data for use in a URL path or query using UTF-8.</p> urlize function (eval_ctx, value, trim_url_limit, nofollow, target, rel, extra_schemes, policies, rel_parts, scheme, rv) <p>Convert URLs in text into clickable links.</p> wordcount function (s) <p>Count the words in that string.</p> wordwrap function (environment, s, width, break_long_words, wrapstring, break_on_hyphens, textwrap, line) <p>Wrap a string to the given width. Existing newlines are treated     as paragraphs to be wrapped separately.</p> xmlattr function (eval_ctx, d, autospace, items, key, value, rv) <p>Create an SGML/XML attribute string based on the items in a dict.</p> tojson function (eval_ctx, value, indent, policies, dumps, kwargs) <p>Serialize an object to a string of JSON, and mark it safe to     render in HTML. This filter is only for use in HTML documents.</p>"},{"location":"environment/#updates","title":"Updates","text":"<ol> <li> <p>Boss-Bot (2025-08-13)</p> </li> <li> <p>Boss-Bot AI Integration Guide (2025-08-13)</p> </li> <li> <p>Cursor Agile Workflow Documentation (2025-08-13)</p> </li> <li> <p>.ai docs (2025-08-13)</p> </li> <li> <p>BossBot CLI (bossctl) Documentation (2025-08-13)</p> </li> <li> <p>Boss-Bot Codebase Structure (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>Cursor Rules Reference (2025-08-13)</p> </li> <li> <p>Download System: gallery-dl and yt-dlp Integration (2025-08-13)</p> </li> <li> <p>Download Client Configuration (2025-08-13)</p> </li> <li> <p>Environment (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> <li> <p>None (2025-08-13)</p> </li> </ol>"},{"location":"environment/#mkdocsyal-file-portion","title":"Mkdocs.yal file (portion)","text":"<pre><code># Basic site information\n\n# Sets the name of your documentation site that appears in the header\n\nsite_name: Boss Bot\n\n# Provides a meta description for SEO purposes\n\nsite_description: AI-powered code generation tooling for efficient software development\n</code></pre>"},{"location":"environment/#list-env-object","title":"List env object","text":"<pre><code>{'J2_STRING': &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;,\n 'conf': {'config_file_path': '/home/runner/work/boss-bot/boss-bot/mkdocs.yml',\n          'copyright': 'Copyright &amp;copy; 2023 - 2025 Malcolm Jones',\n          'dev_addr': _IpAddressValue(host='127.0.0.1', port=8000),\n          'docs_dir': '/home/runner/work/boss-bot/boss-bot/docs',\n          'draft_docs': None,\n          'edit_uri': 'edit/main/docs/',\n          'edit_uri_template': None,\n          'exclude_docs': None,\n          'extra': {'social': [{'icon': 'fontawesome/brands/github',\n                                'link': 'https://github.com/bossjones/boss-bot'}]},\n          'extra_css': ['assets/_mkdocstrings.css', 'css/neoteroi.cards.css'],\n          'extra_javascript': [],\n          'extra_templates': [],\n          'google_analytics': None,\n          'hooks': {},\n          'markdown_extensions': ['toc',\n                                  'tables',\n                                  'fenced_code',\n                                  'abbr',\n                                  'admonition',\n                                  'attr_list',\n                                  'def_list',\n                                  'footnotes',\n                                  'meta',\n                                  'md_in_html',\n                                  'pymdownx.snippets',\n                                  'pymdownx.caret',\n                                  'pymdownx.details',\n                                  'pymdownx.keys',\n                                  'pymdownx.mark',\n                                  'pymdownx.superfences',\n                                  'pymdownx.tilde',\n                                  'pymdownx.emoji',\n                                  'pymdownx.betterem',\n                                  'pymdownx.tabbed',\n                                  'pymdownx.tasklist',\n                                  'pymdownx.highlight',\n                                  'pymdownx.inlinehilite',\n                                  'pymdownx.magiclink',\n                                  &lt;mkdocs_autorefs._internal.references.AutorefsExtension object at 0x7f67a9ca9b50&gt;,\n                                  &lt;mkdocstrings._internal.extension.MkdocstringsExtension object at 0x7f67a740c320&gt;],\n          'mdx_configs': {'pymdownx.betterem': {'smart_enable': 'all'},\n                          'pymdownx.emoji': {'emoji_generator': &lt;function to_svg at 0x7f67a8f122a0&gt;,\n                                             'emoji_index': &lt;function twemoji at 0x7f67a8fd1c60&gt;},\n                          'pymdownx.superfences': {'custom_fences': [{'class': 'mermaid',\n                                                                      'format': functools.partial(&lt;function fence_mermaid at 0x7f67a8b971a0&gt;, custom=True),\n                                                                      'name': 'mermaid'}]},\n                          'pymdownx.tabbed': {'alternate_style': True},\n                          'pymdownx.tasklist': {'custom_checkbox': True},\n                          'toc': {'permalink': True}},\n          'nav': None,\n          'not_in_nav': None,\n          'pages': None,\n          'plugins': {'autorefs': &lt;mkdocs_autorefs._internal.plugin.AutorefsPlugin object at 0x7f67a9f67f50&gt;,\n                      'awesome-pages': &lt;mkdocs_awesome_pages_plugin.plugin.AwesomePagesPlugin object at 0x7f67a90fa570&gt;,\n                      'coverage': &lt;mkdocs_coverage.plugin.MkDocsCoveragePlugin object at 0x7f67a7265e50&gt;,\n                      'gen-files': &lt;mkdocs_gen_files.plugin.GenFilesPlugin object at 0x7f67a744e9c0&gt;,\n                      'git-authors': &lt;mkdocs_git_authors_plugin.plugin.GitAuthorsPlugin object at 0x7f67a822e9c0&gt;,\n                      'git-revision-date-localized': &lt;mkdocs_git_revision_date_localized_plugin.plugin.GitRevisionDateLocalizedPlugin object at 0x7f67a83e6db0&gt;,\n                      'macros': &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;,\n                      'material/blog': &lt;material.plugins.blog.plugin.BlogPlugin object at 0x7f67aa08b530&gt;,\n                      'material/meta': &lt;material.plugins.meta.plugin.MetaPlugin object at 0x7f67a83ba2a0&gt;,\n                      'material/search': &lt;material.plugins.search.plugin.SearchPlugin object at 0x7f67a8fef6e0&gt;,\n                      'material/tags': &lt;material.plugins.tags.plugin.TagsPlugin object at 0x7f67a9cda7e0&gt;,\n                      'mermaid2': &lt;mermaid2.plugin.MarkdownMermaidPlugin object at 0x7f67a9081220&gt;,\n                      'mkdocstrings': &lt;mkdocstrings._internal.plugin.MkdocstringsPlugin object at 0x7f67a7cb19a0&gt;},\n          'remote_branch': 'gh-pages',\n          'remote_name': 'origin',\n          'repo_name': 'bossjones/boss-bot',\n          'repo_url': 'https://github.com/bossjones/boss-bot',\n          'site_author': 'Malcolm Jones',\n          'site_description': 'AI-powered code generation tooling for '\n                              'efficient software development',\n          'site_dir': '/home/runner/work/boss-bot/boss-bot/site',\n          'site_name': 'Boss Bot',\n          'site_url': 'https://bossjones.github.io/boss-bot/docs/',\n          'strict': False,\n          'theme': Theme(name='material', dirs=['/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/material/templates', '/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/mkdocs/templates'], static_templates={'sitemap.xml', '404.html'}, name='material', locale=Locale('en'), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'content.code.copy', 'content.code.select', 'content.tooltips', 'navigation.footer', 'search.share', 'navigation.indexes', 'navigation.sections'], font={'code': 'JetBrains Mono'}, icon={'repo': 'fontawesome/brands/github', 'edit': 'material/pencil', 'view': 'material/eye', 'admonition': {'note': 'octicons/tag-16', 'abstract': 'octicons/checklist-16', 'info': 'octicons/info-16', 'tip': 'octicons/squirrel-16', 'success': 'octicons/check-16', 'question': 'octicons/question-16', 'warning': 'octicons/alert-16', 'failure': 'octicons/x-circle-16', 'danger': 'octicons/zap-16', 'bug': 'octicons/bug-16', 'example': 'octicons/beaker-16', 'quote': 'octicons/quote-16'}}, favicon='images/logo3.png', logo='images/logo3.png', palette=[{'media': '(prefers-color-scheme: light)', 'scheme': 'default', 'primary': 'deep purple', 'accent': 'deep purple', 'toggle': {'icon': 'material/lightbulb', 'name': 'Switch to dark mode'}}, {'media': '(prefers-color-scheme: dark)', 'scheme': 'slate', 'primary': 'deep purple', 'accent': 'deep purple', 'toggle': {'icon': 'material/lightbulb-outline', 'name': 'Switch to light mode'}}]),\n          'use_directory_urls': True,\n          'validation': {'links': {'absolute_links': 20,\n                                   'anchors': 20,\n                                   'not_found': 30,\n                                   'unrecognized_links': 20},\n                         'nav': {'absolute_links': 20,\n                                 'not_found': 30,\n                                 'omitted_files': 20}},\n          'watch': ['/home/runner/work/boss-bot/boss-bot/mkdocs_macro_plugin.py',\n                    '/home/runner/work/boss-bot/boss-bot/docs_templates',\n                    '/home/runner/work/boss-bot/boss-bot/src/boss_bot',\n                    '/home/runner/work/boss-bot/boss-bot/README.md',\n                    '/home/runner/work/boss-bot/boss-bot/CHANGELOG.md',\n                    '/home/runner/work/boss-bot/boss-bot/scripts/docs']},\n 'config': {'force_render_paths': '',\n            'include_dir': '',\n            'include_yaml': [],\n            'j2_block_end_string': '',\n            'j2_block_start_string': '',\n            'j2_comment_end_string': '',\n            'j2_comment_start_string': '',\n            'j2_variable_end_string': '',\n            'j2_variable_start_string': '',\n            'module_name': 'mkdocs_macro_plugin',\n            'modules': [],\n            'on_error_fail': False,\n            'on_undefined': 'keep',\n            'render_by_default': True,\n            'verbose': True},\n 'config_class': &lt;class 'mkdocs.config.base.LegacyConfig'&gt;,\n 'config_scheme': (('module_name',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a80a2ed0&gt;),\n                   ('modules',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a80a23f0&gt;),\n                   ('render_by_default',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a80a2420&gt;),\n                   ('force_render_paths',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;),\n                   ('include_dir',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;),\n                   ('include_yaml',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a80be6f0&gt;),\n                   ('j2_block_start_string',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;),\n                   ('j2_block_end_string',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;),\n                   ('j2_variable_start_string',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;),\n                   ('j2_variable_end_string',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;),\n                   ('j2_comment_start_string',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;),\n                   ('j2_comment_end_string',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a81bcb60&gt;),\n                   ('on_undefined',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a80bec30&gt;),\n                   ('on_error_fail',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a80bec60&gt;),\n                   ('verbose',\n                    &lt;mkdocs.config.config_options.Type object at 0x7f67a80bf3b0&gt;)),\n 'env': &lt;jinja2.environment.Environment object at 0x7f67a72fd4f0&gt;,\n 'filter': &lt;bound method MacrosPlugin.filter of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'filters': {'pretty': &lt;function define_env.&lt;locals&gt;.pretty at 0x7f67a72ebc40&gt;,\n             'relative_url': &lt;function define_env.&lt;locals&gt;.relative_url at 0x7f67a72ebec0&gt;},\n 'force_page_rendering': &lt;bound method MacrosPlugin.force_page_rendering of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'load_config': &lt;bound method BasePlugin.load_config of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'macro': &lt;bound method MacrosPlugin.macro of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'macros': {'context': &lt;function define_env.&lt;locals&gt;.context at 0x7f67a72ebce0&gt;,\n            'doc_env': &lt;function define_env.&lt;locals&gt;.doc_env at 0x7f67a73104a0&gt;,\n            'fix_url': &lt;function fix_url at 0x7f67a809fc40&gt;,\n            'include_ai_files_with_headers': &lt;function define_env.&lt;locals&gt;.include_ai_files_with_headers at 0x7f67a7310360&gt;,\n            'include_ai_markdown': &lt;function define_env.&lt;locals&gt;.include_ai_markdown at 0x7f67a73102c0&gt;,\n            'include_file': &lt;function define_env.&lt;locals&gt;.include_file at 0x7f67a7310400&gt;,\n            'list_contributors': &lt;function define_env.&lt;locals&gt;.list_contributors at 0x7f67a73105e0&gt;,\n            'macros_info': &lt;function define_env.&lt;locals&gt;.macros_info at 0x7f67a72ebd80&gt;,\n            'now': &lt;function define_env.&lt;locals&gt;.now at 0x7f67a72ebe20&gt;,\n            'render_with_page_template': &lt;function define_env.&lt;locals&gt;.render_with_page_template at 0x7f67a7310540&gt;},\n 'markdown': '# Environment\\n'\n             '\\n'\n             '{{ macros_info() }}\\n'\n             '\\n'\n             '\\n'\n             '## Updates\\n'\n             '{% for page in navigation.pages %}\\n'\n             '1. {{ page.title }} ({{ page.update_date }})\\n'\n             '{% endfor %}\\n'\n             '\\n'\n             '\\n'\n             '## Mkdocs.yal file (portion)\\n'\n             '\\n'\n             '```\\n'\n             \"{{ include_file('mkdocs.yml', 0, 5)}}\\n\"\n             '```\\n'\n             '\\n'\n             '## List env object\\n'\n             '\\n'\n             '```\\n'\n             '{{ doc_env() | pprint }}\\n'\n             '```\\n',\n 'on_config': &lt;bound method MacrosPlugin.on_config of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'on_nav': &lt;bound method MacrosPlugin.on_nav of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'on_page_markdown': &lt;bound method MacrosPlugin.on_page_markdown of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'on_post_build': &lt;bound method MacrosPlugin.on_post_build of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'on_pre_build': &lt;bound method MacrosPlugin.on_pre_build of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'on_serve': &lt;bound method MacrosPlugin.on_serve of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'page': Page(title='Environment', url='/boss-bot/docs/environment/'),\n 'post_build_functions': [&lt;function on_post_build at 0x7f67a7310220&gt;],\n 'post_macro_functions': [],\n 'pre_macro_functions': [],\n 'project_dir': '/home/runner/work/boss-bot/boss-bot',\n 'raw_markdown': '# Environment\\n'\n                 '\\n'\n                 '{{ macros_info() }}\\n'\n                 '\\n'\n                 '\\n'\n                 '## Updates\\n'\n                 '{% for page in navigation.pages %}\\n'\n                 '1. {{ page.title }} ({{ page.update_date }})\\n'\n                 '{% endfor %}\\n'\n                 '\\n'\n                 '\\n'\n                 '## Mkdocs.yal file (portion)\\n'\n                 '\\n'\n                 '```\\n'\n                 \"{{ include_file('mkdocs.yml', 0, 5)}}\\n\"\n                 '```\\n'\n                 '\\n'\n                 '## List env object\\n'\n                 '\\n'\n                 '```\\n'\n                 '{{ doc_env() | pprint }}\\n'\n                 '```\\n',\n 'render': &lt;bound method MacrosPlugin.render of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'start_chatting': &lt;bound method MacrosPlugin.start_chatting of &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;&gt;,\n 'supports_multiple_instances': False,\n 'variables': {'config': {'config_file_path': '/home/runner/work/boss-bot/boss-bot/mkdocs.yml',\n                          'copyright': 'Copyright &amp;copy; 2023 - 2025 Malcolm '\n                                       'Jones',\n                          'dev_addr': _IpAddressValue(host='127.0.0.1', port=8000),\n                          'docs_dir': '/home/runner/work/boss-bot/boss-bot/docs',\n                          'draft_docs': None,\n                          'edit_uri': 'edit/main/docs/',\n                          'edit_uri_template': None,\n                          'exclude_docs': None,\n                          'extra': {'social': [{'icon': 'fontawesome/brands/github',\n                                                'link': 'https://github.com/bossjones/boss-bot'}]},\n                          'extra_css': ['assets/_mkdocstrings.css',\n                                        'css/neoteroi.cards.css'],\n                          'extra_javascript': [],\n                          'extra_templates': [],\n                          'google_analytics': None,\n                          'hooks': {},\n                          'markdown_extensions': ['toc',\n                                                  'tables',\n                                                  'fenced_code',\n                                                  'abbr',\n                                                  'admonition',\n                                                  'attr_list',\n                                                  'def_list',\n                                                  'footnotes',\n                                                  'meta',\n                                                  'md_in_html',\n                                                  'pymdownx.snippets',\n                                                  'pymdownx.caret',\n                                                  'pymdownx.details',\n                                                  'pymdownx.keys',\n                                                  'pymdownx.mark',\n                                                  'pymdownx.superfences',\n                                                  'pymdownx.tilde',\n                                                  'pymdownx.emoji',\n                                                  'pymdownx.betterem',\n                                                  'pymdownx.tabbed',\n                                                  'pymdownx.tasklist',\n                                                  'pymdownx.highlight',\n                                                  'pymdownx.inlinehilite',\n                                                  'pymdownx.magiclink',\n                                                  &lt;mkdocs_autorefs._internal.references.AutorefsExtension object at 0x7f67a9ca9b50&gt;,\n                                                  &lt;mkdocstrings._internal.extension.MkdocstringsExtension object at 0x7f67a740c320&gt;],\n                          'mdx_configs': {'pymdownx.betterem': {'smart_enable': 'all'},\n                                          'pymdownx.emoji': {'emoji_generator': &lt;function to_svg at 0x7f67a8f122a0&gt;,\n                                                             'emoji_index': &lt;function twemoji at 0x7f67a8fd1c60&gt;},\n                                          'pymdownx.superfences': {'custom_fences': [{'class': 'mermaid',\n                                                                                      'format': functools.partial(&lt;function fence_mermaid at 0x7f67a8b971a0&gt;, custom=True),\n                                                                                      'name': 'mermaid'}]},\n                                          'pymdownx.tabbed': {'alternate_style': True},\n                                          'pymdownx.tasklist': {'custom_checkbox': True},\n                                          'toc': {'permalink': True}},\n                          'nav': None,\n                          'not_in_nav': None,\n                          'pages': None,\n                          'plugins': {'autorefs': &lt;mkdocs_autorefs._internal.plugin.AutorefsPlugin object at 0x7f67a9f67f50&gt;,\n                                      'awesome-pages': &lt;mkdocs_awesome_pages_plugin.plugin.AwesomePagesPlugin object at 0x7f67a90fa570&gt;,\n                                      'coverage': &lt;mkdocs_coverage.plugin.MkDocsCoveragePlugin object at 0x7f67a7265e50&gt;,\n                                      'gen-files': &lt;mkdocs_gen_files.plugin.GenFilesPlugin object at 0x7f67a744e9c0&gt;,\n                                      'git-authors': &lt;mkdocs_git_authors_plugin.plugin.GitAuthorsPlugin object at 0x7f67a822e9c0&gt;,\n                                      'git-revision-date-localized': &lt;mkdocs_git_revision_date_localized_plugin.plugin.GitRevisionDateLocalizedPlugin object at 0x7f67a83e6db0&gt;,\n                                      'macros': &lt;mkdocs_macros.plugin.MacrosPlugin object at 0x7f67a82505c0&gt;,\n                                      'material/blog': &lt;material.plugins.blog.plugin.BlogPlugin object at 0x7f67aa08b530&gt;,\n                                      'material/meta': &lt;material.plugins.meta.plugin.MetaPlugin object at 0x7f67a83ba2a0&gt;,\n                                      'material/search': &lt;material.plugins.search.plugin.SearchPlugin object at 0x7f67a8fef6e0&gt;,\n                                      'material/tags': &lt;material.plugins.tags.plugin.TagsPlugin object at 0x7f67a9cda7e0&gt;,\n                                      'mermaid2': &lt;mermaid2.plugin.MarkdownMermaidPlugin object at 0x7f67a9081220&gt;,\n                                      'mkdocstrings': &lt;mkdocstrings._internal.plugin.MkdocstringsPlugin object at 0x7f67a7cb19a0&gt;},\n                          'remote_branch': 'gh-pages',\n                          'remote_name': 'origin',\n                          'repo_name': 'bossjones/boss-bot',\n                          'repo_url': 'https://github.com/bossjones/boss-bot',\n                          'site_author': 'Malcolm Jones',\n                          'site_description': 'AI-powered code generation '\n                                              'tooling for efficient software '\n                                              'development',\n                          'site_dir': '/home/runner/work/boss-bot/boss-bot/site',\n                          'site_name': 'Boss Bot',\n                          'site_url': 'https://bossjones.github.io/boss-bot/docs/',\n                          'strict': False,\n                          'theme': Theme(name='material', dirs=['/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/material/templates', '/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/mkdocs/templates'], static_templates={'sitemap.xml', '404.html'}, name='material', locale=Locale('en'), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'content.code.copy', 'content.code.select', 'content.tooltips', 'navigation.footer', 'search.share', 'navigation.indexes', 'navigation.sections'], font={'code': 'JetBrains Mono'}, icon={'repo': 'fontawesome/brands/github', 'edit': 'material/pencil', 'view': 'material/eye', 'admonition': {'note': 'octicons/tag-16', 'abstract': 'octicons/checklist-16', 'info': 'octicons/info-16', 'tip': 'octicons/squirrel-16', 'success': 'octicons/check-16', 'question': 'octicons/question-16', 'warning': 'octicons/alert-16', 'failure': 'octicons/x-circle-16', 'danger': 'octicons/zap-16', 'bug': 'octicons/bug-16', 'example': 'octicons/beaker-16', 'quote': 'octicons/quote-16'}}, favicon='images/logo3.png', logo='images/logo3.png', palette=[{'media': '(prefers-color-scheme: light)', 'scheme': 'default', 'primary': 'deep purple', 'accent': 'deep purple', 'toggle': {'icon': 'material/lightbulb', 'name': 'Switch to dark mode'}}, {'media': '(prefers-color-scheme: dark)', 'scheme': 'slate', 'primary': 'deep purple', 'accent': 'deep purple', 'toggle': {'icon': 'material/lightbulb-outline', 'name': 'Switch to light mode'}}]),\n                          'use_directory_urls': True,\n                          'validation': {'links': {'absolute_links': 20,\n                                                   'anchors': 20,\n                                                   'not_found': 30,\n                                                   'unrecognized_links': 20},\n                                         'nav': {'absolute_links': 20,\n                                                 'not_found': 30,\n                                                 'omitted_files': 20}},\n                          'watch': ['/home/runner/work/boss-bot/boss-bot/mkdocs_macro_plugin.py',\n                                    '/home/runner/work/boss-bot/boss-bot/docs_templates',\n                                    '/home/runner/work/boss-bot/boss-bot/src/boss_bot',\n                                    '/home/runner/work/boss-bot/boss-bot/README.md',\n                                    '/home/runner/work/boss-bot/boss-bot/CHANGELOG.md',\n                                    '/home/runner/work/boss-bot/boss-bot/scripts/docs']},\n               'environment': {'jinja2_version': '3.1.6',\n                               'macros_plugin_version': '1.3.7',\n                               'mkdocs_version': '1.6.1',\n                               'python_version': '3.12.11',\n                               'system': 'Linux',\n                               'system_version': '6.11.0-1018-azure'},\n               'extra': {'social': [{'icon': 'fontawesome/brands/github',\n                                     'link': 'https://github.com/bossjones/boss-bot'}]},\n               'files': &lt;mkdocs.structure.files.Files object at 0x7f67a9d5a0c0&gt;,\n               'filters': {'pretty': &lt;function define_env.&lt;locals&gt;.pretty at 0x7f67a72ebc40&gt;,\n                           'relative_url': &lt;function define_env.&lt;locals&gt;.relative_url at 0x7f67a72ebec0&gt;},\n               'filters_builtin': {'abs': &lt;built-in function abs&gt;,\n                                   'attr': &lt;function do_attr at 0x7f67a9bb96c0&gt;,\n                                   'batch': &lt;function do_batch at 0x7f67a9bb8400&gt;,\n                                   'capitalize': &lt;function do_capitalize at 0x7f67a9baad40&gt;,\n                                   'center': &lt;function do_center at 0x7f67a9bab560&gt;,\n                                   'count': &lt;built-in function len&gt;,\n                                   'd': &lt;function do_default at 0x7f67a9bab420&gt;,\n                                   'default': &lt;function do_default at 0x7f67a9bab420&gt;,\n                                   'dictsort': &lt;function do_dictsort at 0x7f67a9baae80&gt;,\n                                   'e': &lt;function escape at 0x7f67a9afd120&gt;,\n                                   'escape': &lt;function escape at 0x7f67a9afd120&gt;,\n                                   'filesizeformat': &lt;function do_filesizeformat at 0x7f67a9babba0&gt;,\n                                   'first': &lt;function do_first at 0x7f67a9baba60&gt;,\n                                   'float': &lt;function do_float at 0x7f67a9bb80e0&gt;,\n                                   'forceescape': &lt;function do_forceescape at 0x7f67a9baa8e0&gt;,\n                                   'format': &lt;function do_format at 0x7f67a9bb8180&gt;,\n                                   'groupby': &lt;function do_groupby at 0x7f67a9bb8e00&gt;,\n                                   'indent': &lt;function do_indent at 0x7f67a9babd80&gt;,\n                                   'int': &lt;function do_int at 0x7f67a9bb8040&gt;,\n                                   'items': &lt;function do_items at 0x7f67a9baac00&gt;,\n                                   'join': &lt;function do_join at 0x7f67a9bab740&gt;,\n                                   'last': &lt;function do_last at 0x7f67a9bab880&gt;,\n                                   'length': &lt;built-in function len&gt;,\n                                   'list': &lt;function do_list at 0x7f67a9bb9300&gt;,\n                                   'lower': &lt;function do_lower at 0x7f67a9baab60&gt;,\n                                   'map': &lt;function do_map at 0x7f67a9bb9da0&gt;,\n                                   'max': &lt;function do_max at 0x7f67a9bab380&gt;,\n                                   'min': &lt;function do_min at 0x7f67a9bab2e0&gt;,\n                                   'pprint': &lt;function do_pprint at 0x7f67a9babc40&gt;,\n                                   'random': &lt;function do_random at 0x7f67a9babb00&gt;,\n                                   'reject': &lt;function do_reject at 0x7f67a9bba2a0&gt;,\n                                   'rejectattr': &lt;function do_rejectattr at 0x7f67a9bba7a0&gt;,\n                                   'replace': &lt;function do_replace at 0x7f67a9baaa20&gt;,\n                                   'reverse': &lt;function do_reverse at 0x7f67a9bb9620&gt;,\n                                   'round': &lt;function do_round at 0x7f67a9bb8680&gt;,\n                                   'safe': &lt;function do_mark_safe at 0x7f67a9bb9120&gt;,\n                                   'select': &lt;function do_select at 0x7f67a9bba020&gt;,\n                                   'selectattr': &lt;function do_selectattr at 0x7f67a9bba520&gt;,\n                                   'slice': &lt;function do_slice at 0x7f67a9bb85e0&gt;,\n                                   'sort': &lt;function do_sort at 0x7f67a9baaf20&gt;,\n                                   'string': &lt;function soft_str at 0x7f67a9afd6c0&gt;,\n                                   'striptags': &lt;function do_striptags at 0x7f67a9bb82c0&gt;,\n                                   'sum': &lt;function do_sum at 0x7f67a9bb9080&gt;,\n                                   'title': &lt;function do_title at 0x7f67a9baade0&gt;,\n                                   'tojson': &lt;function do_tojson at 0x7f67a9bba5c0&gt;,\n                                   'trim': &lt;function do_trim at 0x7f67a9bb8220&gt;,\n                                   'truncate': &lt;function do_truncate at 0x7f67a9babe20&gt;,\n                                   'unique': &lt;function do_unique at 0x7f67a9bab240&gt;,\n                                   'upper': &lt;function do_upper at 0x7f67a9baaac0&gt;,\n                                   'urlencode': &lt;function do_urlencode at 0x7f67a9baa980&gt;,\n                                   'urlize': &lt;function do_urlize at 0x7f67a9babce0&gt;,\n                                   'wordcount': &lt;function do_wordcount at 0x7f67a9babf60&gt;,\n                                   'wordwrap': &lt;function do_wordwrap at 0x7f67a9babec0&gt;,\n                                   'xmlattr': &lt;function do_xmlattr at 0x7f67a9baaca0&gt;},\n               'git': {'author': 'Malcolm Jones',\n                       'author_email': 'bossjones@theblacktonystark.com',\n                       'commit': '932afc2b459f4a13157b2efeaf681ec0da0673f1',\n                       'committer': 'Malcolm Jones',\n                       'committer_email': 'bossjones@theblacktonystark.com',\n                       'date': datetime.datetime(2025, 8, 12, 19, 41, 12, tzinfo=tzoffset(None, -14400)),\n                       'date_ISO': 'Tue Aug 12 19:41:12 2025 -0400',\n                       'message': 'bump: version 0.12.0 \u2192 0.13.0',\n                       'raw': 'commit '\n                              '932afc2b459f4a13157b2efeaf681ec0da0673f1\\n'\n                              'Author: Malcolm Jones '\n                              '&lt;bossjones@theblacktonystark.com&gt;\\n'\n                              'Date:   Tue Aug 12 19:41:12 2025 -0400\\n'\n                              '\\n'\n                              '    bump: version 0.12.0 \u2192 0.13.0',\n                       'root_dir': '/home/runner/work/boss-bot/boss-bot',\n                       'short_commit': '932afc2',\n                       'short_tag': 'v0.13.0',\n                       'status': True,\n                       'tag': 'v0.13.0'},\n               'macros': {'context': &lt;function define_env.&lt;locals&gt;.context at 0x7f67a72ebce0&gt;,\n                          'doc_env': &lt;function define_env.&lt;locals&gt;.doc_env at 0x7f67a73104a0&gt;,\n                          'fix_url': &lt;function fix_url at 0x7f67a809fc40&gt;,\n                          'include_ai_files_with_headers': &lt;function define_env.&lt;locals&gt;.include_ai_files_with_headers at 0x7f67a7310360&gt;,\n                          'include_ai_markdown': &lt;function define_env.&lt;locals&gt;.include_ai_markdown at 0x7f67a73102c0&gt;,\n                          'include_file': &lt;function define_env.&lt;locals&gt;.include_file at 0x7f67a7310400&gt;,\n                          'list_contributors': &lt;function define_env.&lt;locals&gt;.list_contributors at 0x7f67a73105e0&gt;,\n                          'macros_info': &lt;function define_env.&lt;locals&gt;.macros_info at 0x7f67a72ebd80&gt;,\n                          'now': &lt;function define_env.&lt;locals&gt;.now at 0x7f67a72ebe20&gt;,\n                          'render_with_page_template': &lt;function define_env.&lt;locals&gt;.render_with_page_template at 0x7f67a7310540&gt;},\n               'navigation': &lt;mkdocs.structure.nav.Navigation object at 0x7f67a66ee870&gt;,\n               'page': Page(title='Environment', url='/boss-bot/docs/environment/'),\n               'plugin': {'force_render_paths': '',\n                          'include_dir': '',\n                          'include_yaml': [],\n                          'j2_block_end_string': '',\n                          'j2_block_start_string': '',\n                          'j2_comment_end_string': '',\n                          'j2_comment_start_string': '',\n                          'j2_variable_end_string': '',\n                          'j2_variable_start_string': '',\n                          'module_name': 'mkdocs_macro_plugin',\n                          'modules': [],\n                          'on_error_fail': False,\n                          'on_undefined': 'keep',\n                          'render_by_default': True,\n                          'verbose': True},\n               'social': [{'icon': 'fontawesome/brands/github',\n                           'link': 'https://github.com/bossjones/boss-bot'}]}}\n</code></pre>"},{"location":"langgraph-assistants/","title":"Macro Rendering Error","text":"<p>File: <code>langgraph-assistants.md</code></p> <p>UndefinedError: 'secrets' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/mkdocs_macros/plugin.py\", line 688, in render\n    return md_template.render(**page_variables)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 351, in top-level template code\n  File \"/home/runner/work/boss-bot/boss-bot/.venv/lib/python3.12/site-packages/jinja2/environment.py\", line 490, in getattr\n    return getattr(obj, attribute)\n           ^^^^^^^^^^^^^^^^^^^^^^^\njinja2.exceptions.UndefinedError: 'secrets' is undefined\n</code></pre>"},{"location":"logging/","title":"Loguru Thread-Safe Interceptor Setup Guide","text":""},{"location":"logging/#overview","title":"\ud83c\udfaf Overview","text":"<p>This guide provides comprehensive instructions for setting up thread-safe, async-safe, and multiprocessing-safe logging using Loguru with standard library logging interception. The implementation ensures that all logging calls from any library are captured and handled safely in concurrent environments.</p>"},{"location":"logging/#critical-concepts","title":"\ud83d\udea8 Critical Concepts","text":""},{"location":"logging/#why-early-initialization-matters","title":"Why Early Initialization Matters","text":"<p>Most Python libraries call <code>logging.getLogger()</code> during their import process. If your interceptor isn't configured first:</p> <ul> <li>\u274c Missing logs: Library logs won't be intercepted</li> <li>\u274c Race conditions: Threading conflicts during import</li> <li>\u274c Configuration conflicts: Libraries may set up incompatible handlers</li> <li>\u274c Partial coverage: Some loggers escape interception</li> </ul>"},{"location":"logging/#thread-safety-requirements","title":"Thread Safety Requirements","text":"<p>Standard Python logging has several thread safety limitations:</p> <ul> <li>File corruption in multiprocessing without proper queuing</li> <li>Deadlocks when logging from signal handlers or destructors</li> <li>Lost messages during concurrent writes</li> <li>Inconsistent formatting across threads</li> </ul>"},{"location":"logging/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<pre><code>graph TD\n    A[Application Start] --&gt; B[_early_init FIRST]\n    B --&gt; C[Remove Default Loguru Handler]\n    C --&gt; D[Create InterceptHandler]\n    D --&gt; E[Replace Root Logger Handlers]\n    E --&gt; F[Configure Basic Loguru Handler]\n    F --&gt; G[Intercept Critical Loggers]\n    G --&gt; H[Import Other Modules]\n    H --&gt; I[global_log_config - Full Setup]\n    I --&gt; J[Application Runtime]\n\n    style B fill:#ff6b6b\n    style F fill:#4ecdc4\n    style I fill:#45b7d1\n</code></pre>"},{"location":"logging/#implementation-components","title":"\ud83d\udd27 Implementation Components","text":""},{"location":"logging/#1-early-initialization-function","title":"1. Early Initialization Function","text":"<p>File: <code>my_intercept_logger.py</code></p> <pre><code>def _early_init() -&gt; None:\n    \"\"\"\n    Early initialization of logging system - call BEFORE importing other modules.\n\n    This ensures all logging calls are intercepted from the start and prevents\n    race conditions with threaded imports or configuration conflicts.\n    \"\"\"\n    global _early_init_done\n\n    if _early_init_done:\n        return  # Already initialized\n\n    # Remove default loguru handler immediately\n    logger.remove()\n\n    # Create thread-safe InterceptHandler early\n    intercept_handler = InterceptHandler()\n\n    # Set up basic root logging level\n    logging.root.setLevel(logging.DEBUG)\n\n    # Replace root handler immediately\n    logging.root.handlers = [intercept_handler]\n\n    # Configure basic loguru handler with safety features\n    logger.add(\n        sys.stderr,\n        level=\"INFO\",\n        format=\"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n               \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n               \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n               \"&lt;level&gt;{message}&lt;/level&gt;\",\n        enqueue=True,   # Critical for thread/multiprocessing safety\n        catch=True,     # Prevent logging errors from crashing app\n        backtrace=True,\n        diagnose=True\n    )\n\n    # Immediately intercept common problematic loggers\n    critical_loggers = [\n        \"asyncio\", \"concurrent.futures\", \"multiprocessing\",\n        \"threading\", \"urllib3\", \"requests\", \"httpx\", \"aiohttp\",\n        \"discord\", \"discord.client\", \"discord.gateway\", \"discord.http\"\n    ]\n\n    for logger_name in critical_loggers:\n        log_instance = logging.getLogger(logger_name)\n        log_instance.handlers = [intercept_handler]\n        log_instance.propagate = False\n\n    _early_init_done = True\n</code></pre>"},{"location":"logging/#2-thread-safe-intercepthandler","title":"2. Thread-Safe InterceptHandler","text":"<pre><code>class InterceptHandler(logging.Handler):\n    \"\"\"\n    Thread-safe and async-safe interceptor for standard logging into Loguru.\n\n    This implementation ensures proper thread safety and multiprocessing compatibility\n    by using improved frame inspection and avoiding potential deadlocks.\n    \"\"\"\n\n    def emit(self, record: logging.LogRecord) -&gt; None:\n        \"\"\"Thread-safe emit method that properly handles frame inspection.\"\"\"\n        # Get corresponding Loguru level if it exists\n        try:\n            level = loguru.logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # Improved frame inspection to avoid issues with frozen imports and threading\n        frame, depth = inspect.currentframe(), 0\n        while frame:\n            filename = frame.f_code.co_filename\n            # Check for logging module and frozen/bootstrap code\n            is_logging = filename == logging.__file__\n            is_frozen = \"importlib\" in filename and \"_bootstrap\" in filename\n            if depth &gt; 0 and not (is_logging or is_frozen):\n                break\n            frame = frame.f_back\n            depth += 1\n\n        # Use opt() with proper depth and exception info for thread-safe logging\n        loguru.logger.opt(depth=depth, exception=record.exc_info).log(\n            level,\n            record.getMessage(),\n        )\n</code></pre>"},{"location":"logging/#3-full-configuration-function","title":"3. Full Configuration Function","text":"<pre><code>def global_log_config(log_level: Union[str, int] = logging.DEBUG, json: bool = False) -&gt; _Logger:\n    \"\"\"Configure global logging settings with thread and async safety.\"\"\"\n\n    # Create thread-safe intercept handler\n    intercept_handler = InterceptHandler()\n\n    # Set root logging level\n    logging.root.setLevel(log_level)\n\n    # Replace handlers for all existing loggers\n    for name in logging.root.manager.loggerDict.keys():\n        log_instance = logging.getLogger(name)\n        log_instance.handlers = [intercept_handler]\n        log_instance.propagate = False\n\n    # Configure loguru with thread/async/multiprocessing safety\n    logger.configure(\n        handlers=[\n            {\n                \"sink\": stdout,\n                \"serialize\": json,\n                \"format\": format_record,\n                \"diagnose\": True,\n                \"backtrace\": True,\n                \"enqueue\": True,      # CRITICAL: Enable enqueue for thread/multiprocessing safety\n                \"catch\": True,        # Catch exceptions to prevent app crashes\n            }\n        ],\n    )\n\n    return logger\n</code></pre>"},{"location":"logging/#usage-patterns","title":"\ud83d\ude80 Usage Patterns","text":""},{"location":"logging/#correct-implementation-order","title":"\u2705 Correct Implementation Order","text":"<pre><code># main.py or boss-bot application entry point\nfrom boss_bot.monitoring.logging import early_init\n\n# \ud83d\udd25 STEP 1: Call early_init() FIRST - before ANY other imports\nearly_init()\n\n# \ud83d\udd25 STEP 2: Now safe to import other modules\nimport discord\nimport asyncio\nimport requests\nimport aiohttp\nimport gallery_dl\nimport yt_dlp\n# ... all other imports that use logging\n\n# \ud83d\udd25 STEP 3: Configure full logging features after imports\nfrom boss_bot.monitoring.logging import setup_boss_bot_logging\nfrom boss_bot.core.env import BossSettings\n\n# Initialize boss-bot settings\nsettings = BossSettings()\n\n# Configure logging with boss-bot integration\nlogger = setup_boss_bot_logging(settings)\n\n# \ud83d\udd25 STEP 4: Your boss-bot application code\nasync def main():\n    logger.info(\"Boss-bot starting...\")\n    # Your boss-bot code here\n</code></pre>"},{"location":"logging/#incorrect-implementation-order","title":"\u274c Incorrect Implementation Order","text":"<pre><code># \u274c WRONG: Importing modules before early init\nimport discord        # \u274c This sets up its own logging first\nimport asyncio        # \u274c This may use logging during import\nimport requests       # \u274c This configures urllib3 logging\nimport gallery_dl     # \u274c This may configure its own logging\nimport yt_dlp        # \u274c This configures its own logging\n\n# \u274c TOO LATE: Interceptor misses early logging calls\nfrom boss_bot.monitoring.logging import early_init\nearly_init()         # \u274c Already missed discord's and other libraries' logging setup\n</code></pre>"},{"location":"logging/#safety-features-explained","title":"\ud83d\udee1\ufe0f Safety Features Explained","text":""},{"location":"logging/#1-enqueuetrue-multiprocessing-safety","title":"1. <code>enqueue=True</code> - Multiprocessing Safety","text":"<p>What it does: - Routes log messages through a <code>multiprocessing.SimpleQueue()</code> - Prevents file corruption when multiple processes write to the same log file - Makes logging calls non-blocking - Uses dedicated worker thread for I/O operations</p> <p>When to use: - \u2705 Always for production applications - \u2705 Always when using multiprocessing - \u2705 Always for file sinks in concurrent environments</p> <pre><code>logger.add(\n    \"app.log\",\n    enqueue=True,    # \u2190 Prevents multiprocessing file corruption\n    rotation=\"10 MB\"\n)\n</code></pre>"},{"location":"logging/#2-catchtrue-exception-safety","title":"2. <code>catch=True</code> - Exception Safety","text":"<p>What it does: - Catches exceptions that occur within the logging system itself - Prevents logging errors from crashing your application - Displays error messages on stderr instead of propagating</p> <p>Example: <pre><code>logger.add(\n    broken_sink,     # Sink that might fail\n    catch=True       # \u2190 App continues running even if logging fails\n)\n</code></pre></p>"},{"location":"logging/#3-propagatefalse-duplicate-prevention","title":"3. <code>propagate=False</code> - Duplicate Prevention","text":"<p>What it does: - Prevents intercepted loggers from sending messages to parent loggers - Avoids duplicate log entries - Ensures clean single-path message flow</p> <pre><code>discord_logger = logging.getLogger(\"discord\")\ndiscord_logger.handlers = [intercept_handler]\ndiscord_logger.propagate = False  # \u2190 Prevents duplicates\n</code></pre>"},{"location":"logging/#4-frame-inspection-safety","title":"4. Frame Inspection Safety","text":"<p>Problem with naive approach: <pre><code># \u274c UNSAFE: Can fail with frozen/threaded imports\nframe = logging.currentframe()\nwhile frame.f_code.co_filename == logging.__file__:\n    frame = frame.f_back\n</code></pre></p> <p>Safe approach: <pre><code># \u2705 SAFE: Handles frozen imports and threading\nframe, depth = inspect.currentframe(), 0\nwhile frame:\n    filename = frame.f_code.co_filename\n    is_logging = filename == logging.__file__\n    is_frozen = \"importlib\" in filename and \"_bootstrap\" in filename\n    if depth &gt; 0 and not (is_logging or is_frozen):\n        break\n    frame = frame.f_back\n    depth += 1\n</code></pre></p>"},{"location":"logging/#testing-and-validation","title":"\ud83e\uddea Testing and Validation","text":""},{"location":"logging/#thread-safety-test","title":"Thread Safety Test","text":"<pre><code>def test_thread_safety():\n    \"\"\"Verify logging works correctly across multiple threads.\"\"\"\n    import threading\n    import time\n\n    def worker_thread(thread_id: int):\n        for i in range(10):\n            logger.info(f\"Thread {thread_id}: Message {i}\")\n            time.sleep(0.01)\n\n    # Start multiple threads\n    threads = []\n    for i in range(5):\n        thread = threading.Thread(target=worker_thread, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for completion\n    for thread in threads:\n        thread.join()\n\n    logger.success(\"Thread safety test passed!\")\n</code></pre>"},{"location":"logging/#multiprocessing-test","title":"Multiprocessing Test","text":"<pre><code>def test_multiprocessing():\n    \"\"\"Verify logging works correctly across multiple processes.\"\"\"\n    import multiprocessing\n\n    def worker_process(process_id: int):\n        # Each process needs its own logger setup\n        logger.add(\n            \"multiprocess.log\",\n            enqueue=True,    # \u2190 Critical for multiprocessing\n            format=\"{time} | PID:{process} | {level} | {message}\"\n        )\n\n        for i in range(10):\n            logger.info(f\"Process {process_id}: Message {i}\")\n\n    processes = []\n    for i in range(3):\n        p = multiprocessing.Process(target=worker_process, args=(i,))\n        processes.append(p)\n        p.start()\n\n    for p in processes:\n        p.join()\n</code></pre>"},{"location":"logging/#asyncawait-test","title":"Async/Await Test","text":"<pre><code>async def test_async_logging():\n    \"\"\"Verify logging works correctly in async environments.\"\"\"\n    import asyncio\n\n    async def async_worker(worker_id: int):\n        for i in range(10):\n            logger.info(f\"Async Worker {worker_id}: Message {i}\")\n            await asyncio.sleep(0.01)\n\n    # Run multiple async workers concurrently\n    tasks = [async_worker(i) for i in range(5)]\n    await asyncio.gather(*tasks)\n\n    logger.success(\"Async logging test passed!\")\n\n# Usage\nasyncio.run(test_async_logging())\n</code></pre>"},{"location":"logging/#advanced-configuration","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"logging/#production-file-logging-setup","title":"Production File Logging Setup","text":"<pre><code>def setup_production_logging():\n    \"\"\"Production-ready logging configuration.\"\"\"\n\n    # Console logging - structured for monitoring\n    logger.add(\n        sys.stderr,\n        level=\"INFO\",\n        format=\"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n               \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n               \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; | \"\n               \"&lt;level&gt;{message}&lt;/level&gt;\",\n        enqueue=True,\n        catch=True,\n        diagnose=False,  # \u2190 Disable in production for security\n        backtrace=False  # \u2190 Disable in production for security\n    )\n\n    # File logging - detailed for debugging\n    logger.add(\n        \"logs/app_{time:YYYY-MM-DD}.log\",\n        level=\"DEBUG\",\n        format=\"{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: &lt;8} | \"\n               \"{name}:{function}:{line} | {extra} | {message}\",\n        rotation=\"00:00\",      # \u2190 New file daily\n        retention=\"30 days\",   # \u2190 Keep 30 days\n        compression=\"gz\",      # \u2190 Compress old files\n        enqueue=True,          # \u2190 Multiprocessing safe\n        catch=True,\n        diagnose=True,         # \u2190 Enable for file logs\n        backtrace=True\n    )\n\n    # Error-only file for monitoring alerts\n    logger.add(\n        \"logs/errors_{time:YYYY-MM-DD}.log\",\n        level=\"ERROR\",\n        format=\"{time:YYYY-MM-DD HH:mm:ss.SSS} | {level} | \"\n               \"{name}:{function}:{line} | {message} | {exception}\",\n        rotation=\"100 MB\",\n        retention=\"90 days\",\n        enqueue=True,\n        catch=True,\n        backtrace=True,\n        diagnose=True\n    )\n</code></pre>"},{"location":"logging/#json-structured-logging","title":"JSON Structured Logging","text":"<pre><code>def setup_json_logging():\n    \"\"\"JSON structured logging for log aggregation systems.\"\"\"\n\n    def json_formatter(record):\n        \"\"\"Custom JSON formatter with structured data.\"\"\"\n        return json.dumps({\n            \"timestamp\": record[\"time\"].isoformat(),\n            \"level\": record[\"level\"].name,\n            \"logger\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line\": record[\"line\"],\n            \"message\": record[\"message\"],\n            \"module\": record[\"module\"],\n            \"process\": record[\"process\"].id,\n            \"thread\": record[\"thread\"].id,\n            \"extra\": record[\"extra\"]\n        })\n\n    logger.add(\n        \"logs/structured.jsonl\",\n        format=json_formatter,\n        level=\"INFO\",\n        enqueue=True,\n        catch=True,\n        serialize=False  # \u2190 We handle JSON ourselves\n    )\n</code></pre>"},{"location":"logging/#context-aware-logging","title":"Context-Aware Logging","text":"<pre><code>import contextvars\n\n# Context variables for request tracking\nrequest_id_var = contextvars.ContextVar('request_id', default='no-request')\nuser_id_var = contextvars.ContextVar('user_id', default='anonymous')\n\ndef context_formatter(record):\n    \"\"\"Add context variables to log records.\"\"\"\n    record[\"extra\"][\"request_id\"] = request_id_var.get()\n    record[\"extra\"][\"user_id\"] = user_id_var.get()\n    return (\n        \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n        \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n        \"REQ:{extra[request_id]} | USER:{extra[user_id]} | \"\n        \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; | \"\n        \"&lt;level&gt;{message}&lt;/level&gt;\"\n    )\n\n# Usage in boss-bot Discord command handlers\nasync def handle_discord_command(ctx):\n    request_id_var.set(f\"cmd-{uuid.uuid4().hex[:8]}\")\n    user_id_var.set(ctx.author.id)\n\n    logger.info(\"Processing Discord command\")  # Automatically includes context\n\n# Usage in boss-bot download operations\nasync def handle_download(url: str, user_id: int, guild_id: int):\n    request_id_var.set(f\"dl-{uuid.uuid4().hex[:8]}\")\n    user_id_var.set(user_id)\n\n    # Add boss-bot specific context\n    bound_logger = logger.bind(\n        guild_id=guild_id,\n        download_url=url,\n        component=\"download_manager\"\n    )\n    bound_logger.info(\"Starting download operation\")\n</code></pre>"},{"location":"logging/#common-pitfalls-and-solutions","title":"\ud83d\udea8 Common Pitfalls and Solutions","text":""},{"location":"logging/#problem-1-import-order-issues","title":"Problem 1: Import Order Issues","text":"<p>Symptom: Some logs from libraries don't appear in Loguru</p> <p>Cause: Library imported before <code>_early_init()</code> was called</p> <p>Solution: <pre><code># \u2705 Move early_init() to the very top\nfrom boss_bot.monitoring.logging import early_init\nearly_init()  # Must be first\n\n# Then import everything else\nimport discord\nimport gallery_dl\nimport yt_dlp\nimport langchain\n# ... other boss-bot dependencies\n</code></pre></p>"},{"location":"logging/#problem-2-multiprocessing-file-corruption","title":"Problem 2: Multiprocessing File Corruption","text":"<p>Symptom: Garbled text in log files when using multiprocessing</p> <p>Cause: Multiple processes writing to same file without coordination</p> <p>Solution: <pre><code># \u2705 Always use enqueue=True for file sinks\nlogger.add(\"shared.log\", enqueue=True)\n\n# \u2705 Or use separate files per process\nlogger.add(f\"process_{os.getpid()}.log\")\n</code></pre></p>"},{"location":"logging/#problem-3-logging-in-signal-handlers","title":"Problem 3: Logging in Signal Handlers","text":"<p>Symptom: Deadlocks when logging from signal handlers</p> <p>Cause: Signal handlers interrupt normal execution flow</p> <p>Solution: <pre><code># \u2705 Use enqueue=True to make logging non-blocking\nlogger.add(sys.stderr, enqueue=True, catch=True)\n\ndef signal_handler(signum, frame):\n    logger.warning(f\"Received signal {signum}\")  # Safe with enqueue=True\n</code></pre></p>"},{"location":"logging/#problem-4-memory-leaks-in-long-running-applications","title":"Problem 4: Memory Leaks in Long-Running Applications","text":"<p>Symptom: Memory usage grows over time</p> <p>Cause: Not calling <code>logger.complete()</code> before shutdown</p> <p>Solution: <pre><code>import atexit\n\ndef cleanup_logging():\n    logger.info(\"Shutting down logging...\")\n    logger.complete()  # \u2190 Flush all queued messages\n\natexit.register(cleanup_logging)\n</code></pre></p>"},{"location":"logging/#problem-5-lost-exception-context","title":"Problem 5: Lost Exception Context","text":"<p>Symptom: Exception tracebacks don't show variable values</p> <p>Cause: <code>diagnose=False</code> or production security settings</p> <p>Solution: <pre><code># \u2705 Development: Enable full diagnostics\nlogger.add(\"debug.log\", diagnose=True, backtrace=True)\n\n# \u2705 Production: Separate diagnostic file with restricted access\nlogger.add(\n    \"debug.log\",\n    diagnose=True,\n    backtrace=True,\n    level=\"ERROR\",  # Only for errors\n    # Set file permissions to 600 (owner only)\n)\n</code></pre></p>"},{"location":"logging/#checklist-for-implementation","title":"\ud83d\udccb Checklist for Implementation","text":""},{"location":"logging/#pre-implementation","title":"Pre-Implementation \u2705","text":"<ul> <li> Understand your application's import structure</li> <li> Identify all libraries that use logging</li> <li> Plan early initialization call placement</li> <li> Consider multiprocessing requirements</li> </ul>"},{"location":"logging/#implementation","title":"Implementation \u2705","text":"<ul> <li> Add <code>_early_init()</code> function to logging module</li> <li> Update <code>InterceptHandler</code> with thread-safe frame inspection</li> <li> Add <code>enqueue=True</code> to all handlers</li> <li> Set <code>propagate=False</code> on intercepted loggers</li> <li> Configure <code>catch=True</code> for error resilience</li> </ul>"},{"location":"logging/#testing","title":"Testing \u2705","text":"<ul> <li> Test thread safety with concurrent logging</li> <li> Test multiprocessing with shared log files</li> <li> Test async/await logging patterns</li> <li> Test exception handling and formatting</li> <li> Test early initialization order</li> </ul>"},{"location":"logging/#production-deployment","title":"Production Deployment \u2705","text":"<ul> <li> Disable <code>diagnose=True</code> for security (except debug files)</li> <li> Set up log rotation and retention policies</li> <li> Configure monitoring/alerting on error logs</li> <li> Test graceful shutdown with <code>logger.complete()</code></li> <li> Monitor memory usage in long-running processes</li> </ul>"},{"location":"logging/#additional-resources","title":"\ud83d\udcda Additional Resources","text":""},{"location":"logging/#loguru-documentation","title":"Loguru Documentation","text":"<ul> <li>Official Loguru Documentation</li> <li>Recipes and Examples</li> <li>API Reference</li> </ul>"},{"location":"logging/#thread-safety-references","title":"Thread Safety References","text":"<ul> <li>Python Logging Thread Safety</li> <li>Multiprocessing Logging Best Practices</li> </ul>"},{"location":"logging/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Loguru Performance Tips</li> <li>Async Logging Patterns</li> </ul>"},{"location":"logging/#summary","title":"\ud83c\udfaf Summary","text":"<p>This logging setup provides:</p> <ul> <li>\u2705 Complete interception of all standard library logging calls</li> <li>\u2705 Thread safety for concurrent environments</li> <li>\u2705 Multiprocessing safety with enqueue-based message passing</li> <li>\u2705 Async/await compatibility for modern Python applications</li> <li>\u2705 Production-ready error handling and recovery</li> <li>\u2705 Early initialization to capture all logs from import time</li> <li>\u2705 Zero-deadlock design with non-blocking operations</li> </ul> <p>The key insight is that timing matters: call <code>_early_init()</code> before importing any modules that use logging, and your application will have bulletproof, thread-safe logging from the very first line of code execution.</p>"},{"location":"logging_testing/","title":"Loguru Testing with Logot &amp; Pytest Guide","text":""},{"location":"logging_testing/#overview","title":"\ud83c\udfaf Overview","text":"<p>This guide provides comprehensive instructions for testing Loguru-based logging using Logot, a modern log testing library that offers superior capabilities compared to pytest's built-in <code>caplog</code> fixture. Logot provides advanced log message matching, better async support, and cleaner assertion syntax.</p>"},{"location":"logging_testing/#requirements-compatibility","title":"\ud83d\udea8 Requirements &amp; Compatibility","text":""},{"location":"logging_testing/#python-version-requirements","title":"Python Version Requirements","text":"<ul> <li>Loguru: Python 3.5+</li> <li>Logot: Python 3.8+</li> <li>Compatibility: Use Logot only in projects running Python 3.8+</li> </ul>"},{"location":"logging_testing/#when-to-use-logot-vs-caplog","title":"When to Use Logot vs caplog","text":"<p>Use Logot when: - \u2705 You need advanced log message pattern matching - \u2705 Testing complex logging scenarios with threading/async - \u2705 You want cleaner, more expressive test assertions - \u2705 Testing structured logging with context/binding - \u2705 Running Python 3.8+</p> <p>Use caplog when: - \u2705 You need Python 3.5-3.7 compatibility - \u2705 Simple log testing requirements - \u2705 Minimal dependencies preferred - \u2705 Basic log level and message verification</p>"},{"location":"logging_testing/#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"logging_testing/#standard-installation","title":"Standard Installation","text":"<pre><code># Using uv (recommended)\nuv add --dev 'logot[pytest]'\n\n# Using pip\npip install 'logot[pytest]'\n</code></pre>"},{"location":"logging_testing/#loguru-specific-installation","title":"Loguru-Specific Installation","text":"<pre><code># With Loguru integration\nuv add --dev 'logot[loguru,pytest]'\n\n# Or separate\npip install 'logot[loguru]' 'logot[pytest]'\n</code></pre>"},{"location":"logging_testing/#development-setup","title":"Development Setup","text":"<pre><code># Install all testing dependencies\nuv add --dev pytest 'logot[pytest,loguru]' pytest-asyncio pytest-cov\n\n# Verify installation\nuv run python -c \"import logot; print('Logot installed successfully')\"\n</code></pre>"},{"location":"logging_testing/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"logging_testing/#1-pytest-configuration-pytestini","title":"1. Pytest Configuration (pytest.ini)","text":"<pre><code>[tool:pytest]\n# Basic configuration\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\n\n# Logot configuration\nlogot_capturer = logot.loguru.LoguruCapturer\nlogot_level = DEBUG\nlogot_timeout = 5.0\nlogot_name = tests\n\n# Async support\nasyncio_mode = auto\n\n# Coverage\naddopts =\n    --strict-markers\n    --strict-config\n    --cov=my_intercept_logger\n    --cov-report=term-missing\n    --cov-report=html:htmlcov\n    --cov-fail-under=80\n</code></pre>"},{"location":"logging_testing/#2-pyprojecttoml-configuration","title":"2. pyproject.toml Configuration","text":"<pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\n\n# Logot configuration for Loguru\nlogot_capturer = \"logot.loguru.LoguruCapturer\"\nlogot_level = \"DEBUG\"\nlogot_timeout = 5.0\nlogot_name = \"tests\"\n\n# Async support\nasyncio_mode = \"auto\"\n\n# Test output\naddopts = [\n    \"--strict-markers\",\n    \"--strict-config\",\n    \"--tb=short\",\n    \"--cov=my_intercept_logger\",\n    \"--cov-report=term-missing\"\n]\n\nfilterwarnings = [\n    \"error\",\n    \"ignore::DeprecationWarning\",\n    \"ignore::PendingDeprecationWarning\"\n]\n\nmarkers = [\n    \"slow: marks tests as slow\",\n    \"integration: marks tests as integration tests\",\n    \"unit: marks tests as unit tests\"\n]\n</code></pre>"},{"location":"logging_testing/#3-conftestpy-setup","title":"3. conftest.py Setup","text":"<pre><code>\"\"\"Pytest configuration and fixtures for Loguru testing.\"\"\"\n\nfrom __future__ import annotations\n\nimport pytest\nimport threading\nfrom typing import Generator, Any\nfrom unittest.mock import patch\n\nfrom logot import Logot, logged\nfrom logot.loguru import LoguruCapturer\nfrom loguru import logger\n\n# Import your logging setup\nfrom my_intercept_logger import _early_init, global_log_config, InterceptHandler\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef setup_logging() -&gt; None:\n    \"\"\"Set up logging for the entire test session.\"\"\"\n    # Initialize our thread-safe logging\n    _early_init()\n\n\n@pytest.fixture\ndef reset_logger() -&gt; Generator[None, None, None]:\n    \"\"\"Reset logger state between tests.\"\"\"\n    # Remove all handlers before test\n    logger.remove()\n\n    yield\n\n    # Clean up after test\n    logger.remove()\n    # Re-add default handler for next test\n    logger.add(\n        sys.stderr,\n        level=\"DEBUG\",\n        format=\"&lt;green&gt;{time}&lt;/green&gt; | &lt;level&gt;{level}&lt;/level&gt; | {message}\",\n        enqueue=True,\n        catch=True\n    )\n\n\n@pytest.fixture\ndef logot_instance() -&gt; Logot:\n    \"\"\"Provide a fresh Logot instance for testing.\"\"\"\n    return Logot(capturer=LoguruCapturer)\n\n\n@pytest.fixture\ndef mock_external_service():\n    \"\"\"Mock external services that might use logging.\"\"\"\n    with patch('my_app.external_service') as mock:\n        yield mock\n\n\n# Custom markers for different test types\npytest.mark.unit = pytest.mark.unit\npytest.mark.integration = pytest.mark.integration\npytest.mark.slow = pytest.mark.slow\n</code></pre>"},{"location":"logging_testing/#basic-testing-patterns","title":"\ud83e\uddea Basic Testing Patterns","text":""},{"location":"logging_testing/#1-simple-log-message-testing","title":"1. Simple Log Message Testing","text":"<pre><code>\"\"\"Basic log message testing with Logot.\"\"\"\n\nimport pytest\nfrom logot import Logot, logged\nfrom loguru import logger\n\n\ndef test_basic_logging(logot: Logot) -&gt; None:\n    \"\"\"Test basic log message capture and assertion.\"\"\"\n\n    # Perform action that logs\n    logger.info(\"User logged in successfully\")\n\n    # Assert the log was captured\n    logot.assert_logged(logged.info(\"User logged in successfully\"))\n\n\ndef test_log_levels(logot: Logot) -&gt; None:\n    \"\"\"Test different log levels.\"\"\"\n\n    logger.debug(\"Debug message\")\n    logger.info(\"Info message\")\n    logger.warning(\"Warning message\")\n    logger.error(\"Error message\")\n    logger.critical(\"Critical message\")\n\n    # Assert each level\n    logot.assert_logged(logged.debug(\"Debug message\"))\n    logot.assert_logged(logged.info(\"Info message\"))\n    logot.assert_logged(logged.warning(\"Warning message\"))\n    logot.assert_logged(logged.error(\"Error message\"))\n    logot.assert_logged(logged.critical(\"Critical message\"))\n\n\ndef test_log_with_variables(logot: Logot) -&gt; None:\n    \"\"\"Test logging with variable interpolation.\"\"\"\n\n    user_id = 12345\n    username = \"alice\"\n\n    logger.info(\"User {user} (ID: {id}) logged in\", user=username, id=user_id)\n\n    # Use pattern matching for dynamic content\n    logot.assert_logged(logged.info(\"User alice (ID: 12345) logged in\"))\n\n    # Alternative: Use % patterns for flexible matching\n    logot.assert_logged(logged.info(\"User % (ID: %) logged in\"))\n\n\ndef test_exception_logging(logot: Logot) -&gt; None:\n    \"\"\"Test exception logging with tracebacks.\"\"\"\n\n    try:\n        raise ValueError(\"Something went wrong\")\n    except Exception:\n        logger.exception(\"An error occurred while processing\")\n\n    # Assert exception was logged\n    logot.assert_logged(logged.error(\"An error occurred while processing\"))\n</code></pre>"},{"location":"logging_testing/#2-advanced-pattern-matching","title":"2. Advanced Pattern Matching","text":"<pre><code>\"\"\"Advanced log message pattern matching.\"\"\"\n\nimport pytest\nfrom logot import Logot, logged\nfrom loguru import logger\n\n\ndef test_pattern_matching(logot: Logot) -&gt; None:\n    \"\"\"Test advanced pattern matching capabilities.\"\"\"\n\n    # Log with dynamic data\n    session_id = \"sess_abc123\"\n    timestamp = \"2024-01-15T10:30:00\"\n\n    logger.info(\"Session {session} started at {time}\",\n                session=session_id, time=timestamp)\n\n    # Use % wildcard for any value\n    logot.assert_logged(logged.info(\"Session % started at %\"))\n\n    # Use %{name} for named wildcards\n    logot.assert_logged(logged.info(\"Session %{session} started at %{time}\"))\n\n\ndef test_regex_patterns(logot: Logot) -&gt; None:\n    \"\"\"Test regex pattern matching.\"\"\"\n\n    import re\n\n    logger.info(\"Request ID: req_789xyz processing\")\n\n    # Use regex for complex patterns\n    pattern = re.compile(r\"Request ID: req_\\w+ processing\")\n    logot.assert_logged(logged.info(pattern))\n\n\ndef test_multiple_log_assertions(logot: Logot) -&gt; None:\n    \"\"\"Test multiple log assertions in sequence.\"\"\"\n\n    def process_user_registration(username: str, email: str) -&gt; None:\n        logger.info(\"Starting user registration for {user}\", user=username)\n        logger.debug(\"Validating email: {email}\", email=email)\n        logger.info(\"User {user} registered successfully\", user=username)\n\n    # Execute function\n    process_user_registration(\"alice\", \"alice@example.com\")\n\n    # Assert log sequence\n    logot.assert_logged(logged.info(\"Starting user registration for alice\"))\n    logot.assert_logged(logged.debug(\"Validating email: alice@example.com\"))\n    logot.assert_logged(logged.info(\"User alice registered successfully\"))\n\n\ndef test_log_not_present(logot: Logot) -&gt; None:\n    \"\"\"Test asserting that certain logs are NOT present.\"\"\"\n\n    logger.info(\"Normal operation\")\n\n    # Assert what was logged\n    logot.assert_logged(logged.info(\"Normal operation\"))\n\n    # Assert what was NOT logged\n    logot.assert_not_logged(logged.error(\"Error occurred\"))\n    logot.assert_not_logged(logged.warning(\"Warning message\"))\n</code></pre>"},{"location":"logging_testing/#testing-loguru-specific-features","title":"\ud83c\udfad Testing Loguru-Specific Features","text":""},{"location":"logging_testing/#1-testing-bound-loggers","title":"1. Testing Bound Loggers","text":"<pre><code>\"\"\"Testing Loguru's bind() functionality.\"\"\"\n\nimport pytest\nfrom logot import Logot, logged\nfrom loguru import logger\n\n\ndef test_bound_logger(logot: Logot) -&gt; None:\n    \"\"\"Test logging with bound context.\"\"\"\n\n    # Create bound logger\n    bound_logger = logger.bind(\n        request_id=\"req_123\",\n        user_id=456,\n        component=\"auth\"\n    )\n\n    bound_logger.info(\"User authentication started\")\n    bound_logger.success(\"Authentication completed\")\n\n    # Assert logs with bound context\n    # Note: Exact format depends on your formatter\n    logot.assert_logged(logged.info(\"User authentication started\"))\n    logot.assert_logged(logged.info(\"Authentication completed\"))\n\n\ndef test_contextualize(logot: Logot) -&gt; None:\n    \"\"\"Test Loguru's contextualize() functionality.\"\"\"\n\n    def authenticate_user(user_id: int) -&gt; None:\n        with logger.contextualize(user_id=user_id, operation=\"auth\"):\n            logger.info(\"Starting authentication\")\n            logger.info(\"Checking credentials\")\n            logger.success(\"Authentication successful\")\n\n    # Execute with context\n    authenticate_user(789)\n\n    # Assert logs (context is automatically included)\n    logot.assert_logged(logged.info(\"Starting authentication\"))\n    logot.assert_logged(logged.info(\"Checking credentials\"))\n    logot.assert_logged(logged.info(\"Authentication successful\"))\n\n\ndef test_structured_logging(logot: Logot) -&gt; None:\n    \"\"\"Test structured logging with extra data.\"\"\"\n\n    def process_order(order_id: str, amount: float) -&gt; None:\n        logger.bind(\n            order_id=order_id,\n            amount=amount,\n            currency=\"USD\"\n        ).info(\"Processing payment\")\n\n    process_order(\"order_abc\", 99.99)\n\n    # Assert the log message (extra data included based on formatter)\n    logot.assert_logged(logged.info(\"Processing payment\"))\n</code></pre>"},{"location":"logging_testing/#2-testing-custom-sinks","title":"2. Testing Custom Sinks","text":"<pre><code>\"\"\"Testing custom Loguru sinks.\"\"\"\n\nimport pytest\nfrom logot import Logot, logged\nfrom loguru import logger\nfrom unittest.mock import Mock, patch\nfrom typing import List\nimport json\n\n\ndef test_custom_sink_integration(logot: Logot) -&gt; None:\n    \"\"\"Test logging through custom sinks.\"\"\"\n\n    # Custom sink for testing\n    captured_messages: List[str] = []\n\n    def test_sink(message: str) -&gt; None:\n        captured_messages.append(message.strip())\n\n    # Add custom sink\n    sink_id = logger.add(test_sink, serialize=False, level=\"INFO\")\n\n    try:\n        logger.info(\"Test message for custom sink\")\n        logger.error(\"Error message for custom sink\")\n\n        # Verify sink received messages\n        assert len(captured_messages) == 2\n        assert \"Test message for custom sink\" in captured_messages[0]\n        assert \"Error message for custom sink\" in captured_messages[1]\n\n        # Also verify through Logot\n        logot.assert_logged(logged.info(\"Test message for custom sink\"))\n        logot.assert_logged(logged.error(\"Error message for custom sink\"))\n\n    finally:\n        logger.remove(sink_id)\n\n\ndef test_json_sink(logot: Logot) -&gt; None:\n    \"\"\"Test JSON serialization sink.\"\"\"\n\n    json_messages: List[dict] = []\n\n    def json_sink(message) -&gt; None:\n        # message.record contains the full record\n        record = message.record\n        json_data = {\n            \"timestamp\": record[\"time\"].isoformat(),\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record.get(\"extra\", {})\n        }\n        json_messages.append(json_data)\n\n    # Add JSON sink\n    sink_id = logger.add(json_sink, level=\"DEBUG\")\n\n    try:\n        logger.bind(user_id=123).info(\"User action performed\")\n\n        # Verify JSON structure\n        assert len(json_messages) == 1\n        json_record = json_messages[0]\n\n        assert json_record[\"level\"] == \"INFO\"\n        assert json_record[\"message\"] == \"User action performed\"\n        assert json_record[\"extra\"][\"user_id\"] == 123\n\n        # Also verify through Logot\n        logot.assert_logged(logged.info(\"User action performed\"))\n\n    finally:\n        logger.remove(sink_id)\n\n\n@pytest.mark.asyncio\nasync def test_async_sink(logot: Logot) -&gt; None:\n    \"\"\"Test asynchronous sink functionality.\"\"\"\n\n    async_messages: List[str] = []\n\n    async def async_sink(message) -&gt; None:\n        # Simulate async operation\n        import asyncio\n        await asyncio.sleep(0.01)\n        async_messages.append(message.record[\"message\"])\n\n    # Add async sink\n    sink_id = logger.add(async_sink, level=\"INFO\")\n\n    try:\n        logger.info(\"Async sink test message\")\n\n        # Wait for async processing\n        import asyncio\n        await asyncio.sleep(0.1)\n\n        # Verify async sink received message\n        assert len(async_messages) == 1\n        assert async_messages[0] == \"Async sink test message\"\n\n        # Verify through Logot\n        logot.assert_logged(logged.info(\"Async sink test message\"))\n\n    finally:\n        logger.remove(sink_id)\n</code></pre>"},{"location":"logging_testing/#testing-threading-concurrency","title":"\ud83d\udd04 Testing Threading &amp; Concurrency","text":""},{"location":"logging_testing/#1-thread-safe-logging-tests","title":"1. Thread-Safe Logging Tests","text":"<pre><code>\"\"\"Testing thread-safe logging scenarios.\"\"\"\n\nimport pytest\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom logot import Logot, logged\nfrom loguru import logger\n\n\ndef test_threading_safety(logot: Logot) -&gt; None:\n    \"\"\"Test that logging works correctly across multiple threads.\"\"\"\n\n    def worker_thread(thread_id: int, iterations: int) -&gt; None:\n        for i in range(iterations):\n            logger.info(\"Thread {thread}: iteration {iter}\",\n                       thread=thread_id, iter=i)\n            time.sleep(0.001)  # Small delay\n\n    # Start multiple threads\n    threads = []\n    for thread_id in range(3):\n        thread = threading.Thread(\n            target=worker_thread,\n            args=(thread_id, 5)\n        )\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n\n    # Verify all messages were logged\n    # Should have 3 threads \u00d7 5 iterations = 15 messages\n    for thread_id in range(3):\n        for iteration in range(5):\n            logot.assert_logged(\n                logged.info(f\"Thread {thread_id}: iteration {iteration}\")\n            )\n\n\ndef test_thread_pool_logging(logot: Logot) -&gt; None:\n    \"\"\"Test logging with ThreadPoolExecutor.\"\"\"\n\n    def process_item(item_id: int) -&gt; int:\n        logger.info(\"Processing item {id}\", id=item_id)\n        time.sleep(0.01)  # Simulate work\n        logger.success(\"Completed item {id}\", id=item_id)\n        return item_id\n\n    # Process items in thread pool\n    item_ids = [1, 2, 3, 4, 5]\n\n    with ThreadPoolExecutor(max_workers=3) as executor:\n        futures = [executor.submit(process_item, item_id) for item_id in item_ids]\n\n        # Wait for completion\n        for future in as_completed(futures):\n            result = future.result()\n\n    # Verify all items were logged\n    for item_id in item_ids:\n        logot.assert_logged(logged.info(f\"Processing item {item_id}\"))\n        logot.assert_logged(logged.info(f\"Completed item {item_id}\"))\n\n\ndef test_early_init_thread_safety(logot: Logot) -&gt; None:\n    \"\"\"Test our early initialization is thread-safe.\"\"\"\n\n    from my_intercept_logger import _early_init\n\n    def call_early_init(thread_id: int) -&gt; str:\n        _early_init()\n        logger.info(\"Early init called by thread {id}\", id=thread_id)\n        return f\"thread_{thread_id}\"\n\n    # Call early init from multiple threads simultaneously\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = [executor.submit(call_early_init, i) for i in range(10)]\n        results = [future.result() for future in as_completed(futures)]\n\n    # Verify all threads logged successfully\n    for thread_id in range(10):\n        logot.assert_logged(logged.info(f\"Early init called by thread {thread_id}\"))\n\n    # Should have 10 results\n    assert len(results) == 10\n</code></pre>"},{"location":"logging_testing/#2-async-logging-tests","title":"2. Async Logging Tests","text":"<pre><code>\"\"\"Testing asynchronous logging scenarios.\"\"\"\n\nimport pytest\nimport asyncio\nfrom logot import Logot, logged\nfrom loguru import logger\n\n\n@pytest.mark.asyncio\nasync def test_async_logging(logot: Logot) -&gt; None:\n    \"\"\"Test basic async logging functionality.\"\"\"\n\n    async def async_operation(operation_id: str) -&gt; None:\n        logger.info(\"Starting async operation {id}\", id=operation_id)\n        await asyncio.sleep(0.01)  # Simulate async work\n        logger.success(\"Completed async operation {id}\", id=operation_id)\n\n    # Execute async operation\n    await async_operation(\"op_123\")\n\n    # Verify logs\n    logot.assert_logged(logged.info(\"Starting async operation op_123\"))\n    logot.assert_logged(logged.info(\"Completed async operation op_123\"))\n\n\n@pytest.mark.asyncio\nasync def test_concurrent_async_logging(logot: Logot) -&gt; None:\n    \"\"\"Test concurrent async operations with logging.\"\"\"\n\n    async def async_worker(worker_id: int) -&gt; None:\n        logger.info(\"Async worker {id} starting\", id=worker_id)\n        await asyncio.sleep(0.01)\n        logger.info(\"Async worker {id} finished\", id=worker_id)\n\n    # Run multiple async workers concurrently\n    tasks = [async_worker(i) for i in range(5)]\n    await asyncio.gather(*tasks)\n\n    # Verify all workers logged\n    for worker_id in range(5):\n        logot.assert_logged(logged.info(f\"Async worker {worker_id} starting\"))\n        logot.assert_logged(logged.info(f\"Async worker {worker_id} finished\"))\n\n\n@pytest.mark.asyncio\nasync def test_async_context_logging(logot: Logot) -&gt; None:\n    \"\"\"Test async logging with context management.\"\"\"\n\n    async def process_request(request_id: str) -&gt; None:\n        with logger.contextualize(request_id=request_id):\n            logger.info(\"Request received\")\n            await asyncio.sleep(0.01)\n            logger.info(\"Request processed\")\n\n    # Process multiple requests\n    request_ids = [\"req_1\", \"req_2\", \"req_3\"]\n\n    tasks = [process_request(req_id) for req_id in request_ids]\n    await asyncio.gather(*tasks)\n\n    # Verify all requests logged\n    for _ in request_ids:\n        logot.assert_logged(logged.info(\"Request received\"))\n        logot.assert_logged(logged.info(\"Request processed\"))\n</code></pre>"},{"location":"logging_testing/#testing-intercepthandler-integration","title":"\ud83c\udfaf Testing InterceptHandler Integration","text":""},{"location":"logging_testing/#1-standard-library-integration-tests","title":"1. Standard Library Integration Tests","text":"<pre><code>\"\"\"Testing InterceptHandler with standard library logging.\"\"\"\n\nimport pytest\nimport logging\nfrom logot import Logot, logged\nfrom loguru import logger\nfrom my_intercept_logger import InterceptHandler, global_log_config\n\n\ndef test_intercept_handler_basic(logot: Logot) -&gt; None:\n    \"\"\"Test basic InterceptHandler functionality.\"\"\"\n\n    # Set up standard logging with our interceptor\n    std_logger = logging.getLogger(\"test_module\")\n    std_logger.setLevel(logging.DEBUG)\n    std_logger.addHandler(InterceptHandler())\n    std_logger.propagate = False\n\n    # Log using standard logging\n    std_logger.info(\"Standard logging message\")\n    std_logger.error(\"Standard error message\")\n    std_logger.debug(\"Standard debug message\")\n\n    # Verify messages were intercepted by Loguru\n    logot.assert_logged(logged.info(\"Standard logging message\"))\n    logot.assert_logged(logged.error(\"Standard error message\"))\n    logot.assert_logged(logged.debug(\"Standard debug message\"))\n\n\ndef test_intercept_third_party_libraries(logot: Logot) -&gt; None:\n    \"\"\"Test intercepting logs from third-party libraries.\"\"\"\n\n    # Configure interceptor for common libraries\n    intercept_handler = InterceptHandler()\n\n    loggers_to_intercept = [\n        \"requests\",\n        \"urllib3\",\n        \"asyncio\",\n        \"multiprocessing\"\n    ]\n\n    for logger_name in loggers_to_intercept:\n        lib_logger = logging.getLogger(logger_name)\n        lib_logger.handlers = [intercept_handler]\n        lib_logger.propagate = False\n\n    # Simulate library logging\n    logging.getLogger(\"requests\").info(\"HTTP request started\")\n    logging.getLogger(\"urllib3\").debug(\"Connection pool created\")\n    logging.getLogger(\"asyncio\").warning(\"Event loop slow callback\")\n\n    # Verify interception\n    logot.assert_logged(logged.info(\"HTTP request started\"))\n    logot.assert_logged(logged.debug(\"Connection pool created\"))\n    logot.assert_logged(logged.warning(\"Event loop slow callback\"))\n\n\ndef test_exception_interception(logot: Logot) -&gt; None:\n    \"\"\"Test exception logging through InterceptHandler.\"\"\"\n\n    std_logger = logging.getLogger(\"exception_test\")\n    std_logger.addHandler(InterceptHandler())\n    std_logger.propagate = False\n\n    try:\n        raise ValueError(\"Test exception for interception\")\n    except Exception:\n        std_logger.exception(\"Caught exception in standard logging\")\n\n    # Verify exception was intercepted\n    logot.assert_logged(logged.error(\"Caught exception in standard logging\"))\n\n\ndef test_global_log_config_integration(logot: Logot) -&gt; None:\n    \"\"\"Test our global_log_config function with testing.\"\"\"\n\n    # Configure logging with our setup\n    configured_logger = global_log_config(\n        log_level=logging.DEBUG,\n        json=False\n    )\n\n    # Test various logging operations\n    configured_logger.info(\"Configured logger test\")\n\n    # Test standard library integration\n    std_logger = logging.getLogger(\"integrated_test\")\n    std_logger.info(\"Standard library after configuration\")\n\n    # Verify both work\n    logot.assert_logged(logged.info(\"Configured logger test\"))\n    logot.assert_logged(logged.info(\"Standard library after configuration\"))\n</code></pre>"},{"location":"logging_testing/#performance-load-testing","title":"\ud83d\ude80 Performance &amp; Load Testing","text":""},{"location":"logging_testing/#1-performance-tests","title":"1. Performance Tests","text":"<pre><code>\"\"\"Performance testing for logging infrastructure.\"\"\"\n\nimport pytest\nimport time\nfrom logot import Logot, logged\nfrom loguru import logger\n\n\n@pytest.mark.slow\ndef test_logging_performance(logot: Logot) -&gt; None:\n    \"\"\"Test logging performance under load.\"\"\"\n\n    message_count = 1000\n    start_time = time.time()\n\n    # Log many messages quickly\n    for i in range(message_count):\n        logger.info(\"Performance test message {i}\", i=i)\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    # Performance assertions\n    assert duration &lt; 5.0, f\"Logging {message_count} messages took {duration:.2f}s\"\n\n    # Verify a sample of messages were logged\n    logot.assert_logged(logged.info(\"Performance test message 0\"))\n    logot.assert_logged(logged.info(\"Performance test message 999\"))\n\n    print(f\"Logged {message_count} messages in {duration:.3f}s \"\n          f\"({message_count/duration:.0f} msg/s)\")\n\n\n@pytest.mark.slow\ndef test_concurrent_logging_performance(logot: Logot) -&gt; None:\n    \"\"\"Test performance with concurrent logging.\"\"\"\n\n    import threading\n\n    message_count = 100\n    thread_count = 10\n\n    def log_worker(worker_id: int) -&gt; None:\n        for i in range(message_count):\n            logger.info(\"Worker {worker}: message {msg}\",\n                       worker=worker_id, msg=i)\n\n    start_time = time.time()\n\n    # Start concurrent threads\n    threads = []\n    for worker_id in range(thread_count):\n        thread = threading.Thread(target=log_worker, args=(worker_id,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for completion\n    for thread in threads:\n        thread.join()\n\n    end_time = time.time()\n    duration = end_time - start_time\n    total_messages = message_count * thread_count\n\n    print(f\"Logged {total_messages} messages across {thread_count} threads \"\n          f\"in {duration:.3f}s ({total_messages/duration:.0f} msg/s)\")\n\n    # Verify sample messages\n    logot.assert_logged(logged.info(\"Worker 0: message 0\"))\n    logot.assert_logged(logged.info(\"Worker 9: message 99\"))\n\n\ndef test_enqueue_performance(logot: Logot) -&gt; None:\n    \"\"\"Test performance impact of enqueue=True.\"\"\"\n\n    import tempfile\n    import os\n\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n        log_file = f.name\n\n    try:\n        # Add file sink with enqueue\n        sink_id = logger.add(\n            log_file,\n            level=\"DEBUG\",\n            enqueue=True,  # This is what we're testing\n            catch=True\n        )\n\n        message_count = 500\n        start_time = time.time()\n\n        for i in range(message_count):\n            logger.info(\"Enqueue test message {i}\", i=i)\n\n        # Complete all queued operations\n        logger.complete()\n\n        end_time = time.time()\n        duration = end_time - start_time\n\n        print(f\"Enqueued logging: {message_count} messages in {duration:.3f}s\")\n\n        # Verify messages were logged\n        logot.assert_logged(logged.info(\"Enqueue test message 0\"))\n        logot.assert_logged(logged.info(\"Enqueue test message 499\"))\n\n        logger.remove(sink_id)\n\n    finally:\n        if os.path.exists(log_file):\n            os.unlink(log_file)\n</code></pre>"},{"location":"logging_testing/#advanced-testing-scenarios","title":"\ud83d\udd27 Advanced Testing Scenarios","text":""},{"location":"logging_testing/#1-configuration-testing","title":"1. Configuration Testing","text":"<pre><code>\"\"\"Testing different logging configurations.\"\"\"\n\nimport pytest\nimport tempfile\nimport os\nfrom logot import Logot, logged\nfrom loguru import logger\n\n\ndef test_file_logging_configuration(logot: Logot, reset_logger) -&gt; None:\n    \"\"\"Test file logging configuration.\"\"\"\n\n    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.log') as f:\n        log_file = f.name\n\n    try:\n        # Configure file logging\n        logger.add(\n            log_file,\n            level=\"DEBUG\",\n            format=\"{time} | {level} | {message}\",\n            rotation=\"1 MB\",\n            retention=\"7 days\",\n            enqueue=True\n        )\n\n        logger.info(\"File logging test message\")\n        logger.error(\"File error message\")\n\n        # Complete to ensure file is written\n        logger.complete()\n\n        # Verify through Logot\n        logot.assert_logged(logged.info(\"File logging test message\"))\n        logot.assert_logged(logged.error(\"File error message\"))\n\n        # Verify file was written\n        with open(log_file, 'r') as f:\n            file_content = f.read()\n            assert \"File logging test message\" in file_content\n            assert \"File error message\" in file_content\n\n    finally:\n        if os.path.exists(log_file):\n            os.unlink(log_file)\n\n\ndef test_json_serialization(logot: Logot, reset_logger) -&gt; None:\n    \"\"\"Test JSON serialization configuration.\"\"\"\n\n    json_messages = []\n\n    def json_collector(message) -&gt; None:\n        json_messages.append(message.record)\n\n    # Add JSON sink\n    logger.add(\n        json_collector,\n        serialize=True,  # This enables JSON serialization\n        level=\"INFO\"\n    )\n\n    logger.bind(user_id=123, action=\"login\").info(\"User authentication\")\n\n    # Verify through Logot\n    logot.assert_logged(logged.info(\"User authentication\"))\n\n    # Verify JSON structure\n    assert len(json_messages) == 1\n    record = json_messages[0]\n    assert record[\"message\"] == \"User authentication\"\n    assert record[\"extra\"][\"user_id\"] == 123\n    assert record[\"extra\"][\"action\"] == \"login\"\n\n\ndef test_filtering_configuration(logot: Logot, reset_logger) -&gt; None:\n    \"\"\"Test log filtering configuration.\"\"\"\n\n    def error_only_filter(record) -&gt; bool:\n        return record[\"level\"].no &gt;= 40  # ERROR and above\n\n    # Add filtered sink\n    logger.add(\n        lambda msg: None,  # Dummy sink\n        filter=error_only_filter,\n        level=\"DEBUG\"\n    )\n\n    # Log at different levels\n    logger.debug(\"Debug message\")      # Should be filtered out\n    logger.info(\"Info message\")        # Should be filtered out\n    logger.warning(\"Warning message\")  # Should be filtered out\n    logger.error(\"Error message\")      # Should pass through\n    logger.critical(\"Critical message\") # Should pass through\n\n    # Verify only error and critical passed the filter\n    # Note: These will still be captured by Logot's capturer\n    logot.assert_logged(logged.debug(\"Debug message\"))\n    logot.assert_logged(logged.info(\"Info message\"))\n    logot.assert_logged(logged.warning(\"Warning message\"))\n    logot.assert_logged(logged.error(\"Error message\"))\n    logot.assert_logged(logged.critical(\"Critical message\"))\n</code></pre>"},{"location":"logging_testing/#2-error-scenarios-testing","title":"2. Error Scenarios Testing","text":"<pre><code>\"\"\"Testing error scenarios and edge cases.\"\"\"\n\nimport pytest\nfrom logot import Logot, logged\nfrom loguru import logger\nfrom unittest.mock import patch, Mock\n\n\ndef test_sink_failure_handling(logot: Logot, reset_logger) -&gt; None:\n    \"\"\"Test handling of sink failures.\"\"\"\n\n    def failing_sink(message) -&gt; None:\n        raise RuntimeError(\"Sink failure simulation\")\n\n    # Add failing sink with catch=True\n    logger.add(\n        failing_sink,\n        level=\"INFO\",\n        catch=True  # Should catch and continue\n    )\n\n    # This should not raise an exception\n    logger.info(\"Message to failing sink\")\n\n    # Verify message was still processed by other sinks\n    logot.assert_logged(logged.info(\"Message to failing sink\"))\n\n\ndef test_logging_during_exception_handling(logot: Logot) -&gt; None:\n    \"\"\"Test logging during exception handling.\"\"\"\n\n    def problematic_function() -&gt; None:\n        try:\n            logger.info(\"Entering problematic function\")\n            raise ValueError(\"Simulated error\")\n        except Exception as e:\n            logger.error(\"Error in problematic function: {error}\", error=str(e))\n            raise\n        finally:\n            logger.info(\"Cleaning up after problematic function\")\n\n    # Execute and expect exception\n    with pytest.raises(ValueError, match=\"Simulated error\"):\n        problematic_function()\n\n    # Verify all logging occurred correctly\n    logot.assert_logged(logged.info(\"Entering problematic function\"))\n    logot.assert_logged(logged.error(\"Error in problematic function: Simulated error\"))\n    logot.assert_logged(logged.info(\"Cleaning up after problematic function\"))\n\n\ndef test_circular_logging_prevention(logot: Logot) -&gt; None:\n    \"\"\"Test prevention of circular logging issues.\"\"\"\n\n    def logging_sink(message) -&gt; None:\n        # This sink tries to log - should not cause infinite recursion\n        # Note: This is generally bad practice, just testing safety\n        print(f\"SINK: {message.record['message']}\")\n\n    logger.add(logging_sink, level=\"INFO\")\n\n    # Should not cause infinite recursion\n    logger.info(\"Test message for circular prevention\")\n\n    logot.assert_logged(logged.info(\"Test message for circular prevention\"))\n\n\n@pytest.mark.parametrize(\"log_level,should_log\", [\n    (\"DEBUG\", True),\n    (\"INFO\", True),\n    (\"WARNING\", True),\n    (\"ERROR\", True),\n    (\"CRITICAL\", True),\n])\ndef test_log_level_filtering(logot: Logot, reset_logger, log_level: str, should_log: bool) -&gt; None:\n    \"\"\"Test log level filtering with parameterized tests.\"\"\"\n\n    # Set minimum level to INFO\n    logger.add(lambda msg: None, level=\"INFO\")\n\n    # Log at the specified level\n    getattr(logger, log_level.lower())(\"Test message at {level} level\", level=log_level)\n\n    # Check if it should be logged\n    if should_log and log_level in [\"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]:\n        logot.assert_logged(getattr(logged, log_level.lower())(f\"Test message at {log_level} level\"))\n    elif log_level == \"DEBUG\":\n        # DEBUG should be filtered out but still captured by Logot\n        logot.assert_logged(logged.debug(\"Test message at DEBUG level\"))\n</code></pre>"},{"location":"logging_testing/#best-practices-guidelines","title":"\ud83d\udccb Best Practices &amp; Guidelines","text":""},{"location":"logging_testing/#1-test-organization","title":"1. Test Organization","text":"<pre><code>\"\"\"Best practices for organizing logging tests.\"\"\"\n\nimport pytest\nfrom logot import Logot, logged\n\n\nclass TestUserAuthentication:\n    \"\"\"Group related logging tests in classes.\"\"\"\n\n    def test_successful_login(self, logot: Logot) -&gt; None:\n        \"\"\"Test logging for successful authentication.\"\"\"\n        # Test implementation\n        pass\n\n    def test_failed_login(self, logot: Logot) -&gt; None:\n        \"\"\"Test logging for failed authentication.\"\"\"\n        # Test implementation\n        pass\n\n    def test_logout(self, logot: Logot) -&gt; None:\n        \"\"\"Test logging for user logout.\"\"\"\n        # Test implementation\n        pass\n\n\nclass TestDataProcessing:\n    \"\"\"Group data processing logging tests.\"\"\"\n\n    @pytest.mark.slow\n    def test_large_dataset_processing(self, logot: Logot) -&gt; None:\n        \"\"\"Test logging during large dataset processing.\"\"\"\n        # Test implementation\n        pass\n\n    @pytest.mark.integration\n    def test_database_integration_logging(self, logot: Logot) -&gt; None:\n        \"\"\"Test database operation logging.\"\"\"\n        # Test implementation\n        pass\n\n\n# Use descriptive test names\ndef test_user_registration_logs_all_steps(logot: Logot) -&gt; None:\n    \"\"\"Test that user registration logs each step of the process.\"\"\"\n    # Descriptive name explains what's being tested\n    pass\n\ndef test_payment_processing_logs_security_events(logot: Logot) -&gt; None:\n    \"\"\"Test that payment processing logs all security-relevant events.\"\"\"\n    # Clear intent from the name\n    pass\n</code></pre>"},{"location":"logging_testing/#2-test-data-management","title":"2. Test Data Management","text":"<pre><code>\"\"\"Managing test data for logging tests.\"\"\"\n\nimport pytest\nfrom dataclasses import dataclass\nfrom typing import Dict, Any\n\n\n@dataclass\nclass TestUser:\n    \"\"\"Test user data for logging tests.\"\"\"\n    id: int\n    username: str\n    email: str\n    role: str = \"user\"\n\n\n@dataclass\nclass TestRequest:\n    \"\"\"Test request data for logging tests.\"\"\"\n    id: str\n    method: str\n    path: str\n    user_id: int\n    headers: Dict[str, str] = None\n\n\n@pytest.fixture\ndef test_user() -&gt; TestUser:\n    \"\"\"Provide a test user for logging tests.\"\"\"\n    return TestUser(\n        id=12345,\n        username=\"testuser\",\n        email=\"test@example.com\",\n        role=\"admin\"\n    )\n\n\n@pytest.fixture\ndef test_request(test_user: TestUser) -&gt; TestRequest:\n    \"\"\"Provide a test request for logging tests.\"\"\"\n    return TestRequest(\n        id=\"req_test123\",\n        method=\"POST\",\n        path=\"/api/users\",\n        user_id=test_user.id,\n        headers={\"Content-Type\": \"application/json\"}\n    )\n\n\ndef test_with_test_data(logot: Logot, test_user: TestUser, test_request: TestRequest) -&gt; None:\n    \"\"\"Example test using test data fixtures.\"\"\"\n\n    # Log using test data\n    logger.bind(\n        user_id=test_user.id,\n        request_id=test_request.id\n    ).info(\"Processing request for user {username}\", username=test_user.username)\n\n    # Assert with test data\n    logot.assert_logged(logged.info(f\"Processing request for user {test_user.username}\"))\n</code></pre>"},{"location":"logging_testing/#3-assertion-patterns","title":"3. Assertion Patterns","text":"<pre><code>\"\"\"Best practices for log assertions.\"\"\"\n\nimport pytest\nimport re\nfrom logot import Logot, logged\n\n\ndef test_assertion_best_practices(logot: Logot) -&gt; None:\n    \"\"\"Demonstrate best practices for log assertions.\"\"\"\n\n    # \u2705 Good: Specific assertions\n    logger.info(\"User alice logged in successfully\")\n    logot.assert_logged(logged.info(\"User alice logged in successfully\"))\n\n    # \u2705 Good: Pattern matching for dynamic data\n    user_id = 12345\n    logger.info(\"User {id} performed action\", id=user_id)\n    logot.assert_logged(logged.info(\"User % performed action\"))\n\n    # \u2705 Good: Multiple specific assertions\n    logger.info(\"Starting process\")\n    logger.info(\"Process completed\")\n    logot.assert_logged(logged.info(\"Starting process\"))\n    logot.assert_logged(logged.info(\"Process completed\"))\n\n    # \u2705 Good: Regex for complex patterns\n    logger.info(\"Request ID: req_abc123 processing\")\n    pattern = re.compile(r\"Request ID: req_\\w+ processing\")\n    logot.assert_logged(logged.info(pattern))\n\n\ndef test_assertion_antipatterns(logot: Logot) -&gt; None:\n    \"\"\"Demonstrate what to avoid in log assertions.\"\"\"\n\n    # \u274c Avoid: Too generic assertions\n    logger.info(\"Something happened\")\n    # Don't just check that ANY info log occurred\n\n    # \u274c Avoid: Brittle timestamp assertions\n    logger.info(\"Event at 2024-01-15T10:30:00\")\n    # Don't assert exact timestamps - they change\n\n    # \u2705 Better: Use patterns for timestamps\n    logot.assert_logged(logged.info(\"Event at %\"))\n\n\ndef test_negative_assertions(logot: Logot) -&gt; None:\n    \"\"\"Test what should NOT be logged.\"\"\"\n\n    logger.info(\"Normal operation\")\n\n    # Verify expected logs\n    logot.assert_logged(logged.info(\"Normal operation\"))\n\n    # Verify sensitive data is NOT logged\n    logot.assert_not_logged(logged.info(\"password\"))\n    logot.assert_not_logged(logged.info(\"secret\"))\n    logot.assert_not_logged(logged.error(\"Internal error details\"))\n</code></pre>"},{"location":"logging_testing/#summary","title":"\ud83c\udfaf Summary","text":"<p>This comprehensive guide provides everything needed to test Loguru-based logging using Logot:</p> <ul> <li>\u2705 Complete setup with pytest configuration</li> <li>\u2705 Basic to advanced testing patterns</li> <li>\u2705 Loguru-specific features (bind, contextualize, custom sinks)</li> <li>\u2705 Threading &amp; async safety testing</li> <li>\u2705 InterceptHandler integration testing</li> <li>\u2705 Performance &amp; load testing</li> <li>\u2705 Error scenario testing</li> <li>\u2705 Best practices and anti-patterns</li> </ul>"},{"location":"logging_testing/#key-benefits-of-logot","title":"\ud83d\udd11 Key Benefits of Logot","text":"<ol> <li>Superior to caplog: More expressive assertions and better pattern matching</li> <li>Thread/Async Safe: Handles concurrent logging scenarios correctly</li> <li>Loguru Integration: Designed specifically for modern logging libraries</li> <li>Flexible Patterns: Support for wildcards, regex, and complex matching</li> <li>Clean Syntax: More readable and maintainable test code</li> </ol> <p>With this setup, you can ensure your Loguru logging infrastructure works correctly across all scenarios from simple unit tests to complex integration testing with threading, async operations, and third-party library integration.</p>"},{"location":"logging_typing/","title":"Loguru Type Hints &amp; IDE Support Guide","text":""},{"location":"logging_typing/#overview","title":"\ud83c\udfaf Overview","text":"<p>This guide provides comprehensive instructions for setting up complete type safety with Loguru using MyPy, handling IDE integration challenges, and implementing properly typed logging infrastructure for production applications.</p>"},{"location":"logging_typing/#known-ide-limitations","title":"\ud83d\udea8 Known IDE Limitations","text":""},{"location":"logging_typing/#pyrightpylance-fundamental-issue","title":"PyRight/Pylance Fundamental Issue","text":"<p>Loguru has a documented limitation with PyRight/Pylance in their <code>pyproject.toml</code>:</p> <pre><code>[tool.pyright]\n# Types are defined in a stub file. Unfortunately, type checkers such as Pyright and Mypy are\n# unable to \"merge\" them with the file containing the actual Python implementation. This causes\n# many false positives, therefore type checking is disabled to avoid noisy errors in the editor.\ntypeCheckingMode = \"off\"\n</code></pre> <p>Translation: PyRight/Pylance cannot properly merge <code>.pyi</code> stub files with actual implementation, causing many false positives.</p>"},{"location":"logging_typing/#mypy-works-perfectly","title":"MyPy Works Perfectly","text":"<p>Unlike PyRight, MyPy handles Loguru's stub files correctly and provides excellent type checking without false positives.</p>"},{"location":"logging_typing/#mypy-configuration","title":"\u2699\ufe0f MyPy Configuration","text":""},{"location":"logging_typing/#option-1-mypyini-traditional-config","title":"Option 1: mypy.ini (Traditional Config)","text":"<pre><code>[mypy]\n# Basic configuration\npython_version = 3.8\nwarn_return_any = True\nwarn_unused_configs = True\ndisallow_untyped_defs = True\ndisallow_incomplete_defs = True\ncheck_untyped_defs = True\ndisallow_untyped_decorators = True\n\n# Import handling\nignore_missing_imports = False\nfollow_imports = normal\nshow_error_codes = True\nshow_error_context = True\npretty = True\n\n# Error output\nshow_column_numbers = True\nerror_summary = True\n\n# Cache\ncache_dir = .mypy_cache\n\n# Per-module configurations\n\n# Loguru itself - trust the stub files\n[mypy-loguru.*]\nignore_errors = False\nfollow_imports = normal\nwarn_return_any = False\n\n# Standard library modules\n[mypy-asyncio.*]\nignore_missing_imports = False\n\n[mypy-concurrent.futures.*]\nignore_missing_imports = False\n\n[mypy-multiprocessing.*]\nignore_missing_imports = False\n\n# Third-party libraries that commonly don't have stubs\n[mypy-discord.*]\nignore_missing_imports = True\n\n[mypy-aiohttp.*]\nignore_missing_imports = True\n\n[mypy-requests.*]\nignore_missing_imports = True\n\n[mypy-uvicorn.*]\nignore_missing_imports = True\n\n[mypy-gunicorn.*]\nignore_missing_imports = True\n\n[mypy-celery.*]\nignore_missing_imports = True\n\n[mypy-fastapi.*]\nignore_missing_imports = True\n\n[mypy-pydantic.*]\nignore_missing_imports = True\n\n[mypy-tqdm.*]\nignore_missing_imports = True\n\n[mypy-click.*]\nignore_missing_imports = True\n\n# Testing frameworks\n[mypy-pytest.*]\nignore_missing_imports = True\n\n# Development/debugging tools\n[mypy-pysnooper.*]\nignore_missing_imports = True\n\n[mypy-vcr.*]\nignore_missing_imports = True\n\n[mypy-logging_tree.*]\nignore_missing_imports = True\n\n# Boss-bot application modules\n[mypy-boss_bot.*]\ndisallow_untyped_defs = True\ndisallow_incomplete_defs = True\ncheck_untyped_defs = True\nwarn_return_any = True\n\n[mypy-boss_bot.monitoring.logging.*]\ndisallow_untyped_defs = True\ndisallow_incomplete_defs = True\ncheck_untyped_defs = True\nwarn_return_any = True\n\n# Examples and tests - more relaxed\n[mypy-examples.*]\ndisallow_untyped_defs = False\ndisallow_incomplete_defs = False\n\n[mypy-tests.*]\ndisallow_untyped_defs = False\ndisallow_incomplete_defs = False\nignore_errors = False\n</code></pre>"},{"location":"logging_typing/#option-2-pyprojecttoml-modern-config","title":"Option 2: pyproject.toml (Modern Config)","text":"<pre><code>[tool.mypy]\n# Basic configuration\npython_version = \"3.8\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\ncheck_untyped_defs = true\ndisallow_untyped_decorators = true\n\n# Import handling\nignore_missing_imports = false\nfollow_imports = \"normal\"\nshow_error_codes = true\nshow_error_context = true\npretty = true\n\n# Error output\nshow_column_numbers = true\nerror_summary = true\n\n# Cache\ncache_dir = \".mypy_cache\"\n\n# Loguru-specific settings\n[[tool.mypy.overrides]]\nmodule = \"loguru.*\"\nignore_errors = false\nfollow_imports = \"normal\"\nwarn_return_any = false\n\n# Third-party libraries without stubs\n[[tool.mypy.overrides]]\nmodule = [\n    \"discord.*\",\n    \"aiohttp.*\",\n    \"requests.*\",\n    \"uvicorn.*\",\n    \"gunicorn.*\",\n    \"celery.*\",\n    \"fastapi.*\",\n    \"pydantic.*\",\n    \"tqdm.*\",\n    \"click.*\",\n    \"pytest.*\",\n    \"pysnooper.*\",\n    \"vcr.*\",\n    \"logging_tree.*\",\n    \"pandas.*\",\n    \"numpy.*\"\n]\nignore_missing_imports = true\n\n# Boss-bot application modules\n[[tool.mypy.overrides]]\nmodule = [\"boss_bot.*\", \"boss_bot.monitoring.logging.*\"]\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\ncheck_untyped_defs = true\nwarn_return_any = true\n\n# Test files - more relaxed\n[[tool.mypy.overrides]]\nmodule = [\"tests.*\", \"examples.*\"]\ndisallow_untyped_defs = false\ndisallow_incomplete_defs = false\n</code></pre>"},{"location":"logging_typing/#essential-type-patterns","title":"\ud83d\udd27 Essential Type Patterns","text":""},{"location":"logging_typing/#1-required-imports-for-type-safety","title":"1. Required Imports for Type Safety","text":"<pre><code>from __future__ import annotations  # CRITICAL: Add to every file\n\nimport logging\nimport sys\nimport threading\nfrom types import FrameType\nfrom typing import Union, Optional, Any, Dict, Callable, Awaitable, TextIO, TYPE_CHECKING\n\n# Loguru imports\nimport loguru\nfrom loguru import logger\n\n# Type-only imports\nif TYPE_CHECKING:\n    from loguru import Logger, Message, Record\n</code></pre>"},{"location":"logging_typing/#2-thread-safe-intercepthandler-with-proper-typing","title":"2. Thread-Safe InterceptHandler with Proper Typing","text":"<pre><code>class TypedInterceptHandler(logging.Handler):\n    \"\"\"\n    Thread-safe and async-safe interceptor for standard logging into Loguru.\n\n    This implementation ensures proper thread safety and multiprocessing compatibility\n    by using improved frame inspection and avoiding potential deadlocks.\n    \"\"\"\n\n    def emit(self, record: logging.LogRecord) -&gt; None:\n        \"\"\"Thread-safe emit method that properly handles frame inspection.\"\"\"\n        # Get corresponding Loguru level with proper type annotation\n        try:\n            level: Union[str, int] = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # Improved frame inspection with proper typing\n        frame: Optional[FrameType] = logging.currentframe()\n        depth = 2\n\n        while frame:\n            filename: str = frame.f_code.co_filename\n            # Check for logging module and frozen/bootstrap code\n            is_logging: bool = filename == logging.__file__\n            is_frozen: bool = \"importlib\" in filename and \"_bootstrap\" in filename\n            if depth &gt; 0 and not (is_logging or is_frozen):\n                break\n            frame = frame.f_back\n            depth += 1\n\n        # Use opt() with proper depth and exception info for thread-safe logging\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level,\n            record.getMessage(),\n        )\n</code></pre>"},{"location":"logging_typing/#3-early-initialization-with-complete-type-safety","title":"3. Early Initialization with Complete Type Safety","text":"<pre><code># Global state with proper typing\n_early_init_done: bool = False\n_early_init_lock: threading.Lock = threading.Lock()\n\ndef _early_init() -&gt; None:\n    \"\"\"\n    Early initialization of logging system - call BEFORE importing other modules.\n\n    Thread-safe: Uses double-checked locking to prevent race conditions.\n    \"\"\"\n    global _early_init_done\n\n    # Quick check without lock for performance\n    if _early_init_done:\n        return\n\n    # Thread-safe double-checked locking pattern\n    with _early_init_lock:\n        if _early_init_done:  # Re-check inside lock\n            return  # Another thread already initialized\n\n        # Remove default loguru handler immediately\n        logger.remove()\n\n        # Create thread-safe InterceptHandler early\n        intercept_handler: TypedInterceptHandler = TypedInterceptHandler()\n\n        # Set up basic root logging level\n        logging.root.setLevel(logging.DEBUG)\n\n        # Replace root handler immediately\n        logging.root.handlers = [intercept_handler]\n\n        # Configure basic loguru handler with safety features\n        logger.add(\n            sys.stderr,\n            level=\"INFO\",\n            format=\"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n                   \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n                   \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n                   \"&lt;level&gt;{message}&lt;/level&gt;\",\n            enqueue=True,   # Critical for thread/multiprocessing safety\n            catch=True,     # Prevent logging errors from crashing app\n            backtrace=True,\n            diagnose=True\n        )\n\n        # Immediately intercept critical loggers for Discord.py, async frameworks, and boss-bot\n        critical_loggers: list[str] = [\n            # Python standard library\n            \"asyncio\",\n            \"concurrent.futures\",\n            \"multiprocessing\",\n            \"threading\",\n            \"urllib3\",\n            \"requests\",\n            \"httpx\",\n            \"aiohttp\",\n            # Discord.py framework\n            \"discord\",\n            \"discord.client\",\n            \"discord.gateway\",\n            \"discord.http\",\n            \"discord.voice_client\",\n            \"discord.shard\",\n            \"discord.ext.commands\",\n            \"discord.ext.tasks\",\n            # Boss-bot specific modules\n            \"boss_bot\",\n            \"boss_bot.bot\",\n            \"boss_bot.bot.client\",\n            \"boss_bot.bot.cogs\",\n            \"boss_bot.core\",\n            \"boss_bot.core.downloads\",\n            \"boss_bot.core.queue\",\n            \"boss_bot.cli\",\n            \"boss_bot.monitoring\",\n            \"boss_bot.storage\",\n            \"boss_bot.utils\",\n            # AI/LangChain ecosystem\n            \"langchain\",\n            \"langchain.agents\",\n            \"langchain.chains\",\n            \"langchain.llms\",\n            \"langsmith\",\n            \"openai\",\n            \"anthropic\",\n            # Download tools\n            \"gallery_dl\",\n            \"yt_dlp\",\n            \"youtube_dl\",\n        ]\n\n        for logger_name in critical_loggers:\n            log_instance: logging.Logger = logging.getLogger(logger_name)\n            log_instance.handlers = [intercept_handler]\n            log_instance.propagate = False\n\n        # Mark as completed inside the lock\n        _early_init_done = True\n        logger.debug(\"Early logging initialization completed\")\n</code></pre>"},{"location":"logging_typing/#custom-components-with-type-safety","title":"\ud83c\udfa8 Custom Components with Type Safety","text":""},{"location":"logging_typing/#1-custom-sinks","title":"1. Custom Sinks","text":"<pre><code># Synchronous sink\ndef structured_sink(message: loguru.Message) -&gt; None:\n    \"\"\"Custom sink that processes log messages with full type safety.\"\"\"\n    record: loguru.Record = message.record\n    formatted: str = (\n        f\"{record['time']:%Y-%m-%d %H:%M:%S} | \"\n        f\"{record['level'].name: &lt;8} | \"\n        f\"{record['name']}:{record['function']}:{record['line']} | \"\n        f\"{record['message']}\"\n    )\n    print(formatted)\n\n# Asynchronous sink\nasync def async_sink(message: loguru.Message) -&gt; None:\n    \"\"\"Async sink for external logging services.\"\"\"\n    record: loguru.Record = message.record\n\n    # Simulate async operation\n    await external_log_service.send({\n        \"timestamp\": record[\"time\"].isoformat(),\n        \"level\": record[\"level\"].name,\n        \"message\": record[\"message\"],\n        \"extra\": record.get(\"extra\", {})\n    })\n\n# File-like sink (when serialize=False)\ndef file_like_sink(message: str) -&gt; None:\n    \"\"\"Handle raw string messages when serialize=False.\"\"\"\n    with open(\"custom.log\", \"a\") as f:\n        f.write(message)\n\n# Usage with type safety\nlogger.add(structured_sink, level=\"DEBUG\")\nlogger.add(async_sink, level=\"INFO\")\nlogger.add(file_like_sink, serialize=False, level=\"WARNING\")\n</code></pre>"},{"location":"logging_typing/#2-filters-and-formatters","title":"2. Filters and Formatters","text":"<pre><code>def level_filter(record: loguru.Record) -&gt; bool:\n    \"\"\"Filter function that only allows WARNING and above.\"\"\"\n    return record[\"level\"].no &gt;= 30\n\ndef component_filter(component_name: str) -&gt; Callable[[loguru.Record], bool]:\n    \"\"\"Factory function for component-based filtering.\"\"\"\n    def filter_func(record: loguru.Record) -&gt; bool:\n        return record.get(\"component\") == component_name\n    return filter_func\n\ndef json_formatter(record: loguru.Record) -&gt; str:\n    \"\"\"JSON formatter for structured logging.\"\"\"\n    import json\n\n    log_entry: Dict[str, Any] = {\n        \"timestamp\": record[\"time\"].isoformat(),\n        \"level\": record[\"level\"].name,\n        \"logger\": record[\"name\"],\n        \"function\": record[\"function\"],\n        \"line\": record[\"line\"],\n        \"message\": record[\"message\"],\n        \"extra\": record.get(\"extra\", {})\n    }\n    return json.dumps(log_entry) + \"\\n\"\n\ndef custom_formatter(record: loguru.Record) -&gt; str:\n    \"\"\"Custom log formatter with type safety.\"\"\"\n    return (\n        f\"&lt;green&gt;{record['time']:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | \"\n        f\"&lt;level&gt;{record['level'].name: &lt;8}&lt;/level&gt; | \"\n        f\"&lt;cyan&gt;{record['name']}&lt;/cyan&gt;:&lt;cyan&gt;{record['function']}&lt;/cyan&gt;:&lt;cyan&gt;{record['line']}&lt;/cyan&gt; | \"\n        f\"&lt;level&gt;{record['message']}&lt;/level&gt;\\n{record['exception'] or ''}\"\n    )\n\n# Usage\nlogger.add(\n    \"app.log\",\n    filter=level_filter,\n    format=json_formatter,\n    level=\"DEBUG\"\n)\n\nlogger.add(\n    sys.stderr,\n    filter=component_filter(\"auth\"),\n    format=custom_formatter\n)\n</code></pre>"},{"location":"logging_typing/#3-patch-functions-and-context-management","title":"3. Patch Functions and Context Management","text":"<pre><code>def add_hostname_patch(record: loguru.Record) -&gt; None:\n    \"\"\"Patch function to add hostname to all records.\"\"\"\n    import socket\n    record[\"extra\"][\"hostname\"] = socket.gethostname()\n\ndef add_request_context(record: loguru.Record) -&gt; None:\n    \"\"\"Add request context to log records.\"\"\"\n    # Example using contextvars\n    record[\"extra\"][\"request_id\"] = request_id_var.get(\"unknown\")\n    record[\"extra\"][\"user_id\"] = user_id_var.get(\"anonymous\")\n\n# Context manager with proper typing\nclass LogContext:\n    \"\"\"Context manager for structured logging with type safety.\"\"\"\n\n    def __init__(self, **context: Any) -&gt; None:\n        self.context: Dict[str, Any] = context\n        self.token: Optional[Any] = None\n\n    def __enter__(self) -&gt; loguru.Logger:\n        # Loguru handles contextualize automatically\n        return logger.contextualize(**self.context)\n\n    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n        # Context is automatically restored by loguru\n        pass\n\n# Usage\npatched_logger: loguru.Logger = logger.patch(add_hostname_patch)\npatched_logger.info(\"Message with hostname\")\n\nwith LogContext(operation=\"user_auth\", session_id=\"sess-123\"):\n    logger.info(\"User authentication started\")\n</code></pre>"},{"location":"logging_typing/#4-binding-and-options-with-type-safety","title":"4. Binding and Options with Type Safety","text":"<pre><code>def test_logger_binding() -&gt; loguru.Logger:\n    \"\"\"Test logger binding with proper type annotations.\"\"\"\n    bound_logger: loguru.Logger = logger.bind(\n        request_id=\"req-123\",\n        user_id=456,\n        component=\"auth\",\n        metadata={\"version\": \"1.0.0\", \"env\": \"production\"}\n    )\n\n    bound_logger.info(\"User authenticated successfully\")\n    return bound_logger\n\ndef test_logger_options() -&gt; None:\n    \"\"\"Test logger.opt() with comprehensive type safety.\"\"\"\n    # Basic opt usage\n    logger.opt(colors=True).info(\"Colored message\")\n    logger.opt(depth=1).warning(\"Warning with custom depth\")\n    logger.opt(lazy=True).debug(\"Lazy evaluation: {}\", lambda: expensive_computation())\n\n    # Exception handling with proper typing\n    try:\n        risky_operation()\n    except Exception:\n        logger.opt(exception=True).error(\"Operation failed with exception\")\n\n    # Record manipulation\n    logger.opt(record=True).info(\"Message with record access\",\n                                extra_data=lambda record: record.update(custom=\"value\"))\n\ndef expensive_computation() -&gt; str:\n    \"\"\"Simulate expensive operation for lazy logging.\"\"\"\n    import time\n    time.sleep(0.1)  # Simulate work\n    return \"computed_result\"\n\ndef risky_operation() -&gt; None:\n    \"\"\"Simulate an operation that might raise an exception.\"\"\"\n    raise ValueError(\"Example exception for logging\")\n</code></pre>"},{"location":"logging_typing/#production-configuration-function","title":"\ud83c\udfd7\ufe0f Production Configuration Function","text":"<pre><code>def configure_production_logging(\n    log_level: Union[str, int] = logging.INFO,\n    log_file: Optional[str] = None,\n    use_json: bool = False,\n    enable_async: bool = False,\n    enable_file_rotation: bool = True\n) -&gt; loguru.Logger:\n    \"\"\"\n    Configure production-ready logging with complete type safety.\n\n    Args:\n        log_level: Minimum logging level\n        log_file: Optional file path for file logging\n        use_json: Whether to use JSON formatting\n        enable_async: Whether to enable async sink\n        enable_file_rotation: Whether to enable log rotation\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n\n    # Remove default handler\n    logger.remove()\n\n    # Console handler with conditional formatting\n    console_format: Union[str, Callable[[loguru.Record], str]]\n    if use_json:\n        console_format = json_formatter\n    else:\n        console_format = (\n            \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | \"\n            \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n            \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n            \"&lt;level&gt;{message}&lt;/level&gt;\"\n        )\n\n    logger.add(\n        sys.stderr,\n        level=log_level,\n        format=console_format,\n        filter=level_filter,\n        enqueue=True,   # Thread/multiprocessing safety\n        catch=True,     # Error resilience\n        diagnose=False, # Security: disable in production\n        backtrace=False # Security: disable in production\n    )\n\n    # File handler if specified\n    if log_file:\n        file_config: Dict[str, Any] = {\n            \"level\": log_level,\n            \"format\": json_formatter,\n            \"enqueue\": True,\n            \"catch\": True,\n            \"diagnose\": True,  # Enable for file logs\n            \"backtrace\": True\n        }\n\n        if enable_file_rotation:\n            file_config.update({\n                \"rotation\": \"10 MB\",\n                \"retention\": \"7 days\",\n                \"compression\": \"gz\"\n            })\n\n        logger.add(log_file, **file_config)\n\n    # Error-only file for monitoring\n    logger.add(\n        \"logs/errors.log\",\n        level=\"ERROR\",\n        format=json_formatter,\n        rotation=\"100 MB\",\n        retention=\"30 days\",\n        enqueue=True,\n        catch=True,\n        backtrace=True,\n        diagnose=True\n    )\n\n    # Custom structured sink\n    logger.add(structured_sink, level=\"DEBUG\")\n\n    # Async sink if enabled\n    if enable_async:\n        logger.add(async_sink, level=\"INFO\")\n\n    # Apply patches for additional context\n    enhanced_logger: loguru.Logger = logger.patch(add_hostname_patch)\n\n    return enhanced_logger\n</code></pre>"},{"location":"logging_typing/#installation-usage","title":"\ud83d\ude80 Installation &amp; Usage","text":""},{"location":"logging_typing/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Using uv (recommended)\nuv add --dev mypy\n\n# Using pip\npip install mypy\n\n# Optional: Enhanced loguru typing (requires Python 3.6+)\n# pip install loguru-mypy\n</code></pre>"},{"location":"logging_typing/#2-project-setup","title":"2. Project Setup","text":"<pre><code># Create mypy configuration\ntouch mypy.ini  # Copy configuration from above\n\n# Create typed logger module\ntouch my_intercept_logger.py  # Copy implementation from above\n</code></pre>"},{"location":"logging_typing/#3-type-checking-commands","title":"3. Type Checking Commands","text":"<pre><code># Check specific files\nuv run mypy my_intercept_logger.py\n\n# Check entire project\nuv run mypy .\n\n# With specific config\nuv run mypy --config-file mypy.ini .\n\n# Show detailed error information\nuv run mypy --show-error-codes --show-column-numbers .\n\n# Generate coverage report\nuv run mypy --html-report mypy_report .\n</code></pre>"},{"location":"logging_typing/#4-ide-integration","title":"4. IDE Integration","text":""},{"location":"logging_typing/#vs-code-settings","title":"VS Code Settings","text":"<p>Add to <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"python.linting.mypyEnabled\": true,\n    \"python.linting.enabled\": true,\n    \"python.linting.mypyPath\": \"uv run mypy\",\n    \"python.linting.mypyArgs\": [\n        \"--config-file\", \"mypy.ini\",\n        \"--show-column-numbers\",\n        \"--show-error-codes\"\n    ],\n    \"python.linting.pylintEnabled\": false,\n    \"python.analysis.typeCheckingMode\": \"off\"\n}\n</code></pre>"},{"location":"logging_typing/#pycharm-configuration","title":"PyCharm Configuration","text":"<ol> <li>Go to File \u2192 Settings \u2192 Tools \u2192 External Tools</li> <li>Add new tool:</li> <li>Name: MyPy Type Check</li> <li>Program: <code>uv</code></li> <li>Arguments: <code>run mypy $FilePath$</code></li> <li>Working Directory: <code>$ProjectFileDir$</code></li> </ol>"},{"location":"logging_typing/#vimneovim-with-ale","title":"Vim/Neovim with ALE","text":"<pre><code>let g:ale_linters = {\n\\   'python': ['mypy'],\n\\}\nlet g:ale_python_mypy_executable = 'uv run mypy'\nlet g:ale_python_mypy_options = '--config-file mypy.ini'\n</code></pre>"},{"location":"logging_typing/#testing-type-safety","title":"\ud83e\uddea Testing Type Safety","text":""},{"location":"logging_typing/#complete-test-example","title":"Complete Test Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComprehensive test for Loguru type safety in boss-bot.\nRun with: uv run mypy test_boss_bot_logging_typing.py\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport sys\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nfrom types import FrameType\nfrom typing import Union, Optional, Any, Dict, Callable\n\nimport loguru\nfrom loguru import logger\n\n# Boss-bot specific imports\nfrom boss_bot.core.env import BossSettings\nfrom boss_bot.monitoring.logging import (\n    early_init,\n    setup_boss_bot_logging,\n    setup_thread_safe_logging,\n    ThreadSafeLogConfig,\n)\n\n# Test all the patterns we've defined\ndef test_all_typing_patterns() -&gt; None:\n    \"\"\"Test all typing patterns to ensure MyPy compatibility in boss-bot.\"\"\"\n\n    # Test early initialization\n    early_init()\n\n    # Test boss-bot specific logging setup\n    settings = BossSettings()\n    boss_logger: loguru.Logger = setup_boss_bot_logging(settings)\n\n    # Test basic logging\n    boss_logger.info(\"Boss-bot logging test\")\n    boss_logger.error(\"Error with data: {data}\", data={\"key\": \"value\"})\n\n    # Test binding with boss-bot context\n    bound_logger: loguru.Logger = boss_logger.bind(\n        request_id=\"boss-bot-test-123\",\n        user_id=456,\n        guild_id=789,\n        component=\"download_manager\"\n    )\n    bound_logger.warning(\"Bound logger test\")\n\n    # Test options\n    boss_logger.opt(colors=True).info(\"Colored message\")\n    boss_logger.opt(lazy=True).debug(\"Lazy: {}\", lambda: \"computed\")\n\n    # Test exception handling\n    try:\n        raise ValueError(\"Test exception\")\n    except Exception:\n        boss_logger.opt(exception=True).error(\"Exception test\")\n\n    # Test custom sink\n    boss_logger.add(test_sink, level=\"DEBUG\")\n    boss_logger.debug(\"Custom sink test\")\n\n    # Test configuration from settings\n    config: ThreadSafeLogConfig = ThreadSafeLogConfig.from_boss_settings(settings)\n    configured_logger: loguru.Logger = setup_thread_safe_logging(\n        log_level=config.log_level,\n        enable_file_logging=config.enable_file_logging,\n        log_file_path=config.log_file_path,\n        enable_json_logging=config.enable_json_logging,\n        enable_sensitive_obfuscation=config.enable_sensitive_obfuscation,\n    )\n    configured_logger.success(\"Boss-bot configuration test completed\")\n\ndef test_sink(message: loguru.Message) -&gt; None:\n    \"\"\"Test sink function with proper typing.\"\"\"\n    record: loguru.Record = message.record\n    print(f\"TEST SINK: {record['level'].name} - {record['message']}\")\n\ndef test_filter(record: loguru.Record) -&gt; bool:\n    \"\"\"Test filter function.\"\"\"\n    return record[\"level\"].no &gt;= 20\n\ndef test_formatter(record: loguru.Record) -&gt; str:\n    \"\"\"Test formatter function.\"\"\"\n    return f\"{record['time']} | {record['level'].name} | {record['message']}\\n\"\n\n# Include all our previous implementations here...\n# (TypedInterceptHandler, _early_init, configure_production_logging, etc.)\n\nif __name__ == \"__main__\":\n    print(\"Running comprehensive type safety tests...\")\n    test_all_typing_patterns()\n    print(\"\u2705 All type safety tests passed!\")\n</code></pre>"},{"location":"logging_typing/#run-type-checking","title":"Run Type Checking","text":"<pre><code># Should show \"Success: no issues found\"\nuv run mypy test_boss_bot_logging_typing.py\n\n# Test with strict mode\nuv run mypy --strict test_boss_bot_logging_typing.py\n\n# Check boss-bot logging module\nuv run mypy src/boss_bot/monitoring/logging/\n\n# Check entire boss-bot project\nuv run mypy src/boss_bot/\n\n# Generate detailed report\nuv run mypy --html-report mypy_report src/boss_bot/monitoring/logging/\n</code></pre>"},{"location":"logging_typing/#troubleshooting-common-type-issues","title":"\ud83d\udd0d Troubleshooting Common Type Issues","text":""},{"location":"logging_typing/#problem-1-frame-type-errors","title":"Problem 1: Frame Type Errors","text":"<p>Error: <code>Incompatible types in assignment (expression has type \"Optional[FrameType]\", variable has type \"FrameType\")</code></p> <p>Solution: <pre><code># \u274c Incorrect\nframe, depth = logging.currentframe(), 2\nwhile frame.f_code.co_filename == logging.__file__:  # Error here\n    frame = frame.f_back\n\n# \u2705 Correct\nframe: Optional[FrameType] = logging.currentframe()\ndepth = 2\nwhile frame and frame.f_code.co_filename == logging.__file__:\n    frame = frame.f_back\n    depth += 1\n</code></pre></p>"},{"location":"logging_typing/#problem-2-union-type-issues-with-levels","title":"Problem 2: Union Type Issues with Levels","text":"<p>Error: Type errors when using logger levels</p> <p>Solution: <pre><code># \u274c Incorrect\nlevel = logger.level(record.levelname).name\n\n# \u2705 Correct\ntry:\n    level: Union[str, int] = logger.level(record.levelname).name\nexcept ValueError:\n    level = record.levelno\n</code></pre></p>"},{"location":"logging_typing/#problem-3-custom-sink-type-mismatches","title":"Problem 3: Custom Sink Type Mismatches","text":"<p>Error: Sink function type doesn't match expected signature</p> <p>Solution: <pre><code># \u274c Incorrect - missing type annotations\ndef my_sink(message):\n    print(message)\n\n# \u2705 Correct - proper type annotations\ndef my_sink(message: loguru.Message) -&gt; None:\n    record: loguru.Record = message.record\n    print(f\"{record['time']} - {record['message']}\")\n\n# For string-based sinks (serialize=False)\ndef string_sink(message: str) -&gt; None:\n    print(message.strip())\n</code></pre></p>"},{"location":"logging_typing/#problem-4-async-sink-type-issues","title":"Problem 4: Async Sink Type Issues","text":"<p>Error: Async sink not properly typed</p> <p>Solution: <pre><code># \u274c Incorrect\nasync def async_sink(message):\n    await some_operation(message)\n\n# \u2705 Correct\nasync def async_sink(message: loguru.Message) -&gt; None:\n    record: loguru.Record = message.record\n    await external_service.send(record)\n</code></pre></p>"},{"location":"logging_typing/#problem-5-context-and-extra-type-safety","title":"Problem 5: Context and Extra Type Safety","text":"<p>Error: Type issues with extra context data</p> <p>Solution: <pre><code># \u274c Risky - no type safety\nlogger.bind(user_id=user.id, **extra_data).info(\"Message\")\n\n# \u2705 Safe - explicit typing\nextra_context: Dict[str, Any] = {\n    \"user_id\": user.id,\n    \"session_id\": session.id\n}\ntyped_logger: loguru.Logger = logger.bind(**extra_context)\ntyped_logger.info(\"Message with typed context\")\n</code></pre></p>"},{"location":"logging_typing/#best-practices-checklist","title":"\ud83d\udccb Best Practices Checklist","text":""},{"location":"logging_typing/#essential-setup","title":"Essential Setup \u2705","text":"<ul> <li> Add <code>from __future__ import annotations</code> to every file</li> <li> Install and configure MyPy (not PyRight for loguru)</li> <li> Use proper type imports: <code>FrameType</code>, <code>Union</code>, <code>Optional</code>, etc.</li> <li> Configure MyPy to trust loguru stub files</li> </ul>"},{"location":"logging_typing/#intercepthandler-implementation","title":"InterceptHandler Implementation \u2705","text":"<ul> <li> Use <code>Optional[FrameType]</code> for frame inspection</li> <li> Properly type level as <code>Union[str, int]</code></li> <li> Handle <code>ValueError</code> exceptions from level lookup</li> <li> Use proper depth calculation with type safety</li> </ul>"},{"location":"logging_typing/#custom-components","title":"Custom Components \u2705","text":"<ul> <li> Type all sink functions: <code>(message: loguru.Message) -&gt; None</code></li> <li> Type all filter functions: <code>(record: loguru.Record) -&gt; bool</code></li> <li> Type all formatter functions: <code>(record: loguru.Record) -&gt; str</code></li> <li> Type async sinks: <code>async (message: loguru.Message) -&gt; None</code></li> </ul>"},{"location":"logging_typing/#configuration-functions","title":"Configuration Functions \u2705","text":"<ul> <li> Type all parameters: <code>Union[str, int]</code>, <code>Optional[str]</code>, etc.</li> <li> Return type annotation: <code>-&gt; loguru.Logger</code></li> <li> Type all intermediate variables</li> <li> Use type-safe dictionary unpacking for config</li> </ul>"},{"location":"logging_typing/#production-deployment","title":"Production Deployment \u2705","text":"<ul> <li> Configure CI/CD to run MyPy type checking</li> <li> Set up pre-commit hooks with MyPy</li> <li> Configure IDE for MyPy integration</li> <li> Create type checking documentation for team</li> </ul>"},{"location":"logging_typing/#summary","title":"\ud83c\udfaf Summary","text":"<p>This comprehensive typing setup provides:</p> <ul> <li>\u2705 Complete type safety for all Loguru operations</li> <li>\u2705 MyPy integration that works perfectly with Loguru's stub files</li> <li>\u2705 Production-ready InterceptHandler with proper typing</li> <li>\u2705 Custom component typing for sinks, filters, and formatters</li> <li>\u2705 Thread-safe and async-safe implementations</li> <li>\u2705 IDE integration with proper configuration</li> <li>\u2705 Troubleshooting guide for common type issues</li> </ul>"},{"location":"logging_typing/#key-takeaways","title":"\ud83d\udd11 Key Takeaways","text":"<ol> <li>Use MyPy, not PyRight - Loguru's stub files work perfectly with MyPy</li> <li>Always use <code>from __future__ import annotations</code> - Essential for forward references</li> <li>Type frame inspection carefully - Use <code>Optional[FrameType]</code> and proper null checks</li> <li>Type all custom components - Sinks, filters, formatters need explicit type annotations</li> <li>Trust Loguru's types - The stub files are comprehensive and accurate</li> </ol> <p>With this setup, you'll have bulletproof type safety for your Loguru logging infrastructure that scales from development to production while catching type errors at development time rather than runtime.</p>"},{"location":"people/","title":"People","text":"<p>Information about the people contributing to this project.</p>"},{"location":"people/#made-with-by","title":"Made with  by:","text":"Avatar Contributor Contributions bossjones 14"},{"location":"project_structure/","title":"Boss-Bot Project Structure","text":"<p>This document provides a comprehensive overview of the Boss-Bot project structure with detailed explanations for each module.</p>"},{"location":"project_structure/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Project Root Structure</li> <li>Source Code Structure</li> <li>Test Structure</li> <li>Technology Stack</li> <li>Entry Points and Execution Flow</li> <li>Component Interactions</li> <li>Development Workflow</li> </ul>"},{"location":"project_structure/#project-root-structure","title":"Project Root Structure","text":"<pre><code>boss-bot/                               # Project root directory\n\u251c\u2500\u2500 .claude/                           # Claude AI configuration\n\u2502   \u2514\u2500\u2500 commands/                      # Custom Claude commands\n\u251c\u2500\u2500 .github/                           # GitHub configuration\n\u2502   \u2514\u2500\u2500 workflows/                     # CI/CD workflows\n\u251c\u2500\u2500 ai_docs/                           # AI-related documentation\n\u2502   \u2514\u2500\u2500 plans/                         # AI agent planning documents\n\u251c\u2500\u2500 docs/                              # Project documentation\n\u2502   \u251c\u2500\u2500 EXPERIMENTAL.md               # Experimental features (Epic 5)\n\u2502   \u2514\u2500\u2500 project_structure.md          # This file\n\u251c\u2500\u2500 scripts/                           # Development and deployment scripts\n\u251c\u2500\u2500 src/                               # Source code directory\n\u2502   \u2514\u2500\u2500 boss_bot/                     # Main package\n\u251c\u2500\u2500 tests/                             # Test suite\n\u251c\u2500\u2500 .env.example                       # Environment variable template\n\u251c\u2500\u2500 .gitignore                         # Git ignore patterns\n\u251c\u2500\u2500 CLAUDE.md                          # AI assistant instructions\n\u251c\u2500\u2500 justfile                           # Task automation (like Makefile)\n\u251c\u2500\u2500 pyproject.toml                     # Project configuration and dependencies\n\u251c\u2500\u2500 README.md                          # Project README\n\u2514\u2500\u2500 uv.lock                            # Dependency lock file (uv package manager)\n</code></pre>"},{"location":"project_structure/#source-code-structure","title":"Source Code Structure","text":"<pre><code>src/\n\u2514\u2500\u2500 boss_bot/                           # Main package directory for the Boss-Bot Discord bot\n    \u251c\u2500\u2500 __init__.py                    # Package initialization file\n    \u251c\u2500\u2500 __main__.py                    # Entry point for running the package with `python -m boss_bot`\n    \u251c\u2500\u2500 __version__.py                 # Version information for the package\n    \u251c\u2500\u2500 ai/                            # AI and LLM integration modules (LangChain/LangGraph)\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 agents/                    # LangGraph agents for multi-step AI workflows\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 chains/                    # LangChain chains for composable AI operations\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 memory/                    # Conversation and context memory management\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 prompts/                   # Prompt templates and engineering\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 tools/                     # LangChain tools for AI agent capabilities\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api/                           # REST/GraphQL API layer (future implementation)\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 middleware/                # API middleware (auth, rate limiting, etc.)\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 models/                    # API data models and schemas\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 routes/                    # API route definitions and handlers\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 bot/                           # Discord bot core functionality\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 bot_help.py               # Custom help command implementation\n    \u2502   \u251c\u2500\u2500 client.py                 # Main BossBot client class extending discord.py Bot\n    \u2502   \u251c\u2500\u2500 cogs/                     # Discord command cogs (modular command groups)\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 admin.py             # Administrative commands (bot management)\n    \u2502   \u2502   \u251c\u2500\u2500 downloads.py         # Download commands using strategy pattern\n    \u2502   \u2502   \u2514\u2500\u2500 queue.py             # Queue management commands\n    \u2502   \u251c\u2500\u2500 events/                   # Discord event handlers\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 middleware/               # Bot-specific middleware\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 cli/                          # Command-line interface using Typer\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 commands/                 # CLI subcommands\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2514\u2500\u2500 download.py          # Download-related CLI commands\n    \u2502   \u251c\u2500\u2500 config/                   # CLI configuration management\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 main.py                  # Main CLI entry point and command router\n    \u2502   \u2514\u2500\u2500 utils/                    # CLI utility functions\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 cli.py                        # Legacy CLI module (to be removed)\n    \u251c\u2500\u2500 commands/                      # Legacy commands directory (to be removed)\n    \u2502   \u2514\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 core/                         # Core business logic and services\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 compression/              # Media compression and optimization\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 manager.py           # Main compression management logic\n    \u2502   \u2502   \u251c\u2500\u2500 models.py            # Compression-related data models\n    \u2502   \u2502   \u251c\u2500\u2500 processors/          # Media-specific compression processors\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 audio_processor.py    # Audio file compression\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 base_processor.py     # Abstract base processor\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 image_processor.py    # Image compression\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 video_processor.py    # Video compression\n    \u2502   \u2502   \u2514\u2500\u2500 utils/               # Compression utility functions\n    \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502       \u251c\u2500\u2500 bitrate_calculator.py  # Bitrate calculations for media\n    \u2502   \u2502       \u251c\u2500\u2500 ffmpeg_utils.py       # FFmpeg wrapper utilities\n    \u2502   \u2502       \u2514\u2500\u2500 file_detector.py      # Media file type detection\n    \u2502   \u251c\u2500\u2500 core_queue.py            # Legacy queue module (use queue/manager.py)\n    \u2502   \u251c\u2500\u2500 downloads/               # Download management system\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 clients/             # Download client implementations\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 aio_gallery_dl.py         # Async gallery-dl client wrapper\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 aio_gallery_dl_utils.py   # Gallery-dl utility functions\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 aio_yt_dlp.py             # Async yt-dlp client wrapper\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 config/                    # Client configuration\n    \u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502       \u251c\u2500\u2500 gallery_dl_config.py   # Gallery-dl configuration\n    \u2502   \u2502   \u2502       \u2514\u2500\u2500 gallery_dl_validator.py # Configuration validation\n    \u2502   \u2502   \u251c\u2500\u2500 feature_flags.py     # Feature flag management for strategies\n    \u2502   \u2502   \u251c\u2500\u2500 handlers/            # Legacy platform-specific handlers\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 base_handler.py          # Abstract base handler\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 instagram_handler.py     # Instagram download handler\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 reddit_handler.py        # Reddit download handler\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 twitter_handler.py       # Twitter/X download handler\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 youtube_handler.py       # YouTube download handler\n    \u2502   \u2502   \u251c\u2500\u2500 manager.py           # Download manager coordinating strategies\n    \u2502   \u2502   \u2514\u2500\u2500 strategies/          # Strategy pattern implementations (Epic 5)\n    \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502       \u251c\u2500\u2500 base_strategy.py         # Abstract strategy interface\n    \u2502   \u2502       \u251c\u2500\u2500 instagram_strategy.py    # Instagram strategy (API/CLI modes)\n    \u2502   \u2502       \u251c\u2500\u2500 reddit_strategy.py       # Reddit strategy (API/CLI modes)\n    \u2502   \u2502       \u251c\u2500\u2500 twitter_strategy.py      # Twitter/X strategy (API/CLI modes)\n    \u2502   \u2502       \u2514\u2500\u2500 youtube_strategy.py      # YouTube strategy (API/CLI modes)\n    \u2502   \u251c\u2500\u2500 env.py                   # Environment configuration (BossSettings)\n    \u2502   \u251c\u2500\u2500 queue/                   # Queue management system\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2514\u2500\u2500 manager.py           # QueueManager for download task management\n    \u2502   \u251c\u2500\u2500 services/                # Core service implementations\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 uploads/                 # Upload management system\n    \u2502       \u251c\u2500\u2500 __init__.py\n    \u2502       \u251c\u2500\u2500 config/              # Upload configuration\n    \u2502       \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502       \u251c\u2500\u2500 manager.py           # Upload manager for Discord attachments\n    \u2502       \u251c\u2500\u2500 models.py            # Upload-related data models\n    \u2502       \u251c\u2500\u2500 processors/          # Upload processors\n    \u2502       \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502       \u2502   \u2514\u2500\u2500 discord_processor.py  # Discord-specific upload processing\n    \u2502       \u2514\u2500\u2500 utils/               # Upload utility functions\n    \u2502           \u251c\u2500\u2500 __init__.py\n    \u2502           \u251c\u2500\u2500 batch_processor.py    # Batch upload processing\n    \u2502           \u251c\u2500\u2500 file_detector.py      # File type detection for uploads\n    \u2502           \u2514\u2500\u2500 size_analyzer.py      # File size analysis and chunking\n    \u251c\u2500\u2500 downloaders/                 # Legacy downloaders (to be removed)\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 base.py                 # Legacy base downloader\n    \u251c\u2500\u2500 global_cogs/                 # Legacy global cogs (to be removed)\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 downloads.py            # Legacy download cog\n    \u2502   \u2514\u2500\u2500 queue.py                # Legacy queue cog\n    \u251c\u2500\u2500 integrations/               # External service integrations\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 anthropic/              # Anthropic API integration\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 langsmith/              # LangSmith monitoring integration\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 openai/                 # OpenAI API integration\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 webhooks/               # Webhook integrations\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 main_bot.py                 # Legacy bot entry point (use __main__.py)\n    \u251c\u2500\u2500 monitoring/                 # Monitoring and observability\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 health/                 # Health check system\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 checker.py         # Health check coordinator\n    \u2502   \u2502   \u2514\u2500\u2500 checks/            # Individual health checks\n    \u2502   \u2502       \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 health.py              # Legacy health module\n    \u2502   \u251c\u2500\u2500 health_check.py        # Legacy health check module\n    \u2502   \u251c\u2500\u2500 health_checks/          # Legacy health checks directory\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 logging/                # Logging configuration\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 interceptor.py    # Loguru interceptor for stdlib logging\n    \u2502   \u2502   \u2514\u2500\u2500 logging_config.py  # Main logging configuration\n    \u2502   \u251c\u2500\u2500 logging.py             # Legacy logging module\n    \u2502   \u251c\u2500\u2500 metrics/                # Metrics collection\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 collector.py       # Metrics collector implementation\n    \u2502   \u2502   \u2514\u2500\u2500 exporters/         # Metrics exporters (Prometheus, etc.)\n    \u2502   \u2502       \u2514\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 metrics.py             # Legacy metrics module\n    \u251c\u2500\u2500 schemas/                    # Data schemas and validation\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 ai/                    # AI-related schemas\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 api/                   # API request/response schemas\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 discord/               # Discord-specific schemas\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 storage/                    # Storage management system\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 backends/               # Storage backend implementations\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 cleanup/                # Storage cleanup utilities\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 managers/               # Storage managers\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 quota_manager.py   # User quota management\n    \u2502   \u2502   \u2514\u2500\u2500 validation_manager.py  # File validation manager\n    \u2502   \u251c\u2500\u2500 migrations/             # Database migrations (future)\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 models/                 # Storage data models\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 quotas/                 # Legacy quotas directory\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 quotas_manager.py       # Legacy quota manager\n    \u2502   \u251c\u2500\u2500 validation_manager.py   # Legacy validation manager\n    \u2502   \u2514\u2500\u2500 validations/            # Legacy validations directory\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 utils/                      # Shared utility functions\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 asynctyper.py           # Async wrapper for Typer CLI\n</code></pre>"},{"location":"project_structure/#module-organization-notes","title":"Module Organization Notes","text":""},{"location":"project_structure/#current-issues","title":"Current Issues","text":"<ol> <li>Duplicate modules: Several modules have both legacy and new versions (e.g., health.py vs health/, logging.py vs logging/)</li> <li>Legacy directories: <code>global_cogs/</code>, <code>downloaders/</code>, and <code>commands/</code> should be removed</li> <li>Inconsistent structure: Some modules are split unnecessarily (e.g., storage has both directories and files for the same functionality)</li> </ol>"},{"location":"project_structure/#key-architectural-components","title":"Key Architectural Components","text":""},{"location":"project_structure/#strategy-pattern-epic-5","title":"Strategy Pattern (Epic 5) \u2705","text":"<p>The download strategies in <code>core/downloads/strategies/</code> implement a flexible pattern for platform-specific downloads with: - API-direct mode using gallery-dl/yt-dlp Python APIs - CLI fallback mode using subprocess calls - Feature flag control via environment variables - Automatic fallback on API failures</p>"},{"location":"project_structure/#core-services","title":"Core Services","text":"<ul> <li>QueueManager: Manages download task queue with priority and concurrency control</li> <li>DownloadManager: Coordinates download strategies and handles platform detection</li> <li>BossSettings: Centralized configuration management using pydantic-settings</li> </ul>"},{"location":"project_structure/#discord-integration","title":"Discord Integration","text":"<ul> <li>Custom bot client extending discord.py Bot</li> <li>Modular cog system for command organization</li> <li>Event-driven architecture for Discord interactions</li> </ul>"},{"location":"project_structure/#cli-development","title":"CLI Development","text":"<ul> <li>Typer-based CLI with subcommand architecture</li> <li>Rich formatting for enhanced console output</li> <li>Async support via asynctyper utility</li> </ul>"},{"location":"project_structure/#recommended-cleanup-actions","title":"Recommended Cleanup Actions","text":"<ol> <li>Remove legacy modules and consolidate duplicates</li> <li>Complete migration from handlers to strategies</li> <li>Implement planned AI components in the <code>ai/</code> directory</li> <li>Develop the API layer for programmatic access</li> <li>Expand storage backends beyond local filesystem</li> </ol>"},{"location":"project_structure/#test-structure","title":"Test Structure","text":"<pre><code>tests/                                  # Test suite root\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 conftest.py                        # Global pytest fixtures\n\u251c\u2500\u2500 example_vcr_test.py               # VCR.py example test\n\u251c\u2500\u2500 fixtures/                          # Test data and configuration files\n\u2502   \u251c\u2500\u2500 gallery_dl.conf               # Test gallery-dl configuration\n\u2502   \u251c\u2500\u2500 info.json                     # Test metadata\n\u2502   \u2514\u2500\u2500 *.json                        # Various test data files\n\u251c\u2500\u2500 test_bot/                          # Bot-related tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py                   # Bot-specific fixtures\n\u2502   \u251c\u2500\u2500 test_client.py                # BossBot client tests\n\u2502   \u251c\u2500\u2500 test_cogs/                    # Cog tests (comprehensive)\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 test_admin_dpytest.py    # Admin cog tests with dpytest\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads.py        # Download cog unit tests\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads_direct.py # Direct download tests\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads_dpytest.py # Download tests with dpytest\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads_integration.py    # Download integration tests\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads_upload_integration.py # Upload integration tests\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads_vcr_integration.py    # VCR integration tests\n\u2502   \u2502   \u251c\u2500\u2500 test_queue_dpytest.py    # Queue tests with dpytest\n\u2502   \u2502   \u2514\u2500\u2500 test_youtube_download_dpytest.py # YouTube-specific tests\n\u2502   \u251c\u2500\u2500 test_core.py                  # Core bot functionality tests\n\u2502   \u251c\u2500\u2500 test_download_cog.py         # Legacy download cog tests\n\u2502   \u251c\u2500\u2500 test_help.py                 # Help command tests\n\u2502   \u251c\u2500\u2500 test_queue.py                # Queue functionality tests\n\u2502   \u2514\u2500\u2500 test_queue_cog.py            # Legacy queue cog tests\n\u251c\u2500\u2500 test_cli/                         # CLI tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 cassettes/                   # VCR.py recordings for CLI tests\n\u2502   \u2502   \u2514\u2500\u2500 test_fetch_vcr/         # Recorded HTTP interactions\n\u2502   \u251c\u2500\u2500 test_commands/              # CLI command tests\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 test_download.py       # General download command tests\n\u2502   \u2502   \u2514\u2500\u2500 test_download_reddit.py # Reddit-specific CLI tests\n\u2502   \u251c\u2500\u2500 test_doctor.py              # Doctor command tests\n\u2502   \u2514\u2500\u2500 test_fetch_vcr.py           # VCR fetch tests\n\u251c\u2500\u2500 test_commands/                    # Legacy commands tests\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 test_core/                        # Core logic tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_compression/            # Compression system tests\n\u2502   \u2502   \u251c\u2500\u2500 conftest.py             # Compression test fixtures\n\u2502   \u2502   \u251c\u2500\u2500 test_bitrate_calculator.py # Bitrate calculation tests\n\u2502   \u2502   \u2514\u2500\u2500 test_file_detector.py   # File detection tests\n\u2502   \u251c\u2500\u2500 test_downloads/              # Download system tests\n\u2502   \u2502   \u251c\u2500\u2500 test_strategies/        # Strategy pattern tests\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_twitter_strategy.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_reddit_strategy.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_instagram_strategy.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_youtube_strategy.py\n\u2502   \u2502   \u251c\u2500\u2500 test_clients/           # Download client tests\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_aio_gallery_dl.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_aio_yt_dlp.py\n\u2502   \u2502   \u2514\u2500\u2500 test_manager.py         # Download manager tests\n\u2502   \u251c\u2500\u2500 test_env.py                 # Configuration tests\n\u2502   \u2514\u2500\u2500 test_queue/                 # Queue system tests\n\u2502       \u2514\u2500\u2500 test_manager.py         # Queue manager tests\n\u2514\u2500\u2500 test_integration/                # Full integration tests\n    \u2514\u2500\u2500 test_end_to_end.py          # End-to-end workflow tests\n</code></pre>"},{"location":"project_structure/#technology-stack","title":"Technology Stack","text":""},{"location":"project_structure/#core-framework","title":"Core Framework","text":"<ul> <li>Python 3.12+: Modern Python with full type hints</li> <li>discord.py: Discord bot framework</li> <li>Pydantic/Pydantic-Settings: Data validation and settings management</li> </ul>"},{"location":"project_structure/#cli-ui","title":"CLI &amp; UI","text":"<ul> <li>Typer: CLI framework with subcommand support</li> <li>Rich: Terminal formatting and progress bars</li> <li>asynctyper: Async wrapper for Typer</li> </ul>"},{"location":"project_structure/#download-tools","title":"Download Tools","text":"<ul> <li>gallery-dl: Multi-platform media downloader (API and CLI modes)</li> <li>yt-dlp: YouTube and video platform downloader</li> </ul>"},{"location":"project_structure/#aiml-stack-plannedin-development","title":"AI/ML Stack (Planned/In Development)","text":"<ul> <li>LangChain: Framework for AI chain development</li> <li>LangGraph: Multi-step AI workflow orchestration</li> <li>LangSmith: LLM application monitoring</li> <li>Anthropic/OpenAI: LLM providers</li> </ul>"},{"location":"project_structure/#development-tools","title":"Development Tools","text":"<ul> <li>uv: Fast Python package manager</li> <li>ruff: Python linter and formatter</li> <li>pytest: Testing framework with async support</li> <li>dpytest: Discord.py testing utilities</li> <li>just: Task runner (like Make)</li> </ul>"},{"location":"project_structure/#infrastructure","title":"Infrastructure","text":"<ul> <li>Loguru: Structured logging</li> <li>FFmpeg: Media processing</li> <li>GitHub Actions: CI/CD</li> </ul>"},{"location":"project_structure/#entry-points-and-execution-flow","title":"Entry Points and Execution Flow","text":""},{"location":"project_structure/#discord-bot","title":"Discord Bot","text":"<pre><code># Main entry point\ngoobctl go                    # Starts the Discord bot\n# OR\nuv run python -m boss_bot     # Direct module execution\n</code></pre> <p>Execution Flow: 1. <code>__main__.py</code> \u2192 Entry point 2. <code>bot/client.py</code> \u2192 BossBot initialization 3. <code>core/env.py</code> \u2192 Load configuration from environment 4. Bot registers cogs from <code>bot/cogs/</code> 5. Bot connects to Discord and starts event loop</p>"},{"location":"project_structure/#cli-interface","title":"CLI Interface","text":"<pre><code># CLI entry points\nbossctl &lt;command&gt;             # Main CLI command (via pyproject.toml)\n# OR\nuv run python -m boss_bot.cli.main &lt;command&gt;\n</code></pre> <p>CLI Commands: - <code>bossctl download &lt;platform&gt; &lt;url&gt;</code> \u2192 Download media - <code>bossctl download strategies</code> \u2192 Show strategy configuration - <code>bossctl download info</code> \u2192 Show platform information</p>"},{"location":"project_structure/#component-interactions","title":"Component Interactions","text":""},{"location":"project_structure/#download-flow","title":"Download Flow","text":"<pre><code>User Command \u2192 Discord Cog/CLI Command\n     \u2193\nDownload Manager (checks platform)\n     \u2193\nStrategy Selection (based on URL pattern)\n     \u2193\nFeature Flag Check (API vs CLI mode)\n     \u2193\nExecute Strategy \u2192 API Client or Subprocess\n     \u2193\nQueue Manager (tracks progress)\n     \u2193\nUpload Manager (Discord file handling)\n     \u2193\nResponse to User\n</code></pre>"},{"location":"project_structure/#key-integration-points","title":"Key Integration Points","text":"<ol> <li>Settings Injection: <code>BossSettings</code> is passed through constructors</li> <li>Queue System: Centralized task management via <code>QueueManager</code></li> <li>Strategy Pattern: Platform-specific logic isolated in strategies</li> <li>Event-Driven: Discord events trigger bot actions</li> <li>Async Throughout: All I/O operations use async/await</li> </ol>"},{"location":"project_structure/#development-workflow","title":"Development Workflow","text":""},{"location":"project_structure/#setup","title":"Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/bossjones/boss-bot.git\ncd boss-bot\n\n# Install dependencies\njust uv-update\n\n# Copy environment template\ncp .env.example .env\n# Edit .env with your configuration\n</code></pre>"},{"location":"project_structure/#common-tasks","title":"Common Tasks","text":"<pre><code># Run tests\njust check-test\n\n# Run specific test\njust check-test \"tests/test_bot/test_client.py::test_function\"\n\n# Format code\njust format\n\n# Lint and type check\njust check-code\njust check-type\n\n# Full validation\njust check\n\n# Run bot locally\ngoobctl go\n</code></pre>"},{"location":"project_structure/#environment-variables","title":"Environment Variables","text":"<p>Key configuration via <code>.env</code>: <pre><code># Discord Configuration\nDISCORD_TOKEN=your_token_here\nDISCORD_COMMAND_PREFIX=$\n\n# Feature Flags (Strategy Pattern)\nTWITTER_USE_API_CLIENT=true\nREDDIT_USE_API_CLIENT=true\nINSTAGRAM_USE_API_CLIENT=true\nYOUTUBE_USE_API_CLIENT=true\nDOWNLOAD_API_FALLBACK_TO_CLI=true\n\n# Paths\nBOSS_BOT_DOWNLOAD_DIR=./downloads\n\n# Logging\nBOSS_BOT_LOG_LEVEL=INFO\n</code></pre></p>"},{"location":"project_structure/#testing-guidelines","title":"Testing Guidelines","text":"<ul> <li>All tests use pytest with async support</li> <li>Fixtures follow <code>fixture_*</code> naming convention</li> <li>Mock Discord components with pytest-mock</li> <li>Integration tests validate full workflows</li> <li>Use <code>@pytest.mark.skip_until</code> for WIP features</li> </ul>"},{"location":"project_structure/#code-style","title":"Code Style","text":"<ul> <li>120 character line length</li> <li>Google docstring style</li> <li>Type hints required</li> <li>Ruff for formatting/linting</li> <li>No inline comments unless necessary</li> </ul>"},{"location":"vcr/","title":"pytest-recording: A pytest plugin that allows you recording of network interactions via VCR.py","text":""},{"location":"vcr/#getting-started","title":"Getting Started","text":""},{"location":"vcr/#termonology","title":"Termonology","text":"<p>Before working with pytest-recording, it is important to understand that it is built on top of VCR.py. Below are the terms used in this document, with links to more information.</p> <ul> <li>Cassette: A recording of network interactions.</li> <li>VCR: A library that records and replays network interactions.</li> <li>VCR.py: A Python library that provides a VCR-like interface to network interactions. Learn more here. Originally inspired by Ruby's VCR.</li> <li>pytest-recording: A pytest plugin that provides a VCR-like interface to network interactions. Learn more here. It is an alternative to pytest-vcr.</li> <li>Recording: The process of capturing network interactions and saving them as a cassette.</li> <li>Replaying: The process of playing back network interactions from a cassette.</li> <li>Stubbing: The process of replacing a live URL with a local file or directory of files.</li> <li>Mode: The behavior of VCR.py when recording and replaying network interactions. Learn more here.</li> </ul>"},{"location":"vcr/#set-up-credentials","title":"Set up credentials","text":"<p>An OpenAI, and Claude API key is required to make live calls to the LLM, or to run tests without vcr (see Running tests section).</p>"},{"location":"vcr/#record-modes","title":"Record Modes","text":"<p>VCR supports 4 record modes (with the same behavior as Ruby's VCR):</p>"},{"location":"vcr/#once","title":"once","text":"<ul> <li>Replay previously recorded interactions.</li> <li>Record new interactions if there is no cassette file.</li> <li>Cause an error to be raised for new requests if there is a cassette file.</li> </ul> <p>It is similar to the new_episodes record mode, but will prevent new, unexpected requests from being made (e.g. because the request URI changed).</p> <p>once is the default record mode, used when you do not set one.</p>"},{"location":"vcr/#new_episodes","title":"new_episodes","text":"<ul> <li>Record new interactions.</li> <li>Replay previously recorded interactions. It is similar to the once record mode, but will always record new interactions, even if you have an existing recorded one that is similar, but not identical.</li> </ul> <p>This was the default behavior in versions &lt; 0.3.0</p>"},{"location":"vcr/#none","title":"none","text":"<ul> <li>Replay previously recorded interactions.</li> <li>Cause an error to be raised for any new requests. This is useful when your code makes potentially dangerous HTTP requests. The none record mode guarantees that no new HTTP requests will be made.</li> </ul>"},{"location":"vcr/#all","title":"all","text":"<ul> <li>Record new interactions.</li> <li>Never replay previously recorded interactions. This can be temporarily used to force VCR to re-record a cassette (i.e. to ensure the responses are not out of date) or can be used when you simply want to log all HTTP requests.</li> </ul>"},{"location":"vcr/#additional-recording-options","title":"Additional Recording Options","text":"<p>pytest-recording provides additional options/features to record and replay network interactions. specifically: - Straightforward pytest.mark.vcr, that reflects VCR.use_cassettes API; - Combining multiple VCR cassettes; - Network access blocking; - The rewrite recording mode that rewrites cassettes from scratch.</p>"},{"location":"vcr/#example-step-by-step-guide-to-recording","title":"Example step by step guide to recording","text":"<ol> <li>Run tests without vcr (i.e. without recording) to establish baseline and make sure your tests are working.</li> <li>Run tests with vcr <code>--record-mode=all</code> (i.e. recording) to record responses. For tests that you want to record responses for, use <code>@pytest.mark.vcr()</code>. Learn more here or by looking through existing tests that have cassettes recorded. <code>NOTE: during this recording time, your test MAY fail if the response has not been recorded yet. If so, wait until the recording is complete and re-run the test.</code></li> <li>You should now have a cassette directory in <code>tests</code> directory similar to: <code>/Users/malcolm/dev/malcolm/ada-agent/app/test/surfaces/slack/cassettes</code>. Example of what a cassette file looks like here.</li> <li>Rerun tests with vcr <code>--record-mode=none</code> to replay the cassettes. This is now the default in make local-unittests.</li> </ol>"},{"location":"vcr/#troubleshooting-notes","title":"Troubleshooting NOTES:","text":"<ol> <li>PLEASE NOTE, this has not been ported over to docker yet, so you may need to run it locally till then.</li> <li>If you get an error with a message like <code>VCR.Errors.CannotOverwriteExistingCassette: A cassette for this request already exists and VCR cannot automatically determine how to handle that. Please delete the existing cassette or set the</code>record_mode<code>to</code>all<code>or</code>new_episodes<code>.</code> then you need to re-run the tests with <code>--record-mode=all</code> to overwrite the existing cassette.</li> <li>If you get an error with a message like <code>VCR.Errors.UnhandledHTTPRequestError: Unhandled HTTP request: GET http://example.com/</code>. This can happen if the request is not recorded in the cassette. You can fix this by:<ul> <li>deleting the cassette file and re-running the tests with <code>--record-mode=all</code> to record the response.</li> <li>adding a <code>@pytest.mark.vcr()</code> to the test that is failing.</li> <li>adding a <code>@pytest.mark.vcr(record_mode='all')</code> to the test that is failing.</li> </ul> </li> </ol>"},{"location":"versioning/","title":"Versioning","text":"<p>The firmware, python interface (aka controller), and GUI (aka controller_gui) are versioned independently. Each one gets a version number as following:</p> <ul> <li>Firmware: <code>vXYZ</code></li> <li>Controller: <code>vX.Y.Z</code></li> <li>GUI: <code>vX.Y.Z</code></li> </ul> <p>The overall project is also versioned and tagged on GitHub. Each tag is named <code>vX.Y.Z</code> and is associated with a release.</p> <p>In above examples, <code>X</code>, <code>Y</code>, and <code>Z</code> follow semantic versioning guidelines and are:</p> <ul> <li><code>X</code>: Major version (breaking changes)</li> <li><code>Y</code>: Minor version (non-breaking changes)</li> <li><code>Z</code>: Patch version (bug fixes)</li> </ul> <p>The dependency flow is as following:</p> <pre><code>graph LR\n  A[Firmware] --&gt; B[controller];\n  B --&gt; C[GUI];\n</code></pre> <p>This means, e.g., that the GUI depends on both, the firmware and the controller.</p> <p>Generally, major and minor versions for every part of the project should agree with each other. The patch version can and will differ.</p> <p>!!! note The patch version for the whole project is the sum of all the firmware, controller, and GUI patch versions.</p>"},{"location":"versioning/#major-versions","title":"Major versions","text":"<p>Major versions are reserved for breaking changes. This means that either the hardware, firmware, or the software are incompatible with previous versions.</p>"},{"location":"versioning/#minor-versions","title":"Minor versions","text":"<p>Minor versions are reserved for non-breaking changes, where new features are added. A GUI version <code>v0.1.0</code> will be compatible with firmware versions <code>v010</code> and <code>v020</code> as well as with controller versions <code>v0.1.0</code> and <code>v0.2.0</code>.</p> <p>!!! warning The GUI with version <code>v0.2.0</code> however will not be compatible with firmware or controller versions that are</p> <p>lower than <code>v0.2.0</code>. The same holds true for the controller, which will not be compatible with firmware versions with lower minor versions.</p>"},{"location":"versioning/#patch-versions","title":"Patch versions","text":"<p>Patch versions are reserved for bug fixes.</p>"},{"location":"versioning/#changelog","title":"Changelog","text":"<p>See here.</p>"},{"location":"workflow-rules/","title":"Cursor Workflow Rules","text":"<p>This project has been updated to use the auto rule generator from cursor-auto-rules-agile-workflow.</p> <p>Note: This script can be safely re-run at any time to update the template rules to their latest versions. It will not impact or overwrite any custom rules you've created.</p>"},{"location":"workflow-rules/#core-features","title":"Core Features","text":"<ul> <li>Automated rule generation</li> <li>Standardized documentation formats</li> <li>Supports all 4 Note Types automatically</li> <li>AI behavior control and optimization</li> <li>Flexible workflow integration options</li> </ul>"},{"location":"workflow-rules/#getting-started","title":"Getting Started","text":"<ol> <li>Review the templates in `xnotes/`</li> <li>Choose your preferred workflow approach</li> <li>Start using the AI with confidence!</li> </ol> <p>For demos and tutorials, visit: BMad Code Videos</p>"},{"location":"contributors/dpytest_example/","title":"Discord.py Bot Testing with dpytest","text":"<p>This guide explains how to use <code>dpytest</code> for testing Discord bots built with <code>discord.py</code>. The examples are based on best practices from projects like KoalaBot and the official dpytest documentation.</p>"},{"location":"contributors/dpytest_example/#overview","title":"Overview","text":"<p><code>dpytest</code> is a testing library specifically designed for Discord bots using <code>discord.py</code>. It allows you to: - Simulate Discord messages and interactions - Test bot commands without connecting to Discord - Verify bot responses and behavior - Create isolated test environments</p>"},{"location":"contributors/dpytest_example/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>discord.py (rewrite version)</li> <li>pytest and pytest-asyncio</li> <li>Basic understanding of async/await in Python</li> </ul>"},{"location":"contributors/dpytest_example/#installation","title":"Installation","text":"<pre><code># Install required packages\npip install dpytest pytest pytest-asyncio\n\n# Or with uv (recommended for this project)\nuv add --dev dpytest pytest pytest-asyncio\n</code></pre>"},{"location":"contributors/dpytest_example/#project-structure","title":"Project Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py          # Shared fixtures and configuration\n\u251c\u2500\u2500 test_bot/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_basic_commands.py\n\u2502   \u251c\u2500\u2500 test_cogs/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 test_downloads.py\n\u2502   \u2502   \u2514\u2500\u2500 test_queue.py\n\u2502   \u2514\u2500\u2500 test_error_handling.py\n\u2514\u2500\u2500 fixtures/            # Test data files\n    \u2514\u2500\u2500 sample_data.json\n</code></pre>"},{"location":"contributors/dpytest_example/#setting-up-conftestpy","title":"Setting Up conftest.py","text":"<p>The <code>conftest.py</code> file contains shared fixtures that pytest automatically makes available to all test files:</p> <pre><code>\"\"\"Test configuration and fixtures for Discord bot testing.\"\"\"\n\nimport pytest\nimport pytest_asyncio\nimport discord\nimport discord.ext.commands as commands\nimport discord.ext.test as dpytest\nfrom unittest.mock import Mock, AsyncMock\n\nfrom boss_bot.bot.client import BossBot\nfrom boss_bot.core.env import BossSettings\n\n\n@pytest_asyncio.fixture(scope=\"function\")\nasync def bot_settings():\n    \"\"\"Create test settings for the bot.\"\"\"\n    return BossSettings(\n        discord_token=\"test_token_123\",\n        openai_api_key=\"test_openai_key\",\n        langchain_api_key=\"test_langchain_key\",\n        langchain_hub_api_key=\"test_hub_key\",\n        debug=True,\n        environment=\"testing\"\n    )\n\n\n@pytest_asyncio.fixture(scope=\"function\")\nasync def mock_bot(bot_settings):\n    \"\"\"Create a mocked BossBot instance for testing.\"\"\"\n    # Create bot with proper intents\n    intents = discord.Intents.default()\n    intents.message_content = True\n    intents.members = True\n\n    # Mock external dependencies\n    mock_download_manager = Mock()\n    mock_download_manager.validate_url.return_value = True\n    mock_download_manager.get_active_downloads.return_value = 0\n\n    mock_queue_manager = Mock()\n    mock_queue_manager.queue_size = 0\n    mock_queue_manager.add_to_queue = AsyncMock()\n\n    bot = BossBot(\n        command_prefix=\"!\",\n        intents=intents,\n        settings=bot_settings\n    )\n\n    # Inject mocked dependencies\n    bot.download_manager = mock_download_manager\n    bot.queue_manager = mock_queue_manager\n\n    # Setup bot for testing\n    await bot._async_setup_hook()\n\n    # Load cogs if needed\n    # await bot.load_extension(\"boss_bot.bot.cogs.downloads\")\n\n    # Configure dpytest\n    dpytest.configure(bot)\n\n    yield bot\n\n    # Cleanup after test\n    await dpytest.empty_queue()\n\n\n@pytest_asyncio.fixture(scope=\"function\")\nasync def ctx_mock():\n    \"\"\"Create a mocked Context for direct command testing.\"\"\"\n    ctx = Mock(spec=commands.Context)\n    ctx.send = AsyncMock()\n    ctx.author = Mock()\n    ctx.author.id = 12345\n    ctx.channel = Mock()\n    ctx.channel.id = 67890\n    return ctx\n\n\n@pytest.fixture(scope=\"function\")\ndef sample_urls():\n    \"\"\"Provide sample URLs for testing.\"\"\"\n    return {\n        \"twitter\": \"https://twitter.com/user/status/123456789\",\n        \"reddit\": \"https://reddit.com/r/python/comments/abc123/test\",\n        \"youtube\": \"https://youtube.com/watch?v=dQw4w9WgXcQ\",\n        \"instagram\": \"https://instagram.com/p/ABC123/\",\n        \"invalid\": \"not-a-url\"\n    }\n</code></pre>"},{"location":"contributors/dpytest_example/#basic-command-testing","title":"Basic Command Testing","text":"<p>Here's how to test basic bot commands using dpytest:</p> <pre><code>\"\"\"Test basic bot commands.\"\"\"\n\nimport pytest\nimport pytest_asyncio\nimport discord.ext.test as dpytest\n\n\n@pytest.mark.asyncio\nasync def test_ping_command(mock_bot):\n    \"\"\"Test basic ping command.\"\"\"\n    await dpytest.message(\"!ping\")\n\n    # Verify bot responded\n    assert dpytest.verify().message().contains().content(\"Pong\")\n\n\n@pytest.mark.asyncio\nasync def test_help_command(mock_bot):\n    \"\"\"Test help command shows available commands.\"\"\"\n    await dpytest.message(\"!help\")\n\n    # Check that help message was sent\n    assert dpytest.verify().message().content().contains(\"Commands\")\n\n\n@pytest.mark.asyncio\nasync def test_invalid_command(mock_bot):\n    \"\"\"Test bot handles invalid commands gracefully.\"\"\"\n    await dpytest.message(\"!invalid_command\")\n\n    # Should either ignore or send error message\n    # Depends on your bot's error handling\n    # This example assumes the bot sends an error message\n    assert dpytest.verify().message().contains().content(\"not found\")\n</code></pre>"},{"location":"contributors/dpytest_example/#testing-cogs-and-complex-commands","title":"Testing Cogs and Complex Commands","text":"<p>For testing individual cogs or complex commands:</p> <pre><code>\"\"\"Test download cog functionality.\"\"\"\n\nimport pytest\nimport pytest_asyncio\nimport discord.ext.test as dpytest\nfrom unittest.mock import Mock, AsyncMock, patch\n\nfrom boss_bot.bot.cogs.downloads import DownloadCog\n\n\n@pytest.mark.asyncio\nasync def test_download_command_twitter(mock_bot, sample_urls):\n    \"\"\"Test downloading from Twitter URL.\"\"\"\n    # Mock successful download\n    with patch('boss_bot.core.downloads.strategies.TwitterDownloadStrategy.download') as mock_download:\n        mock_metadata = Mock()\n        mock_metadata.error = None\n        mock_metadata.title = \"Test Tweet\"\n        mock_metadata.download_method = \"cli\"\n        mock_download.return_value = mock_metadata\n\n        await dpytest.message(f\"!download {sample_urls['twitter']}\")\n\n        # Verify success message\n        assert dpytest.verify().message().contains().content(\"\u2705\")\n        assert dpytest.verify().message().contains().content(\"download completed\")\n\n\n@pytest.mark.asyncio\nasync def test_download_command_failure(mock_bot, sample_urls):\n    \"\"\"Test download command with failure.\"\"\"\n    with patch('boss_bot.core.downloads.strategies.TwitterDownloadStrategy.download') as mock_download:\n        mock_metadata = Mock()\n        mock_metadata.error = \"Download failed: Network error\"\n        mock_download.return_value = mock_metadata\n\n        await dpytest.message(f\"!download {sample_urls['twitter']}\")\n\n        # Verify error message\n        assert dpytest.verify().message().contains().content(\"\u274c\")\n        assert dpytest.verify().message().contains().content(\"failed\")\n\n\n@pytest.mark.asyncio\nasync def test_info_command(mock_bot, sample_urls):\n    \"\"\"Test metadata info command.\"\"\"\n    with patch('boss_bot.core.downloads.strategies.TwitterDownloadStrategy.get_metadata') as mock_metadata:\n        mock_meta = Mock()\n        mock_meta.title = \"Sample Tweet Title\"\n        mock_meta.uploader = \"@testuser\"\n        mock_meta.upload_date = \"2024-01-01\"\n        mock_meta.like_count = 42\n        mock_metadata.return_value = mock_meta\n\n        await dpytest.message(f\"!info {sample_urls['twitter']}\")\n\n        # Verify metadata is shown\n        assert dpytest.verify().message().contains().content(\"Twitter Content Info\")\n        assert dpytest.verify().message().contains().content(\"Sample Tweet Title\")\n\n\n@pytest.mark.asyncio\nasync def test_strategies_command(mock_bot):\n    \"\"\"Test strategies configuration command.\"\"\"\n    await dpytest.message(\"!strategies\")\n\n    # Verify configuration is displayed\n    assert dpytest.verify().message().contains().content(\"Download Strategy Configuration\")\n    assert dpytest.verify().message().contains().content(\"Twitter/X\")\n    assert dpytest.verify().message().contains().content(\"CLI Mode\")\n</code></pre>"},{"location":"contributors/dpytest_example/#direct-command-testing-without-dpytest","title":"Direct Command Testing (Without dpytest)","text":"<p>Sometimes you need to test command logic directly without the Discord simulation:</p> <pre><code>\"\"\"Test command logic directly.\"\"\"\n\nimport pytest\nimport pytest_asyncio\nfrom unittest.mock import Mock, AsyncMock\n\nfrom boss_bot.bot.cogs.downloads import DownloadCog\n\n\n@pytest.mark.asyncio\nasync def test_download_callback_direct(ctx_mock, bot_settings):\n    \"\"\"Test download command callback directly.\"\"\"\n    # Create cog instance\n    mock_bot = Mock()\n    mock_bot.settings = bot_settings\n    cog = DownloadCog(mock_bot)\n\n    # Mock strategy\n    mock_strategy = Mock()\n    mock_metadata = Mock()\n    mock_metadata.error = None\n    mock_metadata.title = \"Test Content\"\n    mock_strategy.download = AsyncMock(return_value=mock_metadata)\n\n    # Patch strategy lookup\n    with patch.object(cog, '_get_strategy_for_url', return_value=mock_strategy):\n        # Call command callback directly\n        await cog.download.callback(cog, ctx_mock, \"https://twitter.com/test\")\n\n    # Verify context.send was called with success message\n    assert ctx_mock.send.called\n    sent_args = ctx_mock.send.call_args[0][0]\n    assert \"\u2705\" in sent_args\n\n\n@pytest.mark.asyncio\nasync def test_get_platform_info(bot_settings):\n    \"\"\"Test platform info helper method.\"\"\"\n    mock_bot = Mock()\n    mock_bot.settings = bot_settings\n    cog = DownloadCog(mock_bot)\n\n    # Test different platforms\n    twitter_info = cog._get_platform_info(\"https://twitter.com/test\")\n    assert twitter_info[\"emoji\"] == \"\ud83d\udc26\"\n    assert twitter_info[\"name\"] == \"Twitter/X\"\n\n    reddit_info = cog._get_platform_info(\"https://reddit.com/r/test\")\n    assert reddit_info[\"emoji\"] == \"\ud83e\udd16\"\n    assert reddit_info[\"name\"] == \"Reddit\"\n</code></pre>"},{"location":"contributors/dpytest_example/#testing-error-handling","title":"Testing Error Handling","text":"<pre><code>\"\"\"Test error handling scenarios.\"\"\"\n\nimport pytest\nimport pytest_asyncio\nimport discord.ext.test as dpytest\nfrom unittest.mock import patch\n\n\n@pytest.mark.asyncio\nasync def test_download_network_error(mock_bot, sample_urls):\n    \"\"\"Test handling of network errors during download.\"\"\"\n    with patch('boss_bot.core.downloads.strategies.TwitterDownloadStrategy.download') as mock_download:\n        mock_download.side_effect = ConnectionError(\"Network unreachable\")\n\n        await dpytest.message(f\"!download {sample_urls['twitter']}\")\n\n        # Verify error is handled gracefully\n        assert dpytest.verify().message().contains().content(\"\u274c\")\n        assert dpytest.verify().message().contains().content(\"error\")\n\n\n@pytest.mark.asyncio\nasync def test_invalid_url_handling(mock_bot, sample_urls):\n    \"\"\"Test handling of invalid URLs.\"\"\"\n    await dpytest.message(f\"!download {sample_urls['invalid']}\")\n\n    # Should fall back to queue system or show error\n    # Verify appropriate response based on your bot's behavior\n    assert dpytest.verify().message()  # Some response should be sent\n\n\n@pytest.mark.asyncio\nasync def test_command_permission_error(mock_bot):\n    \"\"\"Test commands with insufficient permissions.\"\"\"\n    # Simulate permission error\n    with patch('discord.ext.commands.Context.send', side_effect=discord.Forbidden(Mock(), \"Insufficient permissions\")):\n        await dpytest.message(\"!download https://twitter.com/test\")\n\n        # dpytest should handle this gracefully\n        # The exact assertion depends on your error handling\n</code></pre>"},{"location":"contributors/dpytest_example/#advanced-testing-patterns","title":"Advanced Testing Patterns","text":""},{"location":"contributors/dpytest_example/#testing-with-different-user-permissions","title":"Testing with Different User Permissions","text":"<pre><code>@pytest.mark.asyncio\nasync def test_admin_only_command(mock_bot):\n    \"\"\"Test command that requires admin permissions.\"\"\"\n    # Get test guild and member\n    config = dpytest.get_config()\n    guild = config.guilds[0]\n\n    # Create admin member\n    admin_member = await dpytest.member_join(permissions=discord.Permissions.all())\n\n    # Test with admin user\n    await dpytest.message(\"!admin_command\", member=admin_member)\n    assert dpytest.verify().message().contains().content(\"Admin command executed\")\n\n    # Test with regular user (should fail)\n    regular_member = await dpytest.member_join()\n    await dpytest.message(\"!admin_command\", member=regular_member)\n    assert dpytest.verify().message().contains().content(\"permission\")\n</code></pre>"},{"location":"contributors/dpytest_example/#testing-reactions-and-interactions","title":"Testing Reactions and Interactions","text":"<pre><code>@pytest.mark.asyncio\nasync def test_reaction_handling(mock_bot):\n    \"\"\"Test bot responds to reactions.\"\"\"\n    # Send initial message\n    await dpytest.message(\"!react_test\")\n\n    # Get the bot's response message\n    message = dpytest.sent_queue[0]\n\n    # Add reaction to the message\n    await dpytest.add_reaction(message, \"\ud83d\udc4d\")\n\n    # Verify bot handles the reaction\n    # (Implementation depends on your bot's reaction handling)\n</code></pre>"},{"location":"contributors/dpytest_example/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest tests/test_bot/test_downloads.py\n\n# Run with verbose output\npytest -v\n\n# Run only async tests\npytest -m asyncio\n\n# Run with coverage\npytest --cov=boss_bot --cov-report=html\n\n# Run specific test\npytest tests/test_bot/test_downloads.py::test_download_command_twitter\n</code></pre>"},{"location":"contributors/dpytest_example/#best-practices","title":"Best Practices","text":""},{"location":"contributors/dpytest_example/#1-test-isolation","title":"1. Test Isolation","text":"<ul> <li>Always use <code>await dpytest.empty_queue()</code> in fixture teardown</li> <li>Use function-scoped fixtures to ensure clean state</li> <li>Mock external dependencies (APIs, file systems, databases)</li> </ul>"},{"location":"contributors/dpytest_example/#2-realistic-testing","title":"2. Realistic Testing","text":"<pre><code># Good: Test actual command flow\nawait dpytest.message(\"!download https://twitter.com/test\")\nassert dpytest.verify().message().contains().content(\"\u2705\")\n\n# Better: Also test edge cases\nawait dpytest.message(\"!download invalid-url\")\nassert dpytest.verify().message().contains().content(\"Invalid\")\n</code></pre>"},{"location":"contributors/dpytest_example/#3-mock-external-services","title":"3. Mock External Services","text":"<pre><code># Always mock external API calls\nwith patch('requests.get') as mock_get:\n    mock_get.return_value.json.return_value = {\"status\": \"success\"}\n    await dpytest.message(\"!download https://api.example.com/media/123\")\n</code></pre>"},{"location":"contributors/dpytest_example/#4-test-command-variations","title":"4. Test Command Variations","text":"<pre><code># Test different argument patterns\nawait dpytest.message(\"!download https://twitter.com/test\")        # Basic\nawait dpytest.message(\"!download https://twitter.com/test --hd\")   # With options\nawait dpytest.message(\"!download\")                                 # Missing args\n</code></pre>"},{"location":"contributors/dpytest_example/#5-use-descriptive-test-names","title":"5. Use Descriptive Test Names","text":"<pre><code>def test_download_twitter_url_with_api_fallback_enabled():\n    \"\"\"Test Twitter download when API fails and fallback is enabled.\"\"\"\n    pass\n</code></pre>"},{"location":"contributors/dpytest_example/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Forgetting to empty the queue: Always use <code>await dpytest.empty_queue()</code> in teardown</li> <li>Not mocking external dependencies: This leads to flaky tests and network dependencies</li> <li>Testing implementation details: Focus on behavior, not internal method calls</li> <li>Inconsistent async/await: Make sure all test functions are properly async</li> <li>Not testing error conditions: Always test both success and failure scenarios</li> </ol>"},{"location":"contributors/dpytest_example/#integration-with-cicd","title":"Integration with CI/CD","text":"<p>Add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\ntestpaths = [\"tests\"]\nmarkers = [\n    \"asyncio: mark test as async\",\n    \"integration: mark test as integration test\",\n    \"slow: mark test as slow running\",\n]\n</code></pre> <p>And in GitHub Actions:</p> <pre><code>- name: Run tests\n  run: |\n    uv run pytest tests/ -v --cov=boss_bot --cov-report=xml\n</code></pre>"},{"location":"contributors/dpytest_example/#conclusion","title":"Conclusion","text":"<p>dpytest provides a powerful framework for testing Discord bots without requiring actual Discord API connections. By following these patterns and best practices, you can create comprehensive test suites that ensure your bot works correctly across different scenarios and edge cases.</p> <p>For more advanced features and API reference, consult the official dpytest documentation.</p>"},{"location":"core/architecture-overview/","title":"Architecture Overview","text":"<p>This document provides essential architectural foundations every developer needs to understand when working with Boss-Bot.</p>"},{"location":"core/architecture-overview/#core-architectural-principles","title":"Core Architectural Principles","text":"<p>Boss-Bot follows a modular, event-driven architecture built on Discord.py with AI capabilities and modern Python patterns:</p> <ul> <li>Discord.py Foundation: Event-driven bot with command cogs</li> <li>Pydantic Configuration: Type-safe environment configuration</li> <li>Async-First: All I/O operations use async/await</li> <li>Dependency Injection: Settings and services passed via constructor</li> <li>Strategy Pattern: Pluggable download implementations</li> <li>AI Integration: LangChain/LangGraph for intelligent features</li> </ul>"},{"location":"core/architecture-overview/#high-level-system-architecture","title":"High-Level System Architecture","text":"<pre><code>graph TB\n    subgraph \"Discord Interface\"\n        DC[Discord Client]\n        COGS[Command Cogs]\n        EVENTS[Event Handlers]\n    end\n\n    subgraph \"Core Business Logic\"\n        QM[Queue Manager]\n        DM[Download Manager]\n        CM[Compression Manager]\n        STRAT[Download Strategies]\n        HAND[Platform Handlers]\n    end\n\n    subgraph \"AI Components\"\n        AGENTS[LangGraph Agents]\n        CHAINS[LangChain Chains]\n        TOOLS[AI Tools]\n    end\n\n    subgraph \"Infrastructure\"\n        STORAGE[Storage Manager]\n        MONITOR[Monitoring]\n        CONFIG[Configuration]\n    end\n\n    DC --&gt; COGS\n    DC --&gt; EVENTS\n    COGS --&gt; QM\n    COGS --&gt; DM\n    COGS --&gt; CM\n    DM --&gt; STRAT\n    DM --&gt; CM\n    STRAT --&gt; HAND\n    COGS --&gt; AGENTS\n    AGENTS --&gt; CHAINS\n    CHAINS --&gt; TOOLS\n    QM --&gt; STORAGE\n    DM --&gt; STORAGE\n    CM --&gt; STORAGE\n    MONITOR --&gt; CONFIG\n</code></pre>"},{"location":"core/architecture-overview/#discordpy-bot-architecture","title":"Discord.py Bot Architecture","text":""},{"location":"core/architecture-overview/#1-bossbot-client-srcboss_botbotclientpy","title":"1. BossBot Client (<code>src/boss_bot/bot/client.py</code>)","text":"<p>The main bot class extending <code>discord.ext.commands.Bot</code>:</p> <pre><code>class BossBot(commands.Bot):\n    \"\"\"Main Discord bot client with dependency injection.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n        self.queue_manager = QueueManager(settings)\n        self.download_manager = DownloadManager(settings)\n        # Bot initialization with intents, command prefix, etc.\n</code></pre> <p>Key Responsibilities: - Discord connection and authentication - Event loop management - Cog loading and unloading - Global error handling - Dependency injection container</p>"},{"location":"core/architecture-overview/#2-command-cogs-srcboss_botbotcogs","title":"2. Command Cogs (<code>src/boss_bot/bot/cogs/</code>)","text":"<p>Commands are organized into logical groups called \"cogs\":</p> <pre><code>class DownloadCog(commands.Cog):\n    \"\"\"Download-related Discord commands.\"\"\"\n\n    def __init__(self, bot: BossBot):\n        self.bot = bot\n        self.queue_manager = bot.queue_manager\n        self.download_manager = bot.download_manager\n\n    @commands.command(name=\"download\")\n    async def download_command(self, ctx: commands.Context, url: str):\n        \"\"\"Download media from supported platforms.\"\"\"\n        # Command implementation\n</code></pre> <p>Current Cogs: - DownloadCog: Media download commands (<code>$download</code>, <code>$dl</code>) - QueueCog: Queue management commands (<code>$queue</code>, <code>$clear</code>, <code>$pause</code>) - AdminCog: Bot information and help commands (<code>$info</code>, <code>$help-detailed</code>, <code>$commands</code>)</p>"},{"location":"core/architecture-overview/#3-event-handlers-srcboss_botbotevents","title":"3. Event Handlers (<code>src/boss_bot/bot/events/</code>)","text":"<p>Handle Discord events and bot lifecycle:</p> <pre><code>@bot.event\nasync def on_ready():\n    \"\"\"Bot startup initialization.\"\"\"\n\n@bot.event\nasync def on_command_error(ctx, error):\n    \"\"\"Global command error handling.\"\"\"\n</code></pre>"},{"location":"core/architecture-overview/#download-system-architecture","title":"Download System Architecture","text":""},{"location":"core/architecture-overview/#current-implementation-handler-pattern","title":"Current Implementation: Handler Pattern","text":"<p>Platform Handlers (<code>src/boss_bot/core/downloads/handlers/</code>):</p> <pre><code>class BaseDownloadHandler(ABC):\n    \"\"\"Abstract base for platform-specific downloads.\"\"\"\n\n    @abstractmethod\n    def download(self, url: str) -&gt; MediaMetadata:\n        \"\"\"Download media and return metadata.\"\"\"\n\n    @abstractmethod\n    def supports_url(self, url: str) -&gt; bool:\n        \"\"\"Check if handler supports URL.\"\"\"\n</code></pre> <p>Implemented Handlers: - <code>TwitterHandler</code>: Twitter/X content via gallery-dl - <code>RedditHandler</code>: Reddit posts via gallery-dl - <code>InstagramHandler</code>: Instagram posts via gallery-dl - <code>YouTubeHandler</code>: YouTube videos via yt-dlp</p>"},{"location":"core/architecture-overview/#experimental-strategy-pattern","title":"Experimental: Strategy Pattern","text":"<p>Download Strategies (<code>src/boss_bot/core/downloads/strategies/</code>):</p> <p>The strategy pattern allows switching between CLI and API implementations:</p> <pre><code>class TwitterDownloadStrategy(BaseDownloadStrategy):\n    \"\"\"Strategy for Twitter downloads with CLI/API choice.\"\"\"\n\n    async def download(self, url: str, **kwargs) -&gt; MediaMetadata:\n        \"\"\"Download using feature-flagged approach.\"\"\"\n\n        if self.feature_flags.use_api_twitter:\n            try:\n                return await self._download_via_api(url, **kwargs)\n            except Exception as e:\n                if self.feature_flags.api_fallback_to_cli:\n                    return await self._download_via_cli(url, **kwargs)\n                raise\n        else:\n            return await self._download_via_cli(url, **kwargs)\n</code></pre> <p>Benefits: - Feature-flagged rollout (API vs CLI) - Automatic fallback mechanisms - Better testing and error handling - Performance improvements</p>"},{"location":"core/architecture-overview/#compression-system-architecture","title":"Compression System Architecture","text":""},{"location":"core/architecture-overview/#compressionmanager-srcboss_botcorecompressionmanagerpy","title":"CompressionManager (<code>src/boss_bot/core/compression/manager.py</code>)","text":"<p>The compression system provides automatic media compression with Discord file size limits in mind:</p> <pre><code>class CompressionManager:\n    \"\"\"Main compression manager that orchestrates all compression operations.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n        self.file_detector = FileTypeDetector()\n\n        # Initialize processors\n        self.video_processor = VideoProcessor(settings)\n        self.audio_processor = AudioProcessor(settings)\n        self.image_processor = ImageProcessor(settings)\n\n    async def compress_file(\n        self,\n        input_path: Path,\n        output_path: Path | None = None,\n        target_size_mb: int | None = None,\n        compression_settings: CompressionSettings | None = None,\n    ) -&gt; CompressionResult:\n        \"\"\"Compress a media file with automatic type detection.\"\"\"\n</code></pre> <p>Key Features: - Automatic file type detection - Target size-based compression (default 50MB for Discord) - Concurrent batch processing - Hardware acceleration support - Detailed compression statistics</p>"},{"location":"core/architecture-overview/#media-processors-srcboss_botcorecompressionprocessors","title":"Media Processors (<code>src/boss_bot/core/compression/processors/</code>)","text":"<p>Processor Architecture:</p> <pre><code>class BaseProcessor(ABC):\n    \"\"\"Abstract base class for media processors.\"\"\"\n\n    @abstractmethod\n    async def compress(self, input_path: Path, target_size_mb: int, output_path: Path) -&gt; CompressionResult:\n        \"\"\"Compress media file.\"\"\"\n\n    @abstractmethod\n    async def get_media_info(self, input_path: Path) -&gt; MediaInfo:\n        \"\"\"Get media file information.\"\"\"\n</code></pre> <p>Implemented Processors: - VideoProcessor: FFmpeg-based video compression with bitrate calculation - AudioProcessor: Audio compression with quality preservation - ImageProcessor: PIL/Pillow-based image compression and optimization</p>"},{"location":"core/architecture-overview/#file-type-detection-srcboss_botcorecompressionutilsfile_detectorpy","title":"File Type Detection (<code>src/boss_bot/core/compression/utils/file_detector.py</code>)","text":"<p>Automatic media type detection:</p> <pre><code>class FileTypeDetector:\n    \"\"\"Detects and categorizes file types for compression.\"\"\"\n\n    def get_media_type(self, file_path: Path) -&gt; MediaType:\n        \"\"\"Determine the media type of a file.\"\"\"\n        # Supports VIDEO, AUDIO, IMAGE, UNKNOWN\n\n# Supported file types:\n# Video: mp4, avi, mkv, mov, flv, wmv, webm, mpeg, 3gp, m4v, mpg, ogv\n# Audio: mp3, wav, m4a, flac, aac, ogg, wma, opus, amr, 3ga\n# Image: jpg, jpeg, png, gif, webp, bmp, tiff, tif, svg, heic, heif\n</code></pre>"},{"location":"core/architecture-overview/#compression-configuration","title":"Compression Configuration","text":"<p>Environment Variables: - <code>COMPRESSION_TARGET_SIZE_MB</code>: Default target size (50MB) - <code>COMPRESSION_FFMPEG_PRESET</code>: FFmpeg preset for video compression (slow) - <code>COMPRESSION_MAX_CONCURRENT</code>: Maximum concurrent operations (3) - <code>COMPRESSION_MIN_VIDEO_BITRATE_KBPS</code>: Minimum video bitrate (125kbps) - <code>COMPRESSION_MIN_AUDIO_BITRATE_KBPS</code>: Minimum audio bitrate (32kbps) - <code>COMPRESSION_IMAGE_MIN_QUALITY</code>: Minimum image quality (10%)</p> <p>Usage Integration: <pre><code># In Discord commands or download workflows\ncompression_manager = CompressionManager(settings)\nresult = await compression_manager.compress_file(\n    input_path=downloaded_file,\n    target_size_mb=25  # Discord Nitro limit\n)\n\nif result.success:\n    await ctx.send(file=discord.File(result.output_path))\n</code></pre></p>"},{"location":"core/architecture-overview/#configuration-management","title":"Configuration Management","text":""},{"location":"core/architecture-overview/#bosssettings-srcboss_botcoreenvpy","title":"BossSettings (<code>src/boss_bot/core/env.py</code>)","text":"<p>Central configuration using Pydantic settings:</p> <pre><code>class BossSettings(BaseSettings):\n    \"\"\"Bot configuration from environment variables.\"\"\"\n\n    # Discord configuration\n    discord_token: SecretStr\n    command_prefix: str = \"$\"\n\n    # Download configuration\n    download_dir: Path = Path(\"./downloads\")\n    max_file_size: int = 100_000_000  # 100MB\n\n    # Compression configuration\n    compression_target_size_mb: int = 50\n    compression_ffmpeg_preset: str = \"slow\"\n    compression_max_concurrent: int = 3\n    compression_min_video_bitrate_kbps: int = 125\n    compression_min_audio_bitrate_kbps: int = 32\n    compression_image_min_quality: int = 10\n\n    # Feature flags (experimental)\n    twitter_use_api_client: bool = False\n    reddit_use_api_client: bool = False\n    download_api_fallback_to_cli: bool = True\n\n    class Config:\n        env_file = \".env\"\n        case_sensitive = False\n</code></pre> <p>Key Features: - Environment variable validation - Type conversion and validation - Secret handling with <code>SecretStr</code> - Default values and documentation - Feature flag support</p>"},{"location":"core/architecture-overview/#queue-management-system","title":"Queue Management System","text":""},{"location":"core/architecture-overview/#queuemanager-srcboss_botcorequeuemanagerpy","title":"QueueManager (<code>src/boss_bot/core/queue/manager.py</code>)","text":"<p>Manages async download queue with rate limiting:</p> <pre><code>class QueueManager:\n    \"\"\"Manages download queue with rate limiting and quotas.\"\"\"\n\n    async def add_to_queue(self, url: str, user_id: int) -&gt; QueueItem:\n        \"\"\"Add download to queue with validation.\"\"\"\n\n    async def process_queue(self):\n        \"\"\"Process queue items with concurrency limits.\"\"\"\n\n    async def get_queue_status(self) -&gt; QueueStatus:\n        \"\"\"Get current queue status and statistics.\"\"\"\n</code></pre> <p>Features: - User-based rate limiting - Priority queuing - Quota management - Progress tracking - Pause/resume functionality</p>"},{"location":"core/architecture-overview/#ai-integration-points","title":"AI Integration Points","text":""},{"location":"core/architecture-overview/#current-state","title":"Current State","text":"<ul> <li>Foundation Ready: LangChain/LangGraph imports and basic structure</li> <li>Future Features: Content analysis, smart downloads, moderation</li> </ul>"},{"location":"core/architecture-overview/#planned-ai-components-srcboss_botai","title":"Planned AI Components (<code>src/boss_bot/ai/</code>)","text":"<pre><code># Future AI agent example\nclass ContentAnalyzerAgent:\n    \"\"\"LangGraph agent for media content analysis.\"\"\"\n\n    async def analyze_content(self, media_metadata: MediaMetadata) -&gt; ContentAnalysis:\n        \"\"\"Analyze downloaded content for insights.\"\"\"\n\n    async def suggest_categories(self, content: str) -&gt; List[str]:\n        \"\"\"Suggest content categories using AI.\"\"\"\n</code></pre>"},{"location":"core/architecture-overview/#storage-and-file-management","title":"Storage and File Management","text":""},{"location":"core/architecture-overview/#storage-managers-srcboss_botstoragemanagers","title":"Storage Managers (<code>src/boss_bot/storage/managers/</code>)","text":"<ul> <li>FileManager: File operations and organization</li> <li>QuotaManager: User/guild storage quotas</li> <li>ValidationManager: File type and security validation</li> </ul>"},{"location":"core/architecture-overview/#security-features","title":"Security Features","text":"<ul> <li>File type validation</li> <li>Virus scanning integration</li> <li>Size limits per user/guild</li> <li>Content filtering</li> </ul>"},{"location":"core/architecture-overview/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"core/architecture-overview/#health-checks-srcboss_botmonitoringhealth","title":"Health Checks (<code>src/boss_bot/monitoring/health/</code>)","text":"<ul> <li>Discord API connectivity</li> <li>Storage system health</li> <li>Download service availability</li> <li>Queue processor status</li> </ul>"},{"location":"core/architecture-overview/#metrics-collection-srcboss_botmonitoringmetrics","title":"Metrics Collection (<code>src/boss_bot/monitoring/metrics/</code>)","text":"<ul> <li>Download success/failure rates</li> <li>Queue processing times</li> <li>Storage usage statistics</li> <li>User activity metrics</li> </ul>"},{"location":"core/architecture-overview/#logging-srcboss_botmonitoringlogging","title":"Logging (<code>src/boss_bot/monitoring/logging/</code>)","text":"<ul> <li>Structured logging with context</li> <li>Error tracking and alerting</li> <li>Performance monitoring</li> <li>Debug information</li> </ul>"},{"location":"core/architecture-overview/#development-patterns","title":"Development Patterns","text":""},{"location":"core/architecture-overview/#dependency-injection","title":"Dependency Injection","text":"<p>All major components receive dependencies via constructor:</p> <pre><code>class DownloadCog(commands.Cog):\n    def __init__(self, bot: BossBot):\n        self.bot = bot\n        self.settings = bot.settings\n        self.queue_manager = bot.queue_manager\n        self.download_manager = bot.download_manager\n</code></pre>"},{"location":"core/architecture-overview/#error-handling","title":"Error Handling","text":"<p>Consistent error handling throughout:</p> <pre><code>@commands.command()\nasync def download(self, ctx: commands.Context, url: str):\n    try:\n        result = await self.download_manager.download(url)\n        await ctx.send(f\"Downloaded: {result.title}\")\n    except QuotaExceededError:\n        await ctx.send(\"Storage quota exceeded!\")\n    except UnsupportedURLError:\n        await ctx.send(\"URL not supported.\")\n    except Exception as e:\n        logger.error(f\"Download failed: {e}\")\n        await ctx.send(\"Download failed. Please try again.\")\n</code></pre>"},{"location":"core/architecture-overview/#async-patterns","title":"Async Patterns","text":"<p>All I/O operations use async/await:</p> <pre><code># Correct async patterns\nasync with aiofiles.open(file_path, 'wb') as f:\n    await f.write(content)\n\n# Execute sync operations in thread pool\nloop = asyncio.get_event_loop()\nresult = await loop.run_in_executor(None, sync_function, arg)\n</code></pre>"},{"location":"core/architecture-overview/#project-structure-evolution","title":"Project Structure Evolution","text":""},{"location":"core/architecture-overview/#current-structure","title":"Current Structure","text":"<ul> <li>Mixed responsibilities between modules</li> <li>Monolithic CLI interface</li> <li>Basic monitoring organization</li> </ul>"},{"location":"core/architecture-overview/#target-structure-see-migrationmd","title":"Target Structure (see MIGRATION.md)","text":"<ul> <li>Clear separation of concerns</li> <li>Modular CLI with subcommands</li> <li>Dedicated AI components</li> <li>Comprehensive monitoring</li> <li>Plugin-based integrations</li> </ul> <p>This architecture provides a solid foundation for scalable Discord bot development with modern Python patterns, comprehensive testing, and future AI capabilities.</p>"},{"location":"core/configuration-management/","title":"Configuration Management","text":"<p>This document covers Boss-Bot's configuration system, including Pydantic settings, environment variables, feature flags, and security considerations for managing bot configuration across environments.</p>"},{"location":"core/configuration-management/#configuration-architecture","title":"Configuration Architecture","text":"<p>Boss-Bot uses a layered configuration approach with clear precedence rules:</p> <pre><code>graph TB\n    subgraph \"Configuration Sources (Precedence Order)\"\n        ENV[Environment Variables] --&gt; DOTENV[.env File]\n        DOTENV --&gt; DEFAULTS[Default Values]\n        DEFAULTS --&gt; COMPUTED[Computed Properties]\n    end\n\n    subgraph \"Configuration Categories\"\n        DISCORD[Discord Settings]\n        DOWNLOAD[Download Settings]\n        FEATURES[Feature Flags]\n        STORAGE[Storage Settings]\n        MONITORING[Monitoring Settings]\n    end\n\n    ENV --&gt; VALIDATION[Pydantic Validation]\n    VALIDATION --&gt; SETTINGS[BossSettings Instance]\n    SETTINGS --&gt; DISCORD\n    SETTINGS --&gt; DOWNLOAD\n    SETTINGS --&gt; FEATURES\n    SETTINGS --&gt; STORAGE\n    SETTINGS --&gt; MONITORING\n</code></pre>"},{"location":"core/configuration-management/#bosssettings-core-configuration","title":"BossSettings Core Configuration","text":""},{"location":"core/configuration-management/#base-settings-class","title":"Base Settings Class","text":"<pre><code># src/boss_bot/core/env.py\nfrom pydantic import BaseSettings, Field, SecretStr, validator\nfrom pathlib import Path\nfrom typing import Optional, List\nfrom enum import Enum\n\nclass LogLevel(str, Enum):\n    \"\"\"Logging level enumeration.\"\"\"\n    DEBUG = \"DEBUG\"\n    INFO = \"INFO\"\n    WARNING = \"WARNING\"\n    ERROR = \"ERROR\"\n    CRITICAL = \"CRITICAL\"\n\nclass BossSettings(BaseSettings):\n    \"\"\"Central configuration for Boss-Bot using Pydantic settings.\n\n    All configuration comes from environment variables with sensible defaults.\n    Uses SecretStr for sensitive values and proper validation.\n    \"\"\"\n\n    # ==========================================\n    # Discord Configuration\n    # ==========================================\n\n    discord_token: SecretStr = Field(\n        ...,  # Required\n        description=\"Discord bot token from Discord Developer Portal\"\n    )\n\n    command_prefix: str = Field(\n        default=\"$\",\n        description=\"Command prefix for Discord bot commands\"\n    )\n\n    guild_ids: Optional[List[int]] = Field(\n        default=None,\n        description=\"Specific guild IDs for development (slash commands)\"\n    )\n\n    # ==========================================\n    # Download System Configuration\n    # ==========================================\n\n    download_dir: Path = Field(\n        default=Path(\"./downloads\"),\n        description=\"Base directory for downloaded media files\"\n    )\n\n    max_file_size: int = Field(\n        default=100_000_000,  # 100MB\n        description=\"Maximum file size in bytes\"\n    )\n\n    max_concurrent_downloads: int = Field(\n        default=3,\n        description=\"Maximum concurrent download operations\"\n    )\n\n    download_timeout: int = Field(\n        default=300,  # 5 minutes\n        description=\"Download timeout in seconds\"\n    )\n\n    # ==========================================\n    # Compression Configuration\n    # ==========================================\n\n    compression_target_size_mb: int = Field(\n        default=50,\n        description=\"Default target size for compression in MB (Discord limit consideration)\"\n    )\n\n    compression_ffmpeg_preset: str = Field(\n        default=\"slow\",\n        description=\"FFmpeg preset for video compression (ultrafast to veryslow)\"\n    )\n\n    compression_ffmpeg_path: Optional[str] = Field(\n        default=None,\n        description=\"Custom path to ffmpeg binary (auto-detected if not specified)\"\n    )\n\n    compression_ffprobe_path: Optional[str] = Field(\n        default=None,\n        description=\"Custom path to ffprobe binary (auto-detected if not specified)\"\n    )\n\n    compression_max_concurrent: int = Field(\n        default=3,\n        description=\"Maximum concurrent compression operations\"\n    )\n\n    compression_min_video_bitrate_kbps: int = Field(\n        default=125,\n        description=\"Minimum video bitrate in kbps (quality threshold)\"\n    )\n\n    compression_min_audio_bitrate_kbps: int = Field(\n        default=32,\n        description=\"Minimum audio bitrate in kbps (quality threshold)\"\n    )\n\n    compression_image_min_quality: int = Field(\n        default=10,\n        description=\"Minimum image quality percentage (1-100)\"\n    )\n\n    compression_hardware_acceleration: bool = Field(\n        default=True,\n        description=\"Enable hardware acceleration for video compression\"\n    )\n\n    compression_max_upload_size_mb: int = Field(\n        default=50,\n        description=\"Target compression size for Discord uploads in MB\"\n    )\n\n    # ==========================================\n    # Upload Configuration\n    # ==========================================\n\n    upload_batch_size_mb: int = Field(\n        default=20,\n        description=\"Maximum batch size for Discord uploads in MB\"\n    )\n\n    upload_max_files_per_batch: int = Field(\n        default=10,\n        description=\"Maximum files per Discord message\"\n    )\n\n    upload_cleanup_after_success: bool = Field(\n        default=True,\n        description=\"Remove downloaded files after successful upload\"\n    )\n\n    upload_enable_progress_updates: bool = Field(\n        default=True,\n        description=\"Show upload progress messages\"\n    )\n\n    # ==========================================\n    # Feature Flags (Experimental)\n    # ==========================================\n\n    twitter_use_api_client: bool = Field(\n        default=False,\n        description=\"Use API-direct client for Twitter downloads\"\n    )\n\n    reddit_use_api_client: bool = Field(\n        default=False,\n        description=\"Use API-direct client for Reddit downloads\"\n    )\n\n    instagram_use_api_client: bool = Field(\n        default=False,\n        description=\"Use API-direct client for Instagram downloads\"\n    )\n\n    youtube_use_api_client: bool = Field(\n        default=False,\n        description=\"Use API-direct client for YouTube downloads\"\n    )\n\n    download_api_fallback_to_cli: bool = Field(\n        default=True,\n        description=\"Fallback to CLI if API client fails\"\n    )\n\n    # ==========================================\n    # External Tool Configuration\n    # ==========================================\n\n    gallery_dl_config_file: Path = Field(\n        default=Path(\"~/.gallery-dl.conf\"),\n        description=\"Path to gallery-dl configuration file\"\n    )\n\n    gallery_dl_cookies_file: Optional[Path] = Field(\n        default=None,\n        description=\"Path to Netscape format cookies file\"\n    )\n\n    gallery_dl_cookies_from_browser: Optional[str] = Field(\n        default=None,\n        description=\"Browser to extract cookies from (firefox, chrome, etc.)\"\n    )\n\n    gallery_dl_user_agent: str = Field(\n        default=\"Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\",\n        description=\"User agent for gallery-dl requests\"\n    )\n\n    yt_dlp_config_file: Optional[Path] = Field(\n        default=None,\n        description=\"Path to yt-dlp configuration file\"\n    )\n\n    # ==========================================\n    # Storage and Quota Configuration\n    # ==========================================\n\n    storage_quota_per_user: int = Field(\n        default=1_000_000_000,  # 1GB\n        description=\"Storage quota per user in bytes\"\n    )\n\n    storage_quota_per_guild: int = Field(\n        default=10_000_000_000,  # 10GB\n        description=\"Storage quota per guild in bytes\"\n    )\n\n    cleanup_enabled: bool = Field(\n        default=True,\n        description=\"Enable automatic file cleanup\"\n    )\n\n    cleanup_after_days: int = Field(\n        default=30,\n        description=\"Days to keep files before cleanup\"\n    )\n\n    # ==========================================\n    # Monitoring and Logging\n    # ==========================================\n\n    log_level: LogLevel = Field(\n        default=LogLevel.INFO,\n        description=\"Logging level\"\n    )\n\n    log_file: Optional[Path] = Field(\n        default=None,\n        description=\"Log file path (stdout if not specified)\"\n    )\n\n    metrics_enabled: bool = Field(\n        default=True,\n        description=\"Enable metrics collection\"\n    )\n\n    health_check_enabled: bool = Field(\n        default=True,\n        description=\"Enable health check endpoints\"\n    )\n\n    # ==========================================\n    # Database Configuration (Future)\n    # ==========================================\n\n    database_url: Optional[SecretStr] = Field(\n        default=None,\n        description=\"Database connection URL\"\n    )\n\n    redis_url: Optional[SecretStr] = Field(\n        default=None,\n        description=\"Redis connection URL for caching\"\n    )\n\n    # ==========================================\n    # Security Configuration\n    # ==========================================\n\n    allowed_file_types: List[str] = Field(\n        default=[\"jpg\", \"jpeg\", \"png\", \"gif\", \"mp4\", \"webm\", \"mp3\", \"wav\"],\n        description=\"Allowed file extensions for downloads\"\n    )\n\n    virus_scan_enabled: bool = Field(\n        default=False,\n        description=\"Enable virus scanning of downloaded files\"\n    )\n\n    rate_limit_per_user: int = Field(\n        default=10,\n        description=\"Rate limit: requests per user per minute\"\n    )\n\n    rate_limit_per_guild: int = Field(\n        default=50,\n        description=\"Rate limit: requests per guild per minute\"\n    )\n\n    # ==========================================\n    # Validation and Computed Properties\n    # ==========================================\n\n    @validator('download_dir')\n    def validate_download_dir(cls, v: Path) -&gt; Path:\n        \"\"\"Ensure download directory exists and is writable.\"\"\"\n        v = v.expanduser().resolve()\n        v.mkdir(parents=True, exist_ok=True)\n\n        # Test write permissions\n        test_file = v / \".write_test\"\n        try:\n            test_file.touch()\n            test_file.unlink()\n        except (PermissionError, OSError) as e:\n            raise ValueError(f\"Download directory not writable: {e}\")\n\n        return v\n\n    @validator('guild_ids', pre=True)\n    def parse_guild_ids(cls, v) -&gt; Optional[List[int]]:\n        \"\"\"Parse guild IDs from comma-separated string or list.\"\"\"\n        if v is None or v == \"\":\n            return None\n\n        if isinstance(v, str):\n            return [int(guild_id.strip()) for guild_id in v.split(\",\")]\n\n        return v\n\n    @validator('allowed_file_types')\n    def normalize_file_types(cls, v: List[str]) -&gt; List[str]:\n        \"\"\"Normalize file extensions (lowercase, no dots).\"\"\"\n        return [ext.lower().lstrip('.') for ext in v]\n\n    @property\n    def is_development(self) -&gt; bool:\n        \"\"\"Check if running in development mode.\"\"\"\n        return self.log_level == LogLevel.DEBUG\n\n    @property\n    def total_storage_quota(self) -&gt; int:\n        \"\"\"Calculate total storage quota across all users/guilds.\"\"\"\n        # This would be implemented based on actual usage patterns\n        return self.storage_quota_per_guild * 10  # Estimate\n\n    class Config:\n        \"\"\"Pydantic configuration.\"\"\"\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n        case_sensitive = False\n        env_nested_delimiter = \"__\"  # For nested config like DISCORD__TOKEN\n\n        # Security: Don't include sensitive values in repr\n        fields = {\n            'discord_token': {'repr': False},\n            'database_url': {'repr': False},\n            'redis_url': {'repr': False},\n        }\n</code></pre>"},{"location":"core/configuration-management/#environment-variable-configuration","title":"Environment Variable Configuration","text":""},{"location":"core/configuration-management/#development-environment-env","title":"Development Environment (.env)","text":"<pre><code># .env file for development\n# =========================\n\n# ==========================================\n# Discord Configuration\n# ==========================================\nDISCORD_TOKEN=\"your_bot_token_here\"\nCOMMAND_PREFIX=\"$\"\nGUILD_IDS=\"123456789,987654321\"  # Optional: specific guilds for development\n\n# ==========================================\n# Download Configuration\n# ==========================================\nDOWNLOAD_DIR=\"./downloads\"\nMAX_FILE_SIZE=100000000  # 100MB\nMAX_CONCURRENT_DOWNLOADS=3\nDOWNLOAD_TIMEOUT=300\n\n# ==========================================\n# Compression Configuration\n# ==========================================\nCOMPRESSION_TARGET_SIZE_MB=50               # Default target size for compression\nCOMPRESSION_FFMPEG_PRESET=\"slow\"           # FFmpeg preset (ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow)\nCOMPRESSION_FFMPEG_PATH=\"\"                  # Custom ffmpeg path (leave empty for auto-detection)\nCOMPRESSION_FFPROBE_PATH=\"\"                 # Custom ffprobe path (leave empty for auto-detection)\nCOMPRESSION_MAX_CONCURRENT=3                # Maximum concurrent compression operations\nCOMPRESSION_MIN_VIDEO_BITRATE_KBPS=125      # Minimum video bitrate (quality threshold)\nCOMPRESSION_MIN_AUDIO_BITRATE_KBPS=32       # Minimum audio bitrate (quality threshold)\nCOMPRESSION_IMAGE_MIN_QUALITY=10            # Minimum image quality percentage (1-100)\nCOMPRESSION_HARDWARE_ACCELERATION=true     # Enable hardware acceleration for video compression\nCOMPRESSION_MAX_UPLOAD_SIZE_MB=50           # Target compression size for Discord uploads\n\n# ==========================================\n# Upload Configuration\n# ==========================================\nUPLOAD_BATCH_SIZE_MB=20                     # Maximum batch size for Discord uploads in MB\nUPLOAD_MAX_FILES_PER_BATCH=10               # Maximum files per Discord message\nUPLOAD_CLEANUP_AFTER_SUCCESS=true          # Remove downloaded files after successful upload\nUPLOAD_ENABLE_PROGRESS_UPDATES=true        # Show upload progress messages\n\n# ==========================================\n# Feature Flags (Experimental Features)\n# ==========================================\nTWITTER_USE_API_CLIENT=false\nREDDIT_USE_API_CLIENT=false\nINSTAGRAM_USE_API_CLIENT=false\nYOUTUBE_USE_API_CLIENT=false           # Enable enhanced YouTube API-direct downloads\nDOWNLOAD_API_FALLBACK_TO_CLI=true      # Auto-fallback to CLI on API errors\n\n# YouTube-specific Configuration\n# ==========================================\n# These are automatically managed by the YouTube strategy\n# .yt_download_history.json     - Download deduplication tracking\n# .yt_performance_metrics.json  - Performance monitoring data\n\n# ==========================================\n# External Tool Configuration\n# ==========================================\nGALLERY_DL_CONFIG_FILE=\"~/.gallery-dl.conf\"\nGALLERY_DL_COOKIES_FROM_BROWSER=\"firefox\"\nGALLERY_DL_USER_AGENT=\"Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\"\n\n# ==========================================\n# Storage and Quota\n# ==========================================\nSTORAGE_QUOTA_PER_USER=1000000000    # 1GB\nSTORAGE_QUOTA_PER_GUILD=10000000000  # 10GB\nCLEANUP_ENABLED=true\nCLEANUP_AFTER_DAYS=30\n\n# ==========================================\n# Monitoring and Logging\n# ==========================================\nLOG_LEVEL=\"DEBUG\"\nLOG_FILE=\"./logs/boss-bot.log\"\nMETRICS_ENABLED=true\nHEALTH_CHECK_ENABLED=true\n\n# ==========================================\n# Security\n# ==========================================\nALLOWED_FILE_TYPES=\"jpg,jpeg,png,gif,mp4,webm,mp3,wav\"\nVIRUS_SCAN_ENABLED=false\nRATE_LIMIT_PER_USER=10\nRATE_LIMIT_PER_GUILD=50\n\n# ==========================================\n# Database (Future)\n# ==========================================\n# DATABASE_URL=\"postgresql://user:pass@localhost/bossbot\"\n# REDIS_URL=\"redis://localhost:6379\"\n</code></pre>"},{"location":"core/configuration-management/#production-environment","title":"Production Environment","text":"<pre><code># Production environment variables\n# ================================\n\n# Security: Use secrets management system in production\nDISCORD_TOKEN=\"${SECRET_DISCORD_TOKEN}\"\nDATABASE_URL=\"${SECRET_DATABASE_URL}\"\n\n# Production-specific settings\nLOG_LEVEL=\"INFO\"\nLOG_FILE=\"/var/log/boss-bot/app.log\"\nDOWNLOAD_DIR=\"/var/lib/boss-bot/downloads\"\n\n# Enhanced security in production\nVIRUS_SCAN_ENABLED=true\nCLEANUP_ENABLED=true\nCLEANUP_AFTER_DAYS=7  # Shorter retention in production\n\n# Conservative feature flags\nTWITTER_USE_API_CLIENT=false\nREDDIT_USE_API_CLIENT=false\nDOWNLOAD_API_FALLBACK_TO_CLI=true\n\n# Tighter rate limits\nRATE_LIMIT_PER_USER=5\nRATE_LIMIT_PER_GUILD=25\nMAX_CONCURRENT_DOWNLOADS=2\n</code></pre>"},{"location":"core/configuration-management/#feature-flag-system","title":"Feature Flag System","text":""},{"location":"core/configuration-management/#downloadfeatureflags","title":"DownloadFeatureFlags","text":"<pre><code># src/boss_bot/core/downloads/feature_flags.py\nfrom boss_bot.core.env import BossSettings\n\nclass DownloadFeatureFlags:\n    \"\"\"Feature flags for download system implementations.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n\n    @property\n    def use_api_twitter(self) -&gt; bool:\n        \"\"\"Use API-direct approach for Twitter downloads.\"\"\"\n        return self.settings.twitter_use_api_client\n\n    @property\n    def use_api_reddit(self) -&gt; bool:\n        \"\"\"Use API-direct approach for Reddit downloads.\"\"\"\n        return self.settings.reddit_use_api_client\n\n    @property\n    def use_api_instagram(self) -&gt; bool:\n        \"\"\"Use API-direct approach for Instagram downloads.\"\"\"\n        return self.settings.instagram_use_api_client\n\n    @property\n    def use_api_youtube(self) -&gt; bool:\n        \"\"\"Use API-direct approach for YouTube downloads.\"\"\"\n        return self.settings.youtube_use_api_client\n\n    @property\n    def api_fallback_to_cli(self) -&gt; bool:\n        \"\"\"Fallback to CLI if API fails.\"\"\"\n        return self.settings.download_api_fallback_to_cli\n\n    def get_strategy_type(self, platform: str) -&gt; str:\n        \"\"\"Get strategy type for platform.\"\"\"\n        platform_flags = {\n            'twitter': self.use_api_twitter,\n            'reddit': self.use_api_reddit,\n            'instagram': self.use_api_instagram,\n            'youtube': self.use_api_youtube,\n        }\n\n        use_api = platform_flags.get(platform.lower(), False)\n        return \"api\" if use_api else \"cli\"\n\n    def should_fallback(self, platform: str, error: Exception) -&gt; bool:\n        \"\"\"Determine if API error should trigger CLI fallback.\"\"\"\n        if not self.api_fallback_to_cli:\n            return False\n\n        # Don't fallback for configuration errors\n        if isinstance(error, (ValueError, TypeError)):\n            return False\n\n        # Fallback for network/API errors\n        return True\n</code></pre>"},{"location":"core/configuration-management/#feature-flag-configuration-patterns","title":"Feature Flag Configuration Patterns","text":"<pre><code># Environment-based feature flag configuration\ndef configure_features_for_environment(env: str) -&gt; Dict[str, bool]:\n    \"\"\"Configure feature flags based on environment.\"\"\"\n\n    if env == \"development\":\n        return {\n            \"twitter_use_api_client\": True,   # Test new features\n            \"reddit_use_api_client\": True,\n            \"download_api_fallback_to_cli\": True,  # Safety net\n        }\n\n    elif env == \"staging\":\n        return {\n            \"twitter_use_api_client\": True,   # Full testing\n            \"reddit_use_api_client\": False,  # Gradual rollout\n            \"download_api_fallback_to_cli\": True,\n        }\n\n    elif env == \"production\":\n        return {\n            \"twitter_use_api_client\": False,  # Conservative\n            \"reddit_use_api_client\": False,\n            \"download_api_fallback_to_cli\": True,\n        }\n\n    return {}  # All defaults (conservative)\n\n# Usage in configuration\nenv = os.getenv(\"ENVIRONMENT\", \"development\")\nfeature_overrides = configure_features_for_environment(env)\n\n# Override environment variables programmatically if needed\nfor key, value in feature_overrides.items():\n    os.environ[key.upper()] = str(value)\n</code></pre>"},{"location":"core/configuration-management/#external-tool-configuration","title":"External Tool Configuration","text":""},{"location":"core/configuration-management/#gallery-dl-configuration","title":"Gallery-dl Configuration","text":"<pre><code># src/boss_bot/core/downloads/clients/config/gallery_dl_config.py\nfrom pydantic import BaseModel, Field, SecretStr, validator\nfrom typing import Optional, List, Dict, Any\nfrom pathlib import Path\n\nclass TwitterConfig(BaseModel):\n    \"\"\"Twitter extractor configuration.\"\"\"\n    quoted: bool = True\n    replies: bool = True\n    retweets: bool = True\n    videos: bool = True\n    cookies: Optional[str] = None\n    filename: str = \"{category}_{user[screen_name]}_{id}_{num}.{extension}\"\n    directory: List[str] = [\"twitter\", \"{user[screen_name]}\"]\n\nclass RedditConfig(BaseModel):\n    \"\"\"Reddit extractor configuration.\"\"\"\n    client_id: Optional[SecretStr] = Field(None, alias=\"client-id\")\n    user_agent: str = Field(alias=\"user-agent\")\n    comments: int = 0\n    morecomments: bool = False\n    videos: bool = True\n    filename: str = \"{category}_{subreddit}_{id}_{num}.{extension}\"\n    directory: List[str] = [\"reddit\", \"{subreddit}\"]\n\n    @validator('user_agent')\n    def validate_user_agent(cls, v):\n        if not v or len(v.strip()) == 0:\n            raise ValueError(\"User agent is required for Reddit\")\n        return v\n\n## Upload System Configuration\n\n### Upload Workflow Configuration\n\nThe upload system integrates seamlessly with the download system to provide automatic Discord file uploads with compression and batching:\n\n```python\n# Upload-specific configuration fields\nclass BossSettings(BaseSettings):\n    # Upload behavior control\n    upload_cleanup_after_success: bool = Field(\n        default=True,\n        description=\"Remove downloaded files after successful upload\",\n        validation_alias=\"UPLOAD_CLEANUP_AFTER_SUCCESS\"\n    )\n\n    upload_enable_progress_updates: bool = Field(\n        default=True,\n        description=\"Show detailed upload progress messages to users\",\n        validation_alias=\"UPLOAD_ENABLE_PROGRESS_UPDATES\"\n    )\n\n    # Discord-specific limits\n    upload_batch_size_mb: int = Field(\n        default=20,\n        description=\"Maximum batch size for Discord uploads in MB (should be under 25MB Discord limit)\",\n        validation_alias=\"UPLOAD_BATCH_SIZE_MB\"\n    )\n\n    upload_max_files_per_batch: int = Field(\n        default=10,\n        description=\"Maximum files per Discord message (Discord's hard limit is 10)\",\n        validation_alias=\"UPLOAD_MAX_FILES_PER_BATCH\"\n    )\n\n    # Compression integration\n    compression_max_upload_size_mb: int = Field(\n        default=50,\n        description=\"Target compression size for Discord uploads in MB\",\n        validation_alias=\"COMPRESSION_MAX_UPLOAD_SIZE_MB\"\n    )\n</code></pre>"},{"location":"core/configuration-management/#upload-configuration-options","title":"Upload Configuration Options","text":""},{"location":"core/configuration-management/#upload-behavior-settings","title":"Upload Behavior Settings","text":"<p>UPLOAD_CLEANUP_AFTER_SUCCESS (default: <code>true</code>) - Controls whether downloaded files are automatically deleted after successful upload - When <code>true</code>: Files are removed to save disk space after Discord upload - When <code>false</code>: Files are preserved locally for manual management - Useful for debugging or when users want to keep local copies</p> <p>UPLOAD_ENABLE_PROGRESS_UPDATES (default: <code>true</code>) - Controls whether detailed upload progress messages are shown to users - When <code>true</code>: Users see compression progress, batch information, and upload status - When <code>false</code>: Minimal feedback for cleaner Discord channels - Can be disabled for high-volume usage scenarios</p>"},{"location":"core/configuration-management/#discord-upload-limits","title":"Discord Upload Limits","text":"<p>UPLOAD_BATCH_SIZE_MB (default: <code>20</code>) - Maximum total size for files in a single Discord message batch - Should be set below Discord's 25MB limit to account for metadata overhead - Larger batches are automatically split into multiple messages - Recommended range: 15-20MB for optimal performance</p> <p>UPLOAD_MAX_FILES_PER_BATCH (default: <code>10</code>) - Maximum number of files per Discord message - Discord's hard limit is 10 files per message - Setting lower values can improve upload reliability - Recommended: 5-10 files depending on average file sizes</p>"},{"location":"core/configuration-management/#compression-integration","title":"Compression Integration","text":"<p>COMPRESSION_MAX_UPLOAD_SIZE_MB (default: <code>50</code>) - Target file size for compression when preparing files for Discord upload - Files larger than Discord's 25MB limit are compressed to this target size - Should be set below the Discord limit to ensure successful uploads - Balance between file quality and upload success</p>"},{"location":"core/configuration-management/#upload-environment-variables-examples","title":"Upload Environment Variables Examples","text":"<pre><code># Development setup (detailed feedback)\nUPLOAD_CLEANUP_AFTER_SUCCESS=false          # Keep files for debugging\nUPLOAD_ENABLE_PROGRESS_UPDATES=true         # Show detailed progress\nUPLOAD_BATCH_SIZE_MB=15                      # Conservative batch size\nUPLOAD_MAX_FILES_PER_BATCH=5                 # Smaller batches for testing\n\n# Production setup (optimized performance)\nUPLOAD_CLEANUP_AFTER_SUCCESS=true           # Clean up to save space\nUPLOAD_ENABLE_PROGRESS_UPDATES=true         # User feedback enabled\nUPLOAD_BATCH_SIZE_MB=20                      # Optimal batch size\nUPLOAD_MAX_FILES_PER_BATCH=10                # Maximum throughput\n\n# High-volume setup (minimal feedback)\nUPLOAD_CLEANUP_AFTER_SUCCESS=true           # Clean up immediately\nUPLOAD_ENABLE_PROGRESS_UPDATES=false        # Reduced message volume\nUPLOAD_BATCH_SIZE_MB=18                      # Conservative for reliability\nUPLOAD_MAX_FILES_PER_BATCH=8                 # Reduced for stability\n\n# Compression optimization for uploads\nCOMPRESSION_MAX_UPLOAD_SIZE_MB=23            # Just under Discord limit\n</code></pre>"},{"location":"core/configuration-management/#upload-configuration-validation","title":"Upload Configuration Validation","text":"<p>The upload system includes validation to ensure settings are within Discord's limits:</p> <pre><code># Validation in BossSettings\n@field_validator(\"upload_batch_size_mb\")\ndef validate_upload_batch_size(cls, v: int) -&gt; int:\n    \"\"\"Validate upload batch size is within Discord limits.\"\"\"\n    if v &gt; 25:\n        raise ValueError(\"Upload batch size cannot exceed Discord's 25MB limit\")\n    if v &lt; 1:\n        raise ValueError(\"Upload batch size must be at least 1MB\")\n    return v\n\n@field_validator(\"upload_max_files_per_batch\")\ndef validate_upload_files_per_batch(cls, v: int) -&gt; int:\n    \"\"\"Validate files per batch is within Discord limits.\"\"\"\n    if v &gt; 10:\n        raise ValueError(\"Discord's maximum is 10 files per message\")\n    if v &lt; 1:\n        raise ValueError(\"Must allow at least 1 file per batch\")\n    return v\n\n@field_validator(\"compression_max_upload_size_mb\")\ndef validate_compression_upload_size(cls, v: int) -&gt; int:\n    \"\"\"Validate compression target is reasonable for uploads.\"\"\"\n    if v &gt; 50:\n        logger.warning(f\"Compression target {v}MB is quite large for Discord uploads\")\n    if v &lt; 10:\n        logger.warning(f\"Compression target {v}MB may result in very low quality\")\n    return v\n</code></pre>"},{"location":"core/configuration-management/#upload-integration-with-download-commands","title":"Upload Integration with Download Commands","text":"<p>The upload system integrates with download commands through the enhanced command interface:</p> <pre><code># Command usage examples\n@commands.command(name=\"download\")\nasync def download_command(self, ctx: commands.Context, url: str, upload: bool = True):\n    \"\"\"Enhanced download command with upload integration.\"\"\"\n\n    # Upload enabled by default\n    if upload:\n        # Full download-to-Discord workflow\n        upload_result = await self.upload_manager.process_downloaded_files(\n            download_dir, ctx, platform_name\n        )\n\n        # Respect upload configuration\n        if self.settings.upload_enable_progress_updates:\n            await ctx.send(f\"\ud83d\udcca Processing {len(media_files)} files...\")\n\n        if self.settings.upload_cleanup_after_success and upload_result.success:\n            # Clean up temporary files\n            shutil.rmtree(download_dir)\n</code></pre>"},{"location":"core/configuration-management/#youtube-specific-configuration","title":"YouTube-Specific Configuration","text":""},{"location":"core/configuration-management/#enhanced-youtube-features","title":"Enhanced YouTube Features","text":"<p>The YouTube download system includes advanced features that require specific configuration:</p> <pre><code># YouTube Strategy Configuration\nyoutube_use_api_client: bool = Field(\n    default=False,\n    description=\"Enable enhanced YouTube API-direct downloads with advanced features\"\n)\n</code></pre> <p>YouTube Feature Overview:</p> <ol> <li>API vs CLI Mode</li> <li><code>YOUTUBE_USE_API_CLIENT=true</code>: Enable API-direct downloads with enhanced features</li> <li><code>YOUTUBE_USE_API_CLIENT=false</code>: Use stable CLI-based approach (default)</li> <li> <p><code>DOWNLOAD_API_FALLBACK_TO_CLI=true</code>: Auto-fallback to CLI on API errors</p> </li> <li> <p>Organized Directory Structure</p> </li> <li>Automatic organization: <code>.downloads/yt-dlp/youtube/{channel_name}/</code></li> <li>Channel name sanitization for filesystem compatibility</li> <li> <p>Consistent metadata files (info.json, description, thumbnails)</p> </li> <li> <p>Quality Optimization for Discord</p> </li> <li>Quality ladder: 720p (50MB) \u2192 480p (25MB) \u2192 360p (10MB)</li> <li>Automatic format selection based on Discord file size limits</li> <li> <p>Web-optimized MP4 output with <code>+faststart</code> flag</p> </li> <li> <p>Deduplication System</p> </li> <li>Automatic duplicate detection via <code>.yt_download_history.json</code></li> <li>Video ID extraction from all YouTube URL formats</li> <li>Force redownload option: <code>force_redownload=True</code></li> <li> <p>History management (max 1000 entries, auto-cleanup)</p> </li> <li> <p>Performance Monitoring</p> </li> <li>Real-time download tracking via <code>.yt_performance_metrics.json</code></li> <li>Method comparison (API vs CLI vs fallback)</li> <li>Statistics accessible via <code>$yt-stats</code> command</li> <li>Metrics retention (max 500 entries, auto-rotation)</li> </ol>"},{"location":"core/configuration-management/#youtube-commands-configuration","title":"YouTube Commands Configuration","text":"<pre><code># Enable enhanced YouTube functionality\nexport YOUTUBE_USE_API_CLIENT=true\nexport DOWNLOAD_API_FALLBACK_TO_CLI=true\n\n# Available YouTube commands:\n# $yt-download &lt;url&gt; [quality] [audio_only]    - Enhanced YouTube downloads\n# $yt-playlist &lt;url&gt; [quality] [max_videos]    - Playlist downloads (max 25)\n# $yt-stats                                     - Performance statistics\n</code></pre> <p>Quality Options: - <code>4K</code>, <code>2160p</code>: Ultra-high definition - <code>1440p</code>, <code>2K</code>: High definition - <code>1080p</code>, <code>FHD</code>: Full HD - <code>720p</code>, <code>HD</code>: HD (default, optimized for Discord) - <code>480p</code>: Standard definition - <code>360p</code>: Low definition - <code>best</code>: Best available quality - <code>worst</code>: Lowest available quality</p> <p>Example Configurations:</p> <pre><code># Conservative setup (stable CLI mode)\nexport YOUTUBE_USE_API_CLIENT=false\nexport DOWNLOAD_API_FALLBACK_TO_CLI=true\n\n# Enhanced setup (API mode with fallback)\nexport YOUTUBE_USE_API_CLIENT=true\nexport DOWNLOAD_API_FALLBACK_TO_CLI=true\n\n# Aggressive setup (API only, no fallback)\nexport YOUTUBE_USE_API_CLIENT=true\nexport DOWNLOAD_API_FALLBACK_TO_CLI=false\n</code></pre>"},{"location":"core/configuration-management/#upload-performance-tuning","title":"Upload Performance Tuning","text":"<p>For High-Volume Usage: <pre><code># Optimize for throughput\nUPLOAD_BATCH_SIZE_MB=20                      # Maximum safe batch size\nUPLOAD_MAX_FILES_PER_BATCH=10                # Maximum files per message\nUPLOAD_ENABLE_PROGRESS_UPDATES=false        # Reduce message spam\n\n# Compression settings for speed\nCOMPRESSION_FFMPEG_PRESET=fast               # Faster compression\nCOMPRESSION_MAX_CONCURRENT=4                 # More concurrent operations\n</code></pre></p> <p>For Quality-Focused Usage: <pre><code># Optimize for quality\nUPLOAD_BATCH_SIZE_MB=15                      # Smaller batches for reliability\nCOMPRESSION_FFMPEG_PRESET=slow               # Higher quality compression\nCOMPRESSION_MAX_UPLOAD_SIZE_MB=20            # Conservative compression target\n</code></pre></p> <p>For Development/Testing: <pre><code># Optimize for debugging\nUPLOAD_CLEANUP_AFTER_SUCCESS=false          # Keep files for inspection\nUPLOAD_ENABLE_PROGRESS_UPDATES=true         # Detailed feedback\nUPLOAD_BATCH_SIZE_MB=10                      # Small batches for testing\nUPLOAD_MAX_FILES_PER_BATCH=3                 # Easy to track batches\n</code></pre></p> <p>class DownloaderConfig(BaseModel):     \"\"\"Downloader configuration.\"\"\"     filesize_min: Optional[int] = Field(None, alias=\"filesize-min\")     filesize_max: Optional[int] = Field(None, alias=\"filesize-max\")     rate: Optional[int] = None     retries: int = 4     timeout: float = 30.0     verify: bool = True</p> <p>class ExtractorConfig(BaseModel):     \"\"\"Main extractor configuration.\"\"\"     base_directory: str = Field(\"./downloads/\", alias=\"base-directory\")     archive: Optional[str] = None     cookies: Optional[str] = None     user_agent: str = Field(alias=\"user-agent\")     twitter: TwitterConfig = TwitterConfig()     reddit: RedditConfig = RedditConfig()</p> <p>class GalleryDLConfig(BaseModel):     \"\"\"Root gallery-dl configuration with validation.\"\"\"     extractor: ExtractorConfig     downloader: DownloaderConfig = DownloaderConfig()</p> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to dictionary for gallery-dl.\"\"\"\n    return self.dict(by_alias=True, exclude_none=True)\n\n@classmethod\ndef from_file(cls, config_file: Path) -&gt; \"GalleryDLConfig\":\n    \"\"\"Load configuration from JSON file.\"\"\"\n    import json\n\n    if not config_file.exists():\n        raise FileNotFoundError(f\"Gallery-dl config not found: {config_file}\")\n\n    with open(config_file, encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    return cls(**data)\n</code></pre> <p>``` </p>"},{"location":"core/configuration-management/#sample-gallery-dl-configuration-file","title":"Sample Gallery-dl Configuration File","text":"<p><code>json {     \"extractor\": {         \"base-directory\": \"./downloads/\",         \"archive\": \"./downloads/.archive.sqlite3\",         \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\",          \"twitter\": {             \"quoted\": true,             \"replies\": true,             \"retweets\": true,             \"videos\": true,             \"filename\": \"{category}_{user[screen_name]}_{id}_{num}.{extension}\",             \"directory\": [\"twitter\", \"{user[screen_name]}\"]         },          \"reddit\": {             \"user-agent\": \"gallery-dl:boss-bot:1.0 (by /u/boss_bot_dev)\",             \"comments\": 0,             \"morecomments\": false,             \"videos\": true,             \"filename\": \"{category}_{subreddit}_{id}_{num}.{extension}\",             \"directory\": [\"reddit\", \"{subreddit}\"]         },          \"instagram\": {             \"videos\": true,             \"highlights\": false,             \"stories\": false,             \"filename\": \"{username}_{shortcode}_{num}.{extension}\",             \"directory\": [\"instagram\", \"{username}\"]         }     },      \"downloader\": {         \"retries\": 4,         \"timeout\": 30.0,         \"verify\": true,         \"rate\": null     },      \"output\": {         \"mode\": \"auto\",         \"progress\": true,         \"log\": \"[{name}][{levelname}] {message}\"     } }</code></p>"},{"location":"core/configuration-management/#configuration-validation-and-security","title":"Configuration Validation and Security","text":""},{"location":"core/configuration-management/#security-best-practices","title":"Security Best Practices","text":"<pre><code># Security considerations in configuration\nclass SecureConfigMixin:\n    \"\"\"Mixin for secure configuration handling.\"\"\"\n\n    @validator('*', pre=True)\n    def no_sql_injection(cls, v):\n        \"\"\"Prevent SQL injection in string fields.\"\"\"\n        if isinstance(v, str):\n            dangerous_patterns = [';', '--', '/*', '*/', 'xp_', 'sp_']\n            if any(pattern in v.lower() for pattern in dangerous_patterns):\n                raise ValueError(\"Potentially dangerous SQL pattern detected\")\n        return v\n\n    @validator('download_dir', 'log_file')\n    def validate_paths(cls, v: Optional[Path]) -&gt; Optional[Path]:\n        \"\"\"Validate and sanitize file paths.\"\"\"\n        if v is None:\n            return v\n\n        # Resolve and normalize path\n        v = Path(v).expanduser().resolve()\n\n        # Prevent path traversal\n        if \"..\" in str(v):\n            raise ValueError(\"Path traversal detected\")\n\n        # Ensure path is within allowed directories\n        allowed_roots = [\n            Path.cwd(),\n            Path.home(),\n            Path(\"/var/log\"),\n            Path(\"/tmp\")\n        ]\n\n        if not any(str(v).startswith(str(root)) for root in allowed_roots):\n            raise ValueError(f\"Path not allowed: {v}\")\n\n        return v\n\n# Apply security mixin\nclass BossSettings(BaseSettings, SecureConfigMixin):\n    # ... existing configuration ...\n    pass\n</code></pre>"},{"location":"core/configuration-management/#configuration-testing","title":"Configuration Testing","text":"<pre><code># tests/test_core/test_configuration.py\nimport pytest\nfrom boss_bot.core.env import BossSettings\n\nclass TestBossSettings:\n    \"\"\"Test configuration validation and security.\"\"\"\n\n    def test_default_configuration(self, monkeypatch):\n        \"\"\"Test default configuration is valid.\"\"\"\n        # Set required fields\n        monkeypatch.setenv(\"DISCORD_TOKEN\", \"test_token\")\n\n        settings = BossSettings()\n\n        assert settings.command_prefix == \"$\"\n        assert settings.max_concurrent_downloads == 3\n        assert settings.log_level.value == \"INFO\"\n\n    def test_environment_override(self, monkeypatch):\n        \"\"\"Test environment variables override defaults.\"\"\"\n        monkeypatch.setenv(\"DISCORD_TOKEN\", \"test_token\")\n        monkeypatch.setenv(\"COMMAND_PREFIX\", \"!\")\n        monkeypatch.setenv(\"MAX_CONCURRENT_DOWNLOADS\", \"5\")\n\n        settings = BossSettings()\n\n        assert settings.command_prefix == \"!\"\n        assert settings.max_concurrent_downloads == 5\n\n    def test_path_validation(self, monkeypatch, tmp_path):\n        \"\"\"Test path validation and creation.\"\"\"\n        monkeypatch.setenv(\"DISCORD_TOKEN\", \"test_token\")\n        monkeypatch.setenv(\"DOWNLOAD_DIR\", str(tmp_path / \"downloads\"))\n\n        settings = BossSettings()\n\n        assert settings.download_dir.exists()\n        assert settings.download_dir.is_dir()\n\n    def test_secret_repr_hidden(self, monkeypatch):\n        \"\"\"Test sensitive values are hidden in repr.\"\"\"\n        monkeypatch.setenv(\"DISCORD_TOKEN\", \"secret_token\")\n\n        settings = BossSettings()\n        settings_repr = repr(settings)\n\n        assert \"secret_token\" not in settings_repr\n        assert \"discord_token=\" not in settings_repr\n\n    def test_feature_flags_integration(self, monkeypatch):\n        \"\"\"Test feature flags work with settings.\"\"\"\n        monkeypatch.setenv(\"DISCORD_TOKEN\", \"test_token\")\n        monkeypatch.setenv(\"TWITTER_USE_API_CLIENT\", \"true\")\n\n        settings = BossSettings()\n        feature_flags = DownloadFeatureFlags(settings)\n\n        assert feature_flags.use_api_twitter is True\n        assert feature_flags.use_api_reddit is False  # Default\n</code></pre>"},{"location":"core/configuration-management/#configuration-management-in-practice","title":"Configuration Management in Practice","text":""},{"location":"core/configuration-management/#dependency-injection-pattern","title":"Dependency Injection Pattern","text":"<pre><code># Inject configuration throughout the application\nclass BossBot(commands.Bot):\n    \"\"\"Main bot with configuration dependency injection.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n        super().__init__(\n            command_prefix=settings.command_prefix,\n            intents=discord.Intents.default()\n        )\n\n        # Initialize subsystems with configuration\n        self.queue_manager = QueueManager(settings)\n        self.download_manager = DownloadManager(settings)\n        self.feature_flags = DownloadFeatureFlags(settings)\n\n# Usage\ndef main():\n    settings = BossSettings()  # Loads from environment\n    bot = BossBot(settings)\n    bot.run(settings.discord_token.get_secret_value())\n</code></pre>"},{"location":"core/configuration-management/#hot-configuration-reloading-future","title":"Hot Configuration Reloading (Future)","text":"<pre><code># Future: Support for configuration reloading without restart\nclass ConfigurationManager:\n    \"\"\"Manage configuration with hot reloading support.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n        self._watchers = []\n\n    async def watch_config_changes(self):\n        \"\"\"Watch for configuration file changes.\"\"\"\n        # Implementation would watch .env file and config files\n        # Reload and validate configuration on changes\n        # Emit events for components to update their configuration\n        pass\n\n    def reload_configuration(self) -&gt; BossSettings:\n        \"\"\"Reload configuration from environment.\"\"\"\n        return BossSettings()\n</code></pre> <p>This configuration system provides type-safe, validated configuration management with security considerations, feature flags, and proper separation of concerns across different environments.</p>"},{"location":"core/development-workflow/","title":"Development Workflow","text":"<p>This document covers daily development practices for Boss-Bot, including Just commands, Claude Code integration, UV package management, and AI-assisted development workflows.</p>"},{"location":"core/development-workflow/#just-command-reference","title":"Just Command Reference","text":"<p>Boss-Bot uses Just as a command runner for development tasks. All commands are defined in the <code>Justfile</code> and organized by category.</p>"},{"location":"core/development-workflow/#essential-daily-commands","title":"Essential Daily Commands","text":"<pre><code># Full development check suite\njust check                    # Run all checks (lint, type, test, format)\n\n# Individual checks\njust check-code              # Lint with ruff\njust check-type              # Type check with pyright/basedpyright\njust check-test              # Run pytest with coverage\njust check-coverage          # Generate detailed coverage report\n\n# Code formatting\njust format                  # Format with ruff and other formatters\njust format-ruff             # Ruff formatting only\n\n# Testing variations\njust check-test \"tests/test_bot/\"                    # Run specific test directory\njust check-test \"tests/test_bot/test_client.py\"     # Run specific test file\njust check-test \"tests/test_bot/test_client.py::test_function_name\"  # Run specific test\n</code></pre>"},{"location":"core/development-workflow/#package-management-with-uv","title":"Package Management with UV","text":"<pre><code># Dependency management\njust uv-update               # Update all dependencies\njust uv-sync                # Sync dependencies with uv.lock\njust uv-add package_name    # Add new dependency\njust uv-remove package_name # Remove dependency\n\n# Virtual environment\njust uv-shell               # Enter UV shell\njust uv-python              # Show Python interpreter path\n\n# Workspace operations\njust uv-workspace-sync      # Sync workspace dependencies\n</code></pre>"},{"location":"core/development-workflow/#development-environment-setup","title":"Development Environment Setup","text":"<pre><code># Initial setup\njust install                # Install all dependencies and setup hooks\njust install-pre-commit     # Setup pre-commit hooks only\n\n# Clean operations\njust clean                  # Clean build artifacts\njust clean-cache           # Clean Python cache files\njust clean-test            # Clean test artifacts\n</code></pre>"},{"location":"core/development-workflow/#documentation-and-release","title":"Documentation and Release","text":"<pre><code># Documentation\njust doc-build              # Build MkDocs documentation\njust doc-serve              # Serve docs locally at localhost:8000\njust doc-deploy             # Deploy documentation\n\n# Release management\njust changelog-generate     # Generate changelog with towncrier\njust release                # Create new release\njust check-security         # Run security audits\n</code></pre>"},{"location":"core/development-workflow/#claude-code-integration","title":"Claude Code Integration","text":""},{"location":"core/development-workflow/#claudemd-configuration","title":"CLAUDE.md Configuration","text":"<p>Boss-Bot is configured for AI-assisted development through Claude Code. The <code>CLAUDE.md</code> file provides comprehensive instructions to Claude about the project structure and development patterns.</p> <p>Key Integration Points:</p> <ol> <li>Architecture Understanding: Claude knows the Discord.py patterns, cog structure, and async patterns</li> <li>Testing Guidance: Automated fixture naming, pytest-mock usage, and Discord testing patterns</li> <li>Code Style: Ruff formatting, type hints, and error handling conventions</li> <li>Build Commands: Claude can run <code>just check</code>, <code>just format</code>, and testing commands</li> </ol>"},{"location":"core/development-workflow/#ai-assisted-development-workflow","title":"AI-Assisted Development Workflow","text":"<pre><code>graph LR\n    subgraph \"Development Cycle\"\n        IDEA[Feature Idea] --&gt; CLAUDE[Claude Analysis]\n        CLAUDE --&gt; PLAN[Implementation Plan]\n        PLAN --&gt; CODE[Write Code]\n        CODE --&gt; TEST[Write Tests]\n        TEST --&gt; CHECK[just check]\n        CHECK --&gt; REVIEW[AI Review]\n        REVIEW --&gt; COMMIT[Commit Changes]\n    end\n</code></pre> <p>Best Practices with Claude Code:</p> <ol> <li>Start with Architecture Questions: Ask Claude to explain patterns before implementing</li> <li>Test-First Development: Ask Claude to generate tests before implementation</li> <li>Code Review: Use Claude to review diffs and suggest improvements</li> <li>Documentation: Claude can generate docstrings and update documentation</li> </ol>"},{"location":"core/development-workflow/#example-claude-interactions","title":"Example Claude Interactions","text":"<pre><code># Ask Claude to explain a pattern\n\"Explain the fixture naming pattern in tests/conftest.py\"\n\n# Request test generation\n\"Generate tests for the new TwitterStrategy class following the project patterns\"\n\n# Code review assistance\n\"Review this Discord cog implementation for best practices\"\n\n# Architecture questions\n\"How should I implement a new download handler for TikTok?\"\n</code></pre>"},{"location":"core/development-workflow/#cursor-ide-integration","title":"Cursor IDE Integration","text":""},{"location":"core/development-workflow/#rule-system-overview","title":"Rule System Overview","text":"<p>Boss-Bot has 29+ specialized Cursor rules across different categories:</p> <p>Core Rules (<code>.cursor/rules/core-rules/</code>): - <code>development-environment-agent.mdc</code>: Environment setup and configuration - <code>phased-implementation-agent.mdc</code>: Feature development planning - <code>security-monitoring-agent.mdc</code>: Security best practices</p> <p>Python Rules (<code>.cursor/rules/py-rules/</code>): - <code>pytest-fixtures-agent.mdc</code>: Fixture naming and organization - <code>pytest-mock-agent.mdc</code>: Mock usage patterns - <code>python-tdd-auto.mdc</code>: Test-driven development automation</p> <p>Testing Rules (<code>.cursor/rules/testing-rules/</code>): - <code>pytest-error-analysis-agent.mdc</code>: Test failure analysis - <code>secure-environment-testing-agent.mdc</code>: Safe testing practices</p>"},{"location":"core/development-workflow/#when-to-use-which-rules","title":"When to Use Which Rules","text":"<pre><code># Development setup\n@dev-environment          # Setting up new development environment\n@phased-implementation    # Planning large features\n\n# Python development\n@pytest-fixtures          # Working with test fixtures\n@pytest-mock             # Mocking in tests\n@python-tdd               # Test-driven development\n\n# Testing and debugging\n@pytest-error-analysis    # Debugging test failures\n@test-modification        # Modifying existing tests\n\n# Project management\n@epic-story-management    # Managing epics and stories\n@workflow-agile           # Agile development workflow\n</code></pre>"},{"location":"core/development-workflow/#cursor-modes-configuration","title":"Cursor Modes Configuration","text":"<pre><code>// .cursor/modes.json\n{\n    \"development\": {\n        \"rules\": [\"@python-tdd\", \"@pytest-fixtures\", \"@dev-environment\"],\n        \"description\": \"Active development with TDD\"\n    },\n    \"testing\": {\n        \"rules\": [\"@pytest-error-analysis\", \"@pytest-mock\", \"@test-modification\"],\n        \"description\": \"Testing and debugging focus\"\n    },\n    \"architecture\": {\n        \"rules\": [\"@phased-implementation\", \"@security-monitoring\"],\n        \"description\": \"Architectural planning and review\"\n    }\n}\n</code></pre>"},{"location":"core/development-workflow/#uv-package-manager-best-practices","title":"UV Package Manager Best Practices","text":""},{"location":"core/development-workflow/#project-configuration","title":"Project Configuration","text":"<pre><code># pyproject.toml - UV workspace configuration\n[tool.uv]\ndev-dependencies = [\n    \"pytest&gt;=7.0.0\",\n    \"pytest-asyncio&gt;=0.21.0\",\n    \"pytest-mock&gt;=3.10.0\",\n    \"ruff&gt;=0.1.0\",\n    \"basedpyright&gt;=1.0.0\"\n]\n\n[tool.uv.workspace]\nmembers = [\"src/boss_bot\"]\n</code></pre>"},{"location":"core/development-workflow/#dependency-management-workflow","title":"Dependency Management Workflow","text":"<pre><code># Add development dependency\njust uv-add --dev pytest-recording\n\n# Add runtime dependency\njust uv-add discord.py\n\n# Update to latest compatible versions\njust uv-update\n\n# Pin specific version\njust uv-add \"pydantic&gt;=2.0.0,&lt;3.0.0\"\n\n# Remove dependency\njust uv-remove old-package\n\n# Show dependency tree\nuv tree\n\n# Audit dependencies for security issues\nuv audit\n</code></pre>"},{"location":"core/development-workflow/#virtual-environment-management","title":"Virtual Environment Management","text":"<pre><code># Create and sync environment\nuv venv\nuv sync\n\n# Activate environment (manual)\nsource .venv/bin/activate\n\n# Or use UV's automatic activation\nuv run python script.py\nuv run pytest\nuv run just check\n\n# Check environment status\nuv python list\nuv python install 3.12\n</code></pre>"},{"location":"core/development-workflow/#pre-commit-hooks-and-code-quality","title":"Pre-commit Hooks and Code Quality","text":""},{"location":"core/development-workflow/#pre-commit-configuration","title":"Pre-commit Configuration","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.0\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n      - id: ruff-format\n\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-merge-conflict\n      - id: check-toml\n      - id: check-yaml\n</code></pre>"},{"location":"core/development-workflow/#quality-gates","title":"Quality Gates","text":"<p>Automated Checks on Commit: 1. Ruff Linting: Code style and common issues 2. Ruff Formatting: Consistent code formatting 3. Type Checking: Static type validation with basedpyright 4. Test Execution: All tests must pass 5. Security Scanning: Dependency vulnerability checks</p> <p>Manual Quality Checks: <pre><code># Full quality check before push\njust check\n\n# Security audit\njust check-security\n\n# Coverage analysis\njust check-coverage\n\n# Documentation build test\njust doc-build\n</code></pre></p>"},{"location":"core/development-workflow/#testing-workflow-integration","title":"Testing Workflow Integration","text":""},{"location":"core/development-workflow/#test-driven-development-tdd","title":"Test-Driven Development (TDD)","text":"<pre><code># TDD workflow with Just commands\njust check-test \"tests/test_new_feature.py\"    # Red: Write failing test\n# Implement feature code\njust check-test \"tests/test_new_feature.py\"    # Green: Make test pass\njust check-code                                # Refactor: Clean up code\njust check                                     # Full validation\n</code></pre>"},{"location":"core/development-workflow/#testing-best-practices","title":"Testing Best Practices","text":"<p>1. Test Organization by Feature <pre><code># Test specific functionality\njust check-test \"tests/test_bot/test_cogs/\"\njust check-test \"tests/test_core/test_downloads/\"\n\n# Test single module\njust check-test \"tests/test_bot/test_client.py\"\n\n# Test with specific markers\njust check-test -m \"not slow\"\njust check-test -m \"asyncio\"\n</code></pre></p> <p>2. VCR Testing for External APIs <pre><code># Record new cassettes (development)\njust check-test \"tests/test_clients/\" --record-mode=all\n\n# Test with existing cassettes (CI)\njust check-test \"tests/test_clients/\" --record-mode=none\n\n# Update specific test recordings\njust check-test \"tests/test_clients/test_twitter.py\" --record-mode=new_episodes\n</code></pre></p> <p>3. Coverage-Driven Development <pre><code># Generate coverage report\njust check-coverage\n\n# View coverage in browser\njust check-coverage --html\nopen htmlcov/index.html\n\n# Focus on uncovered code\njust check-coverage --show-missing\n</code></pre></p>"},{"location":"core/development-workflow/#ai-assisted-debugging-workflow","title":"AI-Assisted Debugging Workflow","text":""},{"location":"core/development-workflow/#error-analysis-with-claude","title":"Error Analysis with Claude","text":"<p>1. Test Failure Analysis <pre><code># Run failing test with verbose output\njust check-test \"tests/test_bot/test_client.py::test_failing\" -v\n\n# Copy output to Claude with context:\n# \"This test is failing. Here's the error output and the test code.\n# What's the issue and how should I fix it?\"\n</code></pre></p> <p>2. Code Review Process <pre><code># Generate diff for review\ngit diff HEAD~1 HEAD &gt; review.diff\n\n# Ask Claude:\n# \"Review this diff for Discord.py best practices, async patterns,\n# and potential issues. Focus on error handling and test coverage.\"\n</code></pre></p> <p>3. Architecture Decision Support <pre><code># Before implementing new features:\n# \"I need to add Instagram download support. Should I use the Handler\n# pattern or Strategy pattern? What are the tradeoffs?\"\n\n# For refactoring decisions:\n# \"Looking at this code, what refactoring would improve maintainability\n# while following the existing patterns?\"\n</code></pre></p>"},{"location":"core/development-workflow/#release-and-deployment-workflow","title":"Release and Deployment Workflow","text":""},{"location":"core/development-workflow/#version-management","title":"Version Management","text":"<pre><code># Semantic versioning with commitizen\njust cz-bump                 # Bump version based on commits\njust cz-changelog           # Generate changelog\n\n# Manual version control\njust towncrier-build        # Build changelog from fragments\njust towncrier-check        # Validate changelog fragments\n</code></pre>"},{"location":"core/development-workflow/#release-checklist","title":"Release Checklist","text":"<pre><code># Pre-release validation\njust check                  # All tests pass\njust check-security         # No security issues\njust doc-build             # Documentation builds\njust check-coverage        # Coverage meets threshold\n\n# Release process\ngit checkout main\ngit pull origin main\njust release               # Creates tag and builds artifacts\ngit push origin main --tags\n\n# Post-release\njust doc-deploy            # Update documentation\n</code></pre>"},{"location":"core/development-workflow/#continuous-integration-integration","title":"Continuous Integration Integration","text":"<p>GitHub Actions Integration: <pre><code># .github/workflows/test.yml integration with Just\n- name: Run tests\n  run: |\n    uv run just check\n\n- name: Check security\n  run: |\n    uv run just check-security\n\n- name: Build docs\n  run: |\n    uv run just doc-build\n</code></pre></p>"},{"location":"core/development-workflow/#environment-and-configuration-management","title":"Environment and Configuration Management","text":""},{"location":"core/development-workflow/#development-environment-setup_1","title":"Development Environment Setup","text":"<pre><code># Complete setup for new developers\ngit clone https://github.com/yourusername/boss-bot.git\ncd boss-bot\n\n# Install UV if not already installed\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Setup development environment\njust install                # Install deps + setup pre-commit\ncp sample.env .env         # Configure environment\nedit .env                  # Add Discord token and settings\n\n# Verify setup\njust check                 # All checks should pass\njust doc-serve            # Documentation should build and serve\n</code></pre>"},{"location":"core/development-workflow/#environment-configuration","title":"Environment Configuration","text":"<pre><code># Development environment variables\nexport DISCORD_TOKEN=\"your_bot_token\"\nexport COMMAND_PREFIX=\"$\"\nexport LOG_LEVEL=\"DEBUG\"\nexport DOWNLOAD_DIR=\"./downloads\"\n\n# Feature flags for experimental features\nexport TWITTER_USE_API_CLIENT=false\nexport DOWNLOAD_API_FALLBACK_TO_CLI=true\n\n# Testing environment\nexport PYTEST_RECORD_MODE=\"none\"  # Don't record new VCR cassettes\nexport TEST_DISCORD_TOKEN=\"test_token\"\n</code></pre>"},{"location":"core/development-workflow/#ide-configuration","title":"IDE Configuration","text":"<p>VSCode Setup (Recommended): <pre><code>// .vscode/settings.json\n{\n    \"python.defaultInterpreter\": \"./.venv/bin/python\",\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.pytestArgs\": [\".\"],\n    \"python.linting.enabled\": false,  // Use ruff instead\n    \"ruff.enable\": true,\n    \"ruff.organizeImports\": true\n}\n</code></pre></p> <p>This development workflow emphasizes automation, AI assistance, and quality gates to ensure maintainable, well-tested code while leveraging modern Python tooling and AI-assisted development practices.</p>"},{"location":"core/discord-integration/","title":"Discord Integration","text":"<p>This document covers Bot-specific patterns for Discord.py integration, including bot client architecture, cog development, event handling, and Discord-specific testing strategies.</p>"},{"location":"core/discord-integration/#discordpy-bot-architecture","title":"Discord.py Bot Architecture","text":""},{"location":"core/discord-integration/#bossbot-client-design","title":"BossBot Client Design","text":"<pre><code># src/boss_bot/bot/client.py\nimport discord\nfrom discord.ext import commands\nfrom boss_bot.core.env import BossSettings\nfrom boss_bot.core.queue.manager import QueueManager\nfrom boss_bot.core.downloads.manager import DownloadManager\n\nclass BossBot(commands.Bot):\n    \"\"\"Main Discord bot client with dependency injection and lifecycle management.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        # Store configuration\n        self.settings = settings\n\n        # Configure intents\n        intents = discord.Intents.default()\n        intents.message_content = True  # Required for reading message content\n        intents.guilds = True\n        intents.members = False  # Don't need member events\n\n        # Initialize bot with configuration\n        super().__init__(\n            command_prefix=settings.command_prefix,\n            intents=intents,\n            help_command=None,  # Custom help command\n            case_insensitive=True,\n            strip_after_prefix=True\n        )\n\n        # Initialize subsystems with dependency injection\n        self.queue_manager = QueueManager(settings)\n        self.download_manager = DownloadManager(settings)\n\n        # Bot state tracking\n        self._startup_complete = False\n        self._shutdown_requested = False\n\n    async def setup_hook(self):\n        \"\"\"Called when bot is starting up. Load cogs and initialize services.\"\"\"\n        print(f\"Logged in as {self.user} (ID: {self.user.id})\")\n\n        # Load cogs\n        await self.load_extension('boss_bot.bot.cogs.downloads')\n        await self.load_extension('boss_bot.bot.cogs.queue')\n        await self.load_extension('boss_bot.bot.cogs.admin')\n\n        # Initialize services\n        await self.queue_manager.start()\n        await self.download_manager.start()\n\n        # Sync slash commands (development only)\n        if self.settings.guild_ids:\n            for guild_id in self.settings.guild_ids:\n                guild = discord.Object(id=guild_id)\n                self.tree.copy_global_to(guild=guild)\n                await self.tree.sync(guild=guild)\n                print(f\"Synced commands to guild {guild_id}\")\n\n        self._startup_complete = True\n        print(\"Bot startup complete!\")\n\n    async def close(self):\n        \"\"\"Graceful shutdown with cleanup.\"\"\"\n        if self._shutdown_requested:\n            return\n\n        self._shutdown_requested = True\n        print(\"Bot shutdown requested...\")\n\n        # Stop services\n        await self.queue_manager.stop()\n        await self.download_manager.stop()\n\n        # Unload cogs\n        for extension in list(self.extensions.keys()):\n            await self.unload_extension(extension)\n\n        # Close Discord connection\n        await super().close()\n        print(\"Bot shutdown complete.\")\n\n    @property\n    def is_ready_and_operational(self) -&gt; bool:\n        \"\"\"Check if bot is fully operational.\"\"\"\n        return (\n            self.is_ready()\n            and self._startup_complete\n            and not self._shutdown_requested\n        )\n</code></pre>"},{"location":"core/discord-integration/#bot-lifecycle-events","title":"Bot Lifecycle Events","text":"<pre><code># src/boss_bot/bot/events/lifecycle.py\nimport logging\nfrom discord.ext import commands\n\nlogger = logging.getLogger(__name__)\n\nclass LifecycleEvents(commands.Cog):\n    \"\"\"Handle bot lifecycle events.\"\"\"\n\n    def __init__(self, bot: BossBot):\n        self.bot = bot\n\n    @commands.Cog.listener()\n    async def on_ready(self):\n        \"\"\"Called when bot connects to Discord.\"\"\"\n        logger.info(f\"Bot connected as {self.bot.user}\")\n        logger.info(f\"Connected to {len(self.bot.guilds)} guilds\")\n\n        # Update presence\n        activity = discord.Activity(\n            type=discord.ActivityType.watching,\n            name=f\"{len(self.bot.guilds)} servers | {self.bot.settings.command_prefix}help\"\n        )\n        await self.bot.change_presence(activity=activity)\n\n    @commands.Cog.listener()\n    async def on_guild_join(self, guild: discord.Guild):\n        \"\"\"Called when bot joins a new guild.\"\"\"\n        logger.info(f\"Joined guild: {guild.name} (ID: {guild.id})\")\n\n        # Send welcome message if possible\n        system_channel = guild.system_channel\n        if system_channel and system_channel.permissions_for(guild.me).send_messages:\n            embed = discord.Embed(\n                title=\"Thanks for adding Boss-Bot!\",\n                description=f\"Use `{self.bot.settings.command_prefix}help` to get started.\",\n                color=discord.Color.green()\n            )\n            await system_channel.send(embed=embed)\n\n    @commands.Cog.listener()\n    async def on_guild_remove(self, guild: discord.Guild):\n        \"\"\"Called when bot leaves a guild.\"\"\"\n        logger.info(f\"Left guild: {guild.name} (ID: {guild.id})\")\n\n        # Cleanup guild-specific data\n        await self.bot.queue_manager.cleanup_guild_data(guild.id)\n</code></pre>"},{"location":"core/discord-integration/#cog-development-patterns","title":"Cog Development Patterns","text":""},{"location":"core/discord-integration/#base-cog-pattern","title":"Base Cog Pattern","text":"<pre><code># src/boss_bot/bot/cogs/base.py\nfrom discord.ext import commands\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from boss_bot.bot.client import BossBot\n\nclass BaseCog(commands.Cog):\n    \"\"\"Base class for all Bot cogs with common functionality.\"\"\"\n\n    def __init__(self, bot: \"BossBot\"):\n        self.bot = bot\n        self.settings = bot.settings\n        self.queue_manager = bot.queue_manager\n        self.download_manager = bot.download_manager\n\n    async def cog_load(self):\n        \"\"\"Called when cog is loaded.\"\"\"\n        print(f\"Loaded cog: {self.__class__.__name__}\")\n\n    async def cog_unload(self):\n        \"\"\"Called when cog is unloaded.\"\"\"\n        print(f\"Unloaded cog: {self.__class__.__name__}\")\n\n    async def cog_command_error(self, ctx: commands.Context, error: commands.CommandError):\n        \"\"\"Handle errors in this cog's commands.\"\"\"\n        if isinstance(error, commands.CommandNotFound):\n            return  # Ignore unknown commands\n\n        if isinstance(error, commands.MissingRequiredArgument):\n            await ctx.send(f\"Missing required argument: `{error.param.name}`\")\n            return\n\n        if isinstance(error, commands.BadArgument):\n            await ctx.send(f\"Invalid argument: {error}\")\n            return\n\n        # Log unexpected errors\n        import logging\n        logger = logging.getLogger(__name__)\n        logger.error(f\"Command error in {ctx.command}: {error}\", exc_info=error)\n\n        await ctx.send(\"An unexpected error occurred. Please try again later.\")\n</code></pre>"},{"location":"core/discord-integration/#download-cog-implementation","title":"Download Cog Implementation","text":"<pre><code># src/boss_bot/bot/cogs/downloads.py\nimport shutil\nimport discord\nfrom pathlib import Path\nfrom discord.ext import commands\nfrom boss_bot.bot.cogs.base import BaseCog\nfrom boss_bot.core.downloads.exceptions import DownloadError, UnsupportedURLError, QuotaExceededError\nfrom boss_bot.core.uploads.manager import UploadManager\nfrom boss_bot.schemas.discord import MediaMetadata\nfrom typing import Optional\n\nclass DownloadCog(BaseCog):\n    \"\"\"Commands for downloading media with upload functionality.\"\"\"\n\n    def __init__(self, bot):\n        super().__init__(bot)\n        # Initialize upload manager for automatic Discord uploads\n        self.upload_manager = UploadManager(bot.settings)\n\n    @commands.command(name=\"download\", aliases=[\"dl\"])\n    async def download_command(self, ctx: commands.Context, url: str, upload: bool = True):\n        \"\"\"Download media and optionally upload to Discord.\n\n        Args:\n            url: URL to download from (Twitter, Reddit, Instagram, YouTube)\n            upload: Whether to upload files to Discord (default: True)\n\n        Examples:\n            $download https://twitter.com/user/status/123\n            $download https://youtube.com/watch?v=abc upload=False\n            $download https://reddit.com/r/pics/comments/abc123/\n        \"\"\"\n        # Validate URL format\n        if not url.startswith(('http://', 'https://')):\n            await ctx.send(\"\u274c Please provide a valid URL starting with http:// or https://\")\n            return\n\n        # Check if bot is operational\n        if not self.bot.is_ready_and_operational:\n            await ctx.send(\"\u26a0\ufe0f Bot is not fully operational. Please try again in a moment.\")\n            return\n\n        # Try to find a strategy that supports this URL\n        strategy = self._get_strategy_for_url(url)\n\n        if not strategy:\n            await ctx.send(\"\u274c URL not supported. Supported platforms: Twitter/X, Reddit, Instagram, YouTube\")\n            return\n\n        # Get platform info for user messages\n        platform_info = self._get_platform_info(url)\n        name = platform_info.get(\"name\", \"Unknown\")\n\n        # Show strategy status\n        if strategy.feature_flags.use_api_twitter and \"twitter\" in name.lower():\n            await ctx.send(f\"\ud83d\ude80 Using experimental API-direct approach for {name}\")\n\n        try:\n            # Create unique download directory for this request\n            request_id = f\"{ctx.author.id}_{ctx.message.id}\"\n            download_subdir = self.download_dir / request_id\n            download_subdir.mkdir(exist_ok=True, parents=True)\n\n            # Temporarily change strategy download directory\n            original_dir = strategy.download_dir\n            strategy.download_dir = download_subdir\n\n            try:\n                metadata = await strategy.download(url)\n\n                # Check if download was successful\n                if metadata.error:\n                    await ctx.send(f\"\u274c {name} download failed: {metadata.error}\")\n                    return\n\n                await ctx.send(f\"\u2705 {name} download completed!\")\n\n                # Show basic metadata if available\n                if metadata.title:\n                    await ctx.send(f\"\ud83d\udcc4 **Title:** {metadata.title}\")\n                if metadata.author:\n                    await ctx.send(f\"\ud83d\udc64 **Author:** {metadata.author}\")\n                if metadata.download_method:\n                    method_emoji = \"\ud83d\ude80\" if metadata.download_method == \"api\" else \"\ud83d\udda5\ufe0f\"\n                    await ctx.send(f\"{method_emoji} Downloaded using {metadata.download_method.upper()} method\")\n\n                # Process and upload files if requested\n                if upload:\n                    await ctx.send(\"\ud83d\udce4 Processing files for upload...\")\n\n                    upload_result = await self.upload_manager.process_downloaded_files(\n                        download_subdir, ctx, name\n                    )\n\n                    if upload_result.success:\n                        await ctx.send(f\"\ud83c\udf89 {upload_result.message}\")\n                    else:\n                        await ctx.send(f\"\u26a0\ufe0f Upload issues: {upload_result.message}\")\n                        if upload_result.error:\n                            await ctx.send(f\"Error details: {upload_result.error}\")\n                else:\n                    await ctx.send(f\"\ud83d\udcc1 Files saved to: `{download_subdir.relative_to(Path.cwd())}`\")\n\n            finally:\n                # Restore original download directory\n                strategy.download_dir = original_dir\n\n                # Cleanup: Remove download directory after upload (optional)\n                if upload and getattr(self.bot.settings, \"upload_cleanup_after_success\", True):\n                    try:\n                        shutil.rmtree(download_subdir)\n                    except Exception as cleanup_error:\n                        print(f\"Cleanup warning: {cleanup_error}\")\n\n        except Exception as e:\n            await ctx.send(f\"\u274c Download error: {e!s}\")\n\n    @commands.command(name=\"download-only\")\n    async def download_only_command(self, ctx: commands.Context, url: str):\n        \"\"\"Download content without uploading to Discord.\n\n        Args:\n            url: URL to download from\n\n        Examples:\n            $download-only https://twitter.com/user/status/123\n            $download-only https://youtube.com/watch?v=abc\n        \"\"\"\n        await self.download_command(ctx, url, upload=False)\n\n    def _create_success_embed(self, metadata: MediaMetadata, user: discord.User) -&gt; discord.Embed:\n        \"\"\"Create embed for successful download.\"\"\"\n        embed = discord.Embed(\n            title=\"Download Completed\",\n            color=discord.Color.green(),\n            timestamp=metadata.download_timestamp\n        )\n\n        embed.add_field(name=\"Platform\", value=metadata.platform.title(), inline=True)\n        embed.add_field(name=\"Method\", value=metadata.download_method.upper(), inline=True)\n\n        if metadata.title:\n            embed.add_field(name=\"Title\", value=metadata.title[:100], inline=False)\n\n        if metadata.author:\n            embed.add_field(name=\"Author\", value=metadata.author, inline=True)\n\n        if metadata.duration:\n            embed.add_field(\n                name=\"Duration\",\n                value=f\"{metadata.duration // 60}:{metadata.duration % 60:02d}\",\n                inline=True\n            )\n\n        if metadata.file_size_bytes:\n            size_mb = metadata.file_size_bytes / 1024 / 1024\n            embed.add_field(name=\"Size\", value=f\"{size_mb:.1f} MB\", inline=True)\n\n        embed.set_footer(text=f\"Downloaded by {user.display_name}\", icon_url=user.avatar.url)\n\n        if metadata.thumbnail:\n            embed.set_thumbnail(url=metadata.thumbnail)\n\n        return embed\n\n    @commands.command(name=\"formats\")\n    async def list_formats_command(self, ctx: commands.Context, url: str):\n        \"\"\"List available formats for a URL (YouTube only).\n\n        Args:\n            url: YouTube URL to check formats for\n        \"\"\"\n        if 'youtube.com' not in url and 'youtu.be' not in url:\n            await ctx.send(\"\u274c Format listing is only supported for YouTube URLs.\")\n            return\n\n        try:\n            # Get format information without downloading\n            formats = await self.download_manager.get_available_formats(url)\n\n            embed = discord.Embed(\n                title=\"Available Formats\",\n                description=f\"Formats for: `{url}`\",\n                color=discord.Color.blue()\n            )\n\n            # Group formats by type\n            video_formats = [f for f in formats if f.get('vcodec') != 'none']\n            audio_formats = [f for f in formats if f.get('acodec') != 'none' and f.get('vcodec') == 'none']\n\n            if video_formats:\n                video_list = []\n                for fmt in video_formats[:10]:  # Limit to 10 formats\n                    resolution = fmt.get('height', 'unknown')\n                    ext = fmt.get('ext', 'unknown')\n                    video_list.append(f\"{resolution}p ({ext})\")\n\n                embed.add_field(\n                    name=\"Video Formats\",\n                    value=\"\\n\".join(video_list),\n                    inline=True\n                )\n\n            if audio_formats:\n                audio_list = []\n                for fmt in audio_formats[:5]:  # Limit to 5 formats\n                    ext = fmt.get('ext', 'unknown')\n                    abr = fmt.get('abr', 'unknown')\n                    audio_list.append(f\"{ext} ({abr}kbps)\")\n\n                embed.add_field(\n                    name=\"Audio Formats\",\n                    value=\"\\n\".join(audio_list),\n                    inline=True\n                )\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:\n            await ctx.send(f\"\u274c Failed to get format information: {e}\")\n\nasync def setup(bot):\n    \"\"\"Setup function for loading the cog.\"\"\"\n    await bot.add_cog(DownloadCog(bot))\n</code></pre>"},{"location":"core/discord-integration/#upload-system-integration","title":"Upload System Integration","text":""},{"location":"core/discord-integration/#upload-workflow","title":"Upload Workflow","text":"<p>The Discord bot now includes an integrated upload system that automatically processes downloaded media and uploads it to Discord with intelligent compression and batching:</p> <pre><code>graph TB\n    subgraph \"Upload Workflow\"\n        DOWNLOAD[Download Complete] --&gt; DETECT[Find Media Files]\n        DETECT --&gt; ANALYZE[Analyze File Sizes]\n        ANALYZE --&gt; COMPRESS{Files &gt; 25MB?}\n        COMPRESS --&gt;|Yes| COMPRESSION[Compress Files]\n        COMPRESS --&gt;|No| BATCH[Create Upload Batches]\n        COMPRESSION --&gt; BATCH\n        BATCH --&gt; UPLOAD[Upload to Discord]\n        UPLOAD --&gt; CLEANUP[Cleanup Files]\n    end\n</code></pre>"},{"location":"core/discord-integration/#upload-manager-integration","title":"Upload Manager Integration","text":"<pre><code># Upload system integration in DownloadCog\nclass DownloadCog(BaseCog):\n    def __init__(self, bot):\n        super().__init__(bot)\n        # Initialize upload manager for automatic Discord uploads\n        self.upload_manager = UploadManager(bot.settings)\n\n    async def process_download_with_upload(self, ctx, url, platform_name):\n        \"\"\"Process download and handle upload workflow.\"\"\"\n\n        # Step 1: Download to temporary directory\n        download_subdir = self.create_unique_download_dir(ctx)\n\n        try:\n            # Step 2: Execute download\n            metadata = await strategy.download(url)\n\n            # Step 3: Process files for upload\n            upload_result = await self.upload_manager.process_downloaded_files(\n                download_subdir, ctx, platform_name\n            )\n\n            # Step 4: Handle results\n            if upload_result.success:\n                await ctx.send(f\"\ud83c\udf89 Upload complete: {upload_result.successful_uploads}/{upload_result.files_processed} files\")\n            else:\n                await ctx.send(f\"\u26a0\ufe0f Upload issues: {upload_result.message}\")\n\n        finally:\n            # Step 5: Cleanup if configured\n            if self.bot.settings.upload_cleanup_after_success:\n                shutil.rmtree(download_subdir)\n</code></pre>"},{"location":"core/discord-integration/#upload-features","title":"Upload Features","text":"<p>Intelligent File Processing: - Media Detection: Automatically finds video, audio, and image files - Size Analysis: Categorizes files by Discord upload limits (25MB default) - Smart Compression: Compresses oversized files using the compression system - Batch Optimization: Groups files into optimal Discord message batches</p> <p>Discord Integration: - Progress Updates: Real-time feedback during processing - Batch Uploads: Respects Discord's 10 file per message limit - Retry Logic: Automatic retries with exponential backoff - Rate Limiting: Handles Discord API rate limits gracefully</p> <p>User Experience: <pre><code># User-friendly upload messages\n@commands.command(name=\"download\")\nasync def download_command(self, ctx: commands.Context, url: str, upload: bool = True):\n    \"\"\"Enhanced download command with upload feedback.\"\"\"\n\n    # Download phase\n    await ctx.send(f\"\u2705 {platform_name} download completed!\")\n\n    if upload:\n        # Upload phase with detailed feedback\n        await ctx.send(\"\ud83d\udce4 Processing files for upload...\")\n        await ctx.send(f\"\ud83d\udcca Found {total_files} media files ({total_size_mb:.1f}MB total)\")\n\n        if oversized_files:\n            await ctx.send(f\"\ud83d\udddc\ufe0f {len(oversized_files)} files need compression\")\n\n        # Per-file compression feedback\n        await ctx.send(f\"\ud83d\udddc\ufe0f Compressing {filename} ({original_mb:.1f}MB \u2192 target: {target_mb}MB)\")\n        await ctx.send(f\"\u2705 Compressed successfully! ({original_mb}MB \u2192 {compressed_mb}MB, ratio: {ratio:.2f})\")\n\n        # Upload batches\n        await ctx.send(f\"\ud83d\udcce Uploading batch 1/3: file1.mp4, file2.jpg (15.2MB)\")\n        await ctx.send(f\"\ud83c\udfaf {platform_name} media files:\", files=discord_files)\n\n        # Final result\n        await ctx.send(f\"\ud83c\udf89 Upload complete: {successful_uploads}/{total_files} files uploaded\")\n</code></pre></p>"},{"location":"core/discord-integration/#upload-configuration","title":"Upload Configuration","text":"<p>The upload system is highly configurable through environment variables:</p> <pre><code># Upload-specific settings in BossSettings\nclass BossSettings(BaseSettings):\n    # Upload behavior\n    upload_cleanup_after_success: bool = True    # Remove files after upload\n    upload_enable_progress_updates: bool = True  # Show detailed progress\n\n    # Batch configuration\n    upload_batch_size_mb: int = 20              # Max batch size (under Discord limit)\n    upload_max_files_per_batch: int = 10        # Max files per message\n\n    # Compression integration\n    compression_max_upload_size_mb: int = 50    # Target compression size for uploads\n</code></pre>"},{"location":"core/discord-integration/#error-handling","title":"Error Handling","text":"<p>The upload system includes comprehensive error handling:</p> <pre><code># Upload error scenarios\nasync def handle_upload_errors(self, upload_result, ctx):\n    \"\"\"Handle various upload error scenarios.\"\"\"\n\n    if not upload_result.success:\n        if \"too large\" in upload_result.message:\n            await ctx.send(\"\ud83d\udca1 Files too large even after compression. Consider external storage.\")\n        elif \"rate limit\" in upload_result.message:\n            await ctx.send(\"\u23f3 Discord rate limit reached. Upload will continue automatically.\")\n        elif upload_result.failed_uploads &gt; 0:\n            success_rate = upload_result.successful_uploads / upload_result.files_processed\n            await ctx.send(f\"\u26a0\ufe0f Partial upload success: {success_rate:.1%} files uploaded\")\n        else:\n            await ctx.send(f\"\u274c Upload failed: {upload_result.error}\")\n</code></pre>"},{"location":"core/discord-integration/#command-examples","title":"Command Examples","text":"<p>Basic Usage: <pre><code>$download https://twitter.com/user/status/123        # Download and upload (default)\n$download https://youtube.com/watch?v=abc upload=True # Explicit upload\n$download-only https://reddit.com/r/pics/comments/def # Download only, no upload\n</code></pre></p> <p>Upload Workflow Messages: <pre><code>User: $download https://twitter.com/example/status/123\n\nBot: \u2705 Twitter/X download completed!\nBot: \ud83d\udce4 Processing files for upload...\nBot: \ud83d\udcca Found 3 media files (45.2MB total)\nBot: \ud83d\udddc\ufe0f 1 files need compression\nBot: \ud83d\udddc\ufe0f Compressing video.mp4 (32.1MB \u2192 target: 23.8MB)\nBot: \u2705 Compressed successfully! (32MB \u2192 24MB, ratio: 0.75)\nBot: \ud83d\udcce Uploading batch 1/1: video_compressed.mp4, image1.jpg, image2.png (23.1MB)\nBot: \ud83c\udfaf Twitter/X media files: [attached files]\nBot: \u2139\ufe0f Compression Info:\n     \ud83d\udddc\ufe0f video_compressed.mp4 (compressed from video.mp4)\nBot: \ud83c\udf89 Upload complete: 3/3 files uploaded\n</code></pre></p>"},{"location":"core/discord-integration/#queue-management-cog","title":"Queue Management Cog","text":"<pre><code># src/boss_bot/bot/cogs/queue.py\nimport discord\nfrom discord.ext import commands\nfrom boss_bot.bot.cogs.base import BaseCog\n\nclass QueueCog(BaseCog):\n    \"\"\"Commands for managing the download queue.\"\"\"\n\n    @commands.command(name=\"queue\", aliases=[\"q\"])\n    async def show_queue(self, ctx: commands.Context):\n        \"\"\"Show current download queue status.\"\"\"\n        queue_status = await self.queue_manager.get_queue_status()\n\n        embed = discord.Embed(\n            title=\"Download Queue Status\",\n            color=discord.Color.blue()\n        )\n\n        embed.add_field(\n            name=\"Queue Stats\",\n            value=f\"Active: {queue_status.active_downloads}\\n\"\n                  f\"Pending: {queue_status.pending_count}\\n\"\n                  f\"Completed Today: {queue_status.completed_today}\",\n            inline=True\n        )\n\n        embed.add_field(\n            name=\"System Stats\",\n            value=f\"Uptime: {queue_status.uptime}\\n\"\n                  f\"Success Rate: {queue_status.success_rate:.1f}%\",\n            inline=True\n        )\n\n        # Show current queue items\n        if queue_status.current_items:\n            queue_list = []\n            for i, item in enumerate(queue_status.current_items[:5]):  # Show first 5\n                user = self.bot.get_user(item.user_id)\n                username = user.display_name if user else \"Unknown\"\n                queue_list.append(f\"{i+1}. {item.platform} - {username}\")\n\n            embed.add_field(\n                name=\"Current Queue\",\n                value=\"\\n\".join(queue_list) or \"Queue is empty\",\n                inline=False\n            )\n\n        await ctx.send(embed=embed)\n\n    @commands.command(name=\"clear_queue\")\n    @commands.has_permissions(manage_guild=True)\n    async def clear_queue(self, ctx: commands.Context):\n        \"\"\"Clear the download queue (Admin only).\"\"\"\n        cleared_count = await self.queue_manager.clear_queue()\n\n        embed = discord.Embed(\n            title=\"Queue Cleared\",\n            description=f\"Removed {cleared_count} items from the queue.\",\n            color=discord.Color.orange()\n        )\n\n        await ctx.send(embed=embed)\n\n    @commands.command(name=\"pause_queue\")\n    @commands.has_permissions(manage_guild=True)\n    async def pause_queue(self, ctx: commands.Context):\n        \"\"\"Pause queue processing (Admin only).\"\"\"\n        await self.queue_manager.pause()\n\n        embed = discord.Embed(\n            title=\"Queue Paused\",\n            description=\"Download queue processing has been paused.\",\n            color=discord.Color.orange()\n        )\n\n        await ctx.send(embed=embed)\n\n    @commands.command(name=\"resume_queue\")\n    @commands.has_permissions(manage_guild=True)\n    async def resume_queue(self, ctx: commands.Context):\n        \"\"\"Resume queue processing (Admin only).\"\"\"\n        await self.queue_manager.resume()\n\n        embed = discord.Embed(\n            title=\"Queue Resumed\",\n            description=\"Download queue processing has been resumed.\",\n            color=discord.Color.green()\n        )\n\n        await ctx.send(embed=embed)\n\nasync def setup(bot):\n    \"\"\"Setup function for loading the cog.\"\"\"\n    await bot.add_cog(QueueCog(bot))\n</code></pre>"},{"location":"core/discord-integration/#admin-and-information-cog","title":"Admin and Information Cog","text":"<pre><code># src/boss_bot/bot/cogs/admin.py\nimport discord\nfrom discord.ext import commands\nfrom boss_bot.bot.client import BossBot\n\nclass AdminCog(commands.Cog):\n    \"\"\"Cog for admin and general bot information commands.\"\"\"\n\n    def __init__(self, bot: BossBot):\n        self.bot = bot\n\n    @commands.command(name=\"info\")\n    async def show_info(self, ctx: commands.Context):\n        \"\"\"Display bot information including prefix and available commands.\"\"\"\n        embed = discord.Embed(\n            title=\"\ud83e\udd16 Boss-Bot Information\",\n            description=\"A Discord Media Download Assistant\",\n            color=discord.Color.blue(),\n        )\n\n        # Bot basic info\n        embed.add_field(\n            name=\"\ud83d\udccb Bot Details\",\n            value=f\"**Version:** {self.bot.version}\\n\"\n            f\"**Prefix:** `{self.bot.command_prefix}`\\n\"\n            f\"**Servers:** {len(self.bot.guilds)}\\n\"\n            f\"**Users:** {len(self.bot.users)}\",\n            inline=True,\n        )\n\n        # Supported platforms\n        embed.add_field(\n            name=\"\ud83c\udf10 Supported Platforms\",\n            value=\"\u2022 Twitter/X \ud83d\udc26\\n\u2022 Reddit \ud83e\udd16\\n\u2022 Instagram \ud83d\udcf7\\n\u2022 YouTube \ud83d\udcfa\\n\u2022 And more!\",\n            inline=True,\n        )\n\n        # Quick help\n        embed.add_field(\n            name=\"\u2753 Need Help?\",\n            value=f\"Use `{self.bot.command_prefix}help` for all commands\\n\"\n            f\"Use `{self.bot.command_prefix}commands` for command list\\n\"\n            f\"Use `{self.bot.command_prefix}prefixes` for prefix info\",\n            inline=False,\n        )\n\n        embed.set_footer(text=\"Boss-Bot | Made with discord.py\")\n        await ctx.send(embed=embed)\n\n    @commands.command(name=\"commands\")\n    async def list_commands(self, ctx: commands.Context):\n        \"\"\"List all available commands organized by category.\"\"\"\n        embed = discord.Embed(\n            title=\"\ud83d\udcda Available Commands\",\n            description=f\"All commands use the prefix: `{self.bot.command_prefix}`\",\n            color=discord.Color.purple(),\n        )\n\n        # Download commands\n        download_commands = [\n            f\"`{self.bot.command_prefix}download &lt;url&gt; [upload=True]` - Download media and upload to Discord\",\n            f\"`{self.bot.command_prefix}download-only &lt;url&gt;` - Download media without uploading to Discord\",\n            f\"`{self.bot.command_prefix}metadata &lt;url&gt;` - Get metadata about a URL without downloading\",\n            f\"`{self.bot.command_prefix}status` - Show current download status\",\n            f\"`{self.bot.command_prefix}strategies` - Show download strategy configuration\",\n            f\"`{self.bot.command_prefix}validate-config [platform]` - Validate platform configuration\",\n            f\"`{self.bot.command_prefix}config-summary [platform]` - Show platform config summary\",\n        ]\n\n        # YouTube-specific commands\n        youtube_commands = [\n            f\"`{self.bot.command_prefix}yt-download &lt;url&gt; [quality] [audio_only]` - YouTube download with quality options\",\n            f\"`{self.bot.command_prefix}yt-playlist &lt;url&gt; [quality] [max_videos]` - Download YouTube playlist (max 25 videos)\",\n            f\"`{self.bot.command_prefix}yt-stats` - Show YouTube download performance statistics\",\n        ]\n\n        embed.add_field(\n            name=\"\ud83d\udce5 Download Commands\",\n            value=\"\\n\".join(download_commands),\n            inline=False,\n        )\n\n        embed.add_field(\n            name=\"\ud83d\udcfa YouTube Commands\",\n            value=\"\\n\".join(youtube_commands),\n            inline=False,\n        )\n\n        # Queue commands\n        queue_commands = [\n            f\"`{self.bot.command_prefix}queue [page]` - Show download queue\",\n            f\"`{self.bot.command_prefix}clear` - Clear the download queue\",\n            f\"`{self.bot.command_prefix}remove &lt;id&gt;` - Remove item from queue\",\n            f\"`{self.bot.command_prefix}pause` - Pause download processing\",\n            f\"`{self.bot.command_prefix}resume` - Resume download processing\",\n        ]\n\n        embed.add_field(\n            name=\"\ud83d\udccb Queue Commands\",\n            value=\"\\n\".join(queue_commands),\n            inline=False,\n        )\n\n        # Admin/Help commands\n        admin_commands = [\n            f\"`{self.bot.command_prefix}info` - Show bot information\",\n            f\"`{self.bot.command_prefix}prefixes` - Show command prefixes\",\n            f\"`{self.bot.command_prefix}commands` - Show this command list\",\n            f\"`{self.bot.command_prefix}help [command]` - Get detailed help\",\n        ]\n\n        embed.add_field(\n            name=\"\u2139\ufe0f Information Commands\",\n            value=\"\\n\".join(admin_commands),\n            inline=False,\n        )\n\n        embed.set_footer(text=\"Use `help &lt;command&gt;` for detailed information about a specific command\")\n        await ctx.send(embed=embed)\n\n    @commands.command(name=\"help-detailed\")\n    async def detailed_help(self, ctx: commands.Context, command_name: str = None):\n        \"\"\"Provide detailed help for a specific command or general usage.\"\"\"\n        if not command_name:\n            # General help overview\n            embed = discord.Embed(\n                title=\"\ud83c\udd98 Boss-Bot Help\",\n                description=\"Welcome to Boss-Bot! Here's how to get started:\",\n                color=discord.Color.gold(),\n            )\n\n            embed.add_field(\n                name=\"\ud83d\ude80 Quick Start\",\n                value=f\"1. Copy any supported URL\\n\"\n                f\"2. Use `{self.bot.command_prefix}download &lt;url&gt;`\\n\"\n                f\"3. Wait for your download to complete!\",\n                inline=False,\n            )\n\n            embed.add_field(\n                name=\"\ud83d\udca1 Pro Tips\",\n                value=f\"\u2022 Use `{self.bot.command_prefix}metadata &lt;url&gt;` to preview content before downloading\\n\"\n                f\"\u2022 Check `{self.bot.command_prefix}queue` to see pending downloads\\n\"\n                f\"\u2022 Use `{self.bot.command_prefix}strategies` to see platform configurations\",\n                inline=False,\n            )\n\n            await ctx.send(embed=embed)\n            return\n\n        # Help for specific command\n        command = self.bot.get_command(command_name)\n        if not command:\n            await ctx.send(\n                f\"\u274c Command `{command_name}` not found. Use `{self.bot.command_prefix}commands` to see all available commands.\"\n            )\n            return\n\n        embed = discord.Embed(\n            title=f\"\ud83d\udcd6 Help: {command.name}\",\n            description=command.help or \"No description available.\",\n            color=discord.Color.blue(),\n        )\n\n        # Command signature\n        embed.add_field(\n            name=\"\ud83d\udcdd Usage\",\n            value=f\"`{self.bot.command_prefix}{command.qualified_name} {command.signature}`\",\n            inline=False,\n        )\n\n        # Add examples for common commands\n        examples = self._get_command_examples(command.name)\n        if examples:\n            embed.add_field(\n                name=\"\ud83d\udca1 Examples\",\n                value=examples,\n                inline=False,\n            )\n\n        await ctx.send(embed=embed)\n\n    def _get_command_examples(self, command_name: str) -&gt; str | None:\n        \"\"\"Get usage examples for specific commands.\"\"\"\n        examples = {\n            \"download\": f\"`{self.bot.command_prefix}download https://twitter.com/user/status/123`\\n\"\n            f\"`{self.bot.command_prefix}download https://reddit.com/r/pics/comments/abc/`\\n\"\n            f\"`{self.bot.command_prefix}download https://youtube.com/watch?v=VIDEO_ID`\",\n            \"yt-download\": f\"`{self.bot.command_prefix}yt-download https://youtube.com/watch?v=VIDEO_ID`\\n\"\n            f\"`{self.bot.command_prefix}yt-download https://youtube.com/watch?v=VIDEO_ID 1080p`\\n\"\n            f\"`{self.bot.command_prefix}yt-download https://youtube.com/watch?v=VIDEO_ID 720p True`\",\n            \"yt-playlist\": f\"`{self.bot.command_prefix}yt-playlist https://youtube.com/playlist?list=PLAYLIST_ID`\\n\"\n            f\"`{self.bot.command_prefix}yt-playlist https://youtube.com/playlist?list=PLAYLIST_ID 480p 5`\",\n            \"yt-stats\": f\"`{self.bot.command_prefix}yt-stats`\",\n            \"metadata\": f\"`{self.bot.command_prefix}metadata https://twitter.com/user/status/123`\\n\"\n            f\"`{self.bot.command_prefix}metadata https://instagram.com/p/POST_ID/`\",\n            \"queue\": f\"`{self.bot.command_prefix}queue`\\n`{self.bot.command_prefix}queue 2`\",\n            \"remove\": f\"`{self.bot.command_prefix}remove download123`\",\n            \"validate-config\": f\"`{self.bot.command_prefix}validate-config`\\n\"\n            f\"`{self.bot.command_prefix}validate-config instagram`\",\n        }\n        return examples.get(command_name)\n\nasync def setup(bot: BossBot):\n    \"\"\"Load the AdminCog.\"\"\"\n    await bot.add_cog(AdminCog(bot))\n</code></pre> <p>Key Features of AdminCog: - Bot Information: <code>$info</code> command shows bot version, server count, and supported platforms - Command Discovery: <code>$commands</code> provides organized command listing by category - Enhanced Help: <code>$help-detailed</code> offers comprehensive help with examples - Prefix Information: <code>$prefixes</code> shows current command prefix configuration - Error Handling: Each command includes proper error handling with user-friendly messages</p> <p>Command Categories: - Download Commands: Media downloading and metadata retrieval - YouTube Commands: YouTube-specific downloads with enhanced features - Queue Commands: Queue management and status monitoring - Information Commands: Bot information and help system</p>"},{"location":"core/discord-integration/#youtube-specific-commands","title":"YouTube-Specific Commands","text":"<p>Boss-Bot provides specialized YouTube commands that offer enhanced functionality beyond the basic <code>$download</code> command.</p>"},{"location":"core/discord-integration/#youtube-download-command","title":"YouTube Download Command","text":"<pre><code>$yt-download &lt;url&gt; [quality] [audio_only]\n</code></pre> <p>Enhanced YouTube downloads with quality control and format options:</p> <pre><code>@commands.command(name=\"yt-download\")\nasync def youtube_download(self, ctx: commands.Context, url: str, quality: str = \"720p\", audio_only: bool = False):\n    \"\"\"YouTube-specific download with quality and format options.\n\n    Args:\n        url: YouTube URL (video, shorts, playlist)\n        quality: Video quality (4K, 1080p, 720p, 480p, 360p, best, worst)\n        audio_only: Download audio only (default: False)\n\n    Examples:\n        $yt-download https://youtube.com/watch?v=VIDEO_ID\n        $yt-download https://youtube.com/watch?v=VIDEO_ID 1080p\n        $yt-download https://youtube.com/watch?v=VIDEO_ID 720p True\n    \"\"\"\n</code></pre> <p>Key Features: - Quality Selection: Choose from 4K, 1080p, 720p, 480p, 360p, best, worst - Audio-Only Mode: Extract audio only with <code>audio_only=True</code> - Organized Storage: Files stored in <code>yt-dlp/youtube/{channel_name}/</code> structure - Discord Optimization: Quality ladder optimized for Discord file size limits - Detailed Metadata: Shows title, channel, duration, views, likes - Deduplication: Automatically detects and skips previously downloaded videos</p> <p>Example Usage: <pre><code>User: $yt-download https://youtube.com/watch?v=dQw4w9WgXcQ 1080p\n\nBot: \ud83d\udcfa Downloading YouTube content with quality: 1080p\nBot: \ud83d\udd17 URL: https://youtube.com/watch?v=dQw4w9WgXcQ\nBot: \ud83d\ude80 Using experimental API-direct approach\nBot: \u2705 YouTube download completed!\nBot: \ud83d\udcdd Title: Never Gonna Give You Up\nBot: \ud83d\udc64 Channel: RickAstleyVEVO\nBot: \u23f1\ufe0f Duration: 3:33\nBot: \ud83d\udc41\ufe0f Views: 1,234,567,890\nBot: \u2764\ufe0f Likes: 12,345,678\nBot: \ud83d\udcc1 Organized in: yt-dlp/youtube/RickAstleyVEVO/\nBot: \ud83d\udce4 Processing files for upload...\nBot: \ud83c\udf89 Successfully uploaded 1 file to Discord!\n</code></pre></p>"},{"location":"core/discord-integration/#youtube-playlist-command","title":"YouTube Playlist Command","text":"<pre><code>$yt-playlist &lt;url&gt; [quality] [max_videos]\n</code></pre> <p>Download YouTube playlists with video limits:</p> <pre><code>@commands.command(name=\"yt-playlist\")\nasync def youtube_playlist(self, ctx: commands.Context, url: str, quality: str = \"720p\", max_videos: int = 10):\n    \"\"\"Download YouTube playlist with video limit.\n\n    Args:\n        url: YouTube playlist URL\n        quality: Video quality for all videos (default: 720p)\n        max_videos: Maximum number of videos to download (default: 10, max: 25)\n\n    Examples:\n        $yt-playlist https://youtube.com/playlist?list=PLAYLIST_ID\n        $yt-playlist https://youtube.com/playlist?list=PLAYLIST_ID 480p 5\n    \"\"\"\n</code></pre> <p>Features: - Batch Processing: Download multiple videos from a playlist - Video Limits: Safety limit of 25 videos maximum per command - Consistent Quality: Apply same quality setting to all videos - Organized Storage: Each video stored in appropriate channel subdirectory - Progress Tracking: Real-time feedback during playlist processing</p> <p>Example Usage: <pre><code>User: $yt-playlist https://youtube.com/playlist?list=PLrW43fNmb-HHN8UCE4RVBCN61o_yNWZEN 720p 3\n\nBot: \ud83d\udcfa Starting YouTube playlist download (max 3 videos, quality: 720p)\nBot: \ud83d\udd17 Playlist: https://youtube.com/playlist?list=PLrW43fNmb-HHN8UCE4RVBCN61o_yNWZEN\nBot: \u26a0\ufe0f Note: Playlist downloads may take several minutes\nBot: \u2705 Playlist download completed!\nBot: \ud83d\udcdd Playlist: My Favorite Videos\nBot: \ud83d\udc64 Channel: Example Channel\nBot: \ud83d\udce4 Processing playlist files for upload...\nBot: \ud83c\udf89 Successfully uploaded 3 files to Discord!\n</code></pre></p>"},{"location":"core/discord-integration/#youtube-statistics-command","title":"YouTube Statistics Command","text":"<pre><code>$yt-stats\n</code></pre> <p>View performance statistics for YouTube downloads:</p> <pre><code>@commands.command(name=\"yt-stats\")\nasync def youtube_stats(self, ctx: commands.Context):\n    \"\"\"Show YouTube download performance statistics.\n\n    Examples:\n        $yt-stats\n    \"\"\"\n</code></pre> <p>Statistics Provided: - Total Downloads: Number of videos downloaded - Average Duration: Mean download time across all downloads - Method Breakdown: API vs CLI vs CLI-fallback usage percentages - Performance Records: Fastest and slowest downloads - Success Rates: Download success/failure ratios</p> <p>Example Output: <pre><code>User: $yt-stats\n\nBot: \ud83d\udcca YouTube Performance Statistics\n\n\ud83d\udcc8 Total Downloads: 142\n\u23f1\ufe0f Average Duration: 8.3s\n\n\ud83d\udd27 Download Methods:\n\ud83d\ude80 API: 98 (69.0%)\n\ud83d\udda5\ufe0f CLI: 32 (22.5%)\n\ud83d\udd04 CLI_FALLBACK: 12 (8.5%)\n\n\ud83c\udfc6 Fastest: 2.1s (api)\n\ud83d\udc0c Slowest: 45.7s (cli_fallback)\n</code></pre></p>"},{"location":"core/discord-integration/#youtube-features-overview","title":"YouTube Features Overview","text":"<p>Organized Directory Structure: <pre><code>.downloads/\n\u251c\u2500\u2500 12345_123456789/          # Individual request folders\n\u251c\u2500\u2500 gallery-dl/               # Gallery-dl downloads\n\u2502   \u251c\u2500\u2500 reddit/\n\u2502   \u2514\u2500\u2500 twitter/\n\u2514\u2500\u2500 yt-dlp/                   # YouTube organized downloads\n    \u2514\u2500\u2500 youtube/              # Platform subdirectory\n        \u251c\u2500\u2500 PewDiePie/        # Channel-based organization\n        \u2502   \u251c\u2500\u2500 video1.mp4\n        \u2502   \u251c\u2500\u2500 video1.info.json\n        \u2502   \u2514\u2500\u2500 video1.webp\n        \u251c\u2500\u2500 MrBeast/\n        \u2514\u2500\u2500 LinusTechTips/\n</code></pre></p> <p>Quality Optimization for Discord: - 720p: Best quality within 50MB limit (Discord Nitro) - 480p: Fallback for 25MB limit (Discord Basic) - 360p: Final fallback for 10MB limit - Format Ladder: <code>best[height&lt;=720][filesize&lt;50M]/best[height&lt;=480][filesize&lt;25M]/best[height&lt;=360][filesize&lt;10M]</code></p> <p>Deduplication System: - Automatic Detection: Prevents re-downloading the same video - User Notification: Informs about duplicate attempts - Force Redownload: Option to bypass with <code>force_redownload=True</code> - Download History: Persistent tracking in <code>.yt_download_history.json</code></p> <p>Performance Monitoring: - Real-time Tracking: Measures download duration and method used - Historical Data: Stores metrics in <code>.yt_performance_metrics.json</code> - Method Comparison: Tracks API vs CLI performance differences - Statistics API: Accessible via <code>yt-stats</code> command</p>"},{"location":"core/discord-integration/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"core/discord-integration/#global-error-handler","title":"Global Error Handler","text":"<pre><code># src/boss_bot/bot/events/error_handler.py\nimport logging\nimport discord\nfrom discord.ext import commands\n\nlogger = logging.getLogger(__name__)\n\nclass ErrorHandler(commands.Cog):\n    \"\"\"Global error handling for the bot.\"\"\"\n\n    def __init__(self, bot):\n        self.bot = bot\n\n    @commands.Cog.listener()\n    async def on_command_error(self, ctx: commands.Context, error: commands.CommandError):\n        \"\"\"Handle command errors globally.\"\"\"\n\n        # Ignore these error types\n        if isinstance(error, (commands.CommandNotFound, commands.DisabledCommand)):\n            return\n\n        # User errors (send helpful message)\n        if isinstance(error, commands.UserInputError):\n            await self._handle_user_input_error(ctx, error)\n            return\n\n        # Permission errors\n        if isinstance(error, commands.CheckFailure):\n            await self._handle_permission_error(ctx, error)\n            return\n\n        # Discord API errors\n        if isinstance(error, discord.HTTPException):\n            await self._handle_discord_error(ctx, error)\n            return\n\n        # Unexpected errors\n        await self._handle_unexpected_error(ctx, error)\n\n    async def _handle_user_input_error(self, ctx: commands.Context, error: commands.UserInputError):\n        \"\"\"Handle user input errors with helpful messages.\"\"\"\n        embed = discord.Embed(\n            title=\"Invalid Command Usage\",\n            color=discord.Color.red()\n        )\n\n        if isinstance(error, commands.MissingRequiredArgument):\n            embed.description = f\"Missing required argument: `{error.param.name}`\"\n            embed.add_field(\n                name=\"Usage\",\n                value=f\"`{ctx.prefix}{ctx.command.qualified_name} {ctx.command.signature}`\",\n                inline=False\n            )\n\n        elif isinstance(error, commands.BadArgument):\n            embed.description = f\"Invalid argument: {error}\"\n\n        elif isinstance(error, commands.TooManyArguments):\n            embed.description = \"Too many arguments provided.\"\n\n        else:\n            embed.description = str(error)\n\n        await ctx.send(embed=embed, delete_after=30)\n\n    async def _handle_permission_error(self, ctx: commands.Context, error: commands.CheckFailure):\n        \"\"\"Handle permission-related errors.\"\"\"\n        embed = discord.Embed(\n            title=\"Permission Denied\",\n            color=discord.Color.red()\n        )\n\n        if isinstance(error, commands.MissingPermissions):\n            perms = \", \".join(error.missing_permissions)\n            embed.description = f\"You need the following permissions: {perms}\"\n\n        elif isinstance(error, commands.BotMissingPermissions):\n            perms = \", \".join(error.missing_permissions)\n            embed.description = f\"I need the following permissions: {perms}\"\n\n        elif isinstance(error, commands.NotOwner):\n            embed.description = \"This command is only available to the bot owner.\"\n\n        else:\n            embed.description = \"You don't have permission to use this command.\"\n\n        await ctx.send(embed=embed, delete_after=30)\n\n    async def _handle_discord_error(self, ctx: commands.Context, error: discord.HTTPException):\n        \"\"\"Handle Discord API errors.\"\"\"\n        if error.status == 403:\n            embed = discord.Embed(\n                title=\"Permission Error\",\n                description=\"I don't have permission to perform this action.\",\n                color=discord.Color.red()\n            )\n        elif error.status == 404:\n            embed = discord.Embed(\n                title=\"Not Found\",\n                description=\"The requested resource was not found.\",\n                color=discord.Color.red()\n            )\n        else:\n            embed = discord.Embed(\n                title=\"Discord API Error\",\n                description=\"An error occurred while communicating with Discord.\",\n                color=discord.Color.red()\n            )\n\n        logger.warning(f\"Discord API error in {ctx.command}: {error}\")\n        await ctx.send(embed=embed, delete_after=30)\n\n    async def _handle_unexpected_error(self, ctx: commands.Context, error: Exception):\n        \"\"\"Handle unexpected errors with logging.\"\"\"\n        logger.error(\n            f\"Unexpected error in command {ctx.command} \"\n            f\"(User: {ctx.author.id}, Guild: {ctx.guild.id if ctx.guild else None}): {error}\",\n            exc_info=error\n        )\n\n        embed = discord.Embed(\n            title=\"Unexpected Error\",\n            description=\"An unexpected error occurred. The error has been logged.\",\n            color=discord.Color.red()\n        )\n\n        await ctx.send(embed=embed, delete_after=30)\n\nasync def setup(bot):\n    \"\"\"Setup function for loading the error handler.\"\"\"\n    await bot.add_cog(ErrorHandler(bot))\n</code></pre>"},{"location":"core/discord-integration/#discord-testing-strategies","title":"Discord Testing Strategies","text":""},{"location":"core/discord-integration/#mocking-discord-objects","title":"Mocking Discord Objects","text":"<pre><code># tests/conftest.py - Discord-specific fixtures\nimport pytest\nfrom unittest.mock import Mock, AsyncMock\nfrom pytest_mock import MockerFixture\nimport discord\nfrom discord.ext import commands\n\n@pytest.fixture(scope=\"function\")\ndef fixture_discord_user_test(mocker: MockerFixture) -&gt; Mock:\n    \"\"\"Create a mocked Discord user.\"\"\"\n    user = mocker.Mock(spec=discord.User)\n    user.id = 12345\n    user.name = \"testuser\"\n    user.display_name = \"Test User\"\n    user.avatar = mocker.Mock()\n    user.avatar.url = \"https://example.com/avatar.png\"\n    return user\n\n@pytest.fixture(scope=\"function\")\ndef fixture_discord_guild_test(mocker: MockerFixture) -&gt; Mock:\n    \"\"\"Create a mocked Discord guild.\"\"\"\n    guild = mocker.Mock(spec=discord.Guild)\n    guild.id = 98765\n    guild.name = \"Test Guild\"\n    guild.owner_id = 12345\n    return guild\n\n@pytest.fixture(scope=\"function\")\ndef fixture_discord_channel_test(mocker: MockerFixture) -&gt; Mock:\n    \"\"\"Create a mocked Discord channel.\"\"\"\n    channel = mocker.Mock(spec=discord.TextChannel)\n    channel.id = 67890\n    channel.name = \"test-channel\"\n    channel.send = mocker.AsyncMock()\n    return channel\n\n@pytest.fixture(scope=\"function\")\ndef fixture_discord_context_test(\n    mocker: MockerFixture,\n    fixture_discord_user_test: Mock,\n    fixture_discord_guild_test: Mock,\n    fixture_discord_channel_test: Mock\n) -&gt; Mock:\n    \"\"\"Create a comprehensive Discord context mock.\"\"\"\n    ctx = mocker.Mock(spec=commands.Context)\n\n    # Essential async methods\n    ctx.send = mocker.AsyncMock()\n    ctx.reply = mocker.AsyncMock()\n\n    # Context properties\n    ctx.author = fixture_discord_user_test\n    ctx.guild = fixture_discord_guild_test\n    ctx.channel = fixture_discord_channel_test\n    ctx.prefix = \"$\"\n\n    # Command information\n    ctx.command = mocker.Mock()\n    ctx.command.name = \"test_command\"\n    ctx.command.qualified_name = \"test_command\"\n    ctx.command.signature = \"&lt;url&gt;\"\n\n    return ctx\n</code></pre>"},{"location":"core/discord-integration/#testing-cog-commands","title":"Testing Cog Commands","text":"<pre><code># tests/test_bot/test_cogs/test_downloads.py\nimport pytest\nfrom unittest.mock import Mock, AsyncMock\nfrom pytest_mock import MockerFixture\n\nfrom boss_bot.bot.cogs.downloads import DownloadCog\nfrom boss_bot.schemas.discord import MediaMetadata\n\nclass TestDownloadCog:\n    \"\"\"Test suite for DownloadCog commands.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_download_command_success(\n        self,\n        mocker: MockerFixture,\n        fixture_discord_context_test: Mock,\n        fixture_mock_bot_test: Mock\n    ):\n        \"\"\"Test successful download command execution.\"\"\"\n        # Setup\n        cog = DownloadCog(fixture_mock_bot_test)\n\n        # Mock queue manager\n        mock_queue_item = mocker.Mock()\n        mock_queue_item.position = 0\n        mock_queue_item.wait_for_completion = mocker.AsyncMock(\n            return_value=MediaMetadata(\n                platform=\"twitter\",\n                url=\"https://twitter.com/test\",\n                title=\"Test Tweet\",\n                author=\"testuser\"\n            )\n        )\n\n        fixture_mock_bot_test.queue_manager.add_to_queue = mocker.AsyncMock(\n            return_value=mock_queue_item\n        )\n\n        fixture_mock_bot_test.is_ready_and_operational = True\n\n        # Execute command\n        await cog.download_command.callback(\n            cog,\n            fixture_discord_context_test,\n            \"https://twitter.com/test\"\n        )\n\n        # Verify queue interaction\n        fixture_mock_bot_test.queue_manager.add_to_queue.assert_called_once()\n\n        # Verify Discord responses\n        assert fixture_discord_context_test.send.call_count &gt;= 1\n\n        # Check final response contains success message\n        final_call = fixture_discord_context_test.send.call_args_list[-1]\n        if 'content' in final_call.kwargs:\n            assert \"\u2705\" in final_call.kwargs['content']\n\n    @pytest.mark.asyncio\n    async def test_download_command_invalid_url(\n        self,\n        mocker: MockerFixture,\n        fixture_discord_context_test: Mock,\n        fixture_mock_bot_test: Mock\n    ):\n        \"\"\"Test download command with invalid URL.\"\"\"\n        cog = DownloadCog(fixture_mock_bot_test)\n        fixture_mock_bot_test.is_ready_and_operational = True\n\n        # Execute with invalid URL\n        await cog.download_command.callback(\n            cog,\n            fixture_discord_context_test,\n            \"not-a-url\"\n        )\n\n        # Verify error response\n        fixture_discord_context_test.send.assert_called_once()\n        sent_message = fixture_discord_context_test.send.call_args[1]['content']\n        assert \"\u274c\" in sent_message\n        assert \"valid URL\" in sent_message\n\n    @pytest.mark.asyncio\n    async def test_download_command_bot_not_ready(\n        self,\n        mocker: MockerFixture,\n        fixture_discord_context_test: Mock,\n        fixture_mock_bot_test: Mock\n    ):\n        \"\"\"Test download command when bot is not operational.\"\"\"\n        cog = DownloadCog(fixture_mock_bot_test)\n        fixture_mock_bot_test.is_ready_and_operational = False\n\n        # Execute command\n        await cog.download_command.callback(\n            cog,\n            fixture_discord_context_test,\n            \"https://twitter.com/test\"\n        )\n\n        # Verify appropriate response\n        fixture_discord_context_test.send.assert_called_once()\n        sent_message = fixture_discord_context_test.send.call_args[1]['content']\n        assert \"\u26a0\ufe0f\" in sent_message\n        assert \"not fully operational\" in sent_message\n</code></pre>"},{"location":"core/discord-integration/#integration-testing-with-dpytest","title":"Integration Testing with dpytest","text":"<pre><code># tests/test_bot/test_integration.py\nimport pytest\nimport dpytest\nfrom boss_bot.bot.client import BossBot\n\n@pytest.mark.asyncio\nasync def test_bot_integration(fixture_settings_test):\n    \"\"\"Test bot integration with dpytest.\"\"\"\n    # Create bot instance\n    bot = BossBot(fixture_settings_test)\n\n    # Configure dpytest\n    dpytest.configure(bot)\n\n    # Load cogs for testing\n    await bot.load_extension('boss_bot.bot.cogs.downloads')\n\n    # Test command execution\n    await dpytest.message(\"$help\")\n    assert dpytest.verify().message().contains().content(\"Available commands\")\n\n    # Test download command with mock\n    await dpytest.message(\"$download https://twitter.com/test\")\n\n    # Verify response\n    assert dpytest.verify().message().contains().content(\"Processing download\")\n\n    # Cleanup\n    await dpytest.empty_queue()\n</code></pre>"},{"location":"core/discord-integration/#custom-help-command","title":"Custom Help Command","text":"<pre><code># src/boss_bot/bot/bot_help.py\nimport discord\nfrom discord.ext import commands\nfrom typing import Optional, List, Mapping\n\nclass BossHelpCommand(commands.HelpCommand):\n    \"\"\"Custom help command with embeds and better formatting.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            command_attrs={\n                'help': 'Show help information for commands',\n                'aliases': ['h']\n            }\n        )\n\n    async def send_bot_help(self, mapping: Mapping[Optional[commands.Cog], List[commands.Command]]):\n        \"\"\"Send help for the entire bot.\"\"\"\n        embed = discord.Embed(\n            title=\"Boss-Bot Help\",\n            description=\"Download media from supported platforms\",\n            color=discord.Color.blue()\n        )\n\n        for cog, commands_list in mapping.items():\n            if not commands_list:\n                continue\n\n            cog_name = getattr(cog, 'qualified_name', 'No Category')\n            command_names = [f\"`{cmd.name}`\" for cmd in commands_list if not cmd.hidden]\n\n            if command_names:\n                embed.add_field(\n                    name=cog_name,\n                    value=\" \".join(command_names),\n                    inline=False\n                )\n\n        embed.add_field(\n            name=\"Supported Platforms\",\n            value=\"Twitter/X, Reddit, Instagram, YouTube\",\n            inline=False\n        )\n\n        embed.set_footer(text=f\"Use {self.context.prefix}help &lt;command&gt; for more info\")\n\n        await self.get_destination().send(embed=embed)\n\n    async def send_command_help(self, command: commands.Command):\n        \"\"\"Send help for a specific command.\"\"\"\n        embed = discord.Embed(\n            title=f\"Command: {command.qualified_name}\",\n            description=command.help or \"No description available\",\n            color=discord.Color.blue()\n        )\n\n        if command.aliases:\n            embed.add_field(\n                name=\"Aliases\",\n                value=\", \".join(f\"`{alias}`\" for alias in command.aliases),\n                inline=False\n            )\n\n        embed.add_field(\n            name=\"Usage\",\n            value=f\"`{self.context.prefix}{command.qualified_name} {command.signature}`\",\n            inline=False\n        )\n\n        await self.get_destination().send(embed=embed)\n\n    async def command_not_found(self, string: str) -&gt; str:\n        \"\"\"Handle command not found.\"\"\"\n        return f\"Command `{string}` not found.\"\n\n    async def send_error_message(self, error: str):\n        \"\"\"Send error message.\"\"\"\n        embed = discord.Embed(\n            title=\"Help Error\",\n            description=error,\n            color=discord.Color.red()\n        )\n        await self.get_destination().send(embed=embed)\n</code></pre> <p>This Discord integration guide provides comprehensive patterns for building robust Discord bots with proper error handling, testing strategies, and modern Discord.py best practices.</p>"},{"location":"core/download-system/","title":"Download System","text":"<p>This document explains Boss-Bot's download system architecture, covering both the current Handler pattern and the experimental Strategy pattern for platform-specific media downloads.</p>"},{"location":"core/download-system/#system-overview","title":"System Overview","text":"<p>Boss-Bot supports downloading media from multiple platforms through a pluggable architecture that allows for different implementation approaches:</p> <p>Supported Platforms: - Twitter/X: Posts, threads, media content - Reddit: Posts, comments, images, videos - Instagram: Posts, stories (experimental) - YouTube: Videos, playlists, audio extraction</p> <p>Implementation Approaches: - Handler Pattern (Current): CLI-based subprocess execution - Strategy Pattern (Experimental): API-direct with CLI fallback</p>"},{"location":"core/download-system/#handler-pattern-current-implementation","title":"Handler Pattern (Current Implementation)","text":""},{"location":"core/download-system/#architecture-overview","title":"Architecture Overview","text":"<p>The Handler pattern provides a stable, CLI-based approach using external tools:</p> <pre><code>graph LR\n    subgraph \"Download Flow\"\n        USER[Discord User] --&gt; COG[Download Cog]\n        COG --&gt; MGR[Download Manager]\n        MGR --&gt; HANDLER[Platform Handler]\n        HANDLER --&gt; CLI[External CLI Tool]\n        CLI --&gt; GALLERY[gallery-dl/yt-dlp]\n        GALLERY --&gt; FILES[Downloaded Files]\n        FILES --&gt; META[MediaMetadata]\n        META --&gt; USER\n    end\n</code></pre>"},{"location":"core/download-system/#base-handler-interface","title":"Base Handler Interface","text":"<pre><code># src/boss_bot/core/downloads/handlers/base_handler.py\nfrom abc import ABC, abstractmethod\nfrom boss_bot.schemas.discord import MediaMetadata\n\nclass BaseDownloadHandler(ABC):\n    \"\"\"Abstract base class for platform-specific download handlers.\"\"\"\n\n    def __init__(self, download_dir: Path):\n        self.download_dir = download_dir\n\n    @abstractmethod\n    def download(self, url: str, **kwargs) -&gt; MediaMetadata:\n        \"\"\"Download media from URL and return metadata.\"\"\"\n        pass\n\n    @abstractmethod\n    def supports_url(self, url: str) -&gt; bool:\n        \"\"\"Check if this handler supports the given URL.\"\"\"\n        pass\n\n    def _build_command(self, url: str, **kwargs) -&gt; List[str]:\n        \"\"\"Build CLI command for external tool.\"\"\"\n        pass\n\n    def _parse_metadata(self, output: str) -&gt; MediaMetadata:\n        \"\"\"Parse CLI output into MediaMetadata.\"\"\"\n        pass\n</code></pre>"},{"location":"core/download-system/#platform-handler-implementations","title":"Platform Handler Implementations","text":""},{"location":"core/download-system/#twitterhandler","title":"TwitterHandler","text":"<pre><code># src/boss_bot/core/downloads/handlers/twitter_handler.py\nclass TwitterHandler(BaseDownloadHandler):\n    \"\"\"Handle Twitter/X downloads using gallery-dl.\"\"\"\n\n    def supports_url(self, url: str) -&gt; bool:\n        \"\"\"Check if URL is from Twitter/X.\"\"\"\n        return any(domain in url.lower() for domain in ['twitter.com', 'x.com'])\n\n    def download(self, url: str, **kwargs) -&gt; MediaMetadata:\n        \"\"\"Download Twitter content via gallery-dl CLI.\"\"\"\n        command = [\n            'gallery-dl',\n            '--extract',                    # Extract metadata only\n            '--write-info-json',           # Save metadata as JSON\n            '--cookies-from-browser', 'firefox',  # Use browser cookies\n            '--user-agent', 'Mozilla/5.0...',    # Set user agent\n            url\n        ]\n\n        result = subprocess.run(command, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise DownloadError(f\"gallery-dl failed: {result.stderr}\")\n\n        return self._parse_twitter_metadata(result.stdout)\n</code></pre>"},{"location":"core/download-system/#youtubehandler","title":"YouTubeHandler","text":"<pre><code># src/boss_bot/core/downloads/handlers/youtube_handler.py\nclass YouTubeHandler(BaseDownloadHandler):\n    \"\"\"Handle YouTube downloads using yt-dlp.\"\"\"\n\n    def download(self, url: str, quality: str = \"720p\", **kwargs) -&gt; MediaMetadata:\n        \"\"\"Download YouTube content with quality selection.\"\"\"\n        command = [\n            'yt-dlp',\n            '--format', f'best[height&lt;={quality[:-1]}]',  # Quality selection\n            '--write-info-json',                          # Save metadata\n            '--extract-flat',                             # Extract info only\n            url\n        ]\n\n        result = subprocess.run(command, capture_output=True, text=True)\n        metadata = self._parse_youtube_metadata(result.stdout)\n\n        # Add quality information\n        metadata.quality = quality\n        metadata.format_info = self._extract_format_info(result.stdout)\n\n        return metadata\n</code></pre>"},{"location":"core/download-system/#enhanced-youtube-strategy-pattern","title":"Enhanced YouTube Strategy Pattern","text":"<p>The YouTube strategy has been significantly enhanced with advanced features for Discord integration:</p> <pre><code># src/boss_bot/core/downloads/strategies/youtube_strategy.py\nclass YouTubeDownloadStrategy(BaseDownloadStrategy):\n    \"\"\"Enhanced YouTube strategy with organized storage and deduplication.\"\"\"\n\n    def _get_youtube_config(self) -&gt; dict[str, Any]:\n        \"\"\"Get optimized yt-dlp configuration for Discord workflow.\"\"\"\n        return {\n            # Organized directory structure\n            \"outtmpl\": {\n                \"default\": f\"{self.download_dir}/yt-dlp/youtube/%(uploader|Unknown)s/%(title).100s-%(id)s.%(ext)s\",\n                \"infojson\": f\"{self.download_dir}/yt-dlp/youtube/%(uploader|Unknown)s/%(title).100s-%(id)s.info.json\",\n                \"thumbnail\": f\"{self.download_dir}/yt-dlp/youtube/%(uploader|Unknown)s/%(title).100s-%(id)s.%(ext)s\",\n                \"description\": f\"{self.download_dir}/yt-dlp/youtube/%(uploader|Unknown)s/%(title).100s-%(id)s.description\"\n            },\n\n            # Quality ladder optimized for Discord limits\n            \"format\": \"best[height&lt;=720][filesize&lt;50M]/best[height&lt;=480][filesize&lt;25M]/best[height&lt;=360][filesize&lt;10M]\",\n\n            # Enhanced metadata preservation\n            \"writeinfojson\": True,\n            \"writedescription\": True,\n            \"writethumbnail\": True,\n            \"writesubtitles\": False,  # Skip subtitles for Discord uploads\n\n            # Performance optimization\n            \"noplaylist\": True,\n            \"ignoreerrors\": False,\n            \"retries\": 3,\n            \"fragment_retries\": 3,\n            \"socket_timeout\": 120,\n            \"read_timeout\": 300,\n\n            # Discord-friendly video settings\n            \"merge_output_format\": \"mp4\",\n            \"postprocessor_args\": [\"-movflags\", \"+faststart\"]  # Web-optimized MP4\n        }\n\n    async def download(self, url: str, **kwargs) -&gt; MediaMetadata:\n        \"\"\"Download with deduplication and performance tracking.\"\"\"\n        # Check for duplicates unless force_redownload is specified\n        duplicate_check = self._check_deduplication(url, **kwargs)\n        if duplicate_check:\n            logger.info(f\"Skipping duplicate download: {url}\")\n            return duplicate_check\n\n        # Performance tracking\n        start_time = time.time()\n\n        # Enhanced error handling with YouTube-specific patterns\n        try:\n            metadata = await self._download_via_api_with_fallbacks(url, **kwargs)\n\n            # Record performance metrics\n            download_duration = time.time() - start_time\n            if not metadata.error:\n                video_id = self._extract_youtube_video_id(url)\n                if video_id:\n                    self._record_download(video_id, metadata)\n                    self._record_performance_metrics(video_id, download_duration, metadata.download_method)\n\n            return metadata\n        except Exception as e:\n            download_duration = time.time() - start_time\n            logger.error(f\"YouTube download failed after {download_duration:.2f}s: {e}\")\n            raise\n</code></pre> <p>Key Strategy Enhancements:</p> <ol> <li>Organized Directory Structure</li> <li><code>yt-dlp/youtube/{channel_name}/</code> hierarchy</li> <li>Automatic channel name sanitization for filesystem compatibility</li> <li> <p>Consistent with gallery-dl organization patterns</p> </li> <li> <p>Discord Optimization</p> </li> <li>Quality ladder: 720p (50MB) \u2192 480p (25MB) \u2192 360p (10MB)</li> <li>MP4 format with web optimization (<code>+faststart</code>)</li> <li> <p>Automatic format selection based on Discord limits</p> </li> <li> <p>Deduplication System</p> </li> <li>Video ID extraction from URLs (supports all YouTube URL formats)</li> <li>Persistent download history in <code>.yt_download_history.json</code></li> <li>Automatic duplicate detection with user notification</li> <li> <p>Force redownload option with <code>force_redownload=True</code></p> </li> <li> <p>Performance Monitoring</p> </li> <li>Real-time download duration tracking</li> <li>Method performance comparison (API vs CLI vs fallback)</li> <li>Historical metrics storage in <code>.yt_performance_metrics.json</code></li> <li> <p>Statistics accessible via Discord commands</p> </li> <li> <p>Enhanced Error Handling</p> </li> <li>YouTube-specific error patterns (age-restriction, copyright, private videos)</li> <li>Retry logic with exponential backoff</li> <li>Fatal vs retryable error classification</li> <li>Automatic API-to-CLI fallback when enabled</li> </ol>"},{"location":"core/download-system/#handler-registration-and-discovery","title":"Handler Registration and Discovery","text":"<pre><code># src/boss_bot/core/downloads/manager.py\nclass DownloadManager:\n    \"\"\"Manages download operations across platforms.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n        self.handlers = self._register_handlers()\n\n    def _register_handlers(self) -&gt; Dict[str, BaseDownloadHandler]:\n        \"\"\"Register all available download handlers.\"\"\"\n        return {\n            'twitter': TwitterHandler(self.settings.download_dir),\n            'reddit': RedditHandler(self.settings.download_dir),\n            'instagram': InstagramHandler(self.settings.download_dir),\n            'youtube': YouTubeHandler(self.settings.download_dir),\n        }\n\n    def get_handler(self, url: str) -&gt; BaseDownloadHandler:\n        \"\"\"Find appropriate handler for URL.\"\"\"\n        for handler in self.handlers.values():\n            if handler.supports_url(url):\n                return handler\n        raise UnsupportedURLError(f\"No handler found for URL: {url}\")\n</code></pre>"},{"location":"core/download-system/#strategy-pattern-experimental","title":"Strategy Pattern (Experimental)","text":""},{"location":"core/download-system/#architecture-overview_1","title":"Architecture Overview","text":"<p>The Strategy pattern enables switching between CLI and API implementations:</p> <pre><code>graph TB\n    subgraph \"Strategy Selection\"\n        FLAGS[Feature Flags] --&gt; STRATEGY[Download Strategy]\n        STRATEGY --&gt; API{Use API?}\n        API --&gt;|Yes| APICLIENT[API Client]\n        API --&gt;|No| CLIHANDLER[CLI Handler]\n        APICLIENT --&gt;|Fallback| CLIHANDLER\n    end\n\n    subgraph \"Implementation\"\n        APICLIENT --&gt; GALLERYAPI[gallery-dl API]\n        CLIHANDLER --&gt; GALLERYCLI[gallery-dl CLI]\n        GALLERYAPI --&gt; METADATA[MediaMetadata]\n        GALLERYCLI --&gt; METADATA\n    end\n</code></pre>"},{"location":"core/download-system/#base-strategy-interface","title":"Base Strategy Interface","text":"<pre><code># src/boss_bot/core/downloads/strategies/base_strategy.py\nfrom abc import ABC, abstractmethod\n\nclass BaseDownloadStrategy(ABC):\n    \"\"\"Strategy interface for download implementations.\"\"\"\n\n    @abstractmethod\n    async def download(self, url: str, **kwargs) -&gt; MediaMetadata:\n        \"\"\"Download using chosen strategy (CLI or API).\"\"\"\n        pass\n\n    @abstractmethod\n    def supports_url(self, url: str) -&gt; bool:\n        \"\"\"Check if strategy supports URL.\"\"\"\n        pass\n</code></pre>"},{"location":"core/download-system/#feature-flag-system","title":"Feature Flag System","text":"<pre><code># src/boss_bot/core/downloads/feature_flags.py\nclass DownloadFeatureFlags:\n    \"\"\"Feature flags for download implementations.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n\n    @property\n    def use_api_twitter(self) -&gt; bool:\n        \"\"\"Use API-direct approach for Twitter downloads.\"\"\"\n        return self.settings.twitter_use_api_client\n\n    @property\n    def use_api_reddit(self) -&gt; bool:\n        \"\"\"Use API-direct approach for Reddit downloads.\"\"\"\n        return self.settings.reddit_use_api_client\n\n    @property\n    def api_fallback_to_cli(self) -&gt; bool:\n        \"\"\"Fallback to CLI if API fails.\"\"\"\n        return self.settings.download_api_fallback_to_cli\n</code></pre>"},{"location":"core/download-system/#strategy-implementation-example","title":"Strategy Implementation Example","text":"<pre><code># src/boss_bot/core/downloads/strategies/twitter_strategy.py\nclass TwitterDownloadStrategy(BaseDownloadStrategy):\n    \"\"\"Strategy for Twitter downloads with CLI/API choice.\"\"\"\n\n    def __init__(self, feature_flags: DownloadFeatureFlags, download_dir: Path):\n        self.feature_flags = feature_flags\n        self.download_dir = download_dir\n\n        # Keep existing CLI handler (unchanged)\n        self.cli_handler = TwitterHandler(download_dir=download_dir)\n\n        # Lazy-loaded API client\n        self._api_client = None\n\n    @property\n    def api_client(self):\n        \"\"\"Lazy load API client only when needed.\"\"\"\n        if self._api_client is None:\n            from boss_bot.core.downloads.clients import AsyncGalleryDL\n            self._api_client = AsyncGalleryDL()\n        return self._api_client\n\n    async def download(self, url: str, **kwargs) -&gt; MediaMetadata:\n        \"\"\"Download using feature-flagged approach.\"\"\"\n\n        if self.feature_flags.use_api_twitter:\n            try:\n                return await self._download_via_api(url, **kwargs)\n            except Exception as e:\n                logger.warning(f\"API download failed: {e}\")\n                if self.feature_flags.api_fallback_to_cli:\n                    return await self._download_via_cli(url, **kwargs)\n                raise\n        else:\n            return await self._download_via_cli(url, **kwargs)\n\n    async def _download_via_cli(self, url: str, **kwargs) -&gt; MediaMetadata:\n        \"\"\"Use existing CLI handler (unchanged behavior).\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None, self.cli_handler.download, url, **kwargs\n        )\n\n    async def _download_via_api(self, url: str, **kwargs) -&gt; MediaMetadata:\n        \"\"\"Use new API client.\"\"\"\n        async with self.api_client as client:\n            async for item in client.download(url, **kwargs):\n                return self._convert_api_response(item)\n</code></pre>"},{"location":"core/download-system/#api-client-implementation","title":"API Client Implementation","text":""},{"location":"core/download-system/#asyncgallerydl-client","title":"AsyncGalleryDL Client","text":"<pre><code># src/boss_bot/core/downloads/clients/aio_gallery_dl.py\nclass AsyncGalleryDL:\n    \"\"\"Asynchronous wrapper around gallery-dl API.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[Dict[str, Any]] = None,\n        config_file: Optional[Path] = None,\n        cookies_file: Optional[Path] = None,\n        cookies_from_browser: Optional[str] = None,\n    ):\n        self.config = config or {}\n        self.config_file = config_file or Path(\"~/.gallery-dl.conf\").expanduser()\n\n        # Apply cookie settings\n        if cookies_file:\n            self.config.setdefault(\"extractor\", {})[\"cookies\"] = str(cookies_file)\n        elif cookies_from_browser:\n            self.config.setdefault(\"extractor\", {})[\"cookies-from-browser\"] = cookies_from_browser\n\n    async def download(self, url: str, **options) -&gt; AsyncIterator[Dict[str, Any]]:\n        \"\"\"Download content from URL asynchronously.\"\"\"\n        loop = asyncio.get_event_loop()\n\n        # Run gallery-dl in thread pool to avoid blocking\n        def _run_gallerydl():\n            import gallery_dl\n            job = gallery_dl.job.DownloadJob(url, self.config)\n            job.run()\n            return job.pathfmt.tempdir\n\n        result = await loop.run_in_executor(None, _run_gallerydl)\n\n        # Yield download results\n        for file_info in result:\n            yield {\n                \"url\": file_info.get(\"url\"),\n                \"filename\": file_info.get(\"filename\"),\n                \"extractor\": file_info.get(\"extractor\"),\n                \"metadata\": file_info\n            }\n\n    async def extract_metadata(self, url: str) -&gt; AsyncIterator[Dict[str, Any]]:\n        \"\"\"Extract metadata without downloading files.\"\"\"\n        loop = asyncio.get_event_loop()\n\n        def _extract_metadata():\n            import gallery_dl\n            extractor = gallery_dl.extractor.find(url)\n            if extractor:\n                return list(extractor.items())\n            return []\n\n        items = await loop.run_in_executor(None, _extract_metadata)\n        for item in items:\n            yield item\n</code></pre>"},{"location":"core/download-system/#configuration-management","title":"Configuration Management","text":"<pre><code># src/boss_bot/core/downloads/clients/config/gallery_dl_config.py\nfrom pydantic import BaseModel, Field, SecretStr\n\nclass TwitterConfig(BaseModel):\n    \"\"\"Twitter extractor configuration.\"\"\"\n    quoted: bool = True\n    replies: bool = True\n    retweets: bool = True\n    videos: bool = True\n    cookies: Optional[str] = None\n    filename: str = \"{category}_{user[screen_name]}_{id}_{num}.{extension}\"\n    directory: List[str] = [\"twitter\", \"{user[screen_name]}\"]\n\nclass GalleryDLConfig(BaseModel):\n    \"\"\"Root gallery-dl configuration with validation.\"\"\"\n    extractor: ExtractorConfig\n    downloader: DownloaderConfig = DownloaderConfig()\n\n    class Config:\n        # Allow loading from JSON files\n        json_encoders = {\n            SecretStr: lambda v: v.get_secret_value() if v else None\n        }\n</code></pre>"},{"location":"core/download-system/#mediametadata-schema","title":"MediaMetadata Schema","text":""},{"location":"core/download-system/#core-metadata-structure","title":"Core Metadata Structure","text":"<pre><code># src/boss_bot/schemas/discord/metadata.py\nfrom pydantic import BaseModel\nfrom typing import Optional, List, Dict, Any\nfrom datetime import datetime\n\nclass MediaMetadata(BaseModel):\n    \"\"\"Unified metadata for downloaded media.\"\"\"\n\n    # Core identification\n    platform: str                      # twitter, reddit, youtube, instagram\n    url: str                           # Original URL\n    id: Optional[str] = None           # Platform-specific ID\n\n    # Content information\n    title: Optional[str] = None        # Post/video title\n    description: Optional[str] = None  # Content description\n    author: Optional[str] = None       # Creator/uploader\n    duration: Optional[int] = None     # Video duration (seconds)\n\n    # Media files\n    files: List[MediaFile] = []        # Downloaded files\n    thumbnail: Optional[str] = None    # Thumbnail URL\n\n    # Platform-specific metadata\n    platform_data: Dict[str, Any] = {}  # Raw platform data\n\n    # Download metadata\n    download_timestamp: datetime = Field(default_factory=datetime.now)\n    download_method: str = \"cli\"       # \"cli\" or \"api\"\n    file_size_bytes: Optional[int] = None\n    quality: Optional[str] = None      # Video quality (720p, 1080p, etc.)\n\n    # Social metadata (when available)\n    like_count: Optional[int] = None\n    retweet_count: Optional[int] = None\n    view_count: Optional[int] = None\n    comment_count: Optional[int] = None\n\nclass MediaFile(BaseModel):\n    \"\"\"Individual media file information.\"\"\"\n    filename: str\n    filepath: Path\n    file_type: str                     # image, video, audio\n    mimetype: str                      # image/jpeg, video/mp4, etc.\n    size_bytes: int\n    checksum: Optional[str] = None     # File integrity check\n</code></pre>"},{"location":"core/download-system/#platform-specific-extensions","title":"Platform-Specific Extensions","text":"<pre><code># Twitter-specific metadata extensions\nclass TwitterMetadata(MediaMetadata):\n    \"\"\"Twitter-specific metadata fields.\"\"\"\n    tweet_id: str\n    user_screen_name: str\n    is_retweet: bool = False\n    is_quote_tweet: bool = False\n    thread_position: Optional[int] = None\n\n    class Config:\n        # Ensure platform is always \"twitter\"\n        schema_extra = {\"platform\": \"twitter\"}\n\n# YouTube-specific metadata extensions\nclass YouTubeMetadata(MediaMetadata):\n    \"\"\"YouTube-specific metadata fields.\"\"\"\n    video_id: str\n    channel_name: str\n    channel_id: str\n    upload_date: datetime\n    categories: List[str] = []\n    tags: List[str] = []\n    is_livestream: bool = False\n    format_info: Dict[str, Any] = {}   # Available formats/qualities\n</code></pre>"},{"location":"core/download-system/#configuration-and-environment-variables","title":"Configuration and Environment Variables","text":""},{"location":"core/download-system/#feature-flag-environment-variables","title":"Feature Flag Environment Variables","text":"<pre><code># Enable API-direct for specific platforms\nexport TWITTER_USE_API_CLIENT=true\nexport REDDIT_USE_API_CLIENT=false\nexport INSTAGRAM_USE_API_CLIENT=false\nexport YOUTUBE_USE_API_CLIENT=false\n\n# Fallback behavior\nexport DOWNLOAD_API_FALLBACK_TO_CLI=true\n\n# Gallery-dl configuration\nexport GALLERY_DL_CONFIG_FILE=\"~/.gallery-dl.conf\"\nexport GALLERY_DL_COOKIES_FROM_BROWSER=\"firefox\"\nexport GALLERY_DL_USER_AGENT=\"Mozilla/5.0 (compatible; BossBot/1.0)\"\n</code></pre>"},{"location":"core/download-system/#bosssettings-integration","title":"BossSettings Integration","text":"<pre><code># src/boss_bot/core/env.py\nclass BossSettings(BaseSettings):\n    \"\"\"Bot configuration with download system settings.\"\"\"\n\n    # Download system configuration\n    download_dir: Path = Field(default=Path(\"./downloads\"))\n    max_file_size: int = Field(default=100_000_000)  # 100MB\n    max_concurrent_downloads: int = Field(default=3)\n\n    # Feature flags for experimental features\n    twitter_use_api_client: bool = Field(default=False)\n    reddit_use_api_client: bool = Field(default=False)\n    instagram_use_api_client: bool = Field(default=False)\n    youtube_use_api_client: bool = Field(default=False)\n    download_api_fallback_to_cli: bool = Field(default=True)\n\n    # Gallery-dl configuration\n    gallery_dl_config_file: Path = Field(default=Path(\"~/.gallery-dl.conf\"))\n    gallery_dl_cookies_file: Optional[Path] = Field(default=None)\n    gallery_dl_cookies_from_browser: Optional[str] = Field(default=None)\n    gallery_dl_user_agent: str = Field(\n        default=\"Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\"\n    )\n</code></pre>"},{"location":"core/download-system/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"core/download-system/#error-hierarchy","title":"Error Hierarchy","text":"<pre><code># src/boss_bot/core/downloads/exceptions.py\nclass DownloadError(Exception):\n    \"\"\"Base exception for download operations.\"\"\"\n    pass\n\nclass UnsupportedURLError(DownloadError):\n    \"\"\"URL is not supported by any handler.\"\"\"\n    pass\n\nclass QuotaExceededError(DownloadError):\n    \"\"\"User or guild storage quota exceeded.\"\"\"\n    pass\n\nclass DownloadTimeoutError(DownloadError):\n    \"\"\"Download operation timed out.\"\"\"\n    pass\n\nclass ExternalToolError(DownloadError):\n    \"\"\"External tool (gallery-dl/yt-dlp) failed.\"\"\"\n    def __init__(self, tool: str, returncode: int, stderr: str):\n        self.tool = tool\n        self.returncode = returncode\n        self.stderr = stderr\n        super().__init__(f\"{tool} failed with code {returncode}: {stderr}\")\n</code></pre>"},{"location":"core/download-system/#retry-and-fallback-logic","title":"Retry and Fallback Logic","text":"<pre><code>async def download_with_retry(\n    strategy: BaseDownloadStrategy,\n    url: str,\n    max_retries: int = 3,\n    **kwargs\n) -&gt; MediaMetadata:\n    \"\"\"Download with automatic retry and fallback.\"\"\"\n\n    last_exception = None\n\n    for attempt in range(max_retries):\n        try:\n            return await strategy.download(url, **kwargs)\n        except (DownloadTimeoutError, ExternalToolError) as e:\n            last_exception = e\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt  # Exponential backoff\n                logger.warning(f\"Download attempt {attempt + 1} failed, retrying in {wait_time}s: {e}\")\n                await asyncio.sleep(wait_time)\n            else:\n                logger.error(f\"All {max_retries} download attempts failed\")\n\n    raise last_exception\n</code></pre>"},{"location":"core/download-system/#performance-considerations","title":"Performance Considerations","text":""},{"location":"core/download-system/#asyncawait-patterns","title":"Async/Await Patterns","text":"<ul> <li>CLI Operations: Execute in thread pool via <code>loop.run_in_executor()</code></li> <li>API Operations: Native async with proper context management</li> <li>File I/O: Use <code>aiofiles</code> for non-blocking file operations</li> <li>Concurrent Downloads: Semaphore-based rate limiting</li> </ul>"},{"location":"core/download-system/#resource-management","title":"Resource Management","text":"<pre><code># Concurrent download limiting\nasync def download_multiple(urls: List[str], max_concurrent: int = 3):\n    \"\"\"Download multiple URLs with concurrency control.\"\"\"\n    semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def download_one(url: str):\n        async with semaphore:\n            return await strategy.download(url)\n\n    tasks = [download_one(url) for url in urls]\n    return await asyncio.gather(*tasks, return_exceptions=True)\n</code></pre>"},{"location":"core/download-system/#compression-integration","title":"Compression Integration","text":""},{"location":"core/download-system/#post-download-compression","title":"Post-Download Compression","text":"<p>Boss-Bot includes automatic compression capabilities to ensure downloaded media meets Discord file size limits:</p> <pre><code># Integration with download workflow\nasync def download_and_compress(\n    self,\n    url: str,\n    target_size_mb: int = 25,  # Discord Nitro limit\n    **kwargs\n) -&gt; CompressionResult:\n    \"\"\"Download and automatically compress media.\"\"\"\n\n    # Standard download process\n    metadata = await self.download_manager.download(url, **kwargs)\n\n    # Check if compression is needed\n    if metadata.file_size_bytes &gt; (target_size_mb * 1024 * 1024):\n        # Initialize compression manager\n        compression_manager = CompressionManager(self.settings)\n\n        # Compress the downloaded file\n        compression_result = await compression_manager.compress_file(\n            input_path=metadata.file_path,\n            target_size_mb=target_size_mb\n        )\n\n        if compression_result.success:\n            # Update metadata with compressed file info\n            metadata.file_path = compression_result.output_path\n            metadata.file_size_bytes = compression_result.compressed_size_bytes\n            metadata.compression_ratio = compression_result.compression_ratio\n\n        return compression_result\n\n    return None  # No compression needed\n</code></pre>"},{"location":"core/download-system/#compression-strategy-selection","title":"Compression Strategy Selection","text":"<p>The system automatically selects appropriate compression strategies based on file type:</p> <pre><code>graph TB\n    subgraph \"Compression Flow\"\n        DOWNLOAD[Downloaded File] --&gt; DETECT[File Type Detection]\n        DETECT --&gt; VIDEO{Video?}\n        DETECT --&gt; AUDIO{Audio?}\n        DETECT --&gt; IMAGE{Image?}\n\n        VIDEO --&gt;|Yes| VIDEOPROC[Video Processor]\n        AUDIO --&gt;|Yes| AUDIOPROC[Audio Processor]\n        IMAGE --&gt;|Yes| IMAGEPROC[Image Processor]\n\n        VIDEOPROC --&gt; FFMPEG[FFmpeg Compression]\n        AUDIOPROC --&gt; AUDIOCOMP[Audio Compression]\n        IMAGEPROC --&gt; PILLOW[PIL/Pillow Optimization]\n\n        FFMPEG --&gt; RESULT[Compressed File]\n        AUDIOCOMP --&gt; RESULT\n        PILLOW --&gt; RESULT\n    end\n</code></pre>"},{"location":"core/download-system/#discord-integration","title":"Discord Integration","text":"<pre><code># Discord cog integration example\n@commands.command(name=\"download\")\nasync def download_command(self, ctx: commands.Context, url: str):\n    \"\"\"Download with automatic compression for Discord.\"\"\"\n\n    # Determine target size based on user/server perks\n    target_size_mb = 25 if ctx.guild.premium_tier &gt;= 2 else 8  # Nitro vs basic\n\n    # Download and compress\n    try:\n        metadata = await self.download_manager.download(url)\n\n        # Check if compression is needed\n        if metadata.file_size_bytes &gt; (target_size_mb * 1024 * 1024):\n            processing_msg = await ctx.send(\"\ud83d\udd04 Compressing file for Discord...\")\n\n            compression_result = await self.compression_manager.compress_file(\n                input_path=metadata.file_path,\n                target_size_mb=target_size_mb\n            )\n\n            if compression_result.success:\n                # Send compressed file\n                await ctx.send(\n                    f\"\u2705 Compressed from {compression_result.original_size_mb:.1f}MB \"\n                    f\"to {compression_result.compressed_size_mb:.1f}MB \"\n                    f\"(ratio: {compression_result.compression_ratio:.2f})\",\n                    file=discord.File(compression_result.output_path)\n                )\n            else:\n                await ctx.send(f\"\u274c Compression failed: {compression_result.error_message}\")\n        else:\n            # Send original file\n            await ctx.send(file=discord.File(metadata.file_path))\n\n    except Exception as e:\n        await ctx.send(f\"\u274c Download failed: {e}\")\n</code></pre>"},{"location":"core/download-system/#compression-configuration","title":"Compression Configuration","text":"<p>Environment Variables: <pre><code># Compression settings\nexport COMPRESSION_TARGET_SIZE_MB=50        # Default target size\nexport COMPRESSION_FFMPEG_PRESET=slow       # Quality vs speed balance\nexport COMPRESSION_MAX_CONCURRENT=3         # Concurrent operations\nexport COMPRESSION_MIN_VIDEO_BITRATE_KBPS=125  # Minimum video quality\nexport COMPRESSION_MIN_AUDIO_BITRATE_KBPS=32   # Minimum audio quality\nexport COMPRESSION_IMAGE_MIN_QUALITY=10     # Minimum image quality (%)\n</code></pre></p> <p>Validation and Feasibility: <pre><code># Pre-compression validation\nasync def validate_compression_feasible(\n    self,\n    file_path: Path,\n    target_size_mb: int\n) -&gt; Tuple[bool, str]:\n    \"\"\"Check if compression to target size is possible.\"\"\"\n\n    media_info = await self.compression_manager.get_media_info(file_path)\n\n    # Check current size vs target\n    current_size_mb = media_info.file_size_bytes / (1024 * 1024)\n    if current_size_mb &lt;= target_size_mb:\n        return False, f\"File already {current_size_mb:.1f}MB (target: {target_size_mb}MB)\"\n\n    # Check minimum quality constraints\n    if media_info.duration_seconds:\n        required_bitrate = target_size_mb * 8 * 1000 / media_info.duration_seconds\n        min_bitrate = 125 + 32  # Video + audio minimum\n\n        if required_bitrate &lt; min_bitrate:\n            return False, f\"Required bitrate {required_bitrate:.0f}kbps below minimum {min_bitrate}kbps\"\n\n    return True, \"\"\n</code></pre></p> <p>Error Handling: <pre><code># Compression-specific error handling\ntry:\n    result = await compression_manager.compress_file(input_path, target_size_mb)\nexcept CompressionError as e:\n    if \"bitrate\" in str(e).lower():\n        await ctx.send(\"\u274c File too long for target size. Try a smaller target or longer video.\")\n    elif \"codec\" in str(e).lower():\n        await ctx.send(\"\u274c Unsupported video codec. Please try a different file.\")\n    else:\n        await ctx.send(f\"\u274c Compression failed: {e}\")\n</code></pre></p> <p>This download system provides both stability through the proven Handler pattern and innovation through the experimental Strategy pattern, enabling gradual adoption of new features while maintaining backward compatibility. The integrated compression system ensures all downloaded media can be shared within Discord's file size constraints while maintaining optimal quality.</p>"},{"location":"core/download-system/#upload-system-integration","title":"Upload System Integration","text":""},{"location":"core/download-system/#enhanced-download-workflow","title":"Enhanced Download Workflow","text":"<p>The download system now integrates seamlessly with the upload system to provide a complete download-to-Discord workflow:</p> <pre><code>graph TB\n    subgraph \"Enhanced Download Workflow\"\n        USER[Discord User] --&gt; CMD[Download Command]\n        CMD --&gt; VALIDATE[URL Validation]\n        VALIDATE --&gt; STRATEGY[Select Strategy]\n        STRATEGY --&gt; DOWNLOAD[Execute Download]\n        DOWNLOAD --&gt; SUCCESS{Download Success?}\n        SUCCESS --&gt;|No| ERROR[Send Error Message]\n        SUCCESS --&gt;|Yes| UPLOAD_CHECK{Upload Requested?}\n        UPLOAD_CHECK --&gt;|No| SAVE[Save to Directory]\n        UPLOAD_CHECK --&gt;|Yes| UPLOAD_FLOW[Upload Workflow]\n\n        subgraph \"Upload Workflow\"\n            UPLOAD_FLOW --&gt; DETECT[Detect Media Files]\n            DETECT --&gt; ANALYZE[Analyze File Sizes]\n            ANALYZE --&gt; COMPRESS{Files &gt; Discord Limit?}\n            COMPRESS --&gt;|Yes| COMPRESSION[Compress Files]\n            COMPRESS --&gt;|No| BATCH[Create Upload Batches]\n            COMPRESSION --&gt; BATCH\n            BATCH --&gt; DISCORD_UPLOAD[Upload to Discord]\n            DISCORD_UPLOAD --&gt; CLEANUP[Cleanup Files]\n        end\n\n        SAVE --&gt; END[Complete]\n        CLEANUP --&gt; END\n        ERROR --&gt; END\n    end\n</code></pre>"},{"location":"core/download-system/#download-upload-integration-architecture","title":"Download-Upload Integration Architecture","text":"<pre><code># Integration architecture in download cog\nclass DownloadCog(commands.Cog):\n    \"\"\"Enhanced download cog with upload integration.\"\"\"\n\n    def __init__(self, bot):\n        self.bot = bot\n        self.settings = bot.settings\n\n        # Core download components\n        self.download_manager = DownloadManager(bot.settings)\n        self.strategies = self._initialize_strategies()\n\n        # New upload integration\n        self.upload_manager = UploadManager(bot.settings)\n\n    async def download_with_upload(self, ctx, url, upload=True):\n        \"\"\"Unified download and upload workflow.\"\"\"\n\n        # Phase 1: Download to isolated directory\n        request_id = f\"{ctx.author.id}_{ctx.message.id}\"\n        download_dir = self.download_manager.create_isolated_dir(request_id)\n\n        try:\n            # Execute download using strategy pattern\n            metadata = await self.execute_download(url, download_dir)\n\n            if metadata.error:\n                await ctx.send(f\"\u274c Download failed: {metadata.error}\")\n                return\n\n            await ctx.send(f\"\u2705 Download completed!\")\n\n            # Phase 2: Upload processing (if requested)\n            if upload:\n                upload_result = await self.upload_manager.process_downloaded_files(\n                    download_dir, ctx, metadata.platform\n                )\n\n                if upload_result.success:\n                    await ctx.send(f\"\ud83c\udf89 {upload_result.message}\")\n                else:\n                    await ctx.send(f\"\u26a0\ufe0f {upload_result.message}\")\n\n        finally:\n            # Phase 3: Cleanup\n            if upload and self.settings.upload_cleanup_after_success:\n                shutil.rmtree(download_dir)\n</code></pre>"},{"location":"core/download-system/#upload-aware-download-configuration","title":"Upload-Aware Download Configuration","text":"<p>The download system now includes upload-specific configuration options:</p> <pre><code># Enhanced BossSettings with upload integration\nclass BossSettings(BaseSettings):\n    # Download configuration (existing)\n    download_dir: Path = Field(default=Path(\"./downloads\"))\n    max_concurrent_downloads: int = Field(default=3)\n\n    # Upload integration configuration (new)\n    upload_cleanup_after_success: bool = Field(\n        default=True,\n        description=\"Remove downloaded files after successful upload\",\n        validation_alias=\"UPLOAD_CLEANUP_AFTER_SUCCESS\"\n    )\n    upload_enable_progress_updates: bool = Field(\n        default=True,\n        description=\"Show upload progress messages\",\n        validation_alias=\"UPLOAD_ENABLE_PROGRESS_UPDATES\"\n    )\n    upload_batch_size_mb: int = Field(\n        default=20,\n        description=\"Maximum batch size for Discord uploads in MB\",\n        validation_alias=\"UPLOAD_BATCH_SIZE_MB\"\n    )\n    upload_max_files_per_batch: int = Field(\n        default=10,\n        description=\"Maximum files per Discord message\",\n        validation_alias=\"UPLOAD_MAX_FILES_PER_BATCH\"\n    )\n\n    # Compression for upload optimization\n    compression_max_upload_size_mb: int = Field(\n        default=50,\n        description=\"Target compression size for Discord uploads in MB\",\n        validation_alias=\"COMPRESSION_MAX_UPLOAD_SIZE_MB\"\n    )\n</code></pre>"},{"location":"core/download-system/#command-interface-changes","title":"Command Interface Changes","text":"<p>The download commands now support upload functionality:</p> <pre><code># Updated command signatures\n@commands.command(name=\"download\")\nasync def download_command(self, ctx: commands.Context, url: str, upload: bool = True):\n    \"\"\"Download content and optionally upload to Discord.\n\n    Args:\n        url: URL to download\n        upload: Whether to upload files to Discord (default: True)\n\n    Examples:\n        $download https://twitter.com/user/status/123         # Download and upload\n        $download https://youtube.com/watch?v=abc upload=False # Download only\n    \"\"\"\n\n@commands.command(name=\"download-only\")\nasync def download_only_command(self, ctx: commands.Context, url: str):\n    \"\"\"Download content without uploading to Discord.\n\n    Args:\n        url: URL to download\n\n    Examples:\n        $download-only https://twitter.com/user/status/123\n    \"\"\"\n    await self.download_command(ctx, url, upload=False)\n</code></pre>"},{"location":"core/download-system/#upload-integration-examples","title":"Upload Integration Examples","text":"<p>Basic Usage: <pre><code># Download and upload (default behavior)\n$download https://twitter.com/user/status/123\n\n# Download only (no upload)\n$download-only https://youtube.com/watch?v=VIDEO_ID\n\n# Explicit upload control\n$download https://reddit.com/r/pics/comments/abc123/ upload=False\n\n# YouTube-specific commands with enhanced features\n$yt-download https://youtube.com/watch?v=VIDEO_ID 1080p          # High quality\n$yt-download https://youtube.com/watch?v=VIDEO_ID 720p True     # Audio only\n$yt-playlist https://youtube.com/playlist?list=PLAYLIST_ID 720p 5 # Playlist (5 videos)\n$yt-stats                                                        # Performance stats\n</code></pre></p> <p>Workflow Messages: <pre><code>User: $download https://twitter.com/example/status/123\n\nBot: \u2705 Twitter/X download completed!\nBot: \ud83d\udce4 Processing files for upload...\nBot: \ud83d\udcca Found 2 media files (15.3MB total)\nBot: \ud83d\udcce Uploading batch 1/1: image1.jpg, video1.mp4 (15.3MB)\nBot: \ud83c\udfaf Twitter/X media files: [files attached]\nBot: \ud83c\udf89 Upload complete: 2/2 files uploaded\n</code></pre></p>"},{"location":"core/download-system/#temporary-directory-management","title":"Temporary Directory Management","text":"<p>The enhanced download system uses temporary directories for upload processing:</p> <pre><code># Temporary directory strategy\nclass DownloadManager:\n    \"\"\"Enhanced download manager with upload integration.\"\"\"\n\n    def create_isolated_download_dir(self, request_id: str) -&gt; Path:\n        \"\"\"Create isolated directory for download-upload workflow.\"\"\"\n        download_subdir = self.download_dir / request_id\n        download_subdir.mkdir(exist_ok=True, parents=True)\n        return download_subdir\n\n    async def download_with_isolation(self, url: str, isolated_dir: Path):\n        \"\"\"Download to isolated directory for upload processing.\"\"\"\n        strategy = self.get_strategy_for_url(url)\n\n        # Temporarily change strategy download directory\n        original_dir = strategy.download_dir\n        strategy.download_dir = isolated_dir\n\n        try:\n            return await strategy.download(url)\n        finally:\n            # Restore original directory\n            strategy.download_dir = original_dir\n</code></pre>"},{"location":"core/download-system/#error-handling-and-fallbacks","title":"Error Handling and Fallbacks","text":"<p>Enhanced error handling for download-upload workflows:</p> <pre><code># Comprehensive error handling\nasync def handle_download_upload_errors(self, ctx, url, upload=True):\n    \"\"\"Handle errors in download-upload workflow.\"\"\"\n\n    try:\n        # Download phase\n        metadata = await self.execute_download(url)\n\n        if metadata.error:\n            await ctx.send(f\"\u274c Download failed: {metadata.error}\")\n            return\n\n        # Upload phase (if requested)\n        if upload:\n            upload_result = await self.upload_manager.process_downloaded_files(\n                download_dir, ctx, metadata.platform\n            )\n\n            if not upload_result.success:\n                # Fallback: Save files locally\n                await ctx.send(f\"\u26a0\ufe0f Upload failed: {upload_result.message}\")\n                await ctx.send(f\"\ud83d\udcc1 Files saved locally to: `{download_dir}`\")\n\n                # Don't cleanup on upload failure\n                return\n\n    except DownloadError as e:\n        await ctx.send(f\"\u274c Download error: {e}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error in download-upload workflow: {e}\")\n        await ctx.send(\"\u274c An unexpected error occurred. Files may be saved locally.\")\n</code></pre>"},{"location":"core/download-system/#performance-optimizations","title":"Performance Optimizations","text":"<p>Upload integration includes several performance optimizations:</p> <p>Concurrent Processing: <pre><code># Concurrent download and upload preparation\nasync def optimized_download_upload(self, url, ctx):\n    \"\"\"Optimized workflow with concurrent operations.\"\"\"\n\n    # Start download\n    download_task = asyncio.create_task(strategy.download(url))\n\n    # Prepare upload manager while downloading\n    upload_manager = UploadManager(self.settings)\n\n    # Wait for download completion\n    metadata = await download_task\n\n    # Process upload immediately\n    if not metadata.error:\n        upload_result = await upload_manager.process_downloaded_files(\n            download_dir, ctx, metadata.platform\n        )\n</code></pre></p> <p>Smart Batching: <pre><code># Intelligent batching for large downloads\nasync def batch_large_downloads(self, media_files, ctx):\n    \"\"\"Handle large downloads with smart batching.\"\"\"\n\n    # Analyze total size\n    total_size_mb = sum(f.size_mb for f in media_files)\n\n    if total_size_mb &gt; 100:  # Large download\n        await ctx.send(f\"\ud83d\udce6 Large download detected ({total_size_mb:.1f}MB). Processing in batches...\")\n\n        # Process in smaller batches for better UX\n        batch_size = 5\n        for i in range(0, len(media_files), batch_size):\n            batch = media_files[i:i + batch_size]\n            await self.process_upload_batch(batch, ctx, i // batch_size + 1)\n</code></pre></p>"},{"location":"core/download-system/#integration-benefits","title":"Integration Benefits","text":"<p>The upload integration provides several key benefits:</p> <ol> <li>Seamless User Experience: Single command for download-to-Discord workflow</li> <li>Automatic Compression: Oversized files are automatically compressed for Discord</li> <li>Intelligent Batching: Files are optimally grouped for Discord's limits</li> <li>Progress Feedback: Real-time updates on download and upload progress</li> <li>Error Resilience: Graceful handling of upload failures with local fallback</li> <li>Resource Management: Automatic cleanup of temporary files</li> <li>Configuration Flexibility: Extensive customization options</li> </ol> <p>This integration transforms the download system from a simple file retrieval tool into a comprehensive media sharing solution optimized for Discord's platform constraints.</p>"},{"location":"core/testing-patterns/","title":"Testing Patterns","text":"<p>This document covers critical testing knowledge for Boss-Bot development, focusing on pytest-mock patterns, Discord command testing, and async testing best practices.</p>"},{"location":"core/testing-patterns/#core-testing-principles","title":"Core Testing Principles","text":""},{"location":"core/testing-patterns/#1-pytest-mock-exclusive-usage","title":"1. pytest-mock Exclusive Usage","text":"<p>\u2705 ALWAYS use <code>mocker</code> fixture <pre><code># \u2705 CORRECT: Use mocker fixture\n@pytest.mark.asyncio\nasync def test_download_command(mocker: MockerFixture):\n    mock_handler = mocker.Mock(spec=TwitterHandler)\n    mock_handler.download.return_value = MediaMetadata(platform=\"twitter\")\n</code></pre></p> <p>\u274c NEVER import unittest.mock directly <pre><code># \u274c WRONG: Don't import unittest.mock\nfrom unittest.mock import Mock, AsyncMock  # DON'T DO THIS\n</code></pre></p>"},{"location":"core/testing-patterns/#2-fixture-naming-conventions","title":"2. Fixture Naming Conventions","text":"<p>All custom fixtures use standardized naming:</p> <pre><code># \u2705 CORRECT: Standardized fixture names\n@pytest.fixture(scope=\"function\")\ndef fixture_settings_test() -&gt; BossSettings:\n    \"\"\"Mock BossSettings for testing.\"\"\"\n\n@pytest.fixture(scope=\"function\")\ndef fixture_mock_bot_test(mocker: MockerFixture) -&gt; Mock:\n    \"\"\"Mock BossBot instance for testing.\"\"\"\n\n@pytest.fixture(scope=\"function\")\ndef fixture_queue_manager_test() -&gt; QueueManager:\n    \"\"\"Real QueueManager instance for testing.\"\"\"\n</code></pre> <p>Naming Pattern: - Prefix: <code>fixture_</code> (always) - Component: <code>settings</code>, <code>bot</code>, <code>queue_manager</code>, etc. - Suffix: <code>_test</code>, <code>_mock</code>, <code>_data</code> (descriptive)</p>"},{"location":"core/testing-patterns/#3-function-scoped-fixtures-for-isolation","title":"3. Function-Scoped Fixtures for Isolation","text":"<pre><code>@pytest.fixture(scope=\"function\")  # Ensures test isolation\ndef fixture_mock_discord_context(mocker: MockerFixture) -&gt; Mock:\n    \"\"\"Create mocked Discord context for command testing.\"\"\"\n    ctx = mocker.Mock(spec=commands.Context)\n    ctx.send = mocker.AsyncMock()\n    ctx.author = mocker.Mock()\n    ctx.author.id = 12345\n    ctx.channel = mocker.Mock()\n    ctx.channel.id = 67890\n    return ctx\n</code></pre>"},{"location":"core/testing-patterns/#discord-command-testing-patterns","title":"Discord Command Testing Patterns","text":""},{"location":"core/testing-patterns/#the-callback-pattern","title":"The <code>.callback()</code> Pattern","text":"<p>\u2705 CORRECT: Use <code>.callback()</code> for command testing <pre><code>@pytest.mark.asyncio\nasync def test_download_command(\n    mocker: MockerFixture,\n    fixture_mock_discord_context: Mock\n):\n    \"\"\"Test download command using callback pattern.\"\"\"\n    # Create cog\n    cog = DownloadCog(bot=None)\n\n    # Mock the download handler\n    mock_download = mocker.patch.object(\n        cog, 'download_handler',\n        return_value=MediaMetadata(platform=\"twitter\", title=\"Test\")\n    )\n\n    # Test command via callback (not direct call)\n    await cog.download.callback(cog, fixture_mock_discord_context, \"https://twitter.com/test\")\n\n    # Verify behavior\n    mock_download.assert_called_once_with(\"https://twitter.com/test\")\n    fixture_mock_discord_context.send.assert_called_once()\n</code></pre></p> <p>\u274c WRONG: Direct command method calls <pre><code># \u274c DON'T DO THIS: Direct call won't work with @commands.command decorator\nawait cog.download(ctx, url)  # This will fail\n</code></pre></p>"},{"location":"core/testing-patterns/#discord-context-mocking","title":"Discord Context Mocking","text":"<p>Always create fully mocked Discord contexts:</p> <pre><code>@pytest.fixture(scope=\"function\")\ndef fixture_discord_context_test(mocker: MockerFixture) -&gt; Mock:\n    \"\"\"Create comprehensive Discord context mock.\"\"\"\n    ctx = mocker.Mock(spec=commands.Context)\n\n    # Essential async methods\n    ctx.send = mocker.AsyncMock()\n    ctx.reply = mocker.AsyncMock()\n\n    # Author information\n    ctx.author = mocker.Mock()\n    ctx.author.id = 12345\n    ctx.author.name = \"testuser\"\n    ctx.author.display_name = \"Test User\"\n\n    # Channel information\n    ctx.channel = mocker.Mock()\n    ctx.channel.id = 67890\n    ctx.channel.name = \"test-channel\"\n\n    # Guild information (optional)\n    ctx.guild = mocker.Mock()\n    ctx.guild.id = 98765\n    ctx.guild.name = \"Test Guild\"\n\n    return ctx\n</code></pre>"},{"location":"core/testing-patterns/#error-handling-in-commands","title":"Error Handling in Commands","text":"<p>Test error scenarios with proper exception handling:</p> <pre><code>@pytest.mark.asyncio\nasync def test_download_command_quota_exceeded(\n    mocker: MockerFixture,\n    fixture_discord_context_test: Mock\n):\n    \"\"\"Test download command when quota is exceeded.\"\"\"\n    cog = DownloadCog(bot=None)\n\n    # Mock quota exceeded exception\n    mocker.patch.object(\n        cog, 'download_handler',\n        side_effect=QuotaExceededError(\"Storage quota exceeded\")\n    )\n\n    # Execute command\n    await cog.download.callback(cog, fixture_discord_context_test, \"test_url\")\n\n    # Verify error message sent to user\n    sent_message = fixture_discord_context_test.send.call_args[0][0]\n    assert \"quota exceeded\" in sent_message.lower()\n</code></pre>"},{"location":"core/testing-patterns/#async-testing-best-practices","title":"Async Testing Best Practices","text":""},{"location":"core/testing-patterns/#1-proper-asyncmock-usage","title":"1. Proper AsyncMock Usage","text":"<pre><code>@pytest.mark.asyncio\nasync def test_async_operation(mocker: MockerFixture):\n    \"\"\"Test async operations with AsyncMock.\"\"\"\n\n    # \u2705 CORRECT: AsyncMock for async methods\n    mock_client = mocker.Mock()\n    mock_client.download = mocker.AsyncMock(\n        return_value={\"status\": \"success\"}\n    )\n\n    # \u2705 CORRECT: AsyncMock for context managers\n    mock_client.__aenter__ = mocker.AsyncMock(return_value=mock_client)\n    mock_client.__aexit__ = mocker.AsyncMock(return_value=None)\n\n    # Test async context manager\n    async with mock_client as client:\n        result = await client.download(\"test_url\")\n        assert result[\"status\"] == \"success\"\n</code></pre>"},{"location":"core/testing-patterns/#2-async-generator-mocking","title":"2. Async Generator Mocking","text":"<pre><code>async def async_generator_mock():\n    \"\"\"Helper to create async generator mock responses.\"\"\"\n    yield {\"extractor\": \"twitter\", \"url\": \"test1.jpg\"}\n    yield {\"extractor\": \"twitter\", \"url\": \"test2.jpg\"}\n\n@pytest.mark.asyncio\nasync def test_async_generator(mocker: MockerFixture):\n    \"\"\"Test code that uses async generators.\"\"\"\n    mock_client = mocker.Mock()\n    mock_client.download = mocker.AsyncMock(\n        return_value=async_generator_mock()\n    )\n\n    items = []\n    async for item in await mock_client.download(\"test_url\"):\n        items.append(item)\n\n    assert len(items) == 2\n    assert items[0][\"extractor\"] == \"twitter\"\n</code></pre>"},{"location":"core/testing-patterns/#3-executor-pattern-testing","title":"3. Executor Pattern Testing","text":"<p>For sync operations in async contexts:</p> <pre><code>@pytest.mark.asyncio\nasync def test_sync_in_async_context(mocker: MockerFixture):\n    \"\"\"Test sync operations executed in thread pool.\"\"\"\n\n    # Mock the sync operation\n    mock_sync_operation = mocker.Mock(return_value=\"sync_result\")\n\n    # Mock executor\n    mock_executor = mocker.patch('asyncio.get_event_loop')\n    mock_loop = mocker.Mock()\n    mock_executor.return_value = mock_loop\n    mock_loop.run_in_executor = mocker.AsyncMock(return_value=\"sync_result\")\n\n    # Test async wrapper\n    loop = asyncio.get_event_loop()\n    result = await loop.run_in_executor(None, mock_sync_operation, \"arg\")\n\n    assert result == \"sync_result\"\n    mock_loop.run_in_executor.assert_called_once_with(None, mock_sync_operation, \"arg\")\n</code></pre>"},{"location":"core/testing-patterns/#vcr-testing-for-api-interactions","title":"VCR Testing for API Interactions","text":""},{"location":"core/testing-patterns/#basic-vcr-configuration","title":"Basic VCR Configuration","text":"<pre><code># tests/conftest.py\n@pytest.fixture(scope=\"session\")\ndef vcr_config():\n    \"\"\"Configure VCR for safe API testing.\"\"\"\n    return {\n        \"record_mode\": \"once\",  # Record once, then replay\n        \"match_on\": [\"method\", \"scheme\", \"host\", \"port\", \"path\", \"query\"],\n        \"filter_headers\": [\n            \"authorization\", \"cookie\", \"x-api-key\", \"user-agent\"\n        ],\n        \"filter_query_parameters\": [\n            \"api_key\", \"access_token\", \"client_secret\"\n        ],\n    }\n</code></pre>"},{"location":"core/testing-patterns/#vcr-test-patterns","title":"VCR Test Patterns","text":"<pre><code>@pytest.mark.asyncio\n@pytest.mark.vcr(cassette_library_dir=\"tests/cassettes\")\nasync def test_api_download_with_vcr():\n    \"\"\"Test API download with VCR recording.\"\"\"\n    config = {\n        \"extractor\": {\n            \"twitter\": {\"videos\": True, \"quoted\": True}\n        }\n    }\n\n    async with AsyncGalleryDL(config=config) as client:\n        items = []\n        async for item in client.download(\"https://twitter.com/example/status/123\"):\n            items.append(item)\n\n        assert len(items) &gt; 0\n        assert items[0][\"extractor\"] == \"twitter\"\n</code></pre>"},{"location":"core/testing-patterns/#security-filtering-for-vcr","title":"Security Filtering for VCR","text":"<pre><code>def filter_request(request):\n    \"\"\"Remove sensitive data from VCR recordings.\"\"\"\n    # Remove authorization headers\n    if 'authorization' in request.headers:\n        request.headers['authorization'] = '&lt;REDACTED&gt;'\n\n    # Filter API keys from query parameters\n    if hasattr(request, 'query') and request.query:\n        filtered_query = []\n        for param in request.query:\n            if 'api_key' in param[0].lower():\n                filtered_query.append((param[0], '&lt;REDACTED&gt;'))\n            else:\n                filtered_query.append(param)\n        request.query = filtered_query\n\n    return request\n</code></pre>"},{"location":"core/testing-patterns/#fixture-organization-and-documentation","title":"Fixture Organization and Documentation","text":""},{"location":"core/testing-patterns/#fixture-documentation-standards","title":"Fixture Documentation Standards","text":"<pre><code>@pytest.fixture(scope=\"function\")\ndef fixture_download_strategy_test(\n    fixture_settings_test: BossSettings,\n    tmp_path: Path\n) -&gt; TwitterDownloadStrategy:\n    \"\"\"Create TwitterDownloadStrategy instance for testing.\n\n    Provides a strategy instance with mocked settings and temporary\n    download directory for isolated testing.\n\n    Args:\n        fixture_settings_test: Mocked BossSettings instance\n        tmp_path: Pytest temporary directory fixture\n\n    Returns:\n        TwitterDownloadStrategy: Configured strategy instance\n\n    Dependencies:\n        - fixture_settings_test: For configuration\n        - tmp_path: For isolated file operations\n    \"\"\"\n    return TwitterDownloadStrategy(\n        settings=fixture_settings_test,\n        download_dir=tmp_path\n    )\n</code></pre>"},{"location":"core/testing-patterns/#conftestpy-organization","title":"Conftest.py Organization","text":"<pre><code># tests/conftest.py - Organized by sections\n\n\"\"\"Test configuration and fixtures for boss-bot.\"\"\"\n\nimport pytest\nfrom unittest.mock import Mock, AsyncMock\nfrom pytest_mock import MockerFixture\n\n# ============================================================================\n# Environment and Settings Fixtures\n# ============================================================================\n\n@pytest.fixture(scope=\"function\")\ndef fixture_env_vars_test() -&gt; dict[str, str]:\n    \"\"\"Provide test environment variables.\"\"\"\n    return {\n        \"DISCORD_TOKEN\": \"test_token\",\n        \"COMMAND_PREFIX\": \"$\",\n        \"DOWNLOAD_DIR\": \"/tmp/downloads\"\n    }\n\n@pytest.fixture(scope=\"function\")\ndef fixture_settings_test(fixture_env_vars_test: dict) -&gt; BossSettings:\n    \"\"\"Create BossSettings instance for testing.\"\"\"\n    return BossSettings(**fixture_env_vars_test)\n\n# ============================================================================\n# Bot and Discord Fixtures\n# ============================================================================\n\n@pytest.fixture(scope=\"function\")\ndef fixture_mock_bot_test(mocker: MockerFixture) -&gt; Mock:\n    \"\"\"Create a mocked BossBot instance for testing.\"\"\"\n    bot = mocker.Mock(spec=BossBot)\n    bot.queue_manager = mocker.Mock()\n    bot.download_manager = mocker.Mock()\n    return bot\n\n# ============================================================================\n# Download System Fixtures\n# ============================================================================\n\n@pytest.fixture(scope=\"function\")\ndef fixture_twitter_handler_test(tmp_path: Path) -&gt; TwitterHandler:\n    \"\"\"Create TwitterHandler instance for testing.\"\"\"\n    return TwitterHandler(download_dir=tmp_path)\n</code></pre>"},{"location":"core/testing-patterns/#test-structure-and-file-organization","title":"Test Structure and File Organization","text":""},{"location":"core/testing-patterns/#test-directory-structure","title":"Test Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py                    # Global fixtures\n\u251c\u2500\u2500 test_bot/\n\u2502   \u251c\u2500\u2500 conftest.py               # Bot-specific fixtures\n\u2502   \u251c\u2500\u2500 test_client.py            # BossBot client tests\n\u2502   \u2514\u2500\u2500 test_cogs/\n\u2502       \u251c\u2500\u2500 test_downloads.py     # Download cog tests\n\u2502       \u2514\u2500\u2500 test_queue.py         # Queue cog tests\n\u251c\u2500\u2500 test_core/\n\u2502   \u251c\u2500\u2500 conftest.py               # Core-specific fixtures\n\u2502   \u251c\u2500\u2500 test_downloads/\n\u2502   \u2502   \u251c\u2500\u2500 test_handlers/        # Handler tests (CLI)\n\u2502   \u2502   \u251c\u2500\u2500 test_strategies/      # Strategy tests (API)\n\u2502   \u2502   \u2514\u2500\u2500 test_clients/         # API client tests with VCR\n\u2502   \u2514\u2500\u2500 test_queue/\n\u2502       \u2514\u2500\u2500 test_manager.py       # Queue manager tests\n\u2514\u2500\u2500 fixtures/                     # Test data files\n    \u251c\u2500\u2500 sample_responses.json\n    \u2514\u2500\u2500 test_media_files/\n</code></pre>"},{"location":"core/testing-patterns/#test-naming-conventions","title":"Test Naming Conventions","text":"<pre><code># Test class organization\nclass TestDownloadCog:\n    \"\"\"Test suite for DownloadCog.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_download_command_success(self):\n        \"\"\"Test successful download command execution.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_download_command_invalid_url(self):\n        \"\"\"Test download command with invalid URL.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_download_command_quota_exceeded(self):\n        \"\"\"Test download command when quota is exceeded.\"\"\"\n\n# Test function naming patterns\ndef test_twitter_handler_supports_url():\n    \"\"\"Test URL support detection for Twitter handler.\"\"\"\n\ndef test_twitter_handler_download_success():\n    \"\"\"Test successful Twitter content download.\"\"\"\n\ndef test_twitter_handler_download_failure():\n    \"\"\"Test Twitter download error handling.\"\"\"\n</code></pre>"},{"location":"core/testing-patterns/#common-testing-pitfalls-to-avoid","title":"Common Testing Pitfalls to Avoid","text":""},{"location":"core/testing-patterns/#1-direct-discord-command-calls","title":"1. Direct Discord Command Calls","text":"<pre><code># \u274c WRONG: This won't work with @commands.command\nawait cog.download(ctx, url)\n\n# \u2705 CORRECT: Use callback pattern\nawait cog.download.callback(cog, ctx, url)\n</code></pre>"},{"location":"core/testing-patterns/#2-missing-asyncmock-for-async-methods","title":"2. Missing AsyncMock for Async Methods","text":"<pre><code># \u274c WRONG: Regular Mock for async method\nctx.send = mocker.Mock()\n\n# \u2705 CORRECT: AsyncMock for async method\nctx.send = mocker.AsyncMock()\n</code></pre>"},{"location":"core/testing-patterns/#3-insufficient-context-mocking","title":"3. Insufficient Context Mocking","text":"<pre><code># \u274c WRONG: Incomplete context mock\nctx = mocker.Mock()\nctx.send = mocker.AsyncMock()\n\n# \u2705 CORRECT: Complete context mock with all required attributes\nctx = mocker.Mock(spec=commands.Context)\nctx.send = mocker.AsyncMock()\nctx.author = mocker.Mock()\nctx.author.id = 12345\nctx.channel = mocker.Mock()\nctx.channel.id = 67890\n</code></pre>"},{"location":"core/testing-patterns/#4-syncasync-mixing","title":"4. Sync/Async Mixing","text":"<pre><code># \u274c WRONG: Missing @pytest.mark.asyncio\ndef test_async_function():\n    result = await some_async_function()\n\n# \u2705 CORRECT: Proper async test decoration\n@pytest.mark.asyncio\nasync def test_async_function():\n    result = await some_async_function()\n</code></pre> <p>Following these testing patterns ensures reliable, maintainable tests that accurately verify Discord bot functionality and async operations.</p>"},{"location":"core/upload-system/","title":"Upload System","text":"<p>This document covers Boss-Bot's upload system architecture, which provides seamless integration between downloaded media and Discord uploads with intelligent compression, batching, and error handling.</p>"},{"location":"core/upload-system/#system-overview","title":"System Overview","text":"<p>The upload system transforms Boss-Bot from a simple download tool into a comprehensive media sharing solution optimized for Discord's platform constraints. It automatically processes downloaded files, compresses oversized content, creates optimal upload batches, and handles Discord-specific limitations.</p> <p>Key Features: - Automatic File Processing: Detects and categorizes media files (video, audio, image) - Intelligent Compression: Compresses files exceeding Discord's 25MB limit - Smart Batching: Groups files into optimal Discord message batches - Progress Feedback: Real-time updates on processing and upload status - Error Resilience: Graceful handling of upload failures with fallback options - Resource Management: Automatic cleanup of temporary files</p>"},{"location":"core/upload-system/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    subgraph \"Upload System Architecture\"\n        USER[Discord User] --&gt; DOWNLOAD[Download Request]\n        DOWNLOAD --&gt; MANAGER[Upload Manager]\n\n        subgraph \"Upload Manager\"\n            MANAGER --&gt; DETECTOR[Media File Detector]\n            MANAGER --&gt; ANALYZER[Size Analyzer]\n            MANAGER --&gt; COMPRESSION[Compression Manager]\n            MANAGER --&gt; PROCESSOR[Discord Processor]\n        end\n\n        subgraph \"Processing Pipeline\"\n            DETECTOR --&gt; DETECT_FILES[Find Media Files]\n            ANALYZER --&gt; SIZE_ANALYSIS[Analyze File Sizes]\n            SIZE_ANALYSIS --&gt; OVERSIZED{Files &gt; 25MB?}\n            OVERSIZED --&gt;|Yes| COMPRESSION\n            OVERSIZED --&gt;|No| BATCHING[Batch Processor]\n            COMPRESSION --&gt; COMPRESS_FILES[Compress Files]\n            COMPRESS_FILES --&gt; BATCHING\n            BATCHING --&gt; BATCH_FILES[Create Upload Batches]\n            BATCH_FILES --&gt; PROCESSOR\n            PROCESSOR --&gt; DISCORD_UPLOAD[Upload to Discord]\n        end\n\n        subgraph \"Discord Integration\"\n            DISCORD_UPLOAD --&gt; RETRY[Retry Logic]\n            RETRY --&gt; RATE_LIMIT[Rate Limit Handling]\n            RATE_LIMIT --&gt; SUCCESS[Upload Success]\n            SUCCESS --&gt; CLEANUP[File Cleanup]\n        end\n    end\n</code></pre>"},{"location":"core/upload-system/#core-components","title":"Core Components","text":""},{"location":"core/upload-system/#upload-manager","title":"Upload Manager","text":"<p>The <code>UploadManager</code> is the main orchestrator that coordinates the entire upload workflow:</p> <pre><code># src/boss_bot/core/uploads/manager.py\nclass UploadManager:\n    \"\"\"Manages the complete upload workflow with compression integration.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n        self.compression_manager = CompressionManager(settings)\n        self.discord_processor = DiscordUploadProcessor(settings)\n        self.file_detector = MediaFileDetector()\n        self.size_analyzer = FileSizeAnalyzer(settings)\n\n    async def process_downloaded_files(\n        self, download_dir: Path, ctx: commands.Context, platform_name: str\n    ) -&gt; UploadResult:\n        \"\"\"Main entry point: Process all files in download directory.\n\n        Workflow:\n        1. Detect media files in directory\n        2. Analyze file sizes to determine upload strategy\n        3. Compress oversized files using compression manager\n        4. Upload files to Discord in optimized batches\n        5. Handle failures gracefully with user feedback\n        \"\"\"\n</code></pre> <p>Key Responsibilities: - Orchestrates the complete upload workflow - Integrates with compression system for oversized files - Provides user feedback throughout the process - Handles errors and edge cases gracefully - Manages temporary file cleanup</p>"},{"location":"core/upload-system/#media-file-detection","title":"Media File Detection","text":"<p>The <code>MediaFileDetector</code> identifies and categorizes media files in download directories:</p> <pre><code># src/boss_bot/core/uploads/utils/file_detector.py\nclass MediaFileDetector:\n    \"\"\"Detects and categorizes media files.\"\"\"\n\n    def __init__(self):\n        self.video_extensions = {\".mp4\", \".avi\", \".mkv\", \".mov\", \".webm\", ...}\n        self.audio_extensions = {\".mp3\", \".wav\", \".m4a\", \".flac\", \".aac\", ...}\n        self.image_extensions = {\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", ...}\n\n    async def find_media_files(self, directory: Path) -&gt; list[MediaFile]:\n        \"\"\"Find all media files in directory recursively.\"\"\"\n</code></pre> <p>Features: - Recursive Directory Search: Finds files in subdirectories - Media Type Classification: Video, audio, image, unknown - Extension-Based Detection: Comprehensive file type recognition - Error Handling: Gracefully handles inaccessible files</p> <p>Supported File Types: - Video: MP4, AVI, MKV, MOV, FLV, WMV, WebM, MPEG, 3GP, M4V, OGV, TS - Audio: MP3, WAV, M4A, FLAC, AAC, OGG, WMA, Opus, AIFF, AU - Image: JPG, JPEG, PNG, GIF, WebP, BMP, TIFF, SVG, ICO</p>"},{"location":"core/upload-system/#size-analysis","title":"Size Analysis","text":"<p>The <code>FileSizeAnalyzer</code> categorizes files based on Discord's upload limits:</p> <pre><code># src/boss_bot/core/uploads/utils/size_analyzer.py\nclass FileSizeAnalyzer:\n    \"\"\"Analyzes file sizes to determine upload strategy.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.discord_limit_mb = 25.0  # Discord's default limit\n        self.max_upload_size_mb = getattr(settings, \"compression_max_upload_size_mb\", 50.0)\n\n    async def analyze_files(self, media_files: list[MediaFile]) -&gt; SizeAnalysis:\n        \"\"\"Analyze media files to categorize by size requirements.\"\"\"\n</code></pre> <p>Analysis Categories: - Acceptable Files: Files \u2264 25MB that can be uploaded directly - Oversized Files: Files &gt; 25MB that require compression - Total Statistics: File count, total size, distribution</p> <p>Configuration: - <code>discord_limit_mb</code>: Discord's file size limit (25MB default) - <code>compression_max_upload_size_mb</code>: Target compression size</p>"},{"location":"core/upload-system/#batch-processing","title":"Batch Processing","text":"<p>The <code>BatchProcessor</code> creates optimal upload batches respecting Discord's limits:</p> <pre><code># src/boss_bot/core/uploads/utils/batch_processor.py\nclass BatchProcessor:\n    \"\"\"Processes media files into batches respecting Discord limits.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        # Discord's hard limits\n        self.max_files_per_message = 10\n        self.max_message_size_mb = 25.0\n\n        # Configurable limits from settings\n        self.preferred_batch_size_mb = getattr(settings, \"upload_batch_size_mb\", 20.0)\n        self.preferred_max_files = min(getattr(settings, \"upload_max_files_per_batch\", 10), 10)\n</code></pre> <p>Batching Strategy: - Size-Based Grouping: Ensures batches don't exceed Discord limits - File Count Limits: Respects Discord's 10 file per message limit - Optimization: Smaller files first for better packing efficiency - Oversized Handling: Single-file batches for files that can't be grouped</p> <p>Discord Limits: - Maximum 10 files per message (hard limit) - Maximum 25MB total per message (hard limit) - Configurable batch size (default: 20MB for safety margin)</p>"},{"location":"core/upload-system/#discord-upload-processing","title":"Discord Upload Processing","text":"<p>The <code>DiscordUploadProcessor</code> handles Discord-specific upload logic:</p> <pre><code># src/boss_bot/core/uploads/processors/discord_processor.py\nclass DiscordUploadProcessor:\n    \"\"\"Handles Discord-specific upload logic.\"\"\"\n\n    def __init__(self, settings: BossSettings):\n        self.settings = settings\n        self.batch_processor = BatchProcessor(settings)\n        self.max_retries = 3\n        self.retry_delay = 2.0\n\n    async def upload_files(\n        self, media_files: list[MediaFile], ctx: commands.Context, platform_name: str\n    ) -&gt; UploadResult:\n        \"\"\"Upload media files to Discord in optimized batches.\"\"\"\n</code></pre> <p>Features: - Batch Upload Management: Processes multiple batches sequentially - Retry Logic: Automatic retries with exponential backoff - Rate Limit Handling: Respects Discord API rate limits - Progress Updates: Real-time feedback to users - Error Recovery: Graceful handling of upload failures</p>"},{"location":"core/upload-system/#data-models","title":"Data Models","text":""},{"location":"core/upload-system/#mediafile","title":"MediaFile","text":"<p>Represents a media file ready for upload:</p> <pre><code>@dataclass\nclass MediaFile:\n    \"\"\"Represents a media file ready for upload.\"\"\"\n    path: Path\n    filename: str\n    size_bytes: int\n    media_type: MediaType\n    is_compressed: bool = False\n    original_path: Path | None = None\n\n    @property\n    def size_mb(self) -&gt; float:\n        \"\"\"Get file size in MB.\"\"\"\n        return self.size_bytes / (1024 * 1024)\n</code></pre>"},{"location":"core/upload-system/#uploadbatch","title":"UploadBatch","text":"<p>Groups files for Discord upload:</p> <pre><code>@dataclass\nclass UploadBatch:\n    \"\"\"A batch of files to upload together.\"\"\"\n    files: list[MediaFile]\n    total_size_bytes: int\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n    @property\n    def total_size_mb(self) -&gt; float:\n        \"\"\"Get total batch size in MB.\"\"\"\n        return self.total_size_bytes / (1024 * 1024)\n</code></pre>"},{"location":"core/upload-system/#uploadresult","title":"UploadResult","text":"<p>Represents the outcome of an upload operation:</p> <pre><code>@dataclass\nclass UploadResult:\n    \"\"\"Result of an upload operation.\"\"\"\n    success: bool\n    message: str\n    files_processed: int\n    successful_uploads: int = 0\n    failed_uploads: int = 0\n    error: str | None = None\n    metadata: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"core/upload-system/#upload-workflow","title":"Upload Workflow","text":""},{"location":"core/upload-system/#step-by-step-process","title":"Step-by-Step Process","text":"<ol> <li> <p>File Discovery <pre><code># Find all media files in download directory\nmedia_files = await self.file_detector.find_media_files(download_dir)\n</code></pre></p> </li> <li> <p>Size Analysis <pre><code># Categorize files by size requirements\nsize_analysis = await self.size_analyzer.analyze_files(media_files)\n</code></pre></p> </li> <li> <p>Compression (if needed) <pre><code># Compress files that exceed Discord limits\ncompressed_files = await self._compress_oversized_files(\n    size_analysis.oversized_files, ctx, platform_name\n)\n</code></pre></p> </li> <li> <p>Batch Creation <pre><code># Create optimized upload batches\nupload_files = size_analysis.acceptable_files + compressed_files\nbatches = self.batch_processor.optimize_batches(upload_files)\n</code></pre></p> </li> <li> <p>Discord Upload <pre><code># Upload each batch to Discord\nupload_result = await self.discord_processor.upload_files(\n    upload_files, ctx, platform_name\n)\n</code></pre></p> </li> <li> <p>Cleanup <pre><code># Remove temporary files if configured\nif self.settings.upload_cleanup_after_success and upload_result.success:\n    shutil.rmtree(download_dir)\n</code></pre></p> </li> </ol>"},{"location":"core/upload-system/#user-experience-flow","title":"User Experience Flow","text":"<pre><code>User: $download https://twitter.com/example/status/123\n\nBot: \u2705 Twitter/X download completed!\nBot: \ud83d\udce4 Processing files for upload...\nBot: \ud83d\udcca Found 3 media files (45.2MB total)\nBot: \ud83d\udddc\ufe0f 1 files need compression\nBot: \ud83d\udddc\ufe0f Compressing video.mp4 (32.1MB \u2192 target: 23.8MB)\nBot: \u2705 Compressed successfully! (32MB \u2192 24MB, ratio: 0.75)\nBot: \ud83d\udcce Uploading batch 1/1: video_compressed.mp4, image1.jpg, image2.png (23.1MB)\nBot: \ud83c\udfaf Twitter/X media files: [attached files]\nBot: \u2139\ufe0f Compression Info:\n     \ud83d\udddc\ufe0f video_compressed.mp4 (compressed from video.mp4)\nBot: \ud83c\udf89 Upload complete: 3/3 files uploaded\n</code></pre>"},{"location":"core/upload-system/#integration-with-download-system","title":"Integration with Download System","text":""},{"location":"core/upload-system/#enhanced-download-commands","title":"Enhanced Download Commands","text":"<p>The upload system integrates seamlessly with the download system:</p> <pre><code>@commands.command(name=\"download\")\nasync def download_command(self, ctx: commands.Context, url: str, upload: bool = True):\n    \"\"\"Download content and optionally upload to Discord.\"\"\"\n\n    # Create isolated download directory\n    request_id = f\"{ctx.author.id}_{ctx.message.id}\"\n    download_dir = self.download_dir / request_id\n\n    try:\n        # Execute download\n        metadata = await strategy.download(url)\n\n        # Process upload if requested\n        if upload:\n            upload_result = await self.upload_manager.process_downloaded_files(\n                download_dir, ctx, platform_name\n            )\n    finally:\n        # Cleanup if configured\n        if upload and self.settings.upload_cleanup_after_success:\n            shutil.rmtree(download_dir)\n</code></pre>"},{"location":"core/upload-system/#temporary-directory-management","title":"Temporary Directory Management","text":"<p>The system uses isolated temporary directories for each download-upload request:</p> <p>Benefits: - Isolation: Each request has its own directory - Concurrency: Multiple users can download simultaneously - Cleanup: Automatic removal after successful upload - Debugging: Files preserved on failure for analysis</p> <p>Directory Structure: <pre><code>downloads/\n\u251c\u2500\u2500 123456789_987654321/    # user_id_message_id\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 video1.mp4\n\u2502   \u2514\u2500\u2500 video1_compressed.mp4\n\u2514\u2500\u2500 234567890_876543210/\n    \u251c\u2500\u2500 audio1.mp3\n    \u2514\u2500\u2500 image2.png\n</code></pre></p>"},{"location":"core/upload-system/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"core/upload-system/#error-categories","title":"Error Categories","text":"<ol> <li>File System Errors</li> <li>File not found or inaccessible</li> <li>Permission issues</li> <li> <p>Disk space limitations</p> </li> <li> <p>Compression Errors</p> </li> <li>FFmpeg failures</li> <li>Insufficient quality for target size</li> <li> <p>Codec compatibility issues</p> </li> <li> <p>Discord API Errors</p> </li> <li>File too large (even after compression)</li> <li>Rate limiting</li> <li>Network connectivity issues</li> <li> <p>Authentication problems</p> </li> <li> <p>Processing Errors</p> </li> <li>Invalid file formats</li> <li>Corrupted media files</li> <li>Unexpected file types</li> </ol>"},{"location":"core/upload-system/#error-handling-strategies","title":"Error Handling Strategies","text":"<pre><code># Comprehensive error handling\nasync def handle_upload_errors(self, upload_result: UploadResult, ctx: commands.Context):\n    \"\"\"Handle various upload error scenarios.\"\"\"\n\n    if not upload_result.success:\n        if \"too large\" in upload_result.message.lower():\n            await ctx.send(\n                \"\ud83d\udca1 Files too large even after compression. \"\n                \"Consider using external storage or lower quality settings.\"\n            )\n        elif \"rate limit\" in upload_result.message.lower():\n            await ctx.send(\n                \"\u23f3 Discord rate limit reached. \"\n                \"Upload will continue automatically when limit resets.\"\n            )\n        elif upload_result.failed_uploads &gt; 0:\n            success_rate = upload_result.successful_uploads / upload_result.files_processed\n            await ctx.send(\n                f\"\u26a0\ufe0f Partial upload success: {success_rate:.1%} files uploaded successfully\"\n            )\n        else:\n            await ctx.send(f\"\u274c Upload failed: {upload_result.error}\")\n</code></pre>"},{"location":"core/upload-system/#retry-logic","title":"Retry Logic","text":"<p>The system includes intelligent retry mechanisms:</p> <pre><code># Retry with exponential backoff\nfor attempt in range(self.max_retries):\n    try:\n        return await self._attempt_batch_upload(batch, ctx, platform_name)\n    except discord.HTTPException as e:\n        if e.status == 429:  # Rate limited\n            retry_after = getattr(e, \"retry_after\", self.retry_delay)\n            if attempt &lt; self.max_retries - 1:\n                await ctx.send(f\"\u23f3 Rate limited. Retrying in {retry_after:.1f}s...\")\n                await asyncio.sleep(retry_after)\n                continue\n</code></pre>"},{"location":"core/upload-system/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"core/upload-system/#concurrent-processing","title":"Concurrent Processing","text":"<pre><code># Optimize compression with concurrency\nasync def compress_files_concurrently(self, oversized_files: list[MediaFile]):\n    \"\"\"Compress multiple files concurrently.\"\"\"\n\n    semaphore = asyncio.Semaphore(self.settings.compression_max_concurrent)\n\n    async def compress_one(media_file: MediaFile):\n        async with semaphore:\n            return await self.compression_manager.compress_file(media_file.path)\n\n    tasks = [compress_one(file) for file in oversized_files]\n    return await asyncio.gather(*tasks, return_exceptions=True)\n</code></pre>"},{"location":"core/upload-system/#memory-management","title":"Memory Management","text":"<ul> <li>Streaming Processing: Files processed individually to minimize memory usage</li> <li>Temporary File Cleanup: Automatic removal of intermediate files</li> <li>Batch Size Limits: Prevents large memory allocations</li> </ul>"},{"location":"core/upload-system/#network-optimization","title":"Network Optimization","text":"<ul> <li>Batch Uploads: Multiple files per Discord message</li> <li>Retry Logic: Handles temporary network issues</li> <li>Rate Limit Respect: Prevents API quota exhaustion</li> </ul>"},{"location":"core/upload-system/#configuration-and-tuning","title":"Configuration and Tuning","text":""},{"location":"core/upload-system/#performance-profiles","title":"Performance Profiles","text":"<p>High-Volume Usage: <pre><code>UPLOAD_BATCH_SIZE_MB=20                      # Maximum safe batch size\nUPLOAD_MAX_FILES_PER_BATCH=10                # Maximum throughput\nUPLOAD_ENABLE_PROGRESS_UPDATES=false        # Reduce message volume\nCOMPRESSION_MAX_CONCURRENT=4                 # More concurrent operations\n</code></pre></p> <p>Quality-Focused Usage: <pre><code>UPLOAD_BATCH_SIZE_MB=15                      # Smaller, more reliable batches\nCOMPRESSION_FFMPEG_PRESET=slow               # Higher quality compression\nCOMPRESSION_MAX_UPLOAD_SIZE_MB=20            # Conservative compression target\n</code></pre></p> <p>Development/Testing: <pre><code>UPLOAD_CLEANUP_AFTER_SUCCESS=false          # Keep files for inspection\nUPLOAD_ENABLE_PROGRESS_UPDATES=true         # Detailed feedback\nUPLOAD_BATCH_SIZE_MB=10                      # Small batches for testing\n</code></pre></p>"},{"location":"core/upload-system/#monitoring-and-metrics","title":"Monitoring and Metrics","text":"<p>The upload system provides comprehensive metrics for monitoring:</p> <pre><code># Upload metrics\nupload_result.metadata = {\n    \"platform\": platform_name,\n    \"total_files\": len(media_files),\n    \"successful_uploads\": successful_uploads,\n    \"failed_uploads\": failed_uploads,\n    \"compression_attempts\": len(oversized_files),\n    \"successful_compressions\": len(compressed_files),\n    \"total_size_mb\": total_size_mb,\n    \"batches_processed\": len(batches),\n    \"processing_time_seconds\": processing_time,\n    \"upload_time_seconds\": upload_time\n}\n</code></pre>"},{"location":"core/upload-system/#security-considerations","title":"Security Considerations","text":""},{"location":"core/upload-system/#file-validation","title":"File Validation","text":"<ul> <li>Extension Checking: Only processes known media file types</li> <li>Size Limits: Enforces maximum file size limits</li> <li>Path Validation: Prevents directory traversal attacks</li> </ul>"},{"location":"core/upload-system/#temporary-file-security","title":"Temporary File Security","text":"<ul> <li>Isolated Directories: Each request uses unique directory</li> <li>Permission Control: Restricted file permissions</li> <li>Automatic Cleanup: Removes sensitive content after processing</li> </ul>"},{"location":"core/upload-system/#discord-integration-security","title":"Discord Integration Security","text":"<ul> <li>Token Protection: Secure handling of Discord bot tokens</li> <li>Rate Limit Compliance: Prevents API abuse</li> <li>Error Information: Sanitized error messages to users</li> </ul>"},{"location":"core/upload-system/#future-enhancements","title":"Future Enhancements","text":""},{"location":"core/upload-system/#planned-features","title":"Planned Features","text":"<ol> <li>Cloud Storage Integration</li> <li>Upload to AWS S3, Google Cloud Storage</li> <li>Generate shareable links for large files</li> <li> <p>Archive old uploads automatically</p> </li> <li> <p>Advanced Compression Options</p> </li> <li>User-selectable quality presets</li> <li>Format conversion (WebM, HEVC)</li> <li> <p>Custom compression parameters</p> </li> <li> <p>Upload Scheduling</p> </li> <li>Queue large uploads for off-peak hours</li> <li>Priority system for different content types</li> <li> <p>Bandwidth throttling options</p> </li> <li> <p>Enhanced User Controls</p> </li> <li>Per-user upload preferences</li> <li>Quality vs. speed trade-off settings</li> <li>Upload notification preferences</li> </ol>"},{"location":"core/upload-system/#extensibility","title":"Extensibility","text":"<p>The upload system is designed for extensibility:</p> <pre><code># Custom upload processors\nclass CustomUploadProcessor(BaseUploadProcessor):\n    \"\"\"Custom upload processor for alternative platforms.\"\"\"\n\n    async def upload_files(self, files: list[MediaFile]) -&gt; UploadResult:\n        \"\"\"Implement custom upload logic.\"\"\"\n        pass\n\n# Register custom processor\nupload_manager.register_processor(\"custom\", CustomUploadProcessor())\n</code></pre> <p>This upload system provides a robust, efficient, and user-friendly solution for sharing downloaded media content through Discord while respecting platform constraints and providing optimal user experience.</p>"},{"location":"development/claude_code/","title":"Claude Code Custom Slash Commands","text":"<p>This guide documents the custom slash commands available in the boss-bot project for Claude Code development workflows. These commands provide specialized automation for common development tasks, testing patterns, and code review processes.</p>"},{"location":"development/claude_code/#available-commands","title":"Available Commands","text":""},{"location":"development/claude_code/#1-context_prime-basic-project-context","title":"1. <code>/context_prime</code> - Basic Project Context","text":"<p>File: <code>.claude/commands/context_prime.md</code></p> <p>Purpose: Establishes basic project context by reading the README and understanding the project structure.</p> <p>Command: <pre><code>READ README.md, THEN run git ls-files to understand the context of the project.\n</code></pre></p> <p>Usage Example: <pre><code>/context_prime\n</code></pre></p> <p>When to Use: - Starting a new Claude Code session - Need quick project overview - Working on unfamiliar parts of the codebase - Onboarding new contributors</p>"},{"location":"development/claude_code/#2-context_prime_w_lead-enhanced-context-priming","title":"2. <code>/context_prime_w_lead</code> - Enhanced Context Priming","text":"<p>File: <code>.claude/commands/context_prime_w_lead.md</code></p> <p>Purpose: Enhanced version that includes specific file reading capabilities alongside basic project context.</p> <p>Command: <pre><code>READ README.md, THEN run git ls-files to understand the context of the project. Be sure to also READ: $ARGUMENTS and nothing else.\n</code></pre></p> <p>Usage Examples: <pre><code>/context_prime_w_lead src/boss_bot/core/env.py\n/context_prime_w_lead tests/conftest.py pyproject.toml\n/context_prime_w_lead src/boss_bot/bot/cogs/downloads.py src/boss_bot/core/downloads/handlers/base_handler.py\n</code></pre></p> <p>When to Use: - Starting work on specific modules or features - Need context for particular files before making changes - Debugging specific components - Code review preparation</p>"},{"location":"development/claude_code/#3-jprompt_ultra_diff_review-multi-llm-code-review","title":"3. <code>/jprompt_ultra_diff_review</code> - Multi-LLM Code Review","text":"<p>File: <code>.claude/commands/jprompt_ultra_diff_review.md</code></p> <p>Purpose: Comprehensive code review workflow using multiple AI models for thorough analysis and synthesis.</p> <p>Workflow: 1. Primary Analysis (openai:o3-mini): Initial code review and analysis 2. Alternative Perspective (anthropic:claude-3-7-sonnet-20250219:4k): Secondary review with different model 3. Technical Deep-dive (gemini:gemini-2.0-flash-thinking-exp): Detailed technical analysis 4. Synthesis: Combine insights from all three models 5. Final Recommendations: Actionable improvement suggestions</p> <p>Usage Examples: <pre><code># Review current staged changes\n/jprompt_ultra_diff_review\n\n# Review specific commit\ngit show abc123 | /jprompt_ultra_diff_review\n\n# Review pull request changes\ngh pr diff 42 | /jprompt_ultra_diff_review\n</code></pre></p> <p>When to Use: - Major feature implementations - Critical bug fixes - Code refactoring efforts - Architecture changes - Pre-merge code review</p>"},{"location":"development/claude_code/#4-tdd_dpytest-test-driven-development-workflow","title":"4. <code>/tdd_dpytest</code> - Test-Driven Development Workflow","text":"<p>File: <code>.claude/commands/tdd_dpytest.md</code></p> <p>Purpose: Comprehensive TDD workflow specifically designed for Discord bot development using dpytest framework.</p> <p>Workflow (RED-GREEN-REFACTOR Cycle): 1. Analyze Requirements: Understand what needs to be implemented 2. Write Failing Tests: Create tests that define expected behavior 3. Run Tests (RED): Verify tests fail as expected 4. Implement Minimal Code: Write just enough code to make tests pass 5. Run Tests (GREEN): Verify tests now pass 6. Refactor: Improve code while keeping tests green 7. Advanced Testing: Add edge cases, concurrency, permissions 8. Integration Testing: Test with actual Discord bot integration 9. Documentation: Update docs and examples</p> <p>Usage Examples: <pre><code># Start TDD for new Discord command\n/tdd_dpytest\n\n# Apply TDD to specific cog feature\n/tdd_dpytest (when working on downloads cog)\n\n# TDD for error handling improvements\n/tdd_dpytest (when implementing robust error handling)\n\n# Implement new platform support with TDD\n/project:tdd_dpytest \"new download command for TikTok platform with URL validation and metadata extraction\"\n\n# Add advanced error handling\n/project:tdd_dpytest \"enhance download command to handle rate limiting and network timeouts gracefully\"\n\n# Complex workflow testing\n/project:tdd_dpytest \"multi-step command workflow for bulk download management with progress tracking\"\n</code></pre></p> <p>When to Use: - Implementing new Discord commands - Adding new cog functionality - Fixing complex bugs with proper test coverage - Refactoring existing Discord bot features</p>"},{"location":"development/claude_code/#5-write_dpytest-quick-test-generation","title":"5. <code>/write_dpytest</code> - Quick Test Generation","text":"<p>File: <code>.claude/commands/write_dpytest.md</code></p> <p>Purpose: Rapid generation of dpytest tests for Discord bot components with support for multiple test types.</p> <p>Supported Test Types: - basic: Simple command testing - cog: Cog-specific functionality testing - direct: Direct method testing without Discord context - error: Error handling and edge case testing - fixture: Custom fixture creation and usage - integration: Full Discord bot integration testing</p> <p>Usage Examples: <pre><code># Generate basic command tests\n/write_dpytest basic\n\n# Create cog-specific tests\n/write_dpytest cog\n\n# Generate error handling tests\n/write_dpytest error\n\n# Create integration tests\n/write_dpytest integration\n</code></pre></p> <p>When to Use: - Quick test creation for new features - Adding missing test coverage - Creating test templates - Prototyping test scenarios</p>"},{"location":"development/claude_code/#workflow-examples","title":"Workflow Examples","text":""},{"location":"development/claude_code/#new-feature-development","title":"New Feature Development","text":"<pre><code># 1. Start with enhanced context\n/context_prime_w_lead src/boss_bot/bot/cogs/downloads.py\n\n# 2. Use TDD workflow for implementation\n/tdd_dpytest\n\n# 3. Generate additional tests as needed\n/write_dpytest error\n\n# 4. Comprehensive code review before merge\n/jprompt_ultra_diff_review\n</code></pre>"},{"location":"development/claude_code/#bug-investigation-and-fix","title":"Bug Investigation and Fix","text":"<pre><code># 1. Get project context with relevant files\n/context_prime_w_lead tests/test_bot/test_downloads.py src/boss_bot/bot/cogs/downloads.py\n\n# 2. Write tests that reproduce the bug\n/write_dpytest error\n\n# 3. Apply TDD to fix the issue\n/tdd_dpytest\n\n# 4. Review the fix\n/jprompt_ultra_diff_review\n</code></pre>"},{"location":"development/claude_code/#code-review-process","title":"Code Review Process","text":"<pre><code># 1. Understand the changes in context\n/context_prime_w_lead [modified files]\n\n# 2. Comprehensive multi-model review\n/jprompt_ultra_diff_review\n\n# 3. If tests need improvement\n/write_dpytest [appropriate type]\n</code></pre>"},{"location":"development/claude_code/#integration-with-boss-bot-development","title":"Integration with Boss-Bot Development","text":""},{"location":"development/claude_code/#quality-standards","title":"Quality Standards","text":"<p>All commands integrate with boss-bot's quality tools: - Testing: <code>just check-test</code> - Linting: <code>just check-code</code> - Type Checking: <code>just check-type</code> - Full Suite: <code>just check</code></p>"},{"location":"development/claude_code/#project-structure","title":"Project Structure","text":"<p>Commands understand boss-bot's modular architecture: - Discord cogs in <code>src/boss_bot/bot/cogs/</code> - Core services in <code>src/boss_bot/core/</code> - Test organization matching <code>src/</code> structure - dpytest integration patterns</p>"},{"location":"development/claude_code/#testing-patterns","title":"Testing Patterns","text":"<p>Commands follow established patterns: - Function-scoped fixtures with <code>fixture_</code> prefix - <code>.callback()</code> pattern for testing decorated commands - Proper async/await testing with <code>@pytest.mark.asyncio</code> - Mock usage via <code>mocker.Mock()</code> and <code>mocker.AsyncMock()</code></p>"},{"location":"development/claude_code/#best-practices","title":"Best Practices","text":""},{"location":"development/claude_code/#command-selection","title":"Command Selection","text":"<ul> <li>Use <code>/context_prime</code> for quick orientation</li> <li>Use <code>/context_prime_w_lead</code> when working on specific files</li> <li>Use <code>/tdd_dpytest</code> for new feature development</li> <li>Use <code>/write_dpytest</code> for quick test additions</li> <li>Use <code>/jprompt_ultra_diff_review</code> for thorough code review</li> </ul>"},{"location":"development/claude_code/#workflow-integration","title":"Workflow Integration","text":"<ul> <li>Start sessions with context priming</li> <li>Use TDD for complex features</li> <li>Apply multi-model review for critical changes</li> <li>Generate tests early and often</li> <li>Review code before committing</li> </ul>"},{"location":"development/claude_code/#common-patterns","title":"Common Patterns","text":"<ul> <li>Always run quality checks after implementing changes</li> <li>Follow the existing fixture naming conventions</li> <li>Use proper async testing patterns for Discord components</li> <li>Document new features and changes</li> <li>Maintain test coverage for all new functionality</li> </ul>"},{"location":"development/claude_code/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/claude_code/#test-failures","title":"Test Failures","text":"<p>If tests fail after using commands: 1. Check test isolation with function-scoped fixtures 2. Verify proper <code>.callback()</code> usage for Discord commands 3. Ensure async/await patterns are correct 4. Review mock setup for Discord components</p>"},{"location":"development/claude_code/#integration-issues","title":"Integration Issues","text":"<p>If commands don't work as expected: 1. Verify file paths and project structure 2. Check that boss-bot dependencies are installed 3. Ensure git context is available 4. Validate dpytest configuration</p>"},{"location":"development/claude_code/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Multi-model review commands may take longer to execute</li> <li>Context priming with many files may be slower</li> <li>TDD workflows are iterative and may require multiple runs</li> <li>Test generation should be followed by manual review and customization</li> </ul>"},{"location":"development/claude_code/custom_slash_commands/","title":"Create a Custom Slash Command for your project in Claude Code","text":"<p>Here's how to use it:</p>"},{"location":"development/claude_code/custom_slash_commands/#how-to-set-up-the-slash-command","title":"How to Set Up the Slash Command","text":"<ol> <li> <p>Create the commands directory (if it doesn't exist):    <pre><code>mkdir -p .claude/commands\n</code></pre></p> </li> <li> <p>Save the command file:    Copy the content from the artifact above and save it as <code>.claude/commands/create-slash-command.md</code></p> </li> <li> <p>Use the command:    <pre><code># In Claude Code, run:\n/project:create-slash-command my-new-command\n</code></pre></p> </li> </ol>"},{"location":"development/claude_code/custom_slash_commands/#how-it-works","title":"How It Works","text":"<p>The slash command will:</p> <ol> <li>Guide you step-by-step through gathering all the necessary information for your project</li> <li>Use our standardized template with XML tags for optimal Claude usage</li> <li>Generate the complete <code>.claude/commands/&lt;name&gt;.md</code> file with all your project-specific information</li> <li>Save it automatically so your team can use the new command immediately</li> </ol>"},{"location":"development/claude_code/custom_slash_commands/#example-usage-flow","title":"Example Usage Flow","text":"<pre><code># Start the command creation process\n&gt; /project:create-slash-command api-setup\n\n# Claude will then ask you questions like:\n# \"What is the name of this project?\"\n# \"What command installs dependencies?\"\n# \"How do you run tests?\"\n# etc.\n\n# After answering all questions, you'll get a new file:\n# .claude/commands/api-setup.md\n\n# Your team can then use:\n&gt; /project:api-setup\n</code></pre>"},{"location":"development/claude_code/custom_slash_commands/#benefits","title":"Benefits","text":"<ul> <li>Consistency: All your project commands follow the same structure</li> <li>Team efficiency: New team members get standardized setup instructions</li> <li>AI optimization: XML tags help Claude parse and use the information effectively</li> <li>Maintainability: Easy to update and modify project setup procedures</li> </ul> <p>The generated commands will include all the key information agents need: installation steps, dependency management, testing, linting, troubleshooting, and AI-specific guidelines.</p>"},{"location":"development/claude_code/claude-sessions/","title":"Claude Code Session Management Commands","text":"<p>Custom slash commands for Claude Code that provide comprehensive development session tracking and documentation. Based on Claude Code's custom slash command system.</p>"},{"location":"development/claude_code/claude-sessions/#overview","title":"\ud83c\udfaf Overview","text":"<p>This is a set of custom slash commands for Claude Code that helps developers maintain continuity across multiple coding sessions with Claude by:</p> <ul> <li>Documenting Progress: Capture what was done, how it was done, and why decisions were made</li> <li>Tracking Changes: Monitor git changes, todo items, and implementation details</li> <li>Knowledge Transfer: Enable future sessions to understand past work without re-analyzing the entire codebase</li> <li>Issue Resolution: Document problems encountered and their solutions for future reference</li> </ul> <p>These commands extend Claude Code's built-in functionality with project-specific session management capabilities.</p>"},{"location":"development/claude_code/claude-sessions/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Start a new session (with optional name)\n/project:session-start authentication-refactor\n# Or without a name\n/project:session-start\n\n# Update progress during development (with optional notes)\n/project:session-update Implemented OAuth with Google\n# Or without notes (auto-summarizes recent activity)\n/project:session-update\n\n# End session with comprehensive summary\n/project:session-end\n\n# View current session status\n/project:session-current\n\n# List all past sessions\n/project:session-list\n</code></pre>"},{"location":"development/claude_code/claude-sessions/#file-structure","title":"\ud83d\udcc1 File Structure","text":"<pre><code>commands/                       # Custom command directory\n\u251c\u2500\u2500 session-start.md           # Command for starting a new session\n\u251c\u2500\u2500 session-update.md          # Command for updating current session\n\u251c\u2500\u2500 session-end.md             # Command for ending and summarizing\n\u251c\u2500\u2500 session-current.md         # Command for viewing current status\n\u251c\u2500\u2500 session-list.md            # Command for listing all sessions\n\u2514\u2500\u2500 session-help.md            # Command for showing help\n\nsessions/                      # Session storage directory\n\u251c\u2500\u2500 .current-session          # Tracks the active session filename\n\u251c\u2500\u2500 2025-01-16-1347.md       # Example session file\n\u2514\u2500\u2500 [YYYY-MM-DD-HHMM-name].md  # Session naming format\n</code></pre>"},{"location":"development/claude_code/claude-sessions/#installation","title":"\ud83d\udee0\ufe0f Installation","text":"<ol> <li> <p>Clone this repository or copy the folders to your project:    <pre><code>git clone git@github.com:iannuttall/claude-sessions.git\n# Or copy the commands and sessions folders to your project root\n</code></pre></p> </li> <li> <p>Create the sessions tracking file:    <pre><code>mkdir -p sessions\ntouch sessions/.current-session\n</code></pre></p> </li> <li> <p>Add to <code>.gitignore</code> if you don't want to track sessions:    <pre><code>sessions/\n</code></pre></p> </li> </ol>"},{"location":"development/claude_code/claude-sessions/#how-it-works","title":"\ud83d\udcdd How It Works","text":"<p>This system provides custom slash commands inspired by Claude Code's custom slash commands feature. While Claude Code typically looks for commands in <code>.claude/commands/</code>, this repository provides a standalone implementation with commands in the <code>commands/</code> directory.</p> <ul> <li>Prefix: All commands use the <code>/project:</code> prefix (for project-specific commands)</li> <li>Arguments: Commands support arguments using the <code>$ARGUMENTS</code> placeholder</li> <li>Execution: Claude reads the command file and executes the instructions within</li> <li>Note: These commands are designed to work with Claude but can be adapted for other AI coding assistants</li> </ul>"},{"location":"development/claude_code/claude-sessions/#command-reference","title":"\ud83d\udccb Command Reference","text":""},{"location":"development/claude_code/claude-sessions/#projectsession-start-name","title":"<code>/project:session-start [name]</code>","text":"<p>Starts a new development session with an optional descriptive name.</p> <p>Parameters: - <code>[name]</code> (optional) - A descriptive name for the session. If omitted, creates a session with just the timestamp.</p> <p>What it does: - Creates a new markdown file with timestamp (format: <code>YYYY-MM-DD-HHMM.md</code> or <code>YYYY-MM-DD-HHMM-name.md</code>) - Sets up session structure with goals and progress sections - Updates <code>.current-session</code> to track active session - Prompts for session goals if not clear from context</p> <p>Examples: <pre><code># With a descriptive name\n/project:session-start refactor-auth-system\n\n# Without a name (just timestamp)\n/project:session-start\n</code></pre></p>"},{"location":"development/claude_code/claude-sessions/#projectsession-update-notes","title":"<code>/project:session-update [notes]</code>","text":"<p>Adds timestamped updates to the current session.</p> <p>Parameters: - <code>[notes]</code> (optional) - Custom notes about the update. If omitted, automatically summarizes recent activities.</p> <p>What it does: - Appends progress notes with timestamp - Captures git status and changes - Tracks todo list progress - Documents issues and solutions - Records implementation details - Auto-generates summary if no notes provided</p> <p>Examples: <pre><code># With custom notes\n/project:session-update Fixed Next.js 15 params Promise issue\n\n# Without notes (auto-summarizes)\n/project:session-update\n</code></pre></p>"},{"location":"development/claude_code/claude-sessions/#projectsession-end","title":"<code>/project:session-end</code>","text":"<p>Ends the current session with a comprehensive summary.</p> <p>What it does: - Generates complete session summary including:   - Duration and timing   - Git changes summary   - Todo items completed/remaining   - Key accomplishments   - Problems and solutions   - Dependencies and configuration changes   - Lessons learned   - Tips for future developers - Clears <code>.current-session</code> file</p>"},{"location":"development/claude_code/claude-sessions/#projectsession-current","title":"<code>/project:session-current</code>","text":"<p>Shows the status of the current active session.</p> <p>What it does: - Displays session name and duration - Shows recent updates - Lists current goals and tasks - Reminds of available commands</p>"},{"location":"development/claude_code/claude-sessions/#projectsession-list","title":"<code>/project:session-list</code>","text":"<p>Lists all session files with summaries.</p> <p>What it does: - Shows all session files sorted by date - Displays session titles and timestamps - Highlights currently active session - Shows brief overview of each session</p>"},{"location":"development/claude_code/claude-sessions/#projectsession-help","title":"<code>/project:session-help</code>","text":"<p>Displays help information about the session system.</p>"},{"location":"development/claude_code/claude-sessions/#best-practices-for-claude-code","title":"\ud83c\udfaf Best Practices for Claude Code","text":""},{"location":"development/claude_code/claude-sessions/#command-usage","title":"Command Usage","text":"<ul> <li>These commands work only within Claude Code interactive sessions</li> <li>Commands are project-specific and available to all team members</li> <li>Arguments are passed directly after the command name</li> </ul>"},{"location":"development/claude_code/claude-sessions/#session-management","title":"Session Management","text":"<ul> <li>Sessions help Claude maintain context across conversations</li> <li>Review past sessions before starting related work</li> <li>Session files serve as documentation for your development process</li> </ul>"},{"location":"development/claude_code/claude-sessions/#customization","title":"\ud83d\udd27 Customization","text":""},{"location":"development/claude_code/claude-sessions/#adapting-for-standard-claude-code-setup","title":"Adapting for Standard Claude Code Setup","text":"<p>If you want to use these with Claude Code's standard directory structure: 1. Copy the <code>commands</code> folder to <code>.claude/commands/</code> in your project 2. Update paths in command files from <code>sessions/</code> to <code>.claude/sessions/</code></p>"},{"location":"development/claude_code/claude-sessions/#creating-your-own-commands","title":"Creating Your Own Commands","text":"<ul> <li>Modify command files to change behavior</li> <li>Create additional session-related commands</li> <li>Organize commands in subdirectories for namespacing (e.g., <code>/project:session:feature:start</code>)</li> <li>Create personal versions in <code>~/.claude/commands/</code> with <code>/user:</code> prefix</li> </ul>"},{"location":"development/claude_code/claude-sessions/#references","title":"\ud83d\udcda References","text":"<ul> <li>Claude Code Slash Commands Documentation</li> <li>Claude Code Memory Management</li> <li>Claude Code Overview</li> </ul>"},{"location":"development/claude_code/claude-sessions/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"development/claude_code/claude-sessions/#starting-sessions","title":"Starting Sessions","text":"<ul> <li>Use descriptive names that indicate the main focus</li> <li>Start sessions for significant features or bug fixes</li> <li>Define clear goals at the beginning</li> </ul>"},{"location":"development/claude_code/claude-sessions/#during-development","title":"During Development","text":"<ul> <li>Update regularly when completing significant tasks</li> <li>Document unexpected issues and their solutions</li> <li>Track breaking changes or important discoveries</li> <li>Note any dependencies added or configuration changes</li> </ul>"},{"location":"development/claude_code/claude-sessions/#ending-sessions","title":"Ending Sessions","text":"<ul> <li>Always end sessions with <code>/project:session-end</code></li> <li>Review the generated summary for completeness</li> <li>Add any missing context before closing</li> </ul>"},{"location":"development/claude_code/claude-sessions/#knowledge-transfer","title":"Knowledge Transfer","text":"<ul> <li>Review relevant past sessions before starting similar work</li> <li>Reference session files in commit messages for context</li> <li>Use session summaries for standup updates or reports</li> </ul>"},{"location":"development/claude_code/claude-sessions/#use-cases","title":"\ud83d\udca1 Use Cases","text":""},{"location":"development/claude_code/claude-sessions/#1-feature-development","title":"1. Feature Development","text":"<pre><code>/project:session-start user-authentication\n# Implement auth logic\n/project:session-update Added middleware and login page\n# Fix issues\n/project:session-update Resolved Next.js 15 async cookie issue\n/project:session-end\n</code></pre>"},{"location":"development/claude_code/claude-sessions/#2-bug-fixing","title":"2. Bug Fixing","text":"<pre><code>/project:session-start fix-email-bounce-handling\n# Investigate issue\n/project:session-update Found AWS SNS webhook misconfiguration\n# Implement fix\n/project:session-update Updated webhook handler and added logging\n/project:session-end\n</code></pre>"},{"location":"development/claude_code/claude-sessions/#3-refactoring","title":"3. Refactoring","text":"<pre><code>/project:session-start database-service-refactor\n# Plan refactoring\n/project:session-update Created new DB service class architecture\n# Execute changes\n/project:session-update Migrated all queries to new service\n/project:session-end\n</code></pre>"},{"location":"development/claude_code/claude-sessions/#benefits-for-ai-agents","title":"\ud83e\udd16 Benefits for AI Agents","text":"<ol> <li>Context Preservation: Sessions provide rich context about past work</li> <li>Decision History: Understand why certain approaches were taken</li> <li>Issue Awareness: Know about problems already encountered and solved</li> <li>Code Evolution: Track how the codebase has changed over time</li> <li>Dependency Tracking: Awareness of what packages and tools are used</li> </ol>"},{"location":"development/claude_code/claude-sessions/#tips-and-tricks","title":"\ud83d\udd0d Tips and Tricks","text":"<ol> <li>Searchable Sessions: Use consistent terminology in updates for easy searching</li> <li>Link Issues: Reference ticket numbers or GitHub issues in updates</li> <li>Code Snippets: Include important code changes in session updates</li> <li>Screenshots: Reference screenshot paths for UI changes</li> <li>Testing Notes: Document test scenarios and results</li> </ol>"},{"location":"development/claude_code/claude-sessions/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"development/claude_code/claude-sessions/#customizing-commands","title":"Customizing Commands","text":"<p>Edit the command files in <code>commands/</code> to: - Change session file format - Add custom sections - Modify summary generation - Adjust git tracking details</p>"},{"location":"development/claude_code/claude-sessions/#session-storage","title":"Session Storage","text":"<ul> <li>Default: <code>sessions/</code></li> <li>Can be changed by updating command files</li> <li>Consider version control needs</li> </ul>"},{"location":"development/claude_code/claude-sessions/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":"<p>No active session found - Start a new session with <code>/project:session-start</code> - Check <code>sessions/.current-session</code> exists</p> <p>Session updates not working - Ensure a session is active - Check file permissions in <code>sessions/</code></p> <p>Missing git information - Verify you're in a git repository - Check git is properly initialized</p>"},{"location":"development/claude_code/claude-sessions/#examples","title":"\ud83d\udcda Examples","text":""},{"location":"development/claude_code/claude-sessions/#complete-feature-implementation-session","title":"Complete Feature Implementation Session","text":"<pre><code># Development Session - 2025-01-16 13:47 - campaign-editor\n\n## Goals\n- [x] Create dedicated campaign editor\n- [x] Add markdown support\n- [x] Implement auto-save\n\n## Progress\n[Multiple detailed updates documenting the implementation]\n\n## Session Summary\nSuccessfully implemented a full-featured campaign editor with markdown support,\nlive preview, and auto-save functionality. Resolved Next.js 15 compatibility\nissues and added proper error handling.\n</code></pre>"},{"location":"development/claude_code/claude-sessions/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>To improve this system: 1. Enhance command instructions for better AI comprehension 2. Add new commands for specific workflows 3. Improve session file formatting 4. Create utilities for session analysis</p>"},{"location":"development/claude_code/claude-sessions/#license","title":"\ud83d\udcc4 License","text":"<p>This session management system is open source and available for use in any project.</p>"},{"location":"development/claude_code/claude-sessions/#credits","title":"Credits","text":"<p>https://github.com/iannuttall/claude-sessions</p> <p>https://www.youtube.com/watch?v=higAxJk_zig</p> <p>Remember: Good documentation today saves hours of debugging tomorrow!</p>"},{"location":"development/fastmcp/","title":"FastMCP Documentation","text":"<p>FastMCP is a high-level, ergonomic Python framework for building MCP (Model Context Protocol) servers. This documentation provides comprehensive guides for building production-ready servers.</p>"},{"location":"development/fastmcp/#documentation-status","title":"Documentation Status","text":""},{"location":"development/fastmcp/#completed-documentation","title":"\u2705 Completed Documentation","text":"<ul> <li> Overview - Introduction and quick start guide</li> <li> Transport Protocols - stdio, streamable-http, SSE comparison and usage</li> <li> Tools - Function registration, validation, context injection</li> <li> Resources - Static and template resources for data exposure</li> <li> Prompts - Message templates and conversation builders</li> <li> Context - Logging, progress reporting, resource access</li> <li> Configuration - Settings and environment variables</li> <li> Authentication - OAuth2 setup for HTTP transports</li> <li> Examples - Complete server implementations and best practices</li> </ul>"},{"location":"development/fastmcp/#missing-core-documentation","title":"\u274c Missing Core Documentation","text":""},{"location":"development/fastmcp/#essential-missing","title":"Essential Missing","text":"<ul> <li> API Reference - Complete class/method documentation with signatures</li> <li> Development Workflow - Local development, testing, debugging</li> <li> Troubleshooting - Common issues, error diagnosis, solutions</li> </ul>"},{"location":"development/fastmcp/#important-missing","title":"Important Missing","text":"<ul> <li> Testing Guide - Unit testing, integration testing, mocking strategies</li> <li> Performance Guide - Optimization, benchmarking, scaling</li> <li> Security Guide - Security best practices beyond OAuth</li> <li> Monitoring &amp; Observability - Logging, metrics, health checks, alerting</li> <li> Migration Guide - From raw MCP SDK to FastMCP</li> </ul>"},{"location":"development/fastmcp/#advanced-topics-missing","title":"Advanced Topics Missing","text":"<ul> <li> Custom Middleware - Request/response middleware patterns and auth customization</li> <li> Session Management - Advanced session handling for StreamableHTTP</li> <li> Event Store Integration - Detailed event store usage and patterns</li> <li> Lifespan Management - Advanced server lifecycle patterns</li> <li> Custom Routes - HTTP endpoint customization beyond examples</li> </ul>"},{"location":"development/fastmcp/#developer-experience-missing","title":"Developer Experience Missing","text":"<ul> <li> Debugging - Debugging FastMCP servers effectively</li> </ul>"},{"location":"development/fastmcp/#production-missing","title":"Production Missing","text":"<ul> <li> Graceful Shutdown - Proper server termination</li> </ul>"},{"location":"development/fastmcp/#reference-missing","title":"Reference Missing","text":"<ul> <li> FAQ - Common questions and answers</li> <li> Limitations - What FastMCP can't do, known issues</li> <li> Comparison - FastMCP vs raw MCP SDK vs other frameworks</li> </ul>"},{"location":"development/fastmcp/#integration-missing","title":"Integration Missing","text":"<ul> <li> Database Integration - ORM patterns, connection pooling</li> <li> Message Queues - Redis, RabbitMQ, async task patterns</li> <li> External APIs - HTTP clients, rate limiting, caching</li> <li> File Systems - File handling, cloud storage integration</li> </ul>"},{"location":"development/fastmcp/#specialized-missing","title":"Specialized Missing","text":"<ul> <li> WebSocket Support - If FastMCP supports WebSocket transport</li> <li> Streaming Responses - Large data streaming patterns</li> <li> Binary Data - Handling images, files, binary content</li> <li> Rate Limiting - Request throttling implementation</li> </ul>"},{"location":"development/fastmcp/#getting-started","title":"Getting Started","text":"<ol> <li>Start with the Overview for a general introduction</li> <li>Follow the Examples for practical implementations</li> <li>Read Tools, Resources, and Prompts for core concepts</li> <li>Configure your server with Configuration</li> <li>Choose your transport in Transport Protocols</li> </ol>"},{"location":"development/fastmcp/#documentation-guidelines","title":"Documentation Guidelines","text":"<p>When creating new documentation:</p> <ol> <li>Read this README.md first to understand the current state</li> <li>Follow existing patterns from completed documentation</li> <li>Include practical examples with working code</li> <li>Add error handling and security considerations</li> <li>Update this checklist when documentation is complete</li> </ol>"},{"location":"development/fastmcp/#file-naming-convention","title":"File Naming Convention","text":"<ul> <li>Use lowercase with hyphens: <code>api-reference.md</code></li> <li>Keep names descriptive but concise</li> <li>Update the checklist with the exact filename and link</li> </ul>"},{"location":"development/fastmcp/#content-structure","title":"Content Structure","text":"<p>Each documentation file should include:</p> <ol> <li>Clear title and overview</li> <li>Table of contents for longer documents</li> <li>Practical examples with complete, working code</li> <li>Best practices section</li> <li>Error handling and troubleshooting tips</li> <li>Links to related documentation</li> </ol>"},{"location":"development/fastmcp/#priority-order","title":"Priority Order","text":"<p>Complete documentation in this suggested order:</p> <ol> <li>API Reference (Essential)</li> <li>Development Workflow (Essential)</li> <li>Troubleshooting (Essential)</li> <li>Testing Guide (Important)</li> <li>Performance Guide (Important)</li> <li>Security Guide (Important)</li> <li>Monitoring &amp; Observability (Important)</li> <li>Migration Guide (Important)</li> </ol>"},{"location":"development/fastmcp/#contributing","title":"Contributing","text":"<p>When adding new documentation:</p> <ol> <li>Create the markdown file in this directory</li> <li>Add practical, tested examples</li> <li>Update this README.md checklist</li> <li>Follow the existing documentation style</li> <li>Include cross-references to related topics</li> </ol>"},{"location":"development/fastmcp/api-reference/","title":"API Reference","text":"<p>Complete reference for all FastMCP classes, methods, and utilities.</p>"},{"location":"development/fastmcp/api-reference/#core-classes","title":"Core Classes","text":""},{"location":"development/fastmcp/api-reference/#fastmcp","title":"FastMCP","text":"<p>The main server class for creating MCP servers.</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\n    name=\"My Server\",\n    instructions=\"Optional server instructions\",\n    debug=True,\n    log_level=\"INFO\"\n)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#constructor-parameters","title":"Constructor Parameters","text":"<ul> <li>name (<code>str | None</code>): Server name (defaults to \"FastMCP\")</li> <li>instructions (<code>str | None</code>): Optional instructions for clients</li> <li>auth_server_provider (<code>OAuthAuthorizationServerProvider | None</code>): OAuth provider for authentication</li> <li>event_store (<code>EventStore | None</code>): Event store for StreamableHTTP sessions</li> <li>tools (<code>list[Tool] | None</code>): Pre-defined tools to register</li> <li>settings: Additional settings passed as keyword arguments</li> </ul>"},{"location":"development/fastmcp/api-reference/#configuration-settings","title":"Configuration Settings","text":"<p>All settings can be configured via environment variables with <code>FASTMCP_</code> prefix:</p> <pre><code># Python configuration\nmcp = FastMCP(\n    debug=True,\n    log_level=\"DEBUG\",\n    host=\"0.0.0.0\",\n    port=8080,\n    warn_on_duplicate_tools=False\n)\n\n# Environment variable configuration\n# FASTMCP_DEBUG=true\n# FASTMCP_LOG_LEVEL=DEBUG\n# FASTMCP_HOST=0.0.0.0\n# FASTMCP_PORT=8080\n</code></pre> <p>Server Settings: - <code>debug</code> (<code>bool</code>): Enable debug mode (default: <code>False</code>) - <code>log_level</code> (<code>str</code>): Logging level - DEBUG, INFO, WARNING, ERROR, CRITICAL (default: <code>\"INFO\"</code>)</p> <p>HTTP Settings: - <code>host</code> (<code>str</code>): Server host (default: <code>\"127.0.0.1\"</code>) - <code>port</code> (<code>int</code>): Server port (default: <code>8000</code>) - <code>mount_path</code> (<code>str</code>): Mount path for routes (default: <code>\"/\"</code>) - <code>sse_path</code> (<code>str</code>): SSE endpoint path (default: <code>\"/sse\"</code>) - <code>message_path</code> (<code>str</code>): Message endpoint path (default: <code>\"/messages/\"</code>) - <code>streamable_http_path</code> (<code>str</code>): StreamableHTTP path (default: <code>\"/mcp\"</code>)</p> <p>StreamableHTTP Settings: - <code>json_response</code> (<code>bool</code>): Use JSON responses (default: <code>False</code>) - <code>stateless_http</code> (<code>bool</code>): Use stateless mode (default: <code>False</code>)</p> <p>Warning Settings: - <code>warn_on_duplicate_resources</code> (<code>bool</code>): Warn on duplicate resources (default: <code>True</code>) - <code>warn_on_duplicate_tools</code> (<code>bool</code>): Warn on duplicate tools (default: <code>True</code>) - <code>warn_on_duplicate_prompts</code> (<code>bool</code>): Warn on duplicate prompts (default: <code>True</code>)</p> <p>Advanced Settings: - <code>dependencies</code> (<code>list[str]</code>): List of dependencies to install - <code>lifespan</code> (<code>Callable</code>): Custom lifespan context manager - <code>auth</code> (<code>AuthSettings | None</code>): Authentication configuration</p>"},{"location":"development/fastmcp/api-reference/#properties","title":"Properties","text":"<p><code>name</code> (<code>str</code>): Server name <pre><code>print(mcp.name)  # \"My Server\"\n</code></pre></p> <p><code>instructions</code> (<code>str | None</code>): Server instructions <pre><code>print(mcp.instructions)  # \"Optional server instructions\"\n</code></pre></p> <p><code>session_manager</code> (<code>StreamableHTTPSessionManager</code>): Session manager for StreamableHTTP <pre><code># Only available after calling streamable_http_app()\nmanager = mcp.session_manager\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#running-the-server","title":"Running the Server","text":"<p><code>run(transport, mount_path=None)</code>: Run the server synchronously <pre><code># Run with stdio (most common for MCP)\nmcp.run(\"stdio\")\n\n# Run with SSE transport\nmcp.run(\"sse\")\n\n# Run with StreamableHTTP transport\nmcp.run(\"streamable-http\")\n</code></pre></p> <p>Async methods: - <code>run_stdio_async()</code>: Run with stdio transport - <code>run_sse_async(mount_path=None)</code>: Run with SSE transport - <code>run_streamable_http_async()</code>: Run with StreamableHTTP transport</p>"},{"location":"development/fastmcp/api-reference/#tool-management","title":"Tool Management","text":"<p><code>@tool(name=None, description=None, annotations=None)</code>: Decorator to register tools <pre><code>@mcp.tool()\ndef simple_tool(x: int) -&gt; str:\n    \"\"\"Convert number to string.\"\"\"\n    return str(x)\n\n@mcp.tool(name=\"custom_name\", description=\"Custom description\")\ndef named_tool(x: int) -&gt; str:\n    return str(x)\n</code></pre></p> <p><code>add_tool(fn, name=None, description=None, annotations=None)</code>: Programmatically register tools <pre><code>def my_function(x: int) -&gt; str:\n    return str(x)\n\nmcp.add_tool(my_function, name=\"convert\", description=\"Convert to string\")\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#resource-management","title":"Resource Management","text":"<p><code>@resource(uri, name=None, description=None, mime_type=None)</code>: Decorator to register resources <pre><code>@mcp.resource(\"data://example\")\ndef get_data() -&gt; str:\n    \"\"\"Static resource.\"\"\"\n    return \"Hello World\"\n\n@mcp.resource(\"data://{param}\")\ndef get_dynamic_data(param: str) -&gt; str:\n    \"\"\"Template resource with parameter.\"\"\"\n    return f\"Data for {param}\"\n</code></pre></p> <p><code>add_resource(resource)</code>: Add a Resource instance <pre><code>from mcp.server.fastmcp.resources import FunctionResource\n\nresource = FunctionResource.from_function(\n    fn=get_data,\n    uri=\"data://example\",\n    name=\"Example Data\"\n)\nmcp.add_resource(resource)\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#prompt-management","title":"Prompt Management","text":"<p><code>@prompt(name=None, description=None)</code>: Decorator to register prompts <pre><code>@mcp.prompt()\ndef analyze_data(table_name: str) -&gt; list[Message]:\n    \"\"\"Generate analysis prompt.\"\"\"\n    return [\n        UserMessage(f\"Analyze table: {table_name}\")\n    ]\n</code></pre></p> <p><code>add_prompt(prompt)</code>: Add a Prompt instance <pre><code>from mcp.server.fastmcp.prompts import Prompt\n\nprompt = Prompt.from_function(analyze_data, name=\"analyzer\")\nmcp.add_prompt(prompt)\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#custom-routes","title":"Custom Routes","text":"<p><code>@custom_route(path, methods, name=None, include_in_schema=True)</code>: Register custom HTTP endpoints <pre><code>from starlette.requests import Request\nfrom starlette.responses import JSONResponse\n\n@mcp.custom_route(\"/health\", methods=[\"GET\"])\nasync def health_check(request: Request) -&gt; JSONResponse:\n    return JSONResponse({\"status\": \"ok\"})\n\n@mcp.custom_route(\"/oauth/callback\", methods=[\"POST\"])\nasync def oauth_callback(request: Request) -&gt; JSONResponse:\n    # Handle OAuth callback\n    return JSONResponse({\"success\": True})\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#app-generation","title":"App Generation","text":"<p><code>sse_app(mount_path=None)</code>: Get Starlette app for SSE transport <pre><code>app = mcp.sse_app()\n# Can be used with ASGI servers like uvicorn directly\n</code></pre></p> <p><code>streamable_http_app()</code>: Get Starlette app for StreamableHTTP transport <pre><code>app = mcp.streamable_http_app()\n# Can be mounted in larger applications\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#protocol-handlers","title":"Protocol Handlers","text":"<p>Internal methods (typically not called directly):</p> <ul> <li><code>list_tools()</code>: List available tools</li> <li><code>call_tool(name, arguments)</code>: Call a tool</li> <li><code>list_resources()</code>: List available resources</li> <li><code>read_resource(uri)</code>: Read a resource</li> <li><code>list_prompts()</code>: List available prompts</li> <li><code>get_prompt(name, arguments)</code>: Get a prompt</li> <li><code>list_resource_templates()</code>: List resource templates</li> </ul>"},{"location":"development/fastmcp/api-reference/#context","title":"Context","text":"<p>Context object providing access to MCP capabilities within tools and resources.</p> <pre><code>from mcp.server.fastmcp import Context\n\n@mcp.tool()\ndef my_tool(x: int, ctx: Context) -&gt; str:\n    # Context is automatically injected\n    ctx.info(f\"Processing {x}\")\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#logging-methods","title":"Logging Methods","text":"<p><code>log(level, message, logger_name=None)</code>: Send log message to client <pre><code>await ctx.log(\"info\", \"Processing data\")\nawait ctx.log(\"error\", \"Something went wrong\", logger_name=\"my_tool\")\n</code></pre></p> <p>Convenience methods: <pre><code>await ctx.debug(\"Debug message\")\nawait ctx.info(\"Info message\")\nawait ctx.warning(\"Warning message\")\nawait ctx.error(\"Error message\")\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#progress-reporting","title":"Progress Reporting","text":"<p><code>report_progress(progress, total=None, message=None)</code>: Report operation progress <pre><code>await ctx.report_progress(50, 100, \"Halfway done\")\nawait ctx.report_progress(75, 100)  # No message\nawait ctx.report_progress(25)  # No total\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#resource-access","title":"Resource Access","text":"<p><code>read_resource(uri)</code>: Read a resource from within a tool <pre><code>content = await ctx.read_resource(\"file://data.txt\")\nfor item in content:\n    print(item.content, item.mime_type)\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#properties_1","title":"Properties","text":"<p><code>request_id</code> (<code>str</code>): Unique request identifier <pre><code>request_id = ctx.request_id\n</code></pre></p> <p><code>client_id</code> (<code>str | None</code>): Client identifier (if available) <pre><code>if ctx.client_id:\n    print(f\"Request from client: {ctx.client_id}\")\n</code></pre></p> <p><code>session</code>: Access to underlying session (advanced usage) <pre><code>session = ctx.session\n</code></pre></p> <p><code>fastmcp</code> (<code>FastMCP</code>): Access to the FastMCP server instance <pre><code>server = ctx.fastmcp\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#tool-components","title":"Tool Components","text":""},{"location":"development/fastmcp/api-reference/#tool","title":"Tool","text":"<p>Internal tool registration class (typically not used directly).</p> <pre><code>from mcp.server.fastmcp.tools import Tool\n\ntool = Tool.from_function(\n    fn=my_function,\n    name=\"custom_name\",\n    description=\"Tool description\",\n    annotations={\"dangerous\": True}\n)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#properties_2","title":"Properties","text":"<ul> <li><code>name</code> (<code>str</code>): Tool name</li> <li><code>description</code> (<code>str</code>): Tool description</li> <li><code>parameters</code> (<code>dict</code>): JSON schema for parameters</li> <li><code>fn_metadata</code> (<code>FuncMetadata</code>): Function metadata</li> <li><code>is_async</code> (<code>bool</code>): Whether tool is async</li> <li><code>context_kwarg</code> (<code>str | None</code>): Context parameter name</li> <li><code>annotations</code> (<code>ToolAnnotations | None</code>): Tool annotations</li> </ul>"},{"location":"development/fastmcp/api-reference/#methods","title":"Methods","text":"<p><code>run(arguments, context=None)</code>: Execute the tool <pre><code>result = await tool.run({\"x\": 5}, context=ctx)\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#resource-components","title":"Resource Components","text":""},{"location":"development/fastmcp/api-reference/#resource","title":"Resource","text":"<p>Base class for all resources.</p> <pre><code>from mcp.server.fastmcp.resources import Resource\n\nclass CustomResource(Resource):\n    async def read(self) -&gt; str | bytes:\n        return \"Custom content\"\n\nresource = CustomResource(\n    uri=\"custom://example\",\n    name=\"Example\",\n    description=\"Custom resource\",\n    mime_type=\"text/plain\"\n)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#properties_3","title":"Properties","text":"<ul> <li><code>uri</code> (<code>AnyUrl</code>): Resource URI</li> <li><code>name</code> (<code>str</code>): Resource name</li> <li><code>description</code> (<code>str | None</code>): Resource description</li> <li><code>mime_type</code> (<code>str</code>): MIME type (default: \"text/plain\")</li> </ul>"},{"location":"development/fastmcp/api-reference/#methods_1","title":"Methods","text":"<p><code>read()</code>: Abstract method to read resource content <pre><code>content = await resource.read()  # Returns str or bytes\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#functionresource","title":"FunctionResource","text":"<p>Resource implementation using a function.</p> <pre><code>from mcp.server.fastmcp.resources import FunctionResource\n\ndef get_data() -&gt; str:\n    return \"Hello World\"\n\nresource = FunctionResource.from_function(\n    fn=get_data,\n    uri=\"data://example\",\n    name=\"Example Data\",\n    description=\"Example resource\",\n    mime_type=\"text/plain\"\n)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#prompt-components","title":"Prompt Components","text":""},{"location":"development/fastmcp/api-reference/#prompt","title":"Prompt","text":"<p>Prompt template class.</p> <pre><code>from mcp.server.fastmcp.prompts import Prompt\n\ndef create_prompt(topic: str) -&gt; list[Message]:\n    return [UserMessage(f\"Tell me about {topic}\")]\n\nprompt = Prompt.from_function(\n    fn=create_prompt,\n    name=\"topic_prompt\",\n    description=\"Generate topic prompt\"\n)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#properties_4","title":"Properties","text":"<ul> <li><code>name</code> (<code>str</code>): Prompt name</li> <li><code>description</code> (<code>str | None</code>): Prompt description</li> <li><code>arguments</code> (<code>list[PromptArgument] | None</code>): Available arguments</li> </ul>"},{"location":"development/fastmcp/api-reference/#methods_2","title":"Methods","text":"<p><code>render(arguments=None)</code>: Render prompt with arguments <pre><code>messages = await prompt.render({\"topic\": \"Python\"})\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#message-classes","title":"Message Classes","text":"<p>Message types for prompt responses.</p> <pre><code>from mcp.server.fastmcp.prompts import Message, UserMessage, AssistantMessage\n\n# Create messages\nuser_msg = UserMessage(\"Hello\")\nassistant_msg = AssistantMessage(\"Hi there!\")\n\n# Generic message\nmsg = Message(role=\"user\", content=\"Hello\")\n</code></pre>"},{"location":"development/fastmcp/api-reference/#promptargument","title":"PromptArgument","text":"<p>Represents a prompt argument.</p> <pre><code>from mcp.server.fastmcp.prompts import PromptArgument\n\narg = PromptArgument(\n    name=\"topic\",\n    description=\"Topic to discuss\",\n    required=True\n)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#utility-types","title":"Utility Types","text":""},{"location":"development/fastmcp/api-reference/#image","title":"Image","text":"<p>Utility class for handling images in tools.</p> <pre><code>from mcp.server.fastmcp import Image\n\n@mcp.tool()\ndef process_image(image_path: str) -&gt; Image:\n    \"\"\"Return an image.\"\"\"\n    return Image.from_path(image_path)\n\n@mcp.tool()\ndef create_image() -&gt; Image:\n    \"\"\"Create image from bytes.\"\"\"\n    image_data = b\"...\"  # Your image bytes\n    return Image.from_bytes(image_data, \"image/png\")\n</code></pre>"},{"location":"development/fastmcp/api-reference/#class-methods","title":"Class Methods","text":"<p><code>from_path(path)</code>: Create image from file path <pre><code>image = Image.from_path(\"/path/to/image.png\")\n</code></pre></p> <p><code>from_bytes(data, mime_type)</code>: Create image from bytes <pre><code>image = Image.from_bytes(image_bytes, \"image/jpeg\")\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#methods_3","title":"Methods","text":"<p><code>to_image_content()</code>: Convert to ImageContent for MCP <pre><code>content = image.to_image_content()\n</code></pre></p>"},{"location":"development/fastmcp/api-reference/#exception-classes","title":"Exception Classes","text":""},{"location":"development/fastmcp/api-reference/#resourceerror","title":"ResourceError","text":"<p>Raised when resource operations fail.</p> <pre><code>from mcp.server.fastmcp.exceptions import ResourceError\n\nraise ResourceError(\"Could not read resource\")\n</code></pre>"},{"location":"development/fastmcp/api-reference/#toolerror","title":"ToolError","text":"<p>Raised when tool execution fails.</p> <pre><code>from mcp.server.fastmcp.exceptions import ToolError\n\nraise ToolError(\"Tool execution failed\")\n</code></pre>"},{"location":"development/fastmcp/api-reference/#type-annotations","title":"Type Annotations","text":""},{"location":"development/fastmcp/api-reference/#toolannotations","title":"ToolAnnotations","text":"<p>Optional annotations for tools providing additional metadata.</p> <pre><code>from mcp.types import ToolAnnotations\n\n@mcp.tool(annotations=ToolAnnotations(dangerous=True))\ndef dangerous_tool() -&gt; str:\n    \"\"\"A dangerous operation.\"\"\"\n    return \"Done\"\n</code></pre>"},{"location":"development/fastmcp/api-reference/#advanced-usage","title":"Advanced Usage","text":""},{"location":"development/fastmcp/api-reference/#custom-lifespan","title":"Custom Lifespan","text":"<pre><code>from contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def custom_lifespan(app: FastMCP):\n    # Startup\n    print(\"Server starting...\")\n    yield\n    # Shutdown\n    print(\"Server stopping...\")\n\nmcp = FastMCP(\"My Server\", lifespan=custom_lifespan)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#authentication-setup","title":"Authentication Setup","text":"<pre><code>from mcp.server.auth.provider import OAuthAuthorizationServerProvider\nfrom mcp.server.auth.settings import AuthSettings\n\n# Set up OAuth provider\nprovider = OAuthAuthorizationServerProvider(...)\n\n# Configure auth settings\nauth_settings = AuthSettings(\n    issuer_url=\"https://example.com\",\n    required_scopes=[\"read\", \"write\"]\n)\n\nmcp = FastMCP(\n    \"Secure Server\",\n    auth_server_provider=provider,\n    auth=auth_settings\n)\n</code></pre>"},{"location":"development/fastmcp/api-reference/#session-management","title":"Session Management","text":"<pre><code># For StreamableHTTP\nmcp = FastMCP(\"My Server\")\napp = mcp.streamable_http_app()\n\n# Access session manager after app creation\nmanager = mcp.session_manager\n</code></pre>"},{"location":"development/fastmcp/api-reference/#integration-with-fastapi","title":"Integration with FastAPI","text":"<pre><code>from fastapi import FastAPI\nfrom starlette.middleware.cors import CORSMiddleware\n\n# Create FastMCP server\nmcp = FastMCP(\"My Server\")\n\n@mcp.tool()\ndef example_tool() -&gt; str:\n    return \"Hello\"\n\n# Create FastAPI app\napp = FastAPI()\n\n# Add CORS middleware\napp.add_middleware(CORSMiddleware, allow_origins=[\"*\"])\n\n# Mount FastMCP\nfastmcp_app = mcp.streamable_http_app()\napp.mount(\"/mcp\", fastmcp_app)\n\n# Add other FastAPI routes\n@app.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\"}\n</code></pre> <p>This reference covers all public APIs in FastMCP. For implementation details, see the Examples and other topic-specific documentation.</p>"},{"location":"development/fastmcp/authentication/","title":"Authentication","text":"<p>FastMCP supports OAuth2 authentication for HTTP transports (SSE and StreamableHTTP). This enables secure access control for servers exposed over the web.</p> <p>Note: Authentication is only available for HTTP transports. The stdio transport runs locally and doesn't require authentication.</p>"},{"location":"development/fastmcp/authentication/#overview","title":"Overview","text":"<p>FastMCP authentication provides:</p> <ul> <li>OAuth2 Authorization Server: Built-in OAuth2 server for issuing tokens</li> <li>Scope-based Access Control: Restrict access to specific capabilities</li> <li>Client Registration: Dynamic or pre-configured client registration</li> <li>Token Management: Access tokens, refresh tokens, and revocation</li> </ul>"},{"location":"development/fastmcp/authentication/#basic-setup","title":"Basic Setup","text":""},{"location":"development/fastmcp/authentication/#1-create-oauth-provider","title":"1. Create OAuth Provider","text":"<pre><code>from mcp.server.auth.provider import OAuthAuthorizationServerProvider\nfrom mcp.server.auth.settings import AuthSettings, ClientRegistrationOptions\nfrom mcp.server.fastmcp import FastMCP\n\nclass SimpleOAuthProvider(OAuthAuthorizationServerProvider):\n    \"\"\"Minimal OAuth provider implementation.\"\"\"\n\n    def __init__(self):\n        # In-memory storage (use database in production)\n        self.clients = {}\n        self.tokens = {}\n        self.auth_codes = {}\n\n    async def get_client(self, client_id: str):\n        \"\"\"Get client information.\"\"\"\n        return self.clients.get(client_id)\n\n    async def create_authorization_code(self, params):\n        \"\"\"Create authorization code for OAuth flow.\"\"\"\n        code = \"auth_code_\" + secrets.token_urlsafe(32)\n        self.auth_codes[code] = {\n            \"client_id\": params.client_id,\n            \"scope\": params.scope,\n            \"expires_at\": time.time() + 600  # 10 minutes\n        }\n        return code\n\n    async def exchange_code_for_tokens(self, code, client_id):\n        \"\"\"Exchange authorization code for access token.\"\"\"\n        if code not in self.auth_codes:\n            raise ValueError(\"Invalid authorization code\")\n\n        token = \"access_token_\" + secrets.token_urlsafe(32)\n        self.tokens[token] = {\n            \"client_id\": client_id,\n            \"scope\": self.auth_codes[code][\"scope\"],\n            \"expires_at\": time.time() + 3600  # 1 hour\n        }\n\n        del self.auth_codes[code]  # Code is single-use\n        return token\n\n    async def validate_token(self, token):\n        \"\"\"Validate access token.\"\"\"\n        token_data = self.tokens.get(token)\n        if not token_data:\n            return None\n\n        if time.time() &gt; token_data[\"expires_at\"]:\n            del self.tokens[token]  # Clean up expired token\n            return None\n\n        return token_data\n\n# Create provider and auth settings\noauth_provider = SimpleOAuthProvider()\n\nauth_settings = AuthSettings(\n    issuer_url=\"https://your-server.com\",\n    required_scopes=[\"read\", \"write\"],\n    client_registration_options=ClientRegistrationOptions(\n        enabled=True,\n        valid_scopes=[\"read\", \"write\", \"admin\"]\n    )\n)\n\n# Create authenticated server\nmcp = FastMCP(\n    \"Authenticated Server\",\n    auth_server_provider=oauth_provider,\n    auth=auth_settings\n)\n</code></pre>"},{"location":"development/fastmcp/authentication/#2-run-with-authentication","title":"2. Run with Authentication","text":"<pre><code>if __name__ == \"__main__\":\n    # Authentication only works with HTTP transports\n    mcp.run(\"streamable-http\")  # or \"sse\"\n</code></pre>"},{"location":"development/fastmcp/authentication/#configuration","title":"Configuration","text":""},{"location":"development/fastmcp/authentication/#auth-settings","title":"Auth Settings","text":"<pre><code>from mcp.server.auth.settings import (\n    AuthSettings,\n    ClientRegistrationOptions,\n    RevocationOptions\n)\n\nauth_settings = AuthSettings(\n    # Required: Your server's public URL\n    issuer_url=\"https://api.example.com\",\n\n    # Optional: Documentation URL for API consumers\n    service_documentation_url=\"https://docs.example.com\",\n\n    # Required scopes for all requests\n    required_scopes=[\"read\"],\n\n    # Client registration settings\n    client_registration_options=ClientRegistrationOptions(\n        enabled=True,\n        client_secret_expiry_seconds=86400,  # 24 hours\n        valid_scopes=[\"read\", \"write\", \"admin\"],\n        default_scopes=[\"read\"]\n    ),\n\n    # Token revocation settings\n    revocation_options=RevocationOptions(\n        enabled=True\n    )\n)\n</code></pre>"},{"location":"development/fastmcp/authentication/#environment-variables","title":"Environment Variables","text":"<pre><code># FastMCP auth settings\nFASTMCP_AUTH__ISSUER_URL=https://api.example.com\nFASTMCP_AUTH__REQUIRED_SCOPES=read,write\nFASTMCP_AUTH__SERVICE_DOCUMENTATION_URL=https://docs.example.com\n\n# Client registration\nFASTMCP_AUTH__CLIENT_REGISTRATION_OPTIONS__ENABLED=true\nFASTMCP_AUTH__CLIENT_REGISTRATION_OPTIONS__VALID_SCOPES=read,write,admin\n</code></pre>"},{"location":"development/fastmcp/authentication/#oauth2-flows","title":"OAuth2 Flows","text":""},{"location":"development/fastmcp/authentication/#authorization-code-flow","title":"Authorization Code Flow","text":"<p>The standard OAuth2 flow for web applications:</p> <pre><code>1. Client redirects user to /auth/authorize\n2. User authenticates and grants permission\n3. Server redirects back with authorization code\n4. Client exchanges code for access token at /auth/token\n5. Client uses access token in Authorization header\n</code></pre>"},{"location":"development/fastmcp/authentication/#client-registration","title":"Client Registration","text":"<p>Dynamic client registration for new applications:</p> <pre><code># Register new client\nPOST /auth/register\nContent-Type: application/json\n\n{\n    \"client_name\": \"My App\",\n    \"redirect_uris\": [\"https://myapp.com/callback\"],\n    \"scope\": \"read write\"\n}\n\n# Response\n{\n    \"client_id\": \"client_123\",\n    \"client_secret\": \"secret_abc\",\n    \"client_id_issued_at\": 1640995200,\n    \"client_secret_expires_at\": 1641081600\n}\n</code></pre>"},{"location":"development/fastmcp/authentication/#complete-example","title":"Complete Example","text":""},{"location":"development/fastmcp/authentication/#github-oauth-integration","title":"GitHub OAuth Integration","text":"<pre><code>import secrets\nimport time\nimport httpx\nfrom typing import Any\n\nfrom mcp.server.auth.provider import OAuthAuthorizationServerProvider\nfrom mcp.server.auth.settings import AuthSettings, ClientRegistrationOptions\nfrom mcp.server.fastmcp import FastMCP\n\nclass GitHubOAuthProvider(OAuthAuthorizationServerProvider):\n    \"\"\"OAuth provider that integrates with GitHub.\"\"\"\n\n    def __init__(self, github_client_id: str, github_client_secret: str):\n        self.github_client_id = github_client_id\n        self.github_client_secret = github_client_secret\n        self.clients = {}\n        self.tokens = {}\n        self.auth_codes = {}\n        self.github_tokens = {}  # Map MCP tokens to GitHub tokens\n\n    async def get_client(self, client_id: str):\n        \"\"\"Get registered client.\"\"\"\n        return self.clients.get(client_id)\n\n    async def register_client(self, registration_data: dict) -&gt; dict:\n        \"\"\"Register a new OAuth client.\"\"\"\n        client_id = f\"client_{secrets.token_urlsafe(16)}\"\n        client_secret = f\"secret_{secrets.token_urlsafe(32)}\"\n\n        self.clients[client_id] = {\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n            \"client_name\": registration_data.get(\"client_name\"),\n            \"redirect_uris\": registration_data.get(\"redirect_uris\", []),\n            \"scope\": registration_data.get(\"scope\", \"read\"),\n            \"created_at\": time.time()\n        }\n\n        return {\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n            \"client_id_issued_at\": int(time.time()),\n            \"client_secret_expires_at\": int(time.time() + 86400)  # 24 hours\n        }\n\n    async def create_authorization_code(self, params: Any) -&gt; str:\n        \"\"\"Create authorization code that will trigger GitHub OAuth.\"\"\"\n        code = f\"auth_{secrets.token_urlsafe(32)}\"\n\n        # Store the original request parameters\n        self.auth_codes[code] = {\n            \"client_id\": params.client_id,\n            \"scope\": params.scope,\n            \"redirect_uri\": params.redirect_uri,\n            \"created_at\": time.time(),\n            \"expires_at\": time.time() + 600  # 10 minutes\n        }\n\n        return code\n\n    async def exchange_code_for_tokens(self, code: str, client_id: str) -&gt; dict:\n        \"\"\"Exchange authorization code for access token.\"\"\"\n        if code not in self.auth_codes:\n            raise ValueError(\"Invalid or expired authorization code\")\n\n        auth_data = self.auth_codes[code]\n        if time.time() &gt; auth_data[\"expires_at\"]:\n            del self.auth_codes[code]\n            raise ValueError(\"Authorization code expired\")\n\n        # For this example, we'll create a token without actually going to GitHub\n        # In a real implementation, you'd exchange with GitHub here\n        access_token = f\"mcp_token_{secrets.token_urlsafe(32)}\"\n\n        self.tokens[access_token] = {\n            \"client_id\": client_id,\n            \"scope\": auth_data[\"scope\"],\n            \"token_type\": \"Bearer\",\n            \"expires_at\": time.time() + 3600,  # 1 hour\n            \"issued_at\": time.time()\n        }\n\n        # Clean up used code\n        del self.auth_codes[code]\n\n        return {\n            \"access_token\": access_token,\n            \"token_type\": \"Bearer\",\n            \"expires_in\": 3600,\n            \"scope\": auth_data[\"scope\"]\n        }\n\n    async def validate_token(self, token: str) -&gt; dict | None:\n        \"\"\"Validate access token.\"\"\"\n        token_data = self.tokens.get(token)\n        if not token_data:\n            return None\n\n        if time.time() &gt; token_data[\"expires_at\"]:\n            del self.tokens[token]\n            return None\n\n        return token_data\n\n    async def revoke_token(self, token: str) -&gt; bool:\n        \"\"\"Revoke access token.\"\"\"\n        if token in self.tokens:\n            del self.tokens[token]\n            return True\n        return False\n\n# Configuration\nGITHUB_CLIENT_ID = \"your_github_app_client_id\"\nGITHUB_CLIENT_SECRET = \"your_github_app_client_secret\"\n\n# Create OAuth provider\noauth_provider = GitHubOAuthProvider(GITHUB_CLIENT_ID, GITHUB_CLIENT_SECRET)\n\n# Create auth settings\nauth_settings = AuthSettings(\n    issuer_url=\"https://your-server.com\",\n    service_documentation_url=\"https://docs.your-server.com\",\n    required_scopes=[\"user\"],\n    client_registration_options=ClientRegistrationOptions(\n        enabled=True,\n        valid_scopes=[\"user\", \"repo\", \"admin\"],\n        default_scopes=[\"user\"]\n    )\n)\n\n# Create authenticated server\nmcp = FastMCP(\n    \"GitHub MCP Server\",\n    auth_server_provider=oauth_provider,\n    auth=auth_settings,\n    host=\"0.0.0.0\",\n    port=8000\n)\n\n# Add protected tools\n@mcp.tool()\ndef get_user_info(ctx) -&gt; dict:\n    \"\"\"Get authenticated user information.\"\"\"\n    # Access token is available in context\n    token = ctx.session.get_access_token()  # Example access\n    return {\"user\": \"authenticated_user\", \"token_type\": \"Bearer\"}\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/authentication/#using-authentication","title":"Using Authentication","text":""},{"location":"development/fastmcp/authentication/#client-side","title":"Client Side","text":"<pre><code>import httpx\n\n# 1. Register client (if needed)\nregistration = httpx.post(\"https://server.com/auth/register\", json={\n    \"client_name\": \"My Client App\",\n    \"redirect_uris\": [\"https://myapp.com/callback\"]\n})\nclient_data = registration.json()\n\n# 2. Start OAuth flow\nauth_url = (\n    f\"https://server.com/auth/authorize\"\n    f\"?client_id={client_data['client_id']}\"\n    f\"&amp;redirect_uri=https://myapp.com/callback\"\n    f\"&amp;scope=read write\"\n    f\"&amp;response_type=code\"\n)\n\n# 3. User visits auth_url, grants permission, gets redirected with code\n\n# 4. Exchange code for token\ntoken_response = httpx.post(\"https://server.com/auth/token\", data={\n    \"grant_type\": \"authorization_code\",\n    \"code\": \"received_auth_code\",\n    \"client_id\": client_data[\"client_id\"],\n    \"client_secret\": client_data[\"client_secret\"]\n})\ntokens = token_response.json()\n\n# 5. Use access token\nheaders = {\"Authorization\": f\"Bearer {tokens['access_token']}\"}\nresponse = httpx.post(\n    \"https://server.com/mcp\",\n    headers=headers,\n    json={\"method\": \"tools/call\", \"params\": {\"name\": \"my_tool\"}}\n)\n</code></pre>"},{"location":"development/fastmcp/authentication/#custom-authentication","title":"Custom Authentication","text":"<pre><code>from starlette.requests import Request\nfrom starlette.responses import JSONResponse\n\n@mcp.custom_route(\"/auth/callback\", methods=[\"GET\"])\nasync def auth_callback(request: Request) -&gt; JSONResponse:\n    \"\"\"Handle OAuth callback.\"\"\"\n    code = request.query_params.get(\"code\")\n    if not code:\n        return JSONResponse({\"error\": \"No authorization code\"}, status_code=400)\n\n    # Process the authorization code\n    # This would typically exchange with external OAuth provider\n\n    return JSONResponse({\"message\": \"Authentication successful\"})\n\n@mcp.custom_route(\"/auth/status\", methods=[\"GET\"])\nasync def auth_status(request: Request) -&gt; JSONResponse:\n    \"\"\"Check authentication status.\"\"\"\n    auth_header = request.headers.get(\"Authorization\")\n    if not auth_header or not auth_header.startswith(\"Bearer \"):\n        return JSONResponse({\"authenticated\": False}, status_code=401)\n\n    token = auth_header.split(\" \")[1]\n    token_data = await oauth_provider.validate_token(token)\n\n    return JSONResponse({\n        \"authenticated\": token_data is not None,\n        \"scopes\": token_data.get(\"scope\", \"\").split() if token_data else []\n    })\n</code></pre>"},{"location":"development/fastmcp/authentication/#security-best-practices","title":"Security Best Practices","text":""},{"location":"development/fastmcp/authentication/#1-use-https-in-production","title":"1. Use HTTPS in Production","text":"<pre><code>auth_settings = AuthSettings(\n    issuer_url=\"https://api.example.com\",  # Always HTTPS in production\n    # ... other settings\n)\n</code></pre>"},{"location":"development/fastmcp/authentication/#2-validate-redirect-uris","title":"2. Validate Redirect URIs","text":"<pre><code>async def register_client(self, registration_data: dict) -&gt; dict:\n    \"\"\"Register client with redirect URI validation.\"\"\"\n    redirect_uris = registration_data.get(\"redirect_uris\", [])\n\n    # Validate redirect URIs\n    for uri in redirect_uris:\n        if not uri.startswith(\"https://\"):\n            raise ValueError(\"Redirect URIs must use HTTPS\")\n        if \"localhost\" in uri and not uri.startswith(\"http://localhost\"):\n            raise ValueError(\"localhost URIs must use HTTP\")\n\n    # ... rest of registration\n</code></pre>"},{"location":"development/fastmcp/authentication/#3-implement-token-expiration","title":"3. Implement Token Expiration","text":"<pre><code>async def validate_token(self, token: str) -&gt; dict | None:\n    \"\"\"Validate token with proper expiration handling.\"\"\"\n    token_data = self.tokens.get(token)\n    if not token_data:\n        return None\n\n    # Check expiration\n    if time.time() &gt; token_data.get(\"expires_at\", 0):\n        # Clean up expired token\n        del self.tokens[token]\n        return None\n\n    return token_data\n</code></pre>"},{"location":"development/fastmcp/authentication/#4-scope-validation","title":"4. Scope Validation","text":"<pre><code>@mcp.tool()\ndef admin_tool(ctx) -&gt; str:\n    \"\"\"Tool that requires admin scope.\"\"\"\n    # Check if user has required scope\n    token_data = ctx.session.get_token_data()  # Hypothetical method\n    scopes = token_data.get(\"scope\", \"\").split()\n\n    if \"admin\" not in scopes:\n        raise PermissionError(\"Admin scope required\")\n\n    return \"Admin operation completed\"\n</code></pre>"},{"location":"development/fastmcp/authentication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/fastmcp/authentication/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Authentication only works with HTTP transports <pre><code># Won't work - stdio doesn't support auth\nmcp.run(\"stdio\")\n\n# Works - HTTP transports support auth\nmcp.run(\"streamable-http\")\nmcp.run(\"sse\")\n</code></pre></p> </li> <li> <p>Missing issuer_url configuration <pre><code># Must provide issuer_url\nauth_settings = AuthSettings(\n    issuer_url=\"https://your-server.com\"  # Required\n)\n</code></pre></p> </li> <li> <p>Provider and settings must be paired <pre><code># Both auth_server_provider AND auth must be provided\nmcp = FastMCP(\n    \"Server\",\n    auth_server_provider=oauth_provider,  # Required if using auth\n    auth=auth_settings                    # Required if using auth\n)\n</code></pre></p> </li> </ol>"},{"location":"development/fastmcp/authentication/#debug-authentication","title":"Debug Authentication","text":"<pre><code>mcp = FastMCP(\n    \"Debug Server\",\n    debug=True,           # Enable debug logging\n    log_level=\"DEBUG\",    # Verbose auth logs\n    auth_server_provider=oauth_provider,\n    auth=auth_settings\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/","title":"Configuration","text":"<p>FastMCP provides extensive configuration options through constructor parameters and environment variables. This guide covers all available settings and how to use them effectively.</p>"},{"location":"development/fastmcp/configuration/#basic-configuration","title":"Basic Configuration","text":""},{"location":"development/fastmcp/configuration/#server-creation","title":"Server Creation","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\n# Minimal configuration\nmcp = FastMCP(\"My Server\")\n\n# With basic settings\nmcp = FastMCP(\n    name=\"My Server\",\n    instructions=\"A helpful server that provides data analysis tools\"\n)\n\n# With transport settings\nmcp = FastMCP(\n    name=\"My Server\",\n    host=\"0.0.0.0\",        # Listen on all interfaces\n    port=8080,             # Custom port\n    debug=True             # Enable debug mode\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#environment-variables","title":"Environment Variables","text":"<p>All settings can be configured via environment variables with the <code>FASTMCP_</code> prefix:</p> <pre><code># Server settings\nFASTMCP_DEBUG=true\nFASTMCP_LOG_LEVEL=DEBUG\n\n# HTTP settings\nFASTMCP_HOST=0.0.0.0\nFASTMCP_PORT=8080\nFASTMCP_MOUNT_PATH=/api/mcp\n\n# StreamableHTTP settings\nFASTMCP_JSON_RESPONSE=true\nFASTMCP_STATELESS_HTTP=false\n</code></pre>"},{"location":"development/fastmcp/configuration/#configuration-file","title":"Configuration File","text":"<p>FastMCP automatically loads settings from a <code>.env</code> file:</p> <pre><code># .env file\nFASTMCP_DEBUG=true\nFASTMCP_LOG_LEVEL=INFO\nFASTMCP_HOST=127.0.0.1\nFASTMCP_PORT=8000\nFASTMCP_WARN_ON_DUPLICATE_TOOLS=false\n</code></pre>"},{"location":"development/fastmcp/configuration/#complete-settings-reference","title":"Complete Settings Reference","text":""},{"location":"development/fastmcp/configuration/#server-settings","title":"Server Settings","text":"<pre><code>mcp = FastMCP(\n    # Core server settings\n    name=\"My Server\",                    # Server name (required)\n    instructions=\"Server description\",   # Instructions for clients\n    debug=False,                        # Enable debug mode\n    log_level=\"INFO\",                   # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL\n)\n</code></pre> <p>Environment Variables: - <code>FASTMCP_DEBUG</code>: Enable debug mode (<code>true</code>/<code>false</code>) - <code>FASTMCP_LOG_LEVEL</code>: Set log level (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>)</p>"},{"location":"development/fastmcp/configuration/#http-settings","title":"HTTP Settings","text":"<pre><code>mcp = FastMCP(\n    # HTTP transport settings\n    host=\"127.0.0.1\",                  # Host to bind to\n    port=8000,                         # Port to listen on\n    mount_path=\"/\",                    # Mount path for SSE endpoints\n    sse_path=\"/sse\",                   # SSE endpoint path\n    message_path=\"/messages/\",         # SSE message endpoint path\n    streamable_http_path=\"/mcp\",       # StreamableHTTP endpoint path\n)\n</code></pre> <p>Environment Variables: - <code>FASTMCP_HOST</code>: Host address (<code>127.0.0.1</code>, <code>0.0.0.0</code>, etc.) - <code>FASTMCP_PORT</code>: Port number (<code>8000</code>, <code>8080</code>, etc.) - <code>FASTMCP_MOUNT_PATH</code>: Mount path (<code>/</code>, <code>/api</code>, etc.) - <code>FASTMCP_SSE_PATH</code>: SSE endpoint (<code>/sse</code>, <code>/events</code>, etc.) - <code>FASTMCP_MESSAGE_PATH</code>: Message endpoint (<code>/messages/</code>, <code>/msg/</code>, etc.) - <code>FASTMCP_STREAMABLE_HTTP_PATH</code>: StreamableHTTP endpoint (<code>/mcp</code>, <code>/stream</code>, etc.)</p>"},{"location":"development/fastmcp/configuration/#streamablehttp-settings","title":"StreamableHTTP Settings","text":"<pre><code>mcp = FastMCP(\n    # StreamableHTTP specific settings\n    json_response=False,               # Return JSON responses instead of raw\n    stateless_http=False,              # Use stateless mode (new transport per request)\n)\n</code></pre> <p>Environment Variables: - <code>FASTMCP_JSON_RESPONSE</code>: Enable JSON responses (<code>true</code>/<code>false</code>) - <code>FASTMCP_STATELESS_HTTP</code>: Enable stateless mode (<code>true</code>/<code>false</code>)</p>"},{"location":"development/fastmcp/configuration/#warning-settings","title":"Warning Settings","text":"<pre><code>mcp = FastMCP(\n    # Warning control\n    warn_on_duplicate_tools=True,      # Warn when registering duplicate tools\n    warn_on_duplicate_resources=True,  # Warn when registering duplicate resources\n    warn_on_duplicate_prompts=True,    # Warn when registering duplicate prompts\n)\n</code></pre> <p>Environment Variables: - <code>FASTMCP_WARN_ON_DUPLICATE_TOOLS</code>: Warn on duplicate tools (<code>true</code>/<code>false</code>) - <code>FASTMCP_WARN_ON_DUPLICATE_RESOURCES</code>: Warn on duplicate resources (<code>true</code>/<code>false</code>) - <code>FASTMCP_WARN_ON_DUPLICATE_PROMPTS</code>: Warn on duplicate prompts (<code>true</code>/<code>false</code>)</p>"},{"location":"development/fastmcp/configuration/#dependencies","title":"Dependencies","text":"<pre><code>mcp = FastMCP(\n    # Runtime dependencies\n    dependencies=[\n        \"requests&gt;=2.25.0\",\n        \"pandas&gt;=1.3.0\",\n        \"numpy\"\n    ]\n)\n</code></pre> <p>Environment Variable: - <code>FASTMCP_DEPENDENCIES</code>: Comma-separated list of dependencies</p>"},{"location":"development/fastmcp/configuration/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"development/fastmcp/configuration/#development-configuration","title":"Development Configuration","text":"<pre><code># development.py\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\n    name=\"Dev Server\",\n    debug=True,                        # Enable debug mode\n    log_level=\"DEBUG\",                 # Verbose logging\n    host=\"127.0.0.1\",                 # Local only\n    port=8000,                         # Standard port\n    warn_on_duplicate_tools=True,      # Catch registration issues\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#production-configuration","title":"Production Configuration","text":"<pre><code># production.py\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\n    name=\"Production Server\",\n    debug=False,                       # Disable debug mode\n    log_level=\"INFO\",                  # Standard logging\n    host=\"0.0.0.0\",                   # Accept external connections\n    port=8080,                         # Production port\n    warn_on_duplicate_tools=False,     # Reduce noise in logs\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code>import os\nfrom mcp.server.fastmcp import FastMCP\n\n# Determine environment\nis_production = os.getenv(\"ENVIRONMENT\") == \"production\"\nis_development = os.getenv(\"ENVIRONMENT\") == \"development\"\n\nmcp = FastMCP(\n    name=os.getenv(\"SERVER_NAME\", \"FastMCP Server\"),\n    debug=not is_production,\n    log_level=\"INFO\" if is_production else \"DEBUG\",\n    host=\"0.0.0.0\" if is_production else \"127.0.0.1\",\n    port=int(os.getenv(\"PORT\", \"8000\")),\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#configuration-class","title":"Configuration Class","text":"<pre><code>from pydantic_settings import BaseSettings\nfrom mcp.server.fastmcp import FastMCP\n\nclass ServerConfig(BaseSettings):\n    \"\"\"Custom configuration with validation.\"\"\"\n\n    server_name: str = \"FastMCP Server\"\n    debug: bool = False\n    log_level: str = \"INFO\"\n    host: str = \"127.0.0.1\"\n    port: int = 8000\n\n    # Custom settings\n    max_connections: int = 100\n    timeout: int = 30\n\n    class Config:\n        env_prefix = \"SERVER_\"\n\nconfig = ServerConfig()\n\nmcp = FastMCP(\n    name=config.server_name,\n    debug=config.debug,\n    log_level=config.log_level,\n    host=config.host,\n    port=config.port,\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#transport-specific-configuration","title":"Transport-Specific Configuration","text":""},{"location":"development/fastmcp/configuration/#stdio-configuration","title":"stdio Configuration","text":"<pre><code># stdio requires minimal configuration\nmcp = FastMCP(\"stdio Server\")\n\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")  # No additional config needed\n</code></pre>"},{"location":"development/fastmcp/configuration/#sse-configuration","title":"SSE Configuration","text":"<pre><code>mcp = FastMCP(\n    \"SSE Server\",\n    host=\"0.0.0.0\",                   # Host for HTTP server\n    port=8000,                        # HTTP port\n    mount_path=\"/github\",             # Custom mount path\n    sse_path=\"/events\",               # Custom SSE endpoint\n    message_path=\"/api/messages/\",    # Custom message endpoint\n    debug=True                        # Enable debug mode\n)\n\nif __name__ == \"__main__\":\n    mcp.run(\"sse\")\n</code></pre>"},{"location":"development/fastmcp/configuration/#streamablehttp-configuration","title":"StreamableHTTP Configuration","text":"<pre><code>mcp = FastMCP(\n    \"StreamableHTTP Server\",\n    host=\"0.0.0.0\",                   # Host for HTTP server\n    port=8080,                        # HTTP port\n    streamable_http_path=\"/stream\",   # Custom endpoint\n    json_response=True,               # JSON responses\n    stateless_http=False,             # Stateful sessions\n    debug=False                       # Production mode\n)\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"development/fastmcp/configuration/#nested-environment-variables","title":"Nested Environment Variables","text":"<p>FastMCP supports nested configuration via double underscores:</p> <pre><code># Authentication settings (when using auth)\nFASTMCP_AUTH__ISSUER_URL=https://auth.example.com\nFASTMCP_AUTH__REQUIRED_SCOPES=read,write\nFASTMCP_AUTH__SERVICE_DOCUMENTATION_URL=https://docs.example.com\n</code></pre>"},{"location":"development/fastmcp/configuration/#custom-lifespan","title":"Custom Lifespan","text":"<pre><code>from contextlib import asynccontextmanager\nfrom mcp.server.fastmcp import FastMCP\n\n@asynccontextmanager\nasync def custom_lifespan(app: FastMCP):\n    \"\"\"Custom lifespan for setup/teardown.\"\"\"\n    print(\"Server starting up...\")\n\n    # Setup code here\n    database = await setup_database()\n\n    try:\n        yield {\"database\": database}\n    finally:\n        # Cleanup code here\n        await database.close()\n        print(\"Server shutting down...\")\n\nasync def setup_database():\n    \"\"\"Mock database setup.\"\"\"\n    return {\"connection\": \"mock_db\"}\n\nmcp = FastMCP(\n    \"Lifespan Server\",\n    lifespan=custom_lifespan\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#event-store-configuration","title":"Event Store Configuration","text":"<pre><code>from mcp.server.streamable_http import EventStore\nfrom mcp.server.fastmcp import FastMCP\n\n# Custom event store for resumability\nevent_store = EventStore()\n\nmcp = FastMCP(\n    \"Resumable Server\",\n    event_store=event_store,\n    stateless_http=False  # Required for resumability\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"development/fastmcp/configuration/#type-validation","title":"Type Validation","text":"<pre><code>from typing import Literal\nfrom pydantic import BaseModel, validator\n\nclass ValidatedConfig(BaseModel):\n    \"\"\"Configuration with validation.\"\"\"\n\n    name: str\n    debug: bool = False\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"] = \"INFO\"\n    host: str = \"127.0.0.1\"\n    port: int = 8000\n\n    @validator(\"port\")\n    def validate_port(cls, v):\n        if not 1 &lt;= v &lt;= 65535:\n            raise ValueError(\"Port must be between 1 and 65535\")\n        return v\n\n    @validator(\"host\")\n    def validate_host(cls, v):\n        if v not in [\"127.0.0.1\", \"0.0.0.0\", \"localhost\"]:\n            # In real scenario, you might want more sophisticated validation\n            pass\n        return v\n\n# Use validated config\nconfig = ValidatedConfig(\n    name=\"Validated Server\",\n    port=8080,\n    debug=True\n)\n\nmcp = FastMCP(\n    name=config.name,\n    debug=config.debug,\n    log_level=config.log_level,\n    host=config.host,\n    port=config.port\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#environment-validation","title":"Environment Validation","text":"<pre><code>import os\nfrom mcp.server.fastmcp import FastMCP\n\ndef get_validated_config():\n    \"\"\"Get configuration with environment validation.\"\"\"\n\n    # Required environment variables\n    required_vars = [\"SERVER_NAME\"]\n    missing = [var for var in required_vars if not os.getenv(var)]\n\n    if missing:\n        raise ValueError(f\"Missing required environment variables: {missing}\")\n\n    # Validate log level\n    log_level = os.getenv(\"LOG_LEVEL\", \"INFO\")\n    if log_level not in [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]:\n        raise ValueError(f\"Invalid log level: {log_level}\")\n\n    # Validate port\n    port = int(os.getenv(\"PORT\", \"8000\"))\n    if not 1 &lt;= port &lt;= 65535:\n        raise ValueError(f\"Invalid port: {port}\")\n\n    return {\n        \"name\": os.getenv(\"SERVER_NAME\"),\n        \"debug\": os.getenv(\"DEBUG\", \"false\").lower() == \"true\",\n        \"log_level\": log_level,\n        \"port\": port,\n        \"host\": os.getenv(\"HOST\", \"127.0.0.1\")\n    }\n\n# Use validated environment config\nconfig = get_validated_config()\nmcp = FastMCP(**config)\n</code></pre>"},{"location":"development/fastmcp/configuration/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"development/fastmcp/configuration/#1-use-environment-variables-for-deployment","title":"1. Use Environment Variables for Deployment","text":"<pre><code># Good: Environment-based configuration\nmcp = FastMCP(\n    name=os.getenv(\"SERVER_NAME\", \"Default Server\"),\n    debug=os.getenv(\"DEBUG\", \"false\").lower() == \"true\",\n    host=os.getenv(\"HOST\", \"127.0.0.1\"),\n    port=int(os.getenv(\"PORT\", \"8000\"))\n)\n\n# Avoid: Hard-coded values\nmcp = FastMCP(\n    name=\"My Server\",\n    debug=True,\n    host=\"192.168.1.100\",  # Don't hard-code IPs\n    port=8000\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#2-separate-configuration-by-environment","title":"2. Separate Configuration by Environment","text":"<pre><code># config/development.py\ndevelopment_config = {\n    \"debug\": True,\n    \"log_level\": \"DEBUG\",\n    \"host\": \"127.0.0.1\",\n    \"warn_on_duplicate_tools\": True\n}\n\n# config/production.py\nproduction_config = {\n    \"debug\": False,\n    \"log_level\": \"INFO\",\n    \"host\": \"0.0.0.0\",\n    \"warn_on_duplicate_tools\": False\n}\n\n# main.py\nimport os\nfrom config import development, production\n\nenv = os.getenv(\"ENVIRONMENT\", \"development\")\nconfig = development_config if env == \"development\" else production_config\n\nmcp = FastMCP(\"My Server\", **config)\n</code></pre>"},{"location":"development/fastmcp/configuration/#3-validate-critical-settings","title":"3. Validate Critical Settings","text":"<pre><code>import os\nfrom mcp.server.fastmcp import FastMCP\n\n# Validate critical settings\nport = int(os.getenv(\"PORT\", \"8000\"))\nif port &lt; 1024 and os.getuid() != 0:  # Unix-specific check\n    raise ValueError(\"Ports below 1024 require root privileges\")\n\nhost = os.getenv(\"HOST\", \"127.0.0.1\")\nif host == \"0.0.0.0\":\n    print(\"WARNING: Server will accept connections from any IP address\")\n\nmcp = FastMCP(\n    name=\"Validated Server\",\n    host=host,\n    port=port\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#4-document-configuration-options","title":"4. Document Configuration Options","text":"<pre><code>class ServerConfig:\n    \"\"\"\n    FastMCP Server Configuration\n\n    Environment Variables:\n        SERVER_NAME: Name of the server (default: \"FastMCP Server\")\n        DEBUG: Enable debug mode (default: false)\n        LOG_LEVEL: Logging level (default: \"INFO\")\n        HOST: Host to bind to (default: \"127.0.0.1\")\n        PORT: Port to listen on (default: 8000)\n\n    Example:\n        export SERVER_NAME=\"My API Server\"\n        export DEBUG=true\n        export LOG_LEVEL=DEBUG\n        export HOST=0.0.0.0\n        export PORT=8080\n    \"\"\"\n\n    def __init__(self):\n        self.name = os.getenv(\"SERVER_NAME\", \"FastMCP Server\")\n        self.debug = os.getenv(\"DEBUG\", \"false\").lower() == \"true\"\n        self.log_level = os.getenv(\"LOG_LEVEL\", \"INFO\")\n        self.host = os.getenv(\"HOST\", \"127.0.0.1\")\n        self.port = int(os.getenv(\"PORT\", \"8000\"))\n\nconfig = ServerConfig()\nmcp = FastMCP(\n    name=config.name,\n    debug=config.debug,\n    log_level=config.log_level,\n    host=config.host,\n    port=config.port\n)\n</code></pre>"},{"location":"development/fastmcp/configuration/#5-use-configuration-files-for-complex-settings","title":"5. Use Configuration Files for Complex Settings","text":"<pre><code># config.yaml\nserver:\n  name: \"Production Server\"\n  debug: false\n  log_level: \"INFO\"\n\nhttp:\n  host: \"0.0.0.0\"\n  port: 8080\n  mount_path: \"/api\"\n\nstreamable_http:\n  json_response: true\n  stateless: false\n\nwarnings:\n  duplicate_tools: false\n  duplicate_resources: false\n  duplicate_prompts: false\n</code></pre> <pre><code>import yaml\nfrom mcp.server.fastmcp import FastMCP\n\ndef load_config(config_file: str) -&gt; dict:\n    \"\"\"Load configuration from YAML file.\"\"\"\n    with open(config_file) as f:\n        config = yaml.safe_load(f)\n\n    # Flatten nested config\n    return {\n        \"name\": config[\"server\"][\"name\"],\n        \"debug\": config[\"server\"][\"debug\"],\n        \"log_level\": config[\"server\"][\"log_level\"],\n        \"host\": config[\"http\"][\"host\"],\n        \"port\": config[\"http\"][\"port\"],\n        \"mount_path\": config[\"http\"][\"mount_path\"],\n        \"json_response\": config[\"streamable_http\"][\"json_response\"],\n        \"stateless_http\": config[\"streamable_http\"][\"stateless\"],\n        \"warn_on_duplicate_tools\": config[\"warnings\"][\"duplicate_tools\"],\n        \"warn_on_duplicate_resources\": config[\"warnings\"][\"duplicate_resources\"],\n        \"warn_on_duplicate_prompts\": config[\"warnings\"][\"duplicate_prompts\"],\n    }\n\nconfig = load_config(\"config.yaml\")\nmcp = FastMCP(**config)\n</code></pre>"},{"location":"development/fastmcp/context/","title":"Context","text":"<p>The Context object provides tools and resources with access to MCP capabilities like logging, progress reporting, and resource access. It's automatically injected into functions that request it through type annotations.</p>"},{"location":"development/fastmcp/context/#overview","title":"Overview","text":"<p>The Context object enables:</p> <ul> <li>Logging: Send messages to the client at different log levels</li> <li>Progress reporting: Update clients on long-running operations</li> <li>Resource access: Read other resources from within tools/resources</li> <li>Request metadata: Access request IDs, client information, and session data</li> </ul> <p>Context is only available during request processing and provides a clean interface to MCP's underlying capabilities.</p>"},{"location":"development/fastmcp/context/#context-injection","title":"Context Injection","text":""},{"location":"development/fastmcp/context/#automatic-injection","title":"Automatic Injection","text":"<p>FastMCP automatically injects Context into functions that request it via type hints:</p> <pre><code>from mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"Server\")\n\n@mcp.tool()\nasync def tool_with_context(x: int, ctx: Context) -&gt; str:\n    \"\"\"Tool that uses context capabilities.\"\"\"\n    await ctx.info(f\"Processing value: {x}\")\n    return f\"Processed: {x}\"\n\n@mcp.resource(\"data://{id}\")\nasync def resource_with_context(id: str, context: Context) -&gt; str:\n    \"\"\"Resource that uses context capabilities.\"\"\"\n    await context.debug(f\"Fetching data for ID: {id}\")\n    return f\"Data for {id}\"\n</code></pre>"},{"location":"development/fastmcp/context/#parameter-naming","title":"Parameter Naming","text":"<p>The context parameter can have any name as long as it's typed as <code>Context</code>:</p> <pre><code>@mcp.tool()\nasync def tool_ctx(x: int, ctx: Context) -&gt; str:\n    await ctx.info(\"Using 'ctx' parameter name\")\n    return str(x)\n\n@mcp.tool()\nasync def tool_context(x: int, context: Context) -&gt; str:\n    await context.info(\"Using 'context' parameter name\")\n    return str(x)\n\n@mcp.tool()\nasync def tool_mcp_context(x: int, mcp_context: Context) -&gt; str:\n    await mcp_context.info(\"Using 'mcp_context' parameter name\")\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/context/#mixed-parameters","title":"Mixed Parameters","text":"<p>Context can be mixed with other parameters in any order:</p> <pre><code>@mcp.tool()\nasync def mixed_params(\n    required_param: str,\n    ctx: Context,\n    optional_param: int = 42\n) -&gt; str:\n    \"\"\"Context mixed with other parameters.\"\"\"\n    await ctx.info(f\"Required: {required_param}, Optional: {optional_param}\")\n    return f\"Result: {required_param}_{optional_param}\"\n</code></pre>"},{"location":"development/fastmcp/context/#logging","title":"Logging","text":""},{"location":"development/fastmcp/context/#log-levels","title":"Log Levels","text":"<p>Send messages to the client at different severity levels:</p> <pre><code>@mcp.tool()\nasync def logging_example(data: str, ctx: Context) -&gt; str:\n    \"\"\"Demonstrate different log levels.\"\"\"\n\n    # Debug information (detailed diagnostic info)\n    await ctx.debug(f\"Starting processing with data: {data}\")\n\n    # General information\n    await ctx.info(\"Processing data...\")\n\n    # Warning about potential issues\n    if len(data) &gt; 1000:\n        await ctx.warning(\"Large data detected, processing may take longer\")\n\n    # Error conditions\n    if not data.strip():\n        await ctx.error(\"Empty data provided\")\n        raise ValueError(\"Data cannot be empty\")\n\n    return f\"Processed: {data}\"\n</code></pre>"},{"location":"development/fastmcp/context/#structured-logging","title":"Structured Logging","text":"<pre><code>@mcp.tool()\nasync def structured_logging(file_path: str, ctx: Context) -&gt; dict:\n    \"\"\"Example of structured logging with context.\"\"\"\n\n    await ctx.info(f\"Processing file: {file_path}\")\n\n    try:\n        # Simulate file processing\n        file_size = len(file_path) * 100  # Mock calculation\n\n        await ctx.debug(f\"File size calculated: {file_size} bytes\")\n\n        if file_size &gt; 10000:\n            await ctx.warning(f\"Large file detected: {file_size} bytes\")\n\n        result = {\n            \"file_path\": file_path,\n            \"size\": file_size,\n            \"status\": \"processed\"\n        }\n\n        await ctx.info(\"File processing completed successfully\")\n        return result\n\n    except Exception as e:\n        await ctx.error(f\"Failed to process file {file_path}: {e}\")\n        raise\n</code></pre>"},{"location":"development/fastmcp/context/#custom-logger-names","title":"Custom Logger Names","text":"<pre><code>@mcp.tool()\nasync def custom_logger(data: str, ctx: Context) -&gt; str:\n    \"\"\"Use custom logger names for organization.\"\"\"\n\n    # Use the low-level log method with custom logger name\n    await ctx.log(\"info\", \"Starting data validation\", logger_name=\"validator\")\n\n    if not data:\n        await ctx.log(\"error\", \"Validation failed: empty data\", logger_name=\"validator\")\n        raise ValueError(\"Data is required\")\n\n    await ctx.log(\"info\", \"Validation passed\", logger_name=\"validator\")\n    await ctx.log(\"info\", \"Processing data\", logger_name=\"processor\")\n\n    result = data.upper()\n\n    await ctx.log(\"info\", \"Processing completed\", logger_name=\"processor\")\n    return result\n</code></pre>"},{"location":"development/fastmcp/context/#progress-reporting","title":"Progress Reporting","text":""},{"location":"development/fastmcp/context/#basic-progress","title":"Basic Progress","text":"<p>Report progress for long-running operations:</p> <pre><code>import asyncio\n\n@mcp.tool()\nasync def long_operation(items: list[str], ctx: Context) -&gt; list[str]:\n    \"\"\"Demonstrate progress reporting.\"\"\"\n\n    total = len(items)\n    results = []\n\n    await ctx.info(f\"Processing {total} items...\")\n\n    for i, item in enumerate(items):\n        # Report progress with current/total and optional message\n        await ctx.report_progress(i, total, f\"Processing {item}\")\n\n        # Simulate work\n        await asyncio.sleep(0.1)\n        results.append(f\"processed_{item}\")\n\n        await ctx.debug(f\"Completed item {i + 1}/{total}\")\n\n    # Report completion\n    await ctx.report_progress(total, total, \"Complete\")\n    await ctx.info(\"All items processed successfully\")\n\n    return results\n</code></pre>"},{"location":"development/fastmcp/context/#percentage-progress","title":"Percentage Progress","text":"<pre><code>@mcp.tool()\nasync def percentage_progress(iterations: int, ctx: Context) -&gt; str:\n    \"\"\"Show progress as percentages.\"\"\"\n\n    await ctx.info(f\"Starting {iterations} iterations\")\n\n    for i in range(iterations):\n        # Calculate percentage\n        progress_pct = (i / iterations) * 100\n\n        # Report with percentage in message\n        await ctx.report_progress(\n            progress=i,\n            total=iterations,\n            message=f\"Progress: {progress_pct:.1f}%\"\n        )\n\n        # Simulate work\n        await asyncio.sleep(0.05)\n\n    await ctx.report_progress(iterations, iterations, \"100% Complete\")\n    return f\"Completed {iterations} iterations\"\n</code></pre>"},{"location":"development/fastmcp/context/#multi-stage-progress","title":"Multi-Stage Progress","text":"<pre><code>@mcp.tool()\nasync def multi_stage_operation(data: list[str], ctx: Context) -&gt; dict:\n    \"\"\"Progress reporting across multiple stages.\"\"\"\n\n    total_items = len(data)\n    stages = [\"validation\", \"processing\", \"optimization\", \"finalization\"]\n\n    results = {\"processed\": [], \"errors\": []}\n\n    for stage_idx, stage in enumerate(stages):\n        await ctx.info(f\"Stage {stage_idx + 1}/{len(stages)}: {stage}\")\n\n        for item_idx, item in enumerate(data):\n            # Calculate overall progress across all stages\n            overall_progress = (stage_idx * total_items + item_idx)\n            overall_total = len(stages) * total_items\n\n            await ctx.report_progress(\n                progress=overall_progress,\n                total=overall_total,\n                message=f\"{stage.title()}: {item} ({item_idx + 1}/{total_items})\"\n            )\n\n            # Simulate stage-specific work\n            if stage == \"validation\" and not item:\n                results[\"errors\"].append(f\"Empty item at index {item_idx}\")\n                continue\n\n            await asyncio.sleep(0.02)  # Simulate work\n\n            if stage == \"processing\":\n                results[\"processed\"].append(f\"{stage}_{item}\")\n\n    await ctx.report_progress(\n        progress=len(stages) * total_items,\n        total=len(stages) * total_items,\n        message=\"All stages complete\"\n    )\n\n    return results\n</code></pre>"},{"location":"development/fastmcp/context/#resource-access","title":"Resource Access","text":""},{"location":"development/fastmcp/context/#reading-resources","title":"Reading Resources","text":"<p>Access other resources from within tools and resources:</p> <pre><code>@mcp.tool()\nasync def config_aware_tool(setting_name: str, ctx: Context) -&gt; str:\n    \"\"\"Tool that reads configuration from resources.\"\"\"\n\n    try:\n        # Read configuration resource\n        config_data = await ctx.read_resource(\"config://app\")\n        await ctx.info(\"Successfully loaded configuration\")\n\n        # Parse configuration (assuming it's JSON)\n        import json\n        config = json.loads(config_data[0].content)  # type: ignore\n\n        if setting_name in config:\n            value = config[setting_name]\n            await ctx.info(f\"Found setting {setting_name}: {value}\")\n            return f\"{setting_name} = {value}\"\n        else:\n            await ctx.warning(f\"Setting {setting_name} not found in configuration\")\n            return f\"Setting {setting_name} not found\"\n\n    except Exception as e:\n        await ctx.error(f\"Failed to read configuration: {e}\")\n        raise\n\n@mcp.tool()\nasync def data_processor(source: str, ctx: Context) -&gt; dict:\n    \"\"\"Process data from various sources.\"\"\"\n\n    await ctx.info(f\"Reading data from {source}\")\n\n    try:\n        # Read data resource\n        resource_data = await ctx.read_resource(f\"data://{source}\")\n\n        content = resource_data[0].content\n        mime_type = resource_data[0].mime_type\n\n        await ctx.debug(f\"Received {len(content)} chars of {mime_type} data\")\n\n        # Process based on MIME type\n        if mime_type == \"application/json\":\n            import json\n            data = json.loads(content)  # type: ignore\n            await ctx.info(\"Parsed JSON data successfully\")\n        else:\n            data = {\"raw_content\": content}\n            await ctx.info(\"Treating as raw text data\")\n\n        return {\n            \"source\": source,\n            \"mime_type\": mime_type,\n            \"processed_data\": data\n        }\n\n    except Exception as e:\n        await ctx.error(f\"Failed to process data from {source}: {e}\")\n        raise\n</code></pre>"},{"location":"development/fastmcp/context/#cross-resource-dependencies","title":"Cross-Resource Dependencies","text":"<pre><code>@mcp.resource(\"report://{report_id}\")\nasync def generate_report(report_id: str, ctx: Context) -&gt; str:\n    \"\"\"Generate a report that depends on other resources.\"\"\"\n\n    await ctx.info(f\"Generating report {report_id}\")\n\n    try:\n        # Read report template\n        template_data = await ctx.read_resource(f\"template://report_{report_id}\")\n        template = template_data[0].content\n\n        await ctx.debug(\"Loaded report template\")\n\n        # Read data for the report\n        data_resource = await ctx.read_resource(f\"data://report_data_{report_id}\")\n        data = data_resource[0].content\n\n        await ctx.debug(\"Loaded report data\")\n\n        # Simple template substitution (in real case, use proper templating)\n        report = template.replace(\"{{ data }}\", str(data))  # type: ignore\n        report = report.replace(\"{{ report_id }}\", report_id)  # type: ignore\n\n        await ctx.info(\"Report generated successfully\")\n        return report\n\n    except Exception as e:\n        await ctx.error(f\"Failed to generate report {report_id}: {e}\")\n        raise ValueError(f\"Report generation failed: {e}\")\n</code></pre>"},{"location":"development/fastmcp/context/#request-metadata","title":"Request Metadata","text":""},{"location":"development/fastmcp/context/#request-information","title":"Request Information","text":"<p>Access information about the current request:</p> <pre><code>@mcp.tool()\nasync def request_info_tool(ctx: Context) -&gt; dict:\n    \"\"\"Tool that provides information about the current request.\"\"\"\n\n    # Get request ID\n    request_id = ctx.request_id\n    await ctx.info(f\"Processing request {request_id}\")\n\n    # Get client ID (if available)\n    client_id = ctx.client_id\n    if client_id:\n        await ctx.info(f\"Request from client: {client_id}\")\n    else:\n        await ctx.debug(\"No client ID available\")\n\n    return {\n        \"request_id\": request_id,\n        \"client_id\": client_id,\n        \"timestamp\": \"2024-01-01T12:00:00Z\"  # You could add actual timestamp\n    }\n</code></pre>"},{"location":"development/fastmcp/context/#session-access","title":"Session Access","text":"<p>Access the underlying session for advanced use cases:</p> <pre><code>@mcp.tool()\nasync def advanced_session_tool(ctx: Context) -&gt; dict:\n    \"\"\"Tool that accesses session information.\"\"\"\n\n    # Access session (for advanced use cases)\n    session = ctx.session\n\n    await ctx.info(\"Accessing session information\")\n\n    # You can use session for advanced MCP operations\n    # This is for advanced users who need direct access\n\n    return {\n        \"request_id\": ctx.request_id,\n        \"has_session\": session is not None,\n        \"session_type\": type(session).__name__\n    }\n</code></pre>"},{"location":"development/fastmcp/context/#error-handling-with-context","title":"Error Handling with Context","text":""},{"location":"development/fastmcp/context/#graceful-error-reporting","title":"Graceful Error Reporting","text":"<pre><code>@mcp.tool()\nasync def robust_tool(data: dict, ctx: Context) -&gt; dict:\n    \"\"\"Tool with comprehensive error handling and context logging.\"\"\"\n\n    try:\n        await ctx.info(\"Starting data processing\")\n\n        # Validate input\n        if not isinstance(data, dict):\n            await ctx.error(\"Invalid input: expected dictionary\")\n            raise TypeError(\"Input must be a dictionary\")\n\n        if \"required_field\" not in data:\n            await ctx.error(\"Missing required field in input data\")\n            raise ValueError(\"required_field is mandatory\")\n\n        await ctx.debug(f\"Processing data with keys: {list(data.keys())}\")\n\n        # Process data\n        result = {\"processed\": True, \"original\": data}\n\n        await ctx.info(\"Data processing completed successfully\")\n        return result\n\n    except TypeError as e:\n        await ctx.error(f\"Type error: {e}\")\n        raise\n    except ValueError as e:\n        await ctx.error(f\"Value error: {e}\")\n        raise\n    except Exception as e:\n        await ctx.error(f\"Unexpected error during processing: {e}\")\n        raise\n</code></pre>"},{"location":"development/fastmcp/context/#resource-error-handling","title":"Resource Error Handling","text":"<pre><code>@mcp.tool()\nasync def safe_resource_reader(resource_uri: str, ctx: Context) -&gt; dict:\n    \"\"\"Safely read resources with error handling.\"\"\"\n\n    await ctx.info(f\"Attempting to read resource: {resource_uri}\")\n\n    try:\n        resource_data = await ctx.read_resource(resource_uri)\n\n        if not resource_data:\n            await ctx.warning(f\"No data returned for resource: {resource_uri}\")\n            return {\"status\": \"empty\", \"uri\": resource_uri}\n\n        content = resource_data[0].content\n        mime_type = resource_data[0].mime_type\n\n        await ctx.info(f\"Successfully read {len(content)} chars of {mime_type} data\")\n\n        return {\n            \"status\": \"success\",\n            \"uri\": resource_uri,\n            \"mime_type\": mime_type,\n            \"content_length\": len(content)\n        }\n\n    except Exception as e:\n        await ctx.error(f\"Failed to read resource {resource_uri}: {e}\")\n\n        return {\n            \"status\": \"error\",\n            \"uri\": resource_uri,\n            \"error\": str(e)\n        }\n</code></pre>"},{"location":"development/fastmcp/context/#advanced-context-usage","title":"Advanced Context Usage","text":""},{"location":"development/fastmcp/context/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\n\n@mcp.tool()\nasync def performance_monitored_tool(data: list[str], ctx: Context) -&gt; dict:\n    \"\"\"Tool that monitors and reports performance metrics.\"\"\"\n\n    start_time = time.time()\n    await ctx.info(f\"Starting processing of {len(data)} items\")\n\n    results = []\n\n    for i, item in enumerate(data):\n        item_start = time.time()\n\n        # Simulate processing\n        await asyncio.sleep(0.01)\n        processed_item = f\"processed_{item}\"\n        results.append(processed_item)\n\n        item_time = time.time() - item_start\n\n        # Report progress with timing\n        await ctx.report_progress(\n            progress=i + 1,\n            total=len(data),\n            message=f\"Item {i + 1}: {item_time:.3f}s\"\n        )\n\n        # Log slow items\n        if item_time &gt; 0.05:\n            await ctx.warning(f\"Slow processing detected for item {i}: {item_time:.3f}s\")\n\n    total_time = time.time() - start_time\n    avg_time = total_time / len(data) if data else 0\n\n    await ctx.info(f\"Processing completed in {total_time:.3f}s (avg: {avg_time:.3f}s per item)\")\n\n    return {\n        \"results\": results,\n        \"performance\": {\n            \"total_time\": total_time,\n            \"average_time\": avg_time,\n            \"items_processed\": len(data)\n        }\n    }\n</code></pre>"},{"location":"development/fastmcp/context/#conditional-logging","title":"Conditional Logging","text":"<pre><code>@mcp.tool()\nasync def conditional_logging_tool(\n    data: list[str],\n    verbose: bool = False,\n    ctx: Context\n) -&gt; list[str]:\n    \"\"\"Tool with conditional logging based on parameters.\"\"\"\n\n    log_level = \"debug\" if verbose else \"info\"\n\n    await ctx.log(log_level, f\"Starting processing with verbose={verbose}\")\n\n    results = []\n\n    for i, item in enumerate(data):\n        if verbose:\n            await ctx.debug(f\"Processing item {i}: {item}\")\n\n        # Simulate processing\n        result = item.upper()\n        results.append(result)\n\n        if verbose:\n            await ctx.debug(f\"Item {i} result: {result}\")\n        elif i % 10 == 0:  # Log every 10th item when not verbose\n            await ctx.info(f\"Processed {i + 1}/{len(data)} items\")\n\n    await ctx.info(f\"Completed processing {len(data)} items\")\n    return results\n</code></pre>"},{"location":"development/fastmcp/context/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/context/#1-always-handle-context-gracefully","title":"1. Always Handle Context Gracefully","text":"<pre><code>@mcp.tool()\nasync def graceful_context_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool that handles context operations gracefully.\"\"\"\n\n    try:\n        await ctx.info(\"Starting processing\")\n\n        # Main processing logic\n        result = data.upper()\n\n        await ctx.info(\"Processing completed successfully\")\n        return result\n\n    except Exception as e:\n        # Always log errors before re-raising\n        await ctx.error(f\"Processing failed: {e}\")\n        raise\n</code></pre>"},{"location":"development/fastmcp/context/#2-use-appropriate-log-levels","title":"2. Use Appropriate Log Levels","text":"<pre><code>@mcp.tool()\nasync def well_logged_tool(config: dict, ctx: Context) -&gt; dict:\n    \"\"\"Tool demonstrating appropriate log level usage.\"\"\"\n\n    # Debug: Detailed diagnostic information\n    await ctx.debug(f\"Tool called with config keys: {list(config.keys())}\")\n\n    # Info: General information about progress\n    await ctx.info(\"Validating configuration\")\n\n    # Warning: Something unexpected but not fatal\n    if \"deprecated_field\" in config:\n        await ctx.warning(\"Configuration uses deprecated field 'deprecated_field'\")\n\n    # Error: Something went wrong\n    if not config.get(\"required_setting\"):\n        await ctx.error(\"Missing required_setting in configuration\")\n        raise ValueError(\"required_setting is mandatory\")\n\n    await ctx.info(\"Configuration validation complete\")\n    return {\"status\": \"valid\", \"config\": config}\n</code></pre>"},{"location":"development/fastmcp/context/#3-provide-meaningful-progress-updates","title":"3. Provide Meaningful Progress Updates","text":"<pre><code>@mcp.tool()\nasync def meaningful_progress_tool(files: list[str], ctx: Context) -&gt; list[dict]:\n    \"\"\"Tool with meaningful progress reporting.\"\"\"\n\n    total_files = len(files)\n    results = []\n\n    await ctx.info(f\"Processing {total_files} files\")\n\n    for i, file_path in enumerate(files):\n        # Descriptive progress messages\n        await ctx.report_progress(\n            progress=i,\n            total=total_files,\n            message=f\"Processing {file_path} ({i + 1}/{total_files})\"\n        )\n\n        # Simulate file processing with sub-steps\n        await ctx.debug(f\"Reading file: {file_path}\")\n\n        # Simulate work\n        await asyncio.sleep(0.1)\n\n        result = {\n            \"file\": file_path,\n            \"size\": len(file_path) * 10,  # Mock size\n            \"status\": \"processed\"\n        }\n        results.append(result)\n\n        await ctx.debug(f\"Completed processing: {file_path}\")\n\n    await ctx.report_progress(total_files, total_files, \"All files processed\")\n    await ctx.info(f\"Successfully processed {total_files} files\")\n\n    return results\n</code></pre>"},{"location":"development/fastmcp/context/#4-handle-resource-dependencies","title":"4. Handle Resource Dependencies","text":"<pre><code>@mcp.tool()\nasync def resource_dependent_tool(config_name: str, ctx: Context) -&gt; dict:\n    \"\"\"Tool that properly handles resource dependencies.\"\"\"\n\n    await ctx.info(f\"Loading configuration: {config_name}\")\n\n    try:\n        # Try to read primary config\n        config_data = await ctx.read_resource(f\"config://{config_name}\")\n        await ctx.info(\"Primary configuration loaded\")\n\n    except Exception as e:\n        await ctx.warning(f\"Primary config failed: {e}\")\n\n        try:\n            # Fallback to default config\n            await ctx.info(\"Attempting to load default configuration\")\n            config_data = await ctx.read_resource(\"config://default\")\n            await ctx.info(\"Default configuration loaded as fallback\")\n\n        except Exception as fallback_error:\n            await ctx.error(f\"Both primary and default configs failed: {fallback_error}\")\n            raise ValueError(\"No configuration available\")\n\n    # Process configuration\n    config_content = config_data[0].content\n    await ctx.debug(f\"Configuration content length: {len(config_content)}\")\n\n    return {\n        \"config_name\": config_name,\n        \"loaded\": True,\n        \"content_length\": len(config_content)\n    }\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/","title":"Custom Middleware","text":"<p>Guide to implementing custom middleware patterns in FastMCP servers, including authentication middleware, custom routes, and request/response processing.</p>"},{"location":"development/fastmcp/custom-middleware/#overview","title":"Overview","text":"<p>FastMCP provides authentication middleware out of the box and supports custom HTTP endpoints through custom routes. While there's no direct API for adding arbitrary ASGI middleware, this guide covers available patterns for customizing request/response processing.</p>"},{"location":"development/fastmcp/custom-middleware/#authentication-middleware-built-in","title":"Authentication Middleware (Built-in)","text":"<p>FastMCP includes sophisticated authentication middleware for OAuth2 flows.</p>"},{"location":"development/fastmcp/custom-middleware/#basic-auth-setup","title":"Basic Auth Setup","text":"<pre><code>from mcp.server.fastmcp import FastMCP\nfrom mcp.server.auth import OAuthAuthorizationServerProvider\nfrom mcp.server.auth.middleware import get_access_token\n\n# Create auth provider\nauth_provider = OAuthAuthorizationServerProvider(\n    provider_id=\"github\",\n    client_id=\"your-github-client-id\",\n    client_secret=\"your-github-client-secret\",\n    authorization_endpoint=\"https://github.com/login/oauth/authorize\",\n    token_endpoint=\"https://github.com/login/oauth/access_token\",\n    scopes=[\"user:email\", \"repo\"],\n)\n\n# Create server with authentication\nmcp = FastMCP(\n    \"authenticated-server\",\n    auth_server_provider=auth_provider\n)\n\n@mcp.tool()\nasync def protected_tool() -&gt; str:\n    \"\"\"Tool that requires authentication.\"\"\"\n    # Access authenticated user context\n    access_token = get_access_token()\n    if not access_token:\n        raise ValueError(\"Authentication required\")\n\n    return f\"Hello, authenticated user! Token: {access_token.token}\"\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#custom-auth-provider","title":"Custom Auth Provider","text":"<pre><code>from mcp.server.auth import AuthorizationServerProvider\nfrom mcp.server.auth.handlers import AuthorizeRequest, TokenRequest\n\nclass CustomAuthProvider(AuthorizationServerProvider):\n    \"\"\"Custom authentication provider.\"\"\"\n\n    def __init__(self, api_key_header: str = \"X-API-Key\"):\n        super().__init__(provider_id=\"custom-api-key\")\n        self.api_key_header = api_key_header\n        self.valid_keys = {\"secret-key-1\", \"secret-key-2\"}\n\n    async def authorize(self, request: AuthorizeRequest) -&gt; str:\n        \"\"\"Handle authorization request.\"\"\"\n        # For API key auth, return a simple authorization code\n        return \"auth-code-12345\"\n\n    async def token(self, request: TokenRequest) -&gt; dict:\n        \"\"\"Handle token exchange.\"\"\"\n        # Validate authorization code and return token\n        if request.code == \"auth-code-12345\":\n            return {\n                \"access_token\": \"api-key-token\",\n                \"token_type\": \"bearer\",\n                \"scope\": \"read write\"\n            }\n        raise ValueError(\"Invalid authorization code\")\n\n    async def validate_token(self, token: str) -&gt; dict | None:\n        \"\"\"Validate access token.\"\"\"\n        if token in self.valid_keys:\n            return {\n                \"token\": token,\n                \"scopes\": [\"read\", \"write\"],\n                \"user_id\": f\"user-{hash(token) % 1000}\"\n            }\n        return None\n\n# Use custom auth provider\nmcp = FastMCP(\n    \"api-key-server\",\n    auth_server_provider=CustomAuthProvider()\n)\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#custom-routes-for-http-endpoints","title":"Custom Routes for HTTP Endpoints","text":"<p>Use custom routes to add arbitrary HTTP endpoints with full request/response control.</p>"},{"location":"development/fastmcp/custom-middleware/#basic-custom-routes","title":"Basic Custom Routes","text":"<pre><code>from starlette.requests import Request\nfrom starlette.responses import JSONResponse, PlainTextResponse\nimport time\n\n@mcp.custom_route(\"/health\", methods=[\"GET\"])\nasync def health_check(request: Request) -&gt; JSONResponse:\n    \"\"\"Health check endpoint.\"\"\"\n    return JSONResponse({\n        \"status\": \"healthy\",\n        \"timestamp\": time.time(),\n        \"server\": \"fastmcp-server\"\n    })\n\n@mcp.custom_route(\"/stats\", methods=[\"GET\"])\nasync def server_stats(request: Request) -&gt; JSONResponse:\n    \"\"\"Server statistics endpoint.\"\"\"\n    return JSONResponse({\n        \"tools_count\": len(mcp._tool_manager.tools),\n        \"resources_count\": len(mcp._resource_manager.resources),\n        \"prompts_count\": len(mcp._prompt_manager.prompts),\n        \"uptime_seconds\": time.time() - start_time\n    })\n\n@mcp.custom_route(\"/webhook\", methods=[\"POST\"])\nasync def webhook_handler(request: Request) -&gt; JSONResponse:\n    \"\"\"Webhook endpoint for external integrations.\"\"\"\n    body = await request.json()\n\n    # Process webhook data\n    event_type = body.get(\"type\")\n    if event_type == \"user_update\":\n        await handle_user_update(body[\"data\"])\n    elif event_type == \"system_notification\":\n        await handle_system_notification(body[\"data\"])\n\n    return JSONResponse({\"status\": \"processed\"})\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#custom-routes-with-authentication","title":"Custom Routes with Authentication","text":"<pre><code>from mcp.server.auth.middleware import get_access_token\nfrom starlette.exceptions import HTTPException\n\n@mcp.custom_route(\"/protected-data\", methods=[\"GET\"])\nasync def protected_data(request: Request) -&gt; JSONResponse:\n    \"\"\"Protected endpoint requiring authentication.\"\"\"\n    # Check authentication in custom route\n    access_token = get_access_token()\n    if not access_token:\n        raise HTTPException(status_code=401, detail=\"Authentication required\")\n\n    # Verify required scopes\n    if \"read\" not in access_token.scopes:\n        raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n\n    return JSONResponse({\n        \"user_id\": access_token.user_id,\n        \"data\": \"sensitive information\",\n        \"scopes\": access_token.scopes\n    })\n\n@mcp.custom_route(\"/admin\", methods=[\"GET\", \"POST\"])\nasync def admin_panel(request: Request) -&gt; JSONResponse:\n    \"\"\"Admin endpoint with scope checking.\"\"\"\n    access_token = get_access_token()\n    if not access_token or \"admin\" not in access_token.scopes:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n\n    if request.method == \"GET\":\n        return JSONResponse({\"admin_data\": \"admin interface\"})\n\n    # Handle admin actions\n    body = await request.json()\n    action = body.get(\"action\")\n\n    if action == \"refresh_tools\":\n        # Refresh tool registration\n        mcp._tool_manager.refresh()\n        return JSONResponse({\"status\": \"tools refreshed\"})\n\n    return JSONResponse({\"status\": \"unknown action\"})\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#requestresponse-processing-patterns","title":"Request/Response Processing Patterns","text":"<p>While FastMCP doesn't provide direct middleware hooks, you can implement processing patterns using custom routes and context managers.</p>"},{"location":"development/fastmcp/custom-middleware/#request-logging-pattern","title":"Request Logging Pattern","text":"<pre><code>import logging\nimport time\nfrom contextlib import asynccontextmanager\nfrom starlette.responses import Response\n\nlogger = logging.getLogger(__name__)\n\n@asynccontextmanager\nasync def request_logger(request: Request):\n    \"\"\"Context manager for request logging.\"\"\"\n    start_time = time.time()\n    request_id = f\"req-{int(start_time * 1000)}\"\n\n    # Log request start\n    logger.info(f\"[{request_id}] {request.method} {request.url.path}\")\n\n    try:\n        yield request_id\n    finally:\n        # Log request completion\n        duration = time.time() - start_time\n        logger.info(f\"[{request_id}] Completed in {duration:.3f}s\")\n\n@mcp.custom_route(\"/api/data\", methods=[\"GET\", \"POST\"])\nasync def data_endpoint(request: Request) -&gt; JSONResponse:\n    \"\"\"Endpoint with request logging.\"\"\"\n    async with request_logger(request) as request_id:\n        # Process request\n        if request.method == \"GET\":\n            data = await fetch_data()\n        else:\n            body = await request.json()\n            data = await process_data(body)\n\n        logger.info(f\"[{request_id}] Processed {len(data)} items\")\n        return JSONResponse({\"data\": data, \"request_id\": request_id})\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#rate-limiting-pattern","title":"Rate Limiting Pattern","text":"<pre><code>import asyncio\nfrom collections import defaultdict\nfrom starlette.exceptions import HTTPException\n\nclass RateLimiter:\n    \"\"\"Simple in-memory rate limiter.\"\"\"\n\n    def __init__(self, max_requests: int = 100, window_seconds: int = 60):\n        self.max_requests = max_requests\n        self.window_seconds = window_seconds\n        self.requests = defaultdict(list)\n\n    async def check_rate_limit(self, identifier: str) -&gt; bool:\n        \"\"\"Check if request is within rate limit.\"\"\"\n        now = time.time()\n        window_start = now - self.window_seconds\n\n        # Clean old requests\n        self.requests[identifier] = [\n            req_time for req_time in self.requests[identifier]\n            if req_time &gt; window_start\n        ]\n\n        # Check current count\n        if len(self.requests[identifier]) &gt;= self.max_requests:\n            return False\n\n        # Record this request\n        self.requests[identifier].append(now)\n        return True\n\nrate_limiter = RateLimiter(max_requests=50, window_seconds=60)\n\n@mcp.custom_route(\"/api/limited\", methods=[\"GET\"])\nasync def rate_limited_endpoint(request: Request) -&gt; JSONResponse:\n    \"\"\"Endpoint with rate limiting.\"\"\"\n    # Use IP address as identifier\n    client_ip = request.client.host if request.client else \"unknown\"\n\n    if not await rate_limiter.check_rate_limit(client_ip):\n        raise HTTPException(\n            status_code=429,\n            detail=\"Rate limit exceeded. Try again later.\"\n        )\n\n    return JSONResponse({\"message\": \"Request processed\", \"ip\": client_ip})\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#response-caching-pattern","title":"Response Caching Pattern","text":"<pre><code>import json\nimport hashlib\nfrom typing import Any\n\nclass ResponseCache:\n    \"\"\"Simple in-memory response cache.\"\"\"\n\n    def __init__(self, ttl_seconds: int = 300):\n        self.ttl_seconds = ttl_seconds\n        self.cache = {}\n\n    def _cache_key(self, request: Request) -&gt; str:\n        \"\"\"Generate cache key from request.\"\"\"\n        key_data = f\"{request.method}:{request.url.path}:{request.url.query}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    async def get(self, request: Request) -&gt; JSONResponse | None:\n        \"\"\"Get cached response if available.\"\"\"\n        cache_key = self._cache_key(request)\n\n        if cache_key in self.cache:\n            cached_data, timestamp = self.cache[cache_key]\n            if time.time() - timestamp &lt; self.ttl_seconds:\n                return JSONResponse(cached_data)\n            else:\n                # Expired, remove from cache\n                del self.cache[cache_key]\n\n        return None\n\n    async def set(self, request: Request, data: Any) -&gt; None:\n        \"\"\"Cache response data.\"\"\"\n        cache_key = self._cache_key(request)\n        self.cache[cache_key] = (data, time.time())\n\nresponse_cache = ResponseCache(ttl_seconds=300)\n\n@mcp.custom_route(\"/api/cached\", methods=[\"GET\"])\nasync def cached_endpoint(request: Request) -&gt; JSONResponse:\n    \"\"\"Endpoint with response caching.\"\"\"\n    # Check cache first\n    cached_response = await response_cache.get(request)\n    if cached_response:\n        # Add cache hit header\n        cached_response.headers[\"X-Cache\"] = \"HIT\"\n        return cached_response\n\n    # Generate fresh response\n    data = await expensive_computation()\n\n    # Cache the response\n    await response_cache.set(request, data)\n\n    response = JSONResponse(data)\n    response.headers[\"X-Cache\"] = \"MISS\"\n    return response\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#context-injection-in-tools","title":"Context Injection in Tools","text":"<p>Access HTTP request context within MCP tools when using HTTP transports.</p>"},{"location":"development/fastmcp/custom-middleware/#request-context-access","title":"Request Context Access","text":"<pre><code>from mcp.server.fastmcp import Context\nfrom starlette.requests import Request\n\n@mcp.tool()\nasync def request_aware_tool(message: str, ctx: Context) -&gt; str:\n    \"\"\"Tool that accesses HTTP request context.\"\"\"\n    # Get request information if available\n    request_info = {}\n\n    # Access session information\n    if hasattr(ctx.session, 'request_context'):\n        request_context = ctx.session.request_context\n        request_info.update({\n            \"session_id\": getattr(request_context, 'session_id', 'unknown'),\n            \"user_agent\": getattr(request_context, 'user_agent', 'unknown'),\n            \"remote_addr\": getattr(request_context, 'remote_addr', 'unknown')\n        })\n\n    # Access authentication context\n    access_token = get_access_token()\n    if access_token:\n        request_info[\"authenticated_user\"] = access_token.user_id\n        request_info[\"scopes\"] = access_token.scopes\n\n    await ctx.info(f\"Processing request with context: {request_info}\")\n\n    return f\"Processed '{message}' with context: {request_info}\"\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"development/fastmcp/custom-middleware/#custom-session-management","title":"Custom Session Management","text":"<pre><code>from mcp.server.streamable_http_manager import SessionManager\nfrom contextlib import asynccontextmanager\n\nclass CustomSessionManager(SessionManager):\n    \"\"\"Custom session manager with additional middleware.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.session_metadata = {}\n\n    @asynccontextmanager\n    async def handle_request(self, session_id: str, request_data: dict):\n        \"\"\"Override request handling with custom logic.\"\"\"\n        # Pre-request processing\n        start_time = time.time()\n        self.session_metadata[session_id] = {\n            \"start_time\": start_time,\n            \"request_count\": self.session_metadata.get(session_id, {}).get(\"request_count\", 0) + 1\n        }\n\n        try:\n            # Call parent implementation\n            async with super().handle_request(session_id, request_data) as result:\n                yield result\n        finally:\n            # Post-request processing\n            duration = time.time() - start_time\n            metadata = self.session_metadata.get(session_id, {})\n            metadata[\"last_duration\"] = duration\n\n            logger.info(f\"Session {session_id}: request #{metadata['request_count']} took {duration:.3f}s\")\n\n# Use custom session manager (advanced - requires FastMCP extension)\n# mcp = FastMCP(\"custom-server\", session_manager_class=CustomSessionManager)\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#middleware-like-decorators","title":"Middleware-like Decorators","text":"<pre><code>from functools import wraps\nfrom typing import Callable, Any\n\ndef with_timing(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to add timing to custom routes.\"\"\"\n    @wraps(func)\n    async def wrapper(request: Request) -&gt; Any:\n        start_time = time.time()\n        try:\n            result = await func(request)\n            return result\n        finally:\n            duration = time.time() - start_time\n            logger.info(f\"{func.__name__} took {duration:.3f}s\")\n    return wrapper\n\ndef with_auth_required(scopes: list[str] = None):\n    \"\"\"Decorator to require authentication on custom routes.\"\"\"\n    def decorator(func: Callable) -&gt; Callable:\n        @wraps(func)\n        async def wrapper(request: Request) -&gt; Any:\n            access_token = get_access_token()\n            if not access_token:\n                raise HTTPException(status_code=401, detail=\"Authentication required\")\n\n            if scopes:\n                if not all(scope in access_token.scopes for scope in scopes):\n                    raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n\n            return await func(request)\n        return wrapper\n    return decorator\n\n# Usage with decorators\n@mcp.custom_route(\"/admin/users\", methods=[\"GET\"])\n@with_timing\n@with_auth_required(scopes=[\"admin\", \"read\"])\nasync def list_users(request: Request) -&gt; JSONResponse:\n    \"\"\"Protected admin endpoint with timing.\"\"\"\n    users = await fetch_all_users()\n    return JSONResponse({\"users\": users})\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/custom-middleware/#security-considerations","title":"Security Considerations","text":"<ol> <li>Always validate authentication in protected endpoints</li> <li>Check scopes for fine-grained access control</li> <li>Sanitize input from custom routes</li> <li>Use HTTPS in production for authentication flows</li> <li>Implement rate limiting for public endpoints</li> <li>Log security events (auth failures, rate limit hits)</li> </ol>"},{"location":"development/fastmcp/custom-middleware/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Cache expensive operations in custom routes</li> <li>Use async patterns for I/O operations</li> <li>Implement connection pooling for external services</li> <li>Monitor response times and set reasonable timeouts</li> <li>Use streaming for large response payloads</li> </ol>"},{"location":"development/fastmcp/custom-middleware/#error-handling","title":"Error Handling","text":"<pre><code>from starlette.exceptions import HTTPException\nfrom starlette.responses import JSONResponse\n\n@mcp.custom_route(\"/api/robust\", methods=[\"POST\"])\nasync def robust_endpoint(request: Request) -&gt; JSONResponse:\n    \"\"\"Endpoint with comprehensive error handling.\"\"\"\n    try:\n        # Validate content type\n        if request.headers.get(\"content-type\") != \"application/json\":\n            raise HTTPException(status_code=400, detail=\"Content-Type must be application/json\")\n\n        # Parse and validate body\n        try:\n            body = await request.json()\n        except ValueError as e:\n            raise HTTPException(status_code=400, detail=f\"Invalid JSON: {e}\")\n\n        # Validate required fields\n        required_fields = [\"action\", \"data\"]\n        missing_fields = [field for field in required_fields if field not in body]\n        if missing_fields:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Missing required fields: {missing_fields}\"\n            )\n\n        # Process request\n        result = await process_action(body[\"action\"], body[\"data\"])\n\n        return JSONResponse({\n            \"status\": \"success\",\n            \"result\": result,\n            \"timestamp\": time.time()\n        })\n\n    except HTTPException:\n        # Re-raise HTTP exceptions\n        raise\n    except Exception as e:\n        # Log unexpected errors\n        logger.exception(f\"Unexpected error in robust_endpoint: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n</code></pre>"},{"location":"development/fastmcp/custom-middleware/#limitations","title":"Limitations","text":"<ol> <li>No Direct Middleware API: Cannot add arbitrary ASGI middleware to the FastMCP server</li> <li>Authentication Only: Built-in middleware only supports authentication patterns</li> <li>Limited Request Context: HTTP request details not directly available in MCP tools</li> <li>Transport Specific: Middleware patterns only apply to HTTP transports (SSE, StreamableHTTP)</li> </ol>"},{"location":"development/fastmcp/custom-middleware/#alternative-approaches","title":"Alternative Approaches","text":"<p>For use cases requiring extensive middleware:</p> <ol> <li>Use Starlette directly with custom MCP integration</li> <li>Proxy pattern: Put a reverse proxy (nginx, Traefik) in front for middleware-like functionality</li> <li>Custom transport: Implement a custom transport with desired middleware capabilities</li> <li>Wrapper pattern: Wrap FastMCP server in a custom ASGI application</li> </ol> <p>This guide covers the available patterns for implementing middleware-like functionality in FastMCP. While not as flexible as full ASGI middleware, these patterns provide powerful customization capabilities for most use cases.</p>"},{"location":"development/fastmcp/custom-routes/","title":"Macro Syntax Error","text":"<p>File: <code>development/fastmcp/custom-routes.md</code></p> <p>Line 371 in Markdown file: expected token 'end of print statement', got ':' <pre><code>                yield f'{{\"id\": {i}, \"name\": \"User{i}\", \"email\": \"user{i}@example.com\"}}'\n</code></pre></p>"},{"location":"development/fastmcp/database-integration/","title":"Database Integration","text":"<p>Comprehensive guide to integrating databases with FastMCP servers, covering SQL and NoSQL databases, connection pooling, ORM patterns, and production-ready database management.</p>"},{"location":"development/fastmcp/database-integration/#overview","title":"Overview","text":"<p>FastMCP provides excellent database integration capabilities through its lifespan management system, dependency injection via Context, and support for all major Python async database libraries. This guide covers patterns for PostgreSQL, MySQL, SQLite, MongoDB, and Redis integration with best practices for production deployments.</p>"},{"location":"development/fastmcp/database-integration/#database-lifecycle-management","title":"Database Lifecycle Management","text":""},{"location":"development/fastmcp/database-integration/#basic-database-integration-pattern","title":"Basic Database Integration Pattern","text":"<p>FastMCP's lifespan management is perfect for database connections:</p> <pre><code>from contextlib import asynccontextmanager\nfrom mcp.server.fastmcp import FastMCP, Context\nimport asyncpg\n\n@asynccontextmanager\nasync def database_lifespan(app: FastMCP):\n    \"\"\"Manage database connection lifecycle.\"\"\"\n    # Initialize database pool\n    pool = await asyncpg.create_pool(\n        \"postgresql://user:password@localhost:5432/mydb\",\n        min_size=5,\n        max_size=20,\n        command_timeout=60\n    )\n\n    # Store pool in app for access in tools\n    app.db_pool = pool\n\n    try:\n        yield {\"db_pool\": pool}\n    finally:\n        # Cleanup on shutdown\n        await pool.close()\n\n# Create FastMCP server with database lifespan\nmcp = FastMCP(\"database-server\", lifespan=database_lifespan)\n\n@mcp.tool()\nasync def get_users(limit: int = 10, ctx: Context) -&gt; list[dict]:\n    \"\"\"Get users from database.\"\"\"\n    async with mcp.db_pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"SELECT id, name, email FROM users LIMIT $1\",\n            limit\n        )\n\n        users = [dict(row) for row in rows]\n        await ctx.info(f\"Retrieved {len(users)} users\")\n        return users\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/database-integration/#postgresql-integration","title":"PostgreSQL Integration","text":""},{"location":"development/fastmcp/database-integration/#asyncpg-with-connection-pooling","title":"AsyncPG with Connection Pooling","text":"<p>AsyncPG provides high-performance PostgreSQL access with connection pooling:</p> <pre><code>import asyncpg\nfrom contextlib import asynccontextmanager\nfrom pydantic import BaseModel\nfrom typing import Optional\nimport os\n\nclass UserCreate(BaseModel):\n    name: str\n    email: str\n    age: Optional[int] = None\n\nclass UserResponse(BaseModel):\n    id: int\n    name: str\n    email: str\n    age: Optional[int]\n\n@asynccontextmanager\nasync def postgres_lifespan(app: FastMCP):\n    \"\"\"PostgreSQL connection management with schema setup.\"\"\"\n    DATABASE_URL = os.getenv(\n        \"DATABASE_URL\",\n        \"postgresql://postgres:password@localhost:5432/fastmcp\"\n    )\n\n    # Create connection pool\n    pool = await asyncpg.create_pool(\n        DATABASE_URL,\n        min_size=5,\n        max_size=20,\n        command_timeout=60,\n        server_settings={\n            'application_name': 'fastmcp-server',\n            'jit': 'off'  # Disable JIT for faster connection\n        }\n    )\n\n    # Initialize database schema\n    async with pool.acquire() as conn:\n        await conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS users (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(100) NOT NULL,\n                email VARCHAR(100) UNIQUE NOT NULL,\n                age INTEGER CHECK (age &gt;= 0 AND age &lt;= 150),\n                created_at TIMESTAMP DEFAULT NOW(),\n                updated_at TIMESTAMP DEFAULT NOW()\n            );\n\n            CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);\n            CREATE INDEX IF NOT EXISTS idx_users_created_at ON users(created_at);\n        \"\"\")\n\n    app.db_pool = pool\n\n    try:\n        yield {\"db_pool\": pool}\n    finally:\n        await pool.close()\n\nmcp = FastMCP(\"postgres-server\", lifespan=postgres_lifespan)\n\n@mcp.tool()\nasync def create_user(user_data: UserCreate, ctx: Context) -&gt; UserResponse:\n    \"\"\"Create a new user with transaction safety.\"\"\"\n    async with mcp.db_pool.acquire() as conn:\n        async with conn.transaction():\n            try:\n                row = await conn.fetchrow(\n                    \"\"\"\n                    INSERT INTO users (name, email, age)\n                    VALUES ($1, $2, $3)\n                    RETURNING id, name, email, age\n                    \"\"\",\n                    user_data.name, user_data.email, user_data.age\n                )\n\n                user = UserResponse(**dict(row))\n                await ctx.info(f\"Created user: {user.name} (ID: {user.id})\")\n                return user\n\n            except asyncpg.UniqueViolationError:\n                await ctx.error(f\"Email {user_data.email} already exists\")\n                raise ValueError(\"Email already exists\")\n            except asyncpg.CheckViolationError as e:\n                await ctx.error(f\"Data validation failed: {e}\")\n                raise ValueError(\"Invalid user data\")\n\n@mcp.tool()\nasync def search_users(\n    search_term: str,\n    limit: int = 20,\n    offset: int = 0,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Search users with pagination.\"\"\"\n    async with mcp.db_pool.acquire() as conn:\n        # Get total count\n        total = await conn.fetchval(\n            \"SELECT COUNT(*) FROM users WHERE name ILIKE $1 OR email ILIKE $1\",\n            f\"%{search_term}%\"\n        )\n\n        # Get paginated results\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT id, name, email, age, created_at\n            FROM users\n            WHERE name ILIKE $1 OR email ILIKE $1\n            ORDER BY created_at DESC\n            LIMIT $2 OFFSET $3\n            \"\"\",\n            f\"%{search_term}%\", limit, offset\n        )\n\n        users = [dict(row) for row in rows]\n\n        await ctx.info(f\"Found {len(users)} users (total: {total})\")\n\n        return {\n            \"users\": users,\n            \"pagination\": {\n                \"total\": total,\n                \"limit\": limit,\n                \"offset\": offset,\n                \"has_more\": offset + len(users) &lt; total\n            }\n        }\n\n@mcp.tool()\nasync def update_user(\n    user_id: int,\n    updates: dict,\n    ctx: Context\n) -&gt; UserResponse:\n    \"\"\"Update user with dynamic fields.\"\"\"\n    allowed_fields = {\"name\", \"email\", \"age\"}\n    update_fields = {k: v for k, v in updates.items() if k in allowed_fields}\n\n    if not update_fields:\n        raise ValueError(\"No valid fields to update\")\n\n    # Build dynamic UPDATE query\n    set_clauses = [f\"{field} = ${i+2}\" for i, field in enumerate(update_fields.keys())]\n    query = f\"\"\"\n        UPDATE users\n        SET {', '.join(set_clauses)}, updated_at = NOW()\n        WHERE id = $1\n        RETURNING id, name, email, age\n    \"\"\"\n\n    async with mcp.db_pool.acquire() as conn:\n        async with conn.transaction():\n            try:\n                row = await conn.fetchrow(query, user_id, *update_fields.values())\n\n                if not row:\n                    raise ValueError(f\"User {user_id} not found\")\n\n                user = UserResponse(**dict(row))\n                await ctx.info(f\"Updated user {user_id}: {list(update_fields.keys())}\")\n                return user\n\n            except asyncpg.UniqueViolationError:\n                await ctx.error(f\"Email conflict during update\")\n                raise ValueError(\"Email already exists\")\n\n@mcp.tool()\nasync def delete_user(user_id: int, ctx: Context) -&gt; dict:\n    \"\"\"Delete user with soft delete option.\"\"\"\n    async with mcp.db_pool.acquire() as conn:\n        # Check if user exists first\n        user = await conn.fetchrow(\"SELECT name FROM users WHERE id = $1\", user_id)\n\n        if not user:\n            await ctx.warning(f\"User {user_id} not found\")\n            return {\"success\": False, \"message\": \"User not found\"}\n\n        # Delete user\n        result = await conn.execute(\"DELETE FROM users WHERE id = $1\", user_id)\n\n        if result == \"DELETE 1\":\n            await ctx.info(f\"Deleted user {user_id} ({user['name']})\")\n            return {\"success\": True, \"message\": f\"User {user['name']} deleted\"}\n        else:\n            return {\"success\": False, \"message\": \"Delete failed\"}\n</code></pre>"},{"location":"development/fastmcp/database-integration/#sqlalchemy-orm-integration","title":"SQLAlchemy ORM Integration","text":"<p>For complex applications requiring ORM features:</p> <pre><code>from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship\nfrom sqlalchemy import String, Integer, DateTime, ForeignKey, func, select, update, delete\nfrom datetime import datetime\nfrom typing import List\n\nclass Base(DeclarativeBase):\n    \"\"\"Base class for all database models.\"\"\"\n    pass\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n    name: Mapped[str] = mapped_column(String(100), nullable=False)\n    email: Mapped[str] = mapped_column(String(100), unique=True, nullable=False)\n    created_at: Mapped[datetime] = mapped_column(DateTime, default=func.now())\n\n    # Relationship to posts\n    posts: Mapped[List[\"Post\"]] = relationship(\"Post\", back_populates=\"author\")\n\nclass Post(Base):\n    __tablename__ = \"posts\"\n\n    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n    title: Mapped[str] = mapped_column(String(200), nullable=False)\n    content: Mapped[str] = mapped_column(String, nullable=False)\n    author_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\"))\n    created_at: Mapped[datetime] = mapped_column(DateTime, default=func.now())\n\n    # Relationship to user\n    author: Mapped[\"User\"] = relationship(\"User\", back_populates=\"posts\")\n\n@asynccontextmanager\nasync def sqlalchemy_lifespan(app: FastMCP):\n    \"\"\"SQLAlchemy async engine and session management.\"\"\"\n    DATABASE_URL = os.getenv(\n        \"DATABASE_URL\",\n        \"postgresql+asyncpg://postgres:password@localhost:5432/fastmcp\"\n    )\n\n    # Create async engine\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,\n        max_overflow=20,\n        pool_pre_ping=True,  # Validate connections\n        echo=os.getenv(\"DEBUG\") == \"true\"  # SQL logging in debug mode\n    )\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    # Create session factory\n    app.async_session = async_sessionmaker(\n        engine,\n        class_=AsyncSession,\n        expire_on_commit=False\n    )\n\n    try:\n        yield {\"engine\": engine}\n    finally:\n        await engine.dispose()\n\nmcp = FastMCP(\"sqlalchemy-server\", lifespan=sqlalchemy_lifespan)\n\n@mcp.tool()\nasync def create_user_with_posts(\n    name: str,\n    email: str,\n    posts: List[dict],\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Create user with initial posts using SQLAlchemy ORM.\"\"\"\n    async with mcp.async_session() as session:\n        async with session.begin():\n            try:\n                # Create user\n                user = User(name=name, email=email)\n                session.add(user)\n                await session.flush()  # Get user ID\n\n                # Create posts\n                post_objects = []\n                for post_data in posts:\n                    post = Post(\n                        title=post_data[\"title\"],\n                        content=post_data[\"content\"],\n                        author_id=user.id\n                    )\n                    session.add(post)\n                    post_objects.append(post)\n\n                await session.commit()\n\n                await ctx.info(f\"Created user {name} with {len(posts)} posts\")\n\n                return {\n                    \"user\": {\n                        \"id\": user.id,\n                        \"name\": user.name,\n                        \"email\": user.email,\n                        \"created_at\": user.created_at.isoformat()\n                    },\n                    \"posts_created\": len(posts)\n                }\n\n            except Exception as e:\n                await session.rollback()\n                await ctx.error(f\"Failed to create user: {e}\")\n                raise\n\n@mcp.tool()\nasync def get_user_with_posts(user_id: int, ctx: Context) -&gt; dict:\n    \"\"\"Get user with their posts using eager loading.\"\"\"\n    async with mcp.async_session() as session:\n        # Eager load posts with user\n        stmt = (\n            select(User)\n            .where(User.id == user_id)\n            .options(selectinload(User.posts))\n        )\n\n        result = await session.execute(stmt)\n        user = result.scalar_one_or_none()\n\n        if not user:\n            await ctx.warning(f\"User {user_id} not found\")\n            return {\"error\": \"User not found\"}\n\n        user_data = {\n            \"id\": user.id,\n            \"name\": user.name,\n            \"email\": user.email,\n            \"created_at\": user.created_at.isoformat(),\n            \"posts\": [\n                {\n                    \"id\": post.id,\n                    \"title\": post.title,\n                    \"content\": post.content,\n                    \"created_at\": post.created_at.isoformat()\n                }\n                for post in user.posts\n            ]\n        }\n\n        await ctx.info(f\"Retrieved user {user.name} with {len(user.posts)} posts\")\n        return user_data\n\n@mcp.tool()\nasync def bulk_update_posts(\n    author_id: int,\n    updates: dict,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Bulk update posts for an author.\"\"\"\n    async with mcp.async_session() as session:\n        async with session.begin():\n            # Update all posts by author\n            stmt = (\n                update(Post)\n                .where(Post.author_id == author_id)\n                .values(**updates)\n                .returning(Post.id)\n            )\n\n            result = await session.execute(stmt)\n            updated_ids = [row[0] for row in result.fetchall()]\n\n            await ctx.info(f\"Updated {len(updated_ids)} posts for author {author_id}\")\n\n            return {\n                \"updated_count\": len(updated_ids),\n                \"updated_post_ids\": updated_ids,\n                \"updates_applied\": updates\n            }\n</code></pre>"},{"location":"development/fastmcp/database-integration/#mysql-integration","title":"MySQL Integration","text":""},{"location":"development/fastmcp/database-integration/#aiomysql-with-connection-pooling","title":"aiomysql with Connection Pooling","text":"<pre><code>import aiomysql\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def mysql_lifespan(app: FastMCP):\n    \"\"\"MySQL connection pool management.\"\"\"\n    pool = await aiomysql.create_pool(\n        host='localhost',\n        port=3306,\n        user='mysql_user',\n        password='mysql_password',\n        db='fastmcp_db',\n        minsize=5,\n        maxsize=20,\n        charset='utf8mb4',\n        autocommit=False\n    )\n\n    # Initialize schema\n    async with pool.acquire() as conn:\n        async with conn.cursor() as cursor:\n            await cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS products (\n                    id INT AUTO_INCREMENT PRIMARY KEY,\n                    name VARCHAR(200) NOT NULL,\n                    price DECIMAL(10,2) NOT NULL,\n                    category VARCHAR(100),\n                    in_stock BOOLEAN DEFAULT TRUE,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    INDEX idx_category (category),\n                    INDEX idx_price (price)\n                ) ENGINE=InnoDB\n            \"\"\")\n            await conn.commit()\n\n    app.mysql_pool = pool\n\n    try:\n        yield {\"mysql_pool\": pool}\n    finally:\n        pool.close()\n        await pool.wait_closed()\n\nmcp = FastMCP(\"mysql-server\", lifespan=mysql_lifespan)\n\n@mcp.tool()\nasync def add_product(\n    name: str,\n    price: float,\n    category: str,\n    in_stock: bool = True,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Add product to MySQL database.\"\"\"\n    async with mcp.mysql_pool.acquire() as conn:\n        async with conn.cursor(aiomysql.DictCursor) as cursor:\n            try:\n                await cursor.execute(\n                    \"\"\"\n                    INSERT INTO products (name, price, category, in_stock)\n                    VALUES (%s, %s, %s, %s)\n                    \"\"\",\n                    (name, price, category, in_stock)\n                )\n\n                product_id = cursor.lastrowid\n                await conn.commit()\n\n                await ctx.info(f\"Added product: {name} (ID: {product_id})\")\n\n                return {\n                    \"id\": product_id,\n                    \"name\": name,\n                    \"price\": price,\n                    \"category\": category,\n                    \"in_stock\": in_stock\n                }\n\n            except Exception as e:\n                await conn.rollback()\n                await ctx.error(f\"Failed to add product: {e}\")\n                raise\n\n@mcp.tool()\nasync def search_products(\n    category: str = None,\n    min_price: float = None,\n    max_price: float = None,\n    in_stock_only: bool = True,\n    ctx: Context\n) -&gt; list[dict]:\n    \"\"\"Search products with filters.\"\"\"\n    async with mcp.mysql_pool.acquire() as conn:\n        async with conn.cursor(aiomysql.DictCursor) as cursor:\n            # Build dynamic query\n            conditions = []\n            params = []\n\n            if category:\n                conditions.append(\"category = %s\")\n                params.append(category)\n\n            if min_price is not None:\n                conditions.append(\"price &gt;= %s\")\n                params.append(min_price)\n\n            if max_price is not None:\n                conditions.append(\"price &lt;= %s\")\n                params.append(max_price)\n\n            if in_stock_only:\n                conditions.append(\"in_stock = TRUE\")\n\n            where_clause = \"WHERE \" + \" AND \".join(conditions) if conditions else \"\"\n\n            query = f\"\"\"\n                SELECT id, name, price, category, in_stock, created_at\n                FROM products\n                {where_clause}\n                ORDER BY created_at DESC\n                LIMIT 100\n            \"\"\"\n\n            await cursor.execute(query, params)\n            products = await cursor.fetchall()\n\n            await ctx.info(f\"Found {len(products)} products\")\n            return products\n</code></pre>"},{"location":"development/fastmcp/database-integration/#sqlite-integration","title":"SQLite Integration","text":"<p>Lightweight database perfect for development and small applications:</p> <pre><code>import aiosqlite\nfrom pathlib import Path\n\n@asynccontextmanager\nasync def sqlite_lifespan(app: FastMCP):\n    \"\"\"SQLite database management.\"\"\"\n    db_path = Path(\"fastmcp.db\")\n\n    # Connect to SQLite\n    conn = await aiosqlite.connect(str(db_path))\n\n    # Enable foreign keys and WAL mode\n    await conn.execute(\"PRAGMA foreign_keys = ON\")\n    await conn.execute(\"PRAGMA journal_mode = WAL\")\n\n    # Create schema\n    await conn.executescript(\"\"\"\n        CREATE TABLE IF NOT EXISTS tasks (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            title TEXT NOT NULL,\n            description TEXT,\n            status TEXT DEFAULT 'pending',\n            priority INTEGER DEFAULT 1,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        );\n\n        CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status);\n        CREATE INDEX IF NOT EXISTS idx_tasks_priority ON tasks(priority);\n    \"\"\")\n\n    await conn.commit()\n    app.sqlite_conn = conn\n\n    try:\n        yield {\"sqlite_conn\": conn}\n    finally:\n        await conn.close()\n\nmcp = FastMCP(\"sqlite-server\", lifespan=sqlite_lifespan)\n\n@mcp.tool()\nasync def create_task(\n    title: str,\n    description: str = \"\",\n    priority: int = 1,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Create a new task.\"\"\"\n    async with mcp.sqlite_conn.execute(\n        \"\"\"\n        INSERT INTO tasks (title, description, priority)\n        VALUES (?, ?, ?)\n        \"\"\",\n        (title, description, priority)\n    ) as cursor:\n        task_id = cursor.lastrowid\n\n    await mcp.sqlite_conn.commit()\n\n    await ctx.info(f\"Created task: {title} (ID: {task_id})\")\n\n    return {\n        \"id\": task_id,\n        \"title\": title,\n        \"description\": description,\n        \"priority\": priority,\n        \"status\": \"pending\"\n    }\n\n@mcp.tool()\nasync def get_tasks(\n    status: str = None,\n    priority: int = None,\n    limit: int = 50,\n    ctx: Context\n) -&gt; list[dict]:\n    \"\"\"Get tasks with optional filtering.\"\"\"\n    query = \"SELECT * FROM tasks\"\n    params = []\n    conditions = []\n\n    if status:\n        conditions.append(\"status = ?\")\n        params.append(status)\n\n    if priority:\n        conditions.append(\"priority = ?\")\n        params.append(priority)\n\n    if conditions:\n        query += \" WHERE \" + \" AND \".join(conditions)\n\n    query += \" ORDER BY priority DESC, created_at DESC LIMIT ?\"\n    params.append(limit)\n\n    async with mcp.sqlite_conn.execute(query, params) as cursor:\n        rows = await cursor.fetchall()\n\n        # Convert rows to dictionaries\n        columns = [description[0] for description in cursor.description]\n        tasks = [dict(zip(columns, row)) for row in rows]\n\n    await ctx.info(f\"Retrieved {len(tasks)} tasks\")\n    return tasks\n</code></pre>"},{"location":"development/fastmcp/database-integration/#nosql-database-integration","title":"NoSQL Database Integration","text":""},{"location":"development/fastmcp/database-integration/#mongodb-with-motor","title":"MongoDB with Motor","text":"<pre><code>from motor.motor_asyncio import AsyncIOMotorClient\nfrom bson import ObjectId\nfrom datetime import datetime\n\n@asynccontextmanager\nasync def mongodb_lifespan(app: FastMCP):\n    \"\"\"MongoDB connection management.\"\"\"\n    client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\n    db = client.fastmcp_db\n\n    # Create indexes\n    await db.users.create_index(\"email\", unique=True)\n    await db.users.create_index(\"created_at\")\n    await db.posts.create_index([(\"author_id\", 1), (\"created_at\", -1)])\n\n    app.mongodb = db\n\n    try:\n        yield {\"mongodb\": db}\n    finally:\n        client.close()\n\nmcp = FastMCP(\"mongodb-server\", lifespan=mongodb_lifespan)\n\n@mcp.tool()\nasync def create_user_mongo(\n    name: str,\n    email: str,\n    metadata: dict = None,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Create user in MongoDB.\"\"\"\n    user_doc = {\n        \"name\": name,\n        \"email\": email,\n        \"created_at\": datetime.utcnow(),\n        \"metadata\": metadata or {}\n    }\n\n    try:\n        result = await mcp.mongodb.users.insert_one(user_doc)\n\n        await ctx.info(f\"Created MongoDB user: {name}\")\n\n        return {\n            \"id\": str(result.inserted_id),\n            \"name\": name,\n            \"email\": email,\n            \"created_at\": user_doc[\"created_at\"].isoformat()\n        }\n\n    except Exception as e:\n        await ctx.error(f\"Failed to create user: {e}\")\n        raise\n\n@mcp.tool()\nasync def find_users_mongo(\n    query: dict = None,\n    limit: int = 20,\n    sort_by: str = \"created_at\",\n    sort_order: int = -1,\n    ctx: Context\n) -&gt; list[dict]:\n    \"\"\"Find users in MongoDB with flexible querying.\"\"\"\n    query = query or {}\n\n    cursor = (\n        mcp.mongodb.users\n        .find(query)\n        .sort(sort_by, sort_order)\n        .limit(limit)\n    )\n\n    users = []\n    async for doc in cursor:\n        doc['_id'] = str(doc['_id'])  # Convert ObjectId to string\n        if 'created_at' in doc:\n            doc['created_at'] = doc['created_at'].isoformat()\n        users.append(doc)\n\n    await ctx.info(f\"Found {len(users)} users matching query\")\n    return users\n\n@mcp.tool()\nasync def aggregate_user_stats(ctx: Context) -&gt; dict:\n    \"\"\"Get user statistics using MongoDB aggregation.\"\"\"\n    pipeline = [\n        {\n            \"$group\": {\n                \"_id\": None,\n                \"total_users\": {\"$sum\": 1},\n                \"avg_metadata_fields\": {\"$avg\": {\"$size\": {\"$objectToArray\": \"$metadata\"}}},\n                \"oldest_user\": {\"$min\": \"$created_at\"},\n                \"newest_user\": {\"$max\": \"$created_at\"}\n            }\n        }\n    ]\n\n    async for result in mcp.mongodb.users.aggregate(pipeline):\n        stats = {\n            \"total_users\": result[\"total_users\"],\n            \"avg_metadata_fields\": result[\"avg_metadata_fields\"],\n            \"oldest_user\": result[\"oldest_user\"].isoformat() if result[\"oldest_user\"] else None,\n            \"newest_user\": result[\"newest_user\"].isoformat() if result[\"newest_user\"] else None\n        }\n\n        await ctx.info(f\"Generated user statistics: {stats['total_users']} total users\")\n        return stats\n\n    return {\"total_users\": 0}\n</code></pre>"},{"location":"development/fastmcp/database-integration/#redis-integration","title":"Redis Integration","text":"<p>For caching and session storage:</p> <pre><code>import redis.asyncio as redis\nimport json\nfrom typing import Any\n\n@asynccontextmanager\nasync def redis_lifespan(app: FastMCP):\n    \"\"\"Redis connection management.\"\"\"\n    redis_client = redis.from_url(\"redis://localhost:6379\")\n\n    # Test connection\n    await redis_client.ping()\n\n    app.redis = redis_client\n\n    try:\n        yield {\"redis\": redis_client}\n    finally:\n        await redis_client.close()\n\nmcp = FastMCP(\"redis-server\", lifespan=redis_lifespan)\n\n@mcp.tool()\nasync def cache_data(\n    key: str,\n    data: Any,\n    ttl: int = 3600,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Cache data in Redis with TTL.\"\"\"\n    serialized_data = json.dumps(data)\n\n    await mcp.redis.setex(key, ttl, serialized_data)\n\n    await ctx.info(f\"Cached data with key: {key} (TTL: {ttl}s)\")\n\n    return {\n        \"key\": key,\n        \"ttl\": ttl,\n        \"data_size\": len(serialized_data)\n    }\n\n@mcp.tool()\nasync def get_cached_data(key: str, ctx: Context) -&gt; dict:\n    \"\"\"Retrieve cached data from Redis.\"\"\"\n    data = await mcp.redis.get(key)\n\n    if data is None:\n        await ctx.warning(f\"Cache miss for key: {key}\")\n        return {\"found\": False, \"key\": key}\n\n    # Get TTL\n    ttl = await mcp.redis.ttl(key)\n\n    try:\n        parsed_data = json.loads(data)\n        await ctx.info(f\"Cache hit for key: {key}\")\n\n        return {\n            \"found\": True,\n            \"key\": key,\n            \"data\": parsed_data,\n            \"ttl_remaining\": ttl\n        }\n    except json.JSONDecodeError:\n        await ctx.error(f\"Failed to parse cached data for key: {key}\")\n        return {\"found\": False, \"key\": key, \"error\": \"Parse error\"}\n\n@mcp.tool()\nasync def redis_stats(ctx: Context) -&gt; dict:\n    \"\"\"Get Redis server statistics.\"\"\"\n    info = await mcp.redis.info()\n\n    stats = {\n        \"redis_version\": info.get(\"redis_version\"),\n        \"used_memory\": info.get(\"used_memory\"),\n        \"used_memory_human\": info.get(\"used_memory_human\"),\n        \"connected_clients\": info.get(\"connected_clients\"),\n        \"total_commands_processed\": info.get(\"total_commands_processed\"),\n        \"keyspace\": {}\n    }\n\n    # Get keyspace info\n    for key, value in info.items():\n        if key.startswith(\"db\"):\n            stats[\"keyspace\"][key] = value\n\n    await ctx.info(\"Retrieved Redis statistics\")\n    return stats\n</code></pre>"},{"location":"development/fastmcp/database-integration/#database-configuration-management","title":"Database Configuration Management","text":""},{"location":"development/fastmcp/database-integration/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code>from pydantic import Field, validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom typing import Literal\n\nclass DatabaseSettings(BaseSettings):\n    \"\"\"Type-safe database configuration.\"\"\"\n    model_config = SettingsConfigDict(\n        env_prefix=\"DB_\",\n        env_file=\".env\",\n        secrets_dir=\"/run/secrets\"\n    )\n\n    # Database connection\n    host: str = Field(default=\"localhost\", description=\"Database host\")\n    port: int = Field(default=5432, description=\"Database port\")\n    user: str = Field(..., description=\"Database user\")\n    password: str = Field(..., description=\"Database password\")\n    name: str = Field(..., description=\"Database name\")\n\n    # Connection pool settings\n    min_pool_size: int = Field(default=5, ge=1, le=50)\n    max_pool_size: int = Field(default=20, ge=1, le=100)\n    pool_timeout: int = Field(default=60, ge=1, le=300)\n\n    # SSL settings\n    ssl_mode: Literal[\"disable\", \"require\", \"verify-ca\", \"verify-full\"] = \"disable\"\n    ssl_cert: str = None\n    ssl_key: str = None\n    ssl_ca: str = None\n\n    @validator('max_pool_size')\n    def validate_pool_sizes(cls, v, values):\n        min_size = values.get('min_pool_size', 5)\n        if v &lt; min_size:\n            raise ValueError('max_pool_size must be &gt;= min_pool_size')\n        return v\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"Generate database URL.\"\"\"\n        return f\"postgresql+asyncpg://{self.user}:{self.password}@{self.host}:{self.port}/{self.name}\"\n\n    @property\n    def ssl_context(self):\n        \"\"\"Generate SSL context if needed.\"\"\"\n        if self.ssl_mode == \"disable\":\n            return None\n\n        import ssl\n        context = ssl.create_default_context()\n\n        if self.ssl_mode == \"require\":\n            context.check_hostname = False\n            context.verify_mode = ssl.CERT_NONE\n        elif self.ssl_ca:\n            context.load_verify_locations(self.ssl_ca)\n\n        if self.ssl_cert and self.ssl_key:\n            context.load_cert_chain(self.ssl_cert, self.ssl_key)\n\n        return context\n\n# Usage\ndb_settings = DatabaseSettings()\n\n@asynccontextmanager\nasync def configured_database_lifespan(app: FastMCP):\n    \"\"\"Database lifespan with configuration.\"\"\"\n    pool = await asyncpg.create_pool(\n        db_settings.url,\n        min_size=db_settings.min_pool_size,\n        max_size=db_settings.max_pool_size,\n        command_timeout=db_settings.pool_timeout,\n        ssl=db_settings.ssl_context\n    )\n\n    app.db_pool = pool\n    app.db_settings = db_settings\n\n    try:\n        yield {\"db_pool\": pool, \"settings\": db_settings}\n    finally:\n        await pool.close()\n</code></pre>"},{"location":"development/fastmcp/database-integration/#testing-database-tools","title":"Testing Database Tools","text":""},{"location":"development/fastmcp/database-integration/#pytest-with-database-fixtures","title":"Pytest with Database Fixtures","text":"<pre><code>import pytest\nimport pytest_asyncio\nfrom unittest.mock import AsyncMock, MagicMock\nimport asyncpg\n\n@pytest_asyncio.fixture\nasync def mock_db_pool():\n    \"\"\"Mock database pool for testing.\"\"\"\n    pool = AsyncMock()\n    connection = AsyncMock()\n\n    # Configure mock connection\n    pool.acquire.return_value.__aenter__.return_value = connection\n    connection.fetchrow.return_value = {\n        \"id\": 1,\n        \"name\": \"Test User\",\n        \"email\": \"test@example.com\"\n    }\n    connection.fetch.return_value = [\n        {\"id\": 1, \"name\": \"User 1\"},\n        {\"id\": 2, \"name\": \"User 2\"}\n    ]\n    connection.execute.return_value = \"INSERT 0 1\"\n\n    return pool\n\n@pytest_asyncio.fixture\nasync def test_mcp_app(mock_db_pool):\n    \"\"\"FastMCP app with mocked database.\"\"\"\n    app = FastMCP(\"test-server\")\n    app.db_pool = mock_db_pool\n    return app\n\n@pytest.mark.asyncio\nasync def test_create_user(test_mcp_app, mock_db_pool):\n    \"\"\"Test user creation with mocked database.\"\"\"\n    from mcp.server.fastmcp import Context\n\n    # Mock context\n    ctx = AsyncMock(spec=Context)\n\n    # Test user creation\n    user_data = UserCreate(name=\"John Doe\", email=\"john@example.com\")\n    result = await create_user(user_data, ctx)\n\n    # Verify database was called\n    mock_db_pool.acquire.assert_called_once()\n\n    # Verify result\n    assert result.name == \"John Doe\"\n    assert result.email == \"john@example.com\"\n\n    # Verify logging\n    ctx.info.assert_called()\n\n@pytest_asyncio.fixture\nasync def test_database():\n    \"\"\"Real database fixture for integration tests.\"\"\"\n    # Create test database\n    pool = await asyncpg.create_pool(\"postgresql://test:test@localhost:5432/test_db\")\n\n    # Setup test schema\n    async with pool.acquire() as conn:\n        await conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS test_users (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(100),\n                email VARCHAR(100) UNIQUE\n            )\n        \"\"\")\n\n    try:\n        yield pool\n    finally:\n        # Cleanup\n        async with pool.acquire() as conn:\n            await conn.execute(\"DROP TABLE IF EXISTS test_users\")\n        await pool.close()\n\n@pytest.mark.asyncio\nasync def test_integration_create_user(test_database):\n    \"\"\"Integration test with real database.\"\"\"\n    app = FastMCP(\"integration-test\")\n    app.db_pool = test_database\n\n    ctx = AsyncMock(spec=Context)\n    user_data = UserCreate(name=\"Integration User\", email=\"integration@test.com\")\n\n    result = await create_user(user_data, ctx)\n\n    # Verify in database\n    async with test_database.acquire() as conn:\n        db_user = await conn.fetchrow(\n            \"SELECT * FROM test_users WHERE email = $1\",\n            \"integration@test.com\"\n        )\n\n        assert db_user is not None\n        assert db_user[\"name\"] == \"Integration User\"\n</code></pre>"},{"location":"development/fastmcp/database-integration/#production-best-practices","title":"Production Best Practices","text":""},{"location":"development/fastmcp/database-integration/#connection-monitoring-and-health-checks","title":"Connection Monitoring and Health Checks","text":"<pre><code>@mcp.custom_route(\"/health/database\", methods=[\"GET\"])\nasync def database_health_check(request) -&gt; JSONResponse:\n    \"\"\"Database health check endpoint.\"\"\"\n    try:\n        # Test database connectivity\n        async with mcp.db_pool.acquire() as conn:\n            await conn.fetchval(\"SELECT 1\")\n\n        # Get pool statistics\n        pool_stats = {\n            \"size\": mcp.db_pool.get_size(),\n            \"idle_connections\": mcp.db_pool.get_idle_size(),\n            \"max_size\": mcp.db_pool._maxsize,\n            \"min_size\": mcp.db_pool._minsize\n        }\n\n        return JSONResponse({\n            \"status\": \"healthy\",\n            \"database\": \"connected\",\n            \"pool_stats\": pool_stats,\n            \"timestamp\": time.time()\n        })\n\n    except Exception as e:\n        return JSONResponse({\n            \"status\": \"unhealthy\",\n            \"database\": \"disconnected\",\n            \"error\": str(e),\n            \"timestamp\": time.time()\n        }, status_code=503)\n\n@mcp.tool()\nasync def database_diagnostics(ctx: Context) -&gt; dict:\n    \"\"\"Get detailed database diagnostics.\"\"\"\n    diagnostics = {}\n\n    try:\n        async with mcp.db_pool.acquire() as conn:\n            # Database version\n            version = await conn.fetchval(\"SELECT version()\")\n            diagnostics[\"database_version\"] = version\n\n            # Connection info\n            diagnostics[\"connection_info\"] = {\n                \"host\": conn.get_dsn_parameters().get(\"host\"),\n                \"port\": conn.get_dsn_parameters().get(\"port\"),\n                \"database\": conn.get_dsn_parameters().get(\"dbname\"),\n                \"user\": conn.get_dsn_parameters().get(\"user\")\n            }\n\n            # Performance metrics\n            metrics = await conn.fetch(\"\"\"\n                SELECT\n                    schemaname,\n                    tablename,\n                    n_tup_ins as inserts,\n                    n_tup_upd as updates,\n                    n_tup_del as deletes\n                FROM pg_stat_user_tables\n                ORDER BY schemaname, tablename\n            \"\"\")\n\n            diagnostics[\"table_stats\"] = [dict(row) for row in metrics]\n\n    except Exception as e:\n        diagnostics[\"error\"] = str(e)\n\n    # Pool diagnostics\n    diagnostics[\"pool_stats\"] = {\n        \"size\": mcp.db_pool.get_size(),\n        \"idle\": mcp.db_pool.get_idle_size(),\n        \"max_size\": mcp.db_pool._maxsize,\n        \"min_size\": mcp.db_pool._minsize\n    }\n\n    await ctx.info(\"Generated database diagnostics\")\n    return diagnostics\n</code></pre>"},{"location":"development/fastmcp/database-integration/#performance-optimization","title":"Performance Optimization","text":"<pre><code>from functools import lru_cache\nimport time\n\n# Query caching\n@lru_cache(maxsize=128)\ndef get_cached_query_result(query_hash: str, cache_time: int):\n    \"\"\"Cache query results based on time intervals.\"\"\"\n    # Cache for 5-minute intervals\n    interval = cache_time // 300\n    return f\"cached_result_{query_hash}_{interval}\"\n\n@mcp.tool()\nasync def optimized_search(\n    search_term: str,\n    use_cache: bool = True,\n    ctx: Context\n) -&gt; list[dict]:\n    \"\"\"Search with intelligent caching.\"\"\"\n\n    if use_cache:\n        cache_key = f\"search_{hash(search_term)}\"\n        cache_time = int(time.time())\n\n        # Try cache first\n        try:\n            cached_result = get_cached_query_result(cache_key, cache_time)\n            if cached_result:\n                await ctx.info(\"Using cached search results\")\n                return cached_result\n        except:\n            pass\n\n    # Execute database query\n    async with mcp.db_pool.acquire() as conn:\n        # Use prepared statement for performance\n        stmt = await conn.prepare(\"\"\"\n            SELECT id, name, email, ts_rank(search_vector, query) as rank\n            FROM users, plainto_tsquery($1) query\n            WHERE search_vector @@ query\n            ORDER BY rank DESC\n            LIMIT 50\n        \"\"\")\n\n        rows = await stmt.fetch(search_term)\n        results = [dict(row) for row in rows]\n\n    await ctx.info(f\"Database search returned {len(results)} results\")\n    return results\n\n# Batch operations for better performance\n@mcp.tool()\nasync def batch_insert_users(\n    users: list[dict],\n    batch_size: int = 100,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Insert users in batches for better performance.\"\"\"\n    total_inserted = 0\n\n    async with mcp.db_pool.acquire() as conn:\n        async with conn.transaction():\n            # Process in batches\n            for i in range(0, len(users), batch_size):\n                batch = users[i:i + batch_size]\n\n                # Prepare batch insert\n                values = [(user[\"name\"], user[\"email\"]) for user in batch]\n\n                await conn.executemany(\n                    \"INSERT INTO users (name, email) VALUES ($1, $2)\",\n                    values\n                )\n\n                total_inserted += len(batch)\n\n                # Report progress\n                await ctx.report_progress(\n                    progress=total_inserted / len(users),\n                    total=1.0,\n                    message=f\"Inserted {total_inserted} of {len(users)} users\"\n                )\n\n    await ctx.info(f\"Batch inserted {total_inserted} users\")\n    return {\n        \"total_inserted\": total_inserted,\n        \"batch_size\": batch_size,\n        \"batches_processed\": (len(users) + batch_size - 1) // batch_size\n    }\n</code></pre>"},{"location":"development/fastmcp/database-integration/#security-best-practices","title":"Security Best Practices","text":""},{"location":"development/fastmcp/database-integration/#sql-injection-prevention","title":"SQL Injection Prevention","text":"<pre><code># \u2705 SAFE: Always use parameterized queries\n@mcp.tool()\nasync def safe_user_search(search_term: str, ctx: Context) -&gt; list[dict]:\n    \"\"\"Safe user search with parameterized query.\"\"\"\n    async with mcp.db_pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"SELECT * FROM users WHERE name ILIKE $1 OR email ILIKE $1\",\n            f\"%{search_term}%\"\n        )\n        return [dict(row) for row in rows]\n\n# \u274c UNSAFE: Never use string formatting\nasync def unsafe_search(search_term: str):\n    query = f\"SELECT * FROM users WHERE name LIKE '%{search_term}%'\"  # DON'T DO THIS\n    # This is vulnerable to SQL injection attacks\n</code></pre>"},{"location":"development/fastmcp/database-integration/#access-control-and-permissions","title":"Access Control and Permissions","text":"<pre><code>from mcp.server.auth.middleware import get_access_token\n\n@mcp.tool()\nasync def admin_database_operation(\n    operation: str,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Database operation requiring admin privileges.\"\"\"\n    # Check authentication and authorization\n    access_token = get_access_token()\n\n    if not access_token:\n        raise ValueError(\"Authentication required\")\n\n    if \"admin\" not in access_token.scopes:\n        raise ValueError(\"Admin privileges required\")\n\n    # Proceed with admin operation\n    async with mcp.db_pool.acquire() as conn:\n        if operation == \"vacuum\":\n            await conn.execute(\"VACUUM ANALYZE\")\n            await ctx.info(\"Database vacuum completed\")\n            return {\"operation\": \"vacuum\", \"status\": \"completed\"}\n\n        elif operation == \"reindex\":\n            await conn.execute(\"REINDEX DATABASE\")\n            await ctx.info(\"Database reindex completed\")\n            return {\"operation\": \"reindex\", \"status\": \"completed\"}\n\n        else:\n            raise ValueError(f\"Unknown admin operation: {operation}\")\n</code></pre> <p>This comprehensive guide provides everything needed to integrate databases effectively with FastMCP servers, from basic connection management to production-ready patterns with security, monitoring, and optimization.</p>"},{"location":"development/fastmcp/debugging/","title":"Debugging","text":"<p>Comprehensive guide to debugging FastMCP servers effectively during development and production, including logging, tracing, error handling, and troubleshooting techniques.</p>"},{"location":"development/fastmcp/debugging/#overview","title":"Overview","text":"<p>FastMCP provides extensive debugging capabilities built on top of Python's logging system with Rich console integration, structured error handling, and context-aware debugging tools. This guide covers all aspects of debugging FastMCP servers from development to production deployment.</p>"},{"location":"development/fastmcp/debugging/#debug-configuration","title":"Debug Configuration","text":""},{"location":"development/fastmcp/debugging/#environment-variables","title":"Environment Variables","text":"<p>FastMCP supports comprehensive debugging configuration through environment variables:</p> <pre><code># Enable debug mode\nexport FASTMCP_DEBUG=true\n\n# Set detailed logging level\nexport FASTMCP_LOG_LEVEL=DEBUG\n\n# Disable warnings for development\nexport FASTMCP_WARN_ON_DUPLICATE_TOOLS=false\nexport FASTMCP_WARN_ON_DUPLICATE_RESOURCES=false\nexport FASTMCP_WARN_ON_DUPLICATE_PROMPTS=false\n\n# Development server configuration\nexport FASTMCP_HOST=0.0.0.0\nexport FASTMCP_PORT=8080\n</code></pre>"},{"location":"development/fastmcp/debugging/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\n# Configure debug settings\nmcp = FastMCP(\"debug-server\")\nmcp.settings.debug = True\nmcp.settings.log_level = \"DEBUG\"\nmcp.settings.warn_on_duplicate_tools = False\n\n# Or pass settings during initialization\nmcp = FastMCP(\n    \"debug-server\",\n    debug=True,\n    log_level=\"DEBUG\",\n    warn_on_duplicate_tools=False\n)\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/debugging/#logging-and-tracing","title":"Logging and Tracing","text":""},{"location":"development/fastmcp/debugging/#rich-console-integration","title":"Rich Console Integration","text":"<p>FastMCP uses Rich console for enhanced debugging output with syntax highlighting and formatted tracebacks:</p> <pre><code>from mcp.server.fastmcp.utilities.logging import configure_logging\n\n# Configure Rich logging with enhanced tracebacks\nconfigure_logging(level=\"DEBUG\")\n\n# Example output will include:\n# - Syntax-highlighted code in tracebacks\n# - Formatted exception display\n# - Structured log messages with timestamps\n</code></pre>"},{"location":"development/fastmcp/debugging/#context-based-logging-in-tools","title":"Context-Based Logging in Tools","text":"<pre><code>from mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"logging-server\")\n\n@mcp.tool()\nasync def debug_tool(operation: str, data: dict, ctx: Context) -&gt; dict:\n    \"\"\"Tool demonstrating comprehensive logging.\"\"\"\n\n    # Debug level logging\n    await ctx.debug(f\"Starting operation: {operation}\")\n    await ctx.debug(f\"Input data keys: {list(data.keys())}\")\n\n    # Info level logging\n    await ctx.info(f\"Processing {operation} with {len(data)} items\")\n\n    # Progress reporting for long operations\n    total_steps = len(data)\n    for i, (key, value) in enumerate(data.items()):\n        await ctx.report_progress(\n            progress=(i + 1) / total_steps,\n            total=1.0,\n            message=f\"Processing {key}\"\n        )\n\n        # Process each item\n        await ctx.debug(f\"Processing item {key}: {value}\")\n\n        # Simulate work\n        import asyncio\n        await asyncio.sleep(0.1)\n\n    # Warning for edge cases\n    if not data:\n        await ctx.warning(\"No data provided for processing\")\n\n    # Error logging (without raising)\n    if operation == \"test_error\":\n        await ctx.error(\"Test error condition detected\")\n\n    result = {\n        \"operation\": operation,\n        \"processed_items\": len(data),\n        \"status\": \"completed\"\n    }\n\n    await ctx.info(f\"Operation {operation} completed successfully\")\n    return result\n\n@mcp.tool()\nasync def request_info_tool(ctx: Context) -&gt; dict:\n    \"\"\"Tool for inspecting request context.\"\"\"\n    return {\n        \"request_id\": ctx.request_id,\n        \"client_id\": ctx.client_id,\n        \"session_available\": hasattr(ctx, 'session'),\n        \"lifespan_context_available\": hasattr(ctx.request_context, 'lifespan_context')\n    }\n</code></pre>"},{"location":"development/fastmcp/debugging/#structured-logging","title":"Structured Logging","text":"<pre><code>import logging\nimport json\n\n# Get FastMCP logger\nlogger = logging.getLogger(\"mcp.server.fastmcp\")\n\n@mcp.tool()\nasync def structured_logging_tool(data: dict, ctx: Context) -&gt; str:\n    \"\"\"Tool demonstrating structured logging patterns.\"\"\"\n\n    # Structured logging with extra fields\n    logger.debug(\n        \"Tool execution started\",\n        extra={\n            \"tool_name\": \"structured_logging_tool\",\n            \"request_id\": ctx.request_id,\n            \"data_size\": len(data),\n            \"data_keys\": list(data.keys())\n        }\n    )\n\n    # Log with JSON context\n    context = {\n        \"operation\": \"data_processing\",\n        \"input_size\": len(data),\n        \"timestamp\": time.time()\n    }\n\n    await ctx.info(f\"Processing context: {json.dumps(context)}\")\n\n    # Process data with detailed logging\n    for key, value in data.items():\n        logger.debug(\n            \"Processing data item\",\n            extra={\n                \"key\": key,\n                \"value_type\": type(value).__name__,\n                \"request_id\": ctx.request_id\n            }\n        )\n\n    return f\"Processed {len(data)} items with structured logging\"\n</code></pre>"},{"location":"development/fastmcp/debugging/#error-handling-and-stack-traces","title":"Error Handling and Stack Traces","text":""},{"location":"development/fastmcp/debugging/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>FastMCP provides a structured exception hierarchy for better debugging:</p> <pre><code>from mcp.server.fastmcp.exceptions import (\n    FastMCPError,\n    ValidationError,\n    ResourceError,\n    ToolError\n)\n\n@mcp.tool()\nasync def error_demonstration_tool(\n    error_type: str,\n    message: str,\n    ctx: Context\n) -&gt; str:\n    \"\"\"Tool demonstrating different error types.\"\"\"\n\n    await ctx.debug(f\"Simulating error type: {error_type}\")\n\n    try:\n        if error_type == \"validation\":\n            raise ValidationError(f\"Validation failed: {message}\")\n        elif error_type == \"resource\":\n            raise ResourceError(f\"Resource error: {message}\")\n        elif error_type == \"tool\":\n            raise ToolError(f\"Tool error: {message}\")\n        elif error_type == \"generic\":\n            raise FastMCPError(f\"Generic FastMCP error: {message}\")\n        elif error_type == \"python\":\n            raise ValueError(f\"Standard Python error: {message}\")\n        else:\n            await ctx.warning(f\"Unknown error type: {error_type}\")\n            return f\"No error simulated for type: {error_type}\"\n\n    except Exception as e:\n        # Log the error with context\n        await ctx.error(f\"Error occurred: {type(e).__name__}: {e}\")\n\n        # Log stack trace in debug mode\n        import traceback\n        await ctx.debug(f\"Stack trace: {traceback.format_exc()}\")\n\n        # Re-raise to let FastMCP handle it\n        raise\n</code></pre>"},{"location":"development/fastmcp/debugging/#custom-error-handling","title":"Custom Error Handling","text":"<pre><code>import traceback\nimport sys\n\n@mcp.tool()\nasync def robust_error_handling_tool(\n    operation: str,\n    fail_probability: float,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Tool with comprehensive error handling.\"\"\"\n\n    await ctx.debug(f\"Starting {operation} with fail rate {fail_probability}\")\n\n    try:\n        # Simulate operation that might fail\n        import random\n        if random.random() &lt; fail_probability:\n            raise RuntimeError(f\"Simulated failure in {operation}\")\n\n        # Successful operation\n        result = {\"operation\": operation, \"status\": \"success\"}\n        await ctx.info(f\"Operation {operation} completed successfully\")\n        return result\n\n    except Exception as e:\n        # Comprehensive error logging\n        error_info = {\n            \"error_type\": type(e).__name__,\n            \"error_message\": str(e),\n            \"operation\": operation,\n            \"fail_probability\": fail_probability\n        }\n\n        # Log error details\n        await ctx.error(f\"Operation failed: {error_info}\")\n\n        # In debug mode, log full traceback\n        if mcp.settings.debug:\n            tb = traceback.format_exc()\n            await ctx.debug(f\"Full traceback:\\n{tb}\")\n\n        # Log system information for debugging\n        await ctx.debug(f\"Python version: {sys.version}\")\n        await ctx.debug(f\"Platform: {sys.platform}\")\n\n        # Return error information instead of raising\n        # (or raise depending on your error handling strategy)\n        return {\n            \"operation\": operation,\n            \"status\": \"error\",\n            \"error\": error_info\n        }\n</code></pre>"},{"location":"development/fastmcp/debugging/#request-and-response-debugging","title":"Request and Response Debugging","text":""},{"location":"development/fastmcp/debugging/#request-tracing","title":"Request Tracing","text":"<pre><code>import time\nimport uuid\n\n@mcp.tool()\nasync def request_tracing_tool(\n    data: str,\n    trace_id: str = None,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Tool demonstrating request tracing patterns.\"\"\"\n\n    # Generate trace ID if not provided\n    if not trace_id:\n        trace_id = str(uuid.uuid4())[:8]\n\n    start_time = time.time()\n\n    await ctx.debug(f\"[{trace_id}] Request started\")\n    await ctx.debug(f\"[{trace_id}] Input data length: {len(data)}\")\n    await ctx.debug(f\"[{trace_id}] Request ID: {ctx.request_id}\")\n    await ctx.debug(f\"[{trace_id}] Client ID: {ctx.client_id}\")\n\n    # Simulate processing stages\n    stages = [\"validation\", \"processing\", \"formatting\", \"completion\"]\n\n    for i, stage in enumerate(stages):\n        stage_start = time.time()\n        await ctx.debug(f\"[{trace_id}] Stage {i+1}: {stage} started\")\n\n        # Simulate stage work\n        import asyncio\n        await asyncio.sleep(0.1)\n\n        stage_duration = time.time() - stage_start\n        await ctx.debug(f\"[{trace_id}] Stage {i+1}: {stage} completed in {stage_duration:.3f}s\")\n\n        # Report progress\n        await ctx.report_progress(\n            progress=(i + 1) / len(stages),\n            total=1.0,\n            message=f\"[{trace_id}] Completed {stage}\"\n        )\n\n    total_duration = time.time() - start_time\n\n    result = {\n        \"trace_id\": trace_id,\n        \"request_id\": ctx.request_id,\n        \"client_id\": ctx.client_id,\n        \"processing_time\": total_duration,\n        \"stages_completed\": len(stages),\n        \"data_length\": len(data)\n    }\n\n    await ctx.info(f\"[{trace_id}] Request completed in {total_duration:.3f}s\")\n\n    return result\n</code></pre>"},{"location":"development/fastmcp/debugging/#response-validation","title":"Response Validation","text":"<pre><code>from pydantic import BaseModel, ValidationError\nfrom typing import Any\n\nclass ToolResponse(BaseModel):\n    \"\"\"Response model for validation.\"\"\"\n    status: str\n    data: Any\n    timestamp: float\n    metadata: dict = {}\n\n@mcp.tool()\nasync def validated_response_tool(\n    operation: str,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Tool demonstrating response validation.\"\"\"\n\n    await ctx.debug(\"Preparing response with validation\")\n\n    # Prepare response data\n    response_data = {\n        \"status\": \"success\",\n        \"data\": f\"Processed {operation}\",\n        \"timestamp\": time.time(),\n        \"metadata\": {\n            \"operation\": operation,\n            \"request_id\": ctx.request_id\n        }\n    }\n\n    # Validate response before returning\n    try:\n        validated_response = ToolResponse(**response_data)\n        await ctx.debug(\"Response validation successful\")\n        return validated_response.dict()\n\n    except ValidationError as e:\n        await ctx.error(f\"Response validation failed: {e}\")\n        # Return error response\n        return {\n            \"status\": \"error\",\n            \"data\": \"Response validation failed\",\n            \"timestamp\": time.time(),\n            \"metadata\": {\"error\": str(e)}\n        }\n</code></pre>"},{"location":"development/fastmcp/debugging/#transport-specific-debugging","title":"Transport-Specific Debugging","text":""},{"location":"development/fastmcp/debugging/#stdio-transport-debugging","title":"Stdio Transport Debugging","text":"<pre><code># stdio_debug_server.py\nimport sys\nimport logging\n\n# Configure logging to stderr for stdio transport\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    stream=sys.stderr  # Important: use stderr to avoid interfering with MCP protocol\n)\n\nmcp = FastMCP(\"stdio-debug-server\")\nmcp.settings.debug = True\nmcp.settings.log_level = \"DEBUG\"\n\n@mcp.tool()\nasync def stdio_debug_tool(message: str, ctx: Context) -&gt; str:\n    \"\"\"Tool for debugging stdio transport.\"\"\"\n\n    # Log to stderr (won't interfere with protocol)\n    logging.debug(f\"Stdio tool called with message: {message}\")\n\n    # Use context logging (goes to MCP client)\n    await ctx.debug(f\"Processing message: {message}\")\n\n    return f\"Processed: {message}\"\n\nif __name__ == \"__main__\":\n    print(\"Starting stdio debug server...\", file=sys.stderr)\n    mcp.run(\"stdio\")\n</code></pre>"},{"location":"development/fastmcp/debugging/#http-transport-debugging","title":"HTTP Transport Debugging","text":"<pre><code>import uvicorn\n\nmcp = FastMCP(\"http-debug-server\")\nmcp.settings.debug = True\nmcp.settings.log_level = \"DEBUG\"\nmcp.settings.host = \"127.0.0.1\"\nmcp.settings.port = 8080\n\n@mcp.custom_route(\"/debug/health\", methods=[\"GET\"])\nasync def debug_health(request):\n    \"\"\"Debug health endpoint.\"\"\"\n    return JSONResponse({\n        \"status\": \"healthy\",\n        \"debug_mode\": mcp.settings.debug,\n        \"log_level\": mcp.settings.log_level,\n        \"tools_count\": len(mcp._tool_manager.tools),\n        \"timestamp\": time.time()\n    })\n\n@mcp.custom_route(\"/debug/logs\", methods=[\"GET\"])\nasync def debug_logs(request):\n    \"\"\"Endpoint to retrieve recent logs.\"\"\"\n    # In a real implementation, you'd collect logs from a handler\n    return JSONResponse({\n        \"message\": \"Log retrieval endpoint\",\n        \"note\": \"Implement log collection for production use\"\n    })\n\n@mcp.tool()\nasync def http_debug_tool(data: dict, ctx: Context) -&gt; dict:\n    \"\"\"Tool for debugging HTTP transport.\"\"\"\n\n    # HTTP-specific debugging\n    await ctx.debug(f\"HTTP tool called with data: {data}\")\n    await ctx.debug(f\"Request ID: {ctx.request_id}\")\n\n    # Simulate HTTP-specific operations\n    await ctx.info(\"Performing HTTP-specific processing\")\n\n    return {\n        \"processed_data\": data,\n        \"transport\": \"http\",\n        \"debug_mode\": mcp.settings.debug\n    }\n\nif __name__ == \"__main__\":\n    print(f\"Starting HTTP debug server on {mcp.settings.host}:{mcp.settings.port}\")\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/debugging/#resource-and-tool-debugging","title":"Resource and Tool Debugging","text":""},{"location":"development/fastmcp/debugging/#resource-debugging","title":"Resource Debugging","text":"<pre><code>from mcp.server.fastmcp.resources.base import Resource\nfrom pydantic import AnyUrl\n\n@mcp.resource(\"debug://static/{name}\")\ndef debug_static_resource(name: str) -&gt; str:\n    \"\"\"Debug static resource with logging.\"\"\"\n\n    # Log resource access\n    logger = logging.getLogger(__name__)\n    logger.debug(f\"Accessing static resource: {name}\")\n\n    resources = {\n        \"config\": \"Debug configuration data\",\n        \"status\": \"Debug status information\",\n        \"logs\": \"Debug log entries\"\n    }\n\n    if name in resources:\n        logger.debug(f\"Resource {name} found\")\n        return resources[name]\n    else:\n        logger.warning(f\"Resource {name} not found\")\n        raise ValueError(f\"Resource not found: {name}\")\n\nclass DebugDynamicResource(Resource):\n    \"\"\"Custom resource class with debugging.\"\"\"\n\n    def __init__(self, uri: str, debug_info: dict):\n        super().__init__(uri=uri)\n        self.debug_info = debug_info\n\n        # Log resource creation\n        logger.debug(f\"Created debug resource: {uri} with info: {debug_info}\")\n\n    async def read(self) -&gt; str:\n        \"\"\"Read resource with debug logging.\"\"\"\n        logger.debug(f\"Reading debug resource: {self.uri}\")\n\n        return json.dumps({\n            \"uri\": self.uri,\n            \"debug_info\": self.debug_info,\n            \"read_timestamp\": time.time()\n        })\n\n@mcp.resource(\"debug://dynamic/{resource_id}\")\nasync def debug_dynamic_resource(resource_id: str) -&gt; DebugDynamicResource:\n    \"\"\"Dynamic resource with debugging.\"\"\"\n\n    debug_info = {\n        \"resource_id\": resource_id,\n        \"creation_time\": time.time(),\n        \"debug_mode\": mcp.settings.debug\n    }\n\n    return DebugDynamicResource(\n        uri=f\"debug://dynamic/{resource_id}\",\n        debug_info=debug_info\n    )\n</code></pre>"},{"location":"development/fastmcp/debugging/#tool-manager-debugging","title":"Tool Manager Debugging","text":"<pre><code>@mcp.tool()\nasync def debug_tool_manager(ctx: Context) -&gt; dict:\n    \"\"\"Tool to inspect tool manager state.\"\"\"\n\n    tool_manager = mcp._tool_manager\n\n    # Get tool information\n    tools_info = {}\n    for name, tool in tool_manager.tools.items():\n        tools_info[name] = {\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"is_async\": tool.is_async,\n            \"context_kwarg\": tool.context_kwarg,\n            \"parameter_count\": len(tool.fn_metadata.parameters)\n        }\n\n    await ctx.debug(f\"Tool manager has {len(tools_info)} tools\")\n\n    return {\n        \"tools_count\": len(tools_info),\n        \"tools\": tools_info,\n        \"warn_on_duplicates\": tool_manager.warn_on_duplicate_tools\n    }\n\n@mcp.tool()\nasync def debug_resource_manager(ctx: Context) -&gt; dict:\n    \"\"\"Tool to inspect resource manager state.\"\"\"\n\n    resource_manager = mcp._resource_manager\n\n    # Get resource information\n    resources_info = {}\n    for uri, resource in resource_manager.resources.items():\n        resources_info[uri] = {\n            \"uri\": uri,\n            \"type\": type(resource).__name__,\n            \"name\": getattr(resource, 'name', None)\n        }\n\n    templates_info = {}\n    for pattern, template in resource_manager._templates.items():\n        templates_info[pattern] = {\n            \"pattern\": pattern,\n            \"type\": type(template).__name__\n        }\n\n    await ctx.debug(f\"Resource manager has {len(resources_info)} resources and {len(templates_info)} templates\")\n\n    return {\n        \"resources_count\": len(resources_info),\n        \"templates_count\": len(templates_info),\n        \"resources\": resources_info,\n        \"templates\": templates_info\n    }\n</code></pre>"},{"location":"development/fastmcp/debugging/#performance-debugging","title":"Performance Debugging","text":""},{"location":"development/fastmcp/debugging/#timing-and-profiling","title":"Timing and Profiling","text":"<pre><code>import time\nimport functools\nimport asyncio\nfrom typing import Callable\n\ndef debug_timing(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to add timing information to tools.\"\"\"\n\n    @functools.wraps(func)\n    async def async_wrapper(*args, **kwargs):\n        start_time = time.time()\n        ctx = kwargs.get('ctx')\n\n        if ctx:\n            await ctx.debug(f\"Starting {func.__name__}\")\n\n        try:\n            result = await func(*args, **kwargs)\n            duration = time.time() - start_time\n\n            if ctx:\n                await ctx.debug(f\"Completed {func.__name__} in {duration:.3f}s\")\n\n            return result\n        except Exception as e:\n            duration = time.time() - start_time\n            if ctx:\n                await ctx.error(f\"Failed {func.__name__} after {duration:.3f}s: {e}\")\n            raise\n\n    @functools.wraps(func)\n    def sync_wrapper(*args, **kwargs):\n        start_time = time.time()\n        ctx = kwargs.get('ctx')\n\n        try:\n            result = func(*args, **kwargs)\n            duration = time.time() - start_time\n\n            if ctx and hasattr(ctx, 'debug'):\n                # Can't await in sync function, but we can log\n                logging.debug(f\"Completed {func.__name__} in {duration:.3f}s\")\n\n            return result\n        except Exception as e:\n            duration = time.time() - start_time\n            logging.error(f\"Failed {func.__name__} after {duration:.3f}s: {e}\")\n            raise\n\n    return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper\n\n@mcp.tool()\n@debug_timing\nasync def performance_debug_tool(\n    operation_count: int,\n    delay_seconds: float,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Tool for performance debugging.\"\"\"\n\n    await ctx.info(f\"Starting performance test: {operation_count} operations with {delay_seconds}s delay\")\n\n    operation_times = []\n\n    for i in range(operation_count):\n        op_start = time.time()\n\n        # Simulate work\n        await asyncio.sleep(delay_seconds)\n\n        op_duration = time.time() - op_start\n        operation_times.append(op_duration)\n\n        await ctx.report_progress(\n            progress=(i + 1) / operation_count,\n            total=1.0,\n            message=f\"Operation {i + 1} completed in {op_duration:.3f}s\"\n        )\n\n    # Calculate statistics\n    avg_time = sum(operation_times) / len(operation_times)\n    min_time = min(operation_times)\n    max_time = max(operation_times)\n\n    result = {\n        \"operation_count\": operation_count,\n        \"total_time\": sum(operation_times),\n        \"average_time\": avg_time,\n        \"min_time\": min_time,\n        \"max_time\": max_time,\n        \"operation_times\": operation_times\n    }\n\n    await ctx.info(f\"Performance test completed: avg={avg_time:.3f}s, min={min_time:.3f}s, max={max_time:.3f}s\")\n\n    return result\n</code></pre>"},{"location":"development/fastmcp/debugging/#memory-debugging","title":"Memory Debugging","text":"<pre><code>import psutil\nimport gc\nimport sys\n\n@mcp.tool()\nasync def memory_debug_tool(ctx: Context) -&gt; dict:\n    \"\"\"Tool for memory debugging and inspection.\"\"\"\n\n    # Get process memory info\n    process = psutil.Process()\n    memory_info = process.memory_info()\n\n    # Get Python garbage collection stats\n    gc_stats = {}\n    for i, stats in enumerate(gc.get_stats()):\n        gc_stats[f\"generation_{i}\"] = stats\n\n    # Get object counts\n    object_counts = {}\n    for obj_type in [dict, list, str, int, float]:\n        count = len([obj for obj in gc.get_objects() if type(obj) is obj_type])\n        object_counts[obj_type.__name__] = count\n\n    await ctx.debug(f\"Memory usage: RSS={memory_info.rss / 1024 / 1024:.2f}MB\")\n    await ctx.debug(f\"GC collections: {gc.get_count()}\")\n\n    result = {\n        \"memory\": {\n            \"rss_mb\": memory_info.rss / 1024 / 1024,\n            \"vms_mb\": memory_info.vms / 1024 / 1024,\n            \"percent\": process.memory_percent()\n        },\n        \"garbage_collection\": {\n            \"counts\": gc.get_count(),\n            \"stats\": gc_stats,\n            \"total_objects\": len(gc.get_objects())\n        },\n        \"object_counts\": object_counts,\n        \"python_info\": {\n            \"version\": sys.version,\n            \"platform\": sys.platform,\n            \"executable\": sys.executable\n        }\n    }\n\n    return result\n</code></pre>"},{"location":"development/fastmcp/debugging/#integration-with-python-debugging-tools","title":"Integration with Python Debugging Tools","text":""},{"location":"development/fastmcp/debugging/#pydebugger-integration","title":"PyDebugger Integration","text":"<pre><code>import pdb\nimport sys\n\n@mcp.tool()\nasync def debugger_tool(\n    message: str,\n    enable_pdb: bool = False,\n    ctx: Context\n) -&gt; str:\n    \"\"\"Tool that can trigger Python debugger.\"\"\"\n\n    await ctx.debug(f\"Debugger tool called with message: {message}\")\n\n    if enable_pdb:\n        await ctx.warning(\"Triggering Python debugger (pdb)\")\n        # Set breakpoint for debugging\n        pdb.set_trace()\n\n    # Process message\n    result = f\"Processed: {message}\"\n\n    await ctx.debug(f\"Returning result: {result}\")\n\n    return result\n\n# For development with debugpy (VS Code debugging)\ndef enable_remote_debugging(port: int = 5678):\n    \"\"\"Enable remote debugging with debugpy.\"\"\"\n    try:\n        import debugpy\n        debugpy.listen(port)\n        print(f\"Waiting for debugger on port {port}...\")\n        debugpy.wait_for_client()\n        print(\"Debugger attached!\")\n    except ImportError:\n        print(\"debugpy not available. Install with: pip install debugpy\")\n\n# Usage in development\nif __name__ == \"__main__\":\n    if \"--debug-remote\" in sys.argv:\n        enable_remote_debugging()\n\n    mcp.run(\"stdio\")\n</code></pre>"},{"location":"development/fastmcp/debugging/#production-debugging","title":"Production Debugging","text":""},{"location":"development/fastmcp/debugging/#health-check-and-diagnostics","title":"Health Check and Diagnostics","text":"<pre><code>@mcp.custom_route(\"/debug/health\", methods=[\"GET\"])\nasync def debug_health_check(request) -&gt; JSONResponse:\n    \"\"\"Comprehensive health check for debugging.\"\"\"\n\n    start_time = time.time()\n\n    health_data = {\n        \"status\": \"healthy\",\n        \"timestamp\": start_time,\n        \"server\": {\n            \"name\": mcp.name,\n            \"debug_mode\": mcp.settings.debug,\n            \"log_level\": mcp.settings.log_level\n        },\n        \"components\": {},\n        \"metrics\": {}\n    }\n\n    # Check tool manager\n    try:\n        tools_count = len(mcp._tool_manager.tools)\n        health_data[\"components\"][\"tools\"] = {\n            \"status\": \"healthy\",\n            \"count\": tools_count\n        }\n    except Exception as e:\n        health_data[\"components\"][\"tools\"] = {\n            \"status\": \"unhealthy\",\n            \"error\": str(e)\n        }\n\n    # Check resource manager\n    try:\n        resources_count = len(mcp._resource_manager.resources)\n        templates_count = len(mcp._resource_manager._templates)\n        health_data[\"components\"][\"resources\"] = {\n            \"status\": \"healthy\",\n            \"resources_count\": resources_count,\n            \"templates_count\": templates_count\n        }\n    except Exception as e:\n        health_data[\"components\"][\"resources\"] = {\n            \"status\": \"unhealthy\",\n            \"error\": str(e)\n        }\n\n    # System metrics\n    try:\n        process = psutil.Process()\n        health_data[\"metrics\"] = {\n            \"memory_mb\": process.memory_info().rss / 1024 / 1024,\n            \"cpu_percent\": process.cpu_percent(),\n            \"uptime_seconds\": time.time() - process.create_time()\n        }\n    except Exception as e:\n        health_data[\"metrics\"] = {\"error\": str(e)}\n\n    # Calculate response time\n    health_data[\"response_time_ms\"] = (time.time() - start_time) * 1000\n\n    return JSONResponse(health_data)\n\n@mcp.custom_route(\"/debug/config\", methods=[\"GET\"])\nasync def debug_config(request) -&gt; JSONResponse:\n    \"\"\"Debug endpoint to show current configuration.\"\"\"\n\n    config_data = {\n        \"settings\": {\n            \"debug\": mcp.settings.debug,\n            \"log_level\": mcp.settings.log_level,\n            \"host\": mcp.settings.host,\n            \"port\": mcp.settings.port,\n            \"warn_on_duplicate_tools\": mcp.settings.warn_on_duplicate_tools,\n            \"warn_on_duplicate_resources\": mcp.settings.warn_on_duplicate_resources,\n            \"warn_on_duplicate_prompts\": mcp.settings.warn_on_duplicate_prompts\n        },\n        \"environment\": {\n            key: value for key, value in os.environ.items()\n            if key.startswith(\"FASTMCP_\")\n        },\n        \"python\": {\n            \"version\": sys.version,\n            \"platform\": sys.platform,\n            \"executable\": sys.executable\n        }\n    }\n\n    return JSONResponse(config_data)\n</code></pre>"},{"location":"development/fastmcp/debugging/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/debugging/#debug-guidelines","title":"Debug Guidelines","text":"<ol> <li>Use Environment Variables: Configure debugging through <code>FASTMCP_*</code> environment variables for easy deployment</li> <li>Structured Logging: Include context information in all log messages</li> <li>Progress Reporting: Use <code>ctx.report_progress()</code> for long-running operations</li> <li>Error Context: Preserve error context when wrapping exceptions</li> <li>Transport Considerations: Remember stdio uses stderr for logging to avoid protocol interference</li> </ol>"},{"location":"development/fastmcp/debugging/#performance-debugging_1","title":"Performance Debugging","text":"<ol> <li>Timing Decorators: Use timing decorators for performance-critical tools</li> <li>Memory Monitoring: Monitor memory usage in long-running servers</li> <li>Request Tracing: Implement request tracing for complex workflows</li> <li>Health Checks: Implement comprehensive health check endpoints</li> </ol>"},{"location":"development/fastmcp/debugging/#security-considerations","title":"Security Considerations","text":"<ol> <li>Log Sanitization: Don't log sensitive information in production</li> <li>Debug Mode: Disable debug mode in production unless necessary</li> <li>Error Messages: Don't expose internal details in error messages</li> <li>Access Control: Secure debug endpoints with proper authentication</li> </ol> <p>This comprehensive debugging guide provides all the tools and techniques needed to effectively debug FastMCP servers throughout the development lifecycle, from local development to production troubleshooting.</p>"},{"location":"development/fastmcp/development-workflow/","title":"Development Workflow","text":"<p>Complete guide for developing FastMCP servers locally, including setup, testing, debugging, and deployment.</p>"},{"location":"development/fastmcp/development-workflow/#quick-start","title":"Quick Start","text":""},{"location":"development/fastmcp/development-workflow/#1-environment-setup","title":"1. Environment Setup","text":"<p>Python Version: FastMCP requires Python 3.10 or later.</p> <p>Package Manager: Use <code>uv</code> exclusively (never pip): <pre><code># Install uv if you haven't already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create new project\nuv init my-mcp-server\ncd my-mcp-server\n\n# Add FastMCP dependency\nuv add mcp\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#2-project-structure","title":"2. Project Structure","text":"<p>Create a well-organized project structure: <pre><code>my-mcp-server/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 uv.lock\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_mcp_server/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 main.py          # Entry point\n\u2502       \u251c\u2500\u2500 tools/           # Tool implementations\n\u2502       \u251c\u2500\u2500 resources/       # Resource handlers\n\u2502       \u2514\u2500\u2500 prompts/         # Prompt templates\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_tools.py\n\u2502   \u251c\u2500\u2500 test_resources.py\n\u2502   \u2514\u2500\u2500 test_integration.py\n\u2514\u2500\u2500 examples/\n    \u2514\u2500\u2500 client_examples.py\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#3-basic-server-template","title":"3. Basic Server Template","text":"<p>Create <code>src/my_mcp_server/main.py</code>: <pre><code>\"\"\"My MCP Server - A FastMCP server implementation.\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP, Context\n\n# Create server with descriptive name and instructions\nmcp = FastMCP(\n    name=\"My MCP Server\",\n    instructions=\"A helpful server that provides X, Y, and Z capabilities\"\n)\n\n@mcp.tool()\ndef example_tool(input_text: str, ctx: Context) -&gt; str:\n    \"\"\"Example tool with logging and error handling.\"\"\"\n    ctx.info(f\"Processing: {input_text}\")\n\n    try:\n        result = input_text.upper()\n        ctx.info(f\"Result: {result}\")\n        return result\n    except Exception as e:\n        ctx.error(f\"Tool failed: {e}\")\n        raise\n\n@mcp.resource(\"data://example\")\ndef example_resource() -&gt; str:\n    \"\"\"Example static resource.\"\"\"\n    return \"Hello from my MCP server!\"\n\n@mcp.prompt()\ndef example_prompt(topic: str) -&gt; str:\n    \"\"\"Generate a prompt about a topic.\"\"\"\n    return f\"Please explain {topic} in simple terms.\"\n\nif __name__ == \"__main__\":\n    # Run with stdio transport for MCP clients\n    mcp.run(\"stdio\")\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#development-workflow_1","title":"Development Workflow","text":""},{"location":"development/fastmcp/development-workflow/#1-local-development-setup","title":"1. Local Development Setup","text":"<p>Install development dependencies: <pre><code># Add development tools\nuv add --dev pytest pytest-anyio pytest-examples\nuv add --dev ruff pyright\nuv add --dev uvicorn  # For HTTP transport testing\n</code></pre></p> <p>Configure your <code>pyproject.toml</code>: <pre><code>[project]\nname = \"my-mcp-server\"\nversion = \"0.1.0\"\ndescription = \"My FastMCP server\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [\"mcp\"]\n\n[project.scripts]\nmy-mcp-server = \"my_mcp_server.main:main\"\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py310\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\"]\n\n[tool.pyright]\nvenvPath = \".\"\nvenv = \".venv\"\ninclude = [\"src\", \"tests\"]\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#2-code-quality-setup","title":"2. Code Quality Setup","text":"<p>Install pre-commit hooks: <pre><code>uv add --dev pre-commit\nuv run pre-commit install\n</code></pre></p> <p>Create <code>.pre-commit-config.yaml</code>: <pre><code>repos:\n  - repo: local\n    hooks:\n      - id: ruff-format\n        name: Ruff Format\n        entry: uv run ruff format\n        language: system\n        types: [python]\n        pass_filenames: false\n      - id: ruff-check\n        name: Ruff Check\n        entry: uv run ruff check --fix\n        language: system\n        types: [python]\n        pass_filenames: false\n      - id: pyright\n        name: Type Check\n        entry: uv run pyright\n        language: system\n        types: [python]\n        pass_filenames: false\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#3-development-loop","title":"3. Development Loop","text":"<p>Daily development workflow: <pre><code># 1. Code formatting (run frequently)\nuv run ruff format .\n\n# 2. Linting and auto-fixes\nuv run ruff check . --fix\n\n# 3. Type checking\nuv run pyright\n\n# 4. Run tests\nuv run pytest\n\n# 5. Test your server manually\nuv run python -m my_mcp_server.main\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#testing","title":"Testing","text":""},{"location":"development/fastmcp/development-workflow/#1-unit-testing-setup","title":"1. Unit Testing Setup","text":"<p>Basic test structure (<code>tests/test_tools.py</code>): <pre><code>import pytest\nfrom mcp.server.fastmcp import FastMCP, Context\nfrom mcp.shared.memory import create_connected_server_and_client_session\n\n@pytest.mark.anyio\nasync def test_example_tool():\n    \"\"\"Test tool functionality.\"\"\"\n    mcp = FastMCP(\"Test Server\")\n\n    @mcp.tool()\n    def test_tool(x: int) -&gt; int:\n        return x * 2\n\n    # Test with connected session\n    async with create_connected_server_and_client_session(mcp._mcp_server) as (\n        server_session,\n        client_session,\n    ):\n        # Call tool through client\n        result = await client_session.call_tool(\"test_tool\", {\"x\": 5})\n        assert result.content[0].text == \"10\"\n</code></pre></p> <p>Testing with context (<code>tests/test_context.py</code>): <pre><code>import pytest\nfrom unittest.mock import AsyncMock\nfrom mcp.server.fastmcp import FastMCP, Context\n\n@pytest.mark.anyio\nasync def test_tool_with_context():\n    \"\"\"Test tool that uses context.\"\"\"\n    mcp = FastMCP(\"Test Server\")\n\n    @mcp.tool()\n    async def context_tool(message: str, ctx: Context) -&gt; str:\n        await ctx.info(f\"Processing: {message}\")\n        return f\"Processed: {message}\"\n\n    # Create mock context\n    mock_context = AsyncMock(spec=Context)\n\n    # Test tool directly\n    tool = mcp._tool_manager._tools[\"context_tool\"]\n    result = await tool.run({\"message\": \"test\"}, context=mock_context)\n\n    assert result == \"Processed: test\"\n    mock_context.info.assert_called_once_with(\"Processing: test\")\n</code></pre></p> <p>Resource testing (<code>tests/test_resources.py</code>): <pre><code>import pytest\nfrom mcp.server.fastmcp import FastMCP\n\n@pytest.mark.anyio\nasync def test_static_resource():\n    \"\"\"Test static resource.\"\"\"\n    mcp = FastMCP(\"Test Server\")\n\n    @mcp.resource(\"data://test\")\n    def test_resource() -&gt; str:\n        return \"test data\"\n\n    # Test resource reading\n    content = await mcp.read_resource(\"data://test\")\n    assert content[0].content == \"test data\"\n\n@pytest.mark.anyio\nasync def test_template_resource():\n    \"\"\"Test template resource.\"\"\"\n    mcp = FastMCP(\"Test Server\")\n\n    @mcp.resource(\"data://{param}\")\n    def template_resource(param: str) -&gt; str:\n        return f\"data for {param}\"\n\n    # Test with parameter\n    content = await mcp.read_resource(\"data://example\")\n    assert content[0].content == \"data for example\"\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#2-integration-testing","title":"2. Integration Testing","text":"<p>HTTP transport testing: <pre><code>import pytest\nimport httpx\nfrom mcp.server.fastmcp import FastMCP\n\n@pytest.mark.anyio\nasync def test_sse_server():\n    \"\"\"Test SSE transport integration.\"\"\"\n    mcp = FastMCP(\"Test Server\", port=8001)\n\n    @mcp.tool()\n    def test_tool() -&gt; str:\n        return \"success\"\n\n    # Start server in background\n    import asyncio\n    server_task = asyncio.create_task(mcp.run_sse_async())\n\n    try:\n        # Wait for server to start\n        await asyncio.sleep(0.1)\n\n        # Test health endpoint (if you add one)\n        async with httpx.AsyncClient() as client:\n            response = await client.get(\"http://localhost:8001/health\")\n            assert response.status_code == 200\n    finally:\n        server_task.cancel()\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#3-running-tests","title":"3. Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=src --cov-report=html\n\n# Run specific test file\nuv run pytest tests/test_tools.py\n\n# Run with verbose output\nuv run pytest -v\n\n# Run tests in parallel\nuv run pytest -n auto\n\n# Debug failing tests\nuv run pytest -s --tb=long\n</code></pre>"},{"location":"development/fastmcp/development-workflow/#debugging","title":"Debugging","text":""},{"location":"development/fastmcp/development-workflow/#1-local-debugging","title":"1. Local Debugging","text":"<p>Enable debug mode: <pre><code>mcp = FastMCP(\n    \"My Server\",\n    debug=True,           # Enable debug mode\n    log_level=\"DEBUG\"     # Verbose logging\n)\n</code></pre></p> <p>Environment variables: <pre><code># Set via environment\nexport FASTMCP_DEBUG=true\nexport FASTMCP_LOG_LEVEL=DEBUG\n\n# Run your server\nuv run python -m my_mcp_server.main\n</code></pre></p> <p>Rich logging (recommended): <pre><code># Install rich for better logging\nuv add rich\n\n# Rich logging is automatically used when available\nuv run python -m my_mcp_server.main\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#2-tool-debugging","title":"2. Tool Debugging","text":"<p>Add logging to tools: <pre><code>@mcp.tool()\nasync def debug_tool(data: dict, ctx: Context) -&gt; str:\n    \"\"\"Tool with comprehensive logging.\"\"\"\n    ctx.debug(f\"Input data: {data}\")\n    ctx.info(\"Processing started\")\n\n    try:\n        # Your tool logic\n        result = process_data(data)\n        ctx.info(f\"Processing completed: {result}\")\n        return result\n    except ValueError as e:\n        ctx.warning(f\"Invalid input: {e}\")\n        return f\"Error: {e}\"\n    except Exception as e:\n        ctx.error(f\"Unexpected error: {e}\")\n        raise\n</code></pre></p> <p>Progress reporting for long operations: <pre><code>@mcp.tool()\nasync def long_operation(items: list[str], ctx: Context) -&gt; str:\n    \"\"\"Tool with progress reporting.\"\"\"\n    total = len(items)\n    results = []\n\n    for i, item in enumerate(items):\n        # Report progress\n        await ctx.report_progress(i, total, f\"Processing {item}\")\n\n        # Process item\n        result = await process_item(item)\n        results.append(result)\n\n    await ctx.report_progress(total, total, \"Complete\")\n    return f\"Processed {total} items\"\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#3-http-transport-debugging","title":"3. HTTP Transport Debugging","text":"<p>Debug HTTP endpoints: <pre><code>from starlette.requests import Request\nfrom starlette.responses import JSONResponse\n\n@mcp.custom_route(\"/debug\", methods=[\"GET\"])\nasync def debug_endpoint(request: Request) -&gt; JSONResponse:\n    \"\"\"Debug endpoint to inspect server state.\"\"\"\n    return JSONResponse({\n        \"tools\": len(mcp._tool_manager._tools),\n        \"resources\": len(mcp._resource_manager._resources),\n        \"prompts\": len(mcp._prompt_manager._prompts),\n    })\n\n# Test the endpoint\n# curl http://localhost:8000/debug\n</code></pre></p> <p>Request logging: <pre><code>import logging\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\n\nclass LoggingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        logger = logging.getLogger(\"request\")\n        logger.info(f\"{request.method} {request.url}\")\n        response = await call_next(request)\n        logger.info(f\"Response: {response.status_code}\")\n        return response\n\n# Add to your server (advanced usage)\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#4-client-testing","title":"4. Client Testing","text":"<p>Manual client testing: <pre><code>\"\"\"Test client for manual debugging.\"\"\"\nimport asyncio\nfrom mcp.client.stdio import stdio_client\nfrom mcp.client.session import ClientSession\n\nasync def test_client():\n    \"\"\"Connect to your server and test it.\"\"\"\n    # Start your server with stdio: python -m my_mcp_server.main\n\n    async with stdio_client() as (read, write):\n        async with ClientSession(read, write) as session:\n            # Initialize\n            await session.initialize()\n\n            # List tools\n            tools = await session.list_tools()\n            print(f\"Available tools: {[t.name for t in tools.tools]}\")\n\n            # Call a tool\n            result = await session.call_tool(\"example_tool\", {\"input_text\": \"hello\"})\n            print(f\"Tool result: {result.content}\")\n\n            # Read a resource\n            resource = await session.read_resource(\"data://example\")\n            print(f\"Resource: {resource.contents}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(test_client())\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/development-workflow/#1-code-organization","title":"1. Code Organization","text":"<p>Separate concerns: <pre><code># tools/calculator.py\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n# resources/data.py\n@mcp.resource(\"data://users/{user_id}\")\ndef get_user_data(user_id: str) -&gt; dict:\n    \"\"\"Get user data by ID.\"\"\"\n    return load_user_data(user_id)\n\n# main.py\nfrom .tools import calculator\nfrom .resources import data\n</code></pre></p> <p>Use type hints everywhere: <pre><code>from typing import List, Dict, Optional\n\n@mcp.tool()\ndef process_items(\n    items: List[str],\n    options: Optional[Dict[str, str]] = None\n) -&gt; Dict[str, int]:\n    \"\"\"Process items with type safety.\"\"\"\n    return {item: len(item) for item in items}\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#2-error-handling","title":"2. Error Handling","text":"<p>Graceful error handling: <pre><code>@mcp.tool()\nasync def robust_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with comprehensive error handling.\"\"\"\n    try:\n        # Validate input\n        if not data.strip():\n            raise ValueError(\"Data cannot be empty\")\n\n        # Process\n        result = await process_data(data)\n\n        # Validate output\n        if not result:\n            ctx.warning(\"Processing returned empty result\")\n            return \"No data processed\"\n\n        return result\n\n    except ValueError as e:\n        ctx.error(f\"Validation error: {e}\")\n        return f\"Error: {e}\"\n    except Exception as e:\n        ctx.error(f\"Unexpected error: {e}\")\n        # Log full traceback for debugging\n        import traceback\n        ctx.debug(traceback.format_exc())\n        raise\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#3-performance-considerations","title":"3. Performance Considerations","text":"<p>Async best practices: <pre><code>import asyncio\n\n@mcp.tool()\nasync def batch_process(items: List[str], ctx: Context) -&gt; List[str]:\n    \"\"\"Process items concurrently.\"\"\"\n    ctx.info(f\"Processing {len(items)} items concurrently\")\n\n    # Use asyncio.gather for concurrent processing\n    tasks = [process_single_item(item) for item in items]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Handle any exceptions\n    processed = []\n    for i, result in enumerate(results):\n        if isinstance(result, Exception):\n            ctx.warning(f\"Item {i} failed: {result}\")\n            processed.append(f\"Error: {result}\")\n        else:\n            processed.append(result)\n\n    return processed\n</code></pre></p> <p>Resource caching: <pre><code>import functools\nfrom typing import Dict, Any\n\n# Simple in-memory cache\n_cache: Dict[str, Any] = {}\n\n@mcp.resource(\"data://cached/{key}\")\n@functools.lru_cache(maxsize=100)\ndef cached_resource(key: str) -&gt; str:\n    \"\"\"Resource with caching.\"\"\"\n    # This will be cached automatically\n    return expensive_computation(key)\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#4-testing-strategies","title":"4. Testing Strategies","text":"<p>Mock external dependencies: <pre><code>import pytest\nfrom unittest.mock import patch, AsyncMock\n\n@pytest.mark.anyio\nasync def test_tool_with_external_api():\n    \"\"\"Test tool that calls external API.\"\"\"\n    mcp = FastMCP(\"Test Server\")\n\n    @mcp.tool()\n    async def api_tool(query: str) -&gt; str:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(f\"https://api.example.com?q={query}\")\n            return response.json()[\"result\"]\n\n    # Mock the external API\n    with patch('httpx.AsyncClient') as mock_client:\n        mock_response = AsyncMock()\n        mock_response.json.return_value = {\"result\": \"mocked data\"}\n        mock_client.return_value.__aenter__.return_value.get.return_value = mock_response\n\n        # Test the tool\n        tool = mcp._tool_manager._tools[\"api_tool\"]\n        result = await tool.run({\"query\": \"test\"})\n        assert \"mocked data\" in result\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#5-environment-configuration","title":"5. Environment Configuration","text":"<p>Configuration management: <pre><code>from pydantic_settings import BaseSettings\n\nclass ServerSettings(BaseSettings):\n    \"\"\"Server configuration.\"\"\"\n    debug: bool = False\n    log_level: str = \"INFO\"\n    api_key: str = \"\"\n    database_url: str = \"sqlite:///data.db\"\n\n    class Config:\n        env_prefix = \"MCP_\"\n\nsettings = ServerSettings()\n\nmcp = FastMCP(\n    \"My Server\",\n    debug=settings.debug,\n    log_level=settings.log_level\n)\n</code></pre></p> <p>Environment files: <pre><code># .env\nMCP_DEBUG=true\nMCP_LOG_LEVEL=DEBUG\nMCP_API_KEY=your-secret-key\nMCP_DATABASE_URL=postgresql://user:pass@localhost/db\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#deployment","title":"Deployment","text":""},{"location":"development/fastmcp/development-workflow/#1-building-for-distribution","title":"1. Building for Distribution","text":"<p>Create distributable package: <pre><code># Build wheel\nuv build\n\n# Install locally for testing\nuv pip install dist/my_mcp_server-0.1.0-py3-none-any.whl\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#2-container-deployment","title":"2. Container Deployment","text":"<p>Dockerfile: <pre><code>FROM python:3.11-slim\n\n# Install uv\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\n\n# Set working directory\nWORKDIR /app\n\n# Copy project files\nCOPY pyproject.toml uv.lock ./\nCOPY src/ src/\n\n# Install dependencies\nRUN uv sync --frozen\n\n# Run server\nCMD [\"uv\", \"run\", \"python\", \"-m\", \"my_mcp_server.main\"]\n</code></pre></p>"},{"location":"development/fastmcp/development-workflow/#3-production-considerations","title":"3. Production Considerations","text":"<p>Graceful shutdown: <pre><code>import signal\nimport asyncio\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(app: FastMCP):\n    \"\"\"Handle startup and shutdown.\"\"\"\n    print(\"Server starting...\")\n\n    # Setup signal handlers\n    loop = asyncio.get_event_loop()\n    stop_event = asyncio.Event()\n\n    def signal_handler():\n        print(\"Shutdown signal received\")\n        stop_event.set()\n\n    for sig in (signal.SIGTERM, signal.SIGINT):\n        loop.add_signal_handler(sig, signal_handler)\n\n    try:\n        yield\n    finally:\n        print(\"Server shutting down...\")\n\nmcp = FastMCP(\"Production Server\", lifespan=lifespan)\n</code></pre></p> <p>This workflow guide provides everything needed for professional FastMCP development, from initial setup through production deployment.</p>"},{"location":"development/fastmcp/event-store-integration/","title":"Event Store Integration","text":"<p>Comprehensive guide to implementing event stores in FastMCP for connection resumability, session persistence, and robust client-server communication patterns.</p>"},{"location":"development/fastmcp/event-store-integration/#overview","title":"Overview","text":"<p>Event stores in FastMCP enable connection resumability by persisting events that can be replayed when clients reconnect. This is particularly valuable for unstable network connections, long-running operations, and distributed deployments where clients need to recover from disconnections.</p>"},{"location":"development/fastmcp/event-store-integration/#event-store-architecture","title":"Event Store Architecture","text":"<p>FastMCP's event store system consists of three core components:</p> <ol> <li>EventStore Interface: Abstract base class defining storage and replay operations</li> <li>StreamableHTTP Transport: Integration layer that automatically stores and replays events</li> <li>Event Management: Automatic event ID generation, stream association, and chronological ordering</li> </ol>"},{"location":"development/fastmcp/event-store-integration/#core-interface","title":"Core Interface","text":"<pre><code>from abc import ABC, abstractmethod\nfrom mcp.types import JSONRPCMessage\n\nclass EventStore(ABC):\n    \"\"\"Interface for resumability support via event storage.\"\"\"\n\n    @abstractmethod\n    async def store_event(\n        self,\n        stream_id: str,\n        message: JSONRPCMessage\n    ) -&gt; str:\n        \"\"\"Store an event and return unique event ID.\"\"\"\n        pass\n\n    @abstractmethod\n    async def replay_events_after(\n        self,\n        last_event_id: str,\n        send_callback,\n    ) -&gt; str | None:\n        \"\"\"Replay events after specified ID via callback.\"\"\"\n        pass\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#built-in-event-store-implementation","title":"Built-in Event Store Implementation","text":"<p>FastMCP includes an in-memory event store suitable for development and single-instance deployments.</p>"},{"location":"development/fastmcp/event-store-integration/#inmemoryeventstore-usage","title":"InMemoryEventStore Usage","text":"<pre><code>from mcp.server.fastmcp import FastMCP\nfrom examples.servers.simple_streamablehttp.mcp_simple_streamablehttp.event_store import InMemoryEventStore\n\n# Create event store with size limits\nevent_store = InMemoryEventStore(max_events_per_stream=1000)\n\n# Create FastMCP server with event store\nmcp = FastMCP(\n    name=\"resumable-server\",\n    event_store=event_store\n)\n\n@mcp.tool()\nasync def long_running_task(duration: int, ctx: Context) -&gt; str:\n    \"\"\"Tool that benefits from resumability.\"\"\"\n    import asyncio\n\n    # Send progress updates during long operation\n    for i in range(duration):\n        await ctx.info(f\"Processing step {i+1}/{duration}\")\n        await asyncio.sleep(1)\n\n    await ctx.info(\"Task completed!\")\n    return f\"Completed {duration}-second task\"\n\nif __name__ == \"__main__\":\n    # StreamableHTTP automatically uses event store for resumability\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#event-store-configuration","title":"Event Store Configuration","text":"<pre><code># Configure event store behavior\nevent_store = InMemoryEventStore(\n    max_events_per_stream=500  # Limit memory usage per stream\n)\n\n# Environment variable configuration\nimport os\nos.environ['FASTMCP_STATELESS_HTTP'] = 'false'  # Required for event stores\n\nmcp = FastMCP(\"configured-server\", event_store=event_store)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#custom-event-store-implementations","title":"Custom Event Store Implementations","text":""},{"location":"development/fastmcp/event-store-integration/#database-backed-event-store","title":"Database-Backed Event Store","text":"<pre><code>import asyncio\nimport json\nimport time\nfrom datetime import datetime\nfrom uuid import uuid4\nimport aiosqlite\nfrom mcp.server.streamable_http import EventStore\nfrom mcp.types import JSONRPCMessage\n\nclass SQLiteEventStore(EventStore):\n    \"\"\"SQLite-based event store for persistent storage.\"\"\"\n\n    def __init__(self, db_path: str = \"events.db\"):\n        self.db_path = db_path\n        self._initialized = False\n\n    async def _ensure_initialized(self):\n        \"\"\"Initialize database schema if needed.\"\"\"\n        if not self._initialized:\n            async with aiosqlite.connect(self.db_path) as db:\n                await db.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS events (\n                        event_id TEXT PRIMARY KEY,\n                        stream_id TEXT NOT NULL,\n                        message TEXT NOT NULL,\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        sequence_number INTEGER\n                    )\n                \"\"\")\n                await db.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_stream_sequence\n                    ON events(stream_id, sequence_number)\n                \"\"\")\n                await db.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_event_id_created\n                    ON events(event_id, created_at)\n                \"\"\")\n                await db.commit()\n            self._initialized = True\n\n    async def store_event(\n        self,\n        stream_id: str,\n        message: JSONRPCMessage\n    ) -&gt; str:\n        \"\"\"Store event in SQLite database.\"\"\"\n        await self._ensure_initialized()\n\n        event_id = f\"{int(time.time() * 1000000)}-{uuid4().hex[:8]}\"\n        message_json = json.dumps(message.model_dump())\n\n        async with aiosqlite.connect(self.db_path) as db:\n            # Get next sequence number for stream\n            cursor = await db.execute(\n                \"SELECT COALESCE(MAX(sequence_number), 0) + 1 FROM events WHERE stream_id = ?\",\n                (stream_id,)\n            )\n            sequence_number = (await cursor.fetchone())[0]\n\n            # Store event\n            await db.execute(\n                \"\"\"INSERT INTO events (event_id, stream_id, message, sequence_number)\n                   VALUES (?, ?, ?, ?)\"\"\",\n                (event_id, stream_id, message_json, sequence_number)\n            )\n            await db.commit()\n\n        return event_id\n\n    async def replay_events_after(\n        self,\n        last_event_id: str,\n        send_callback,\n    ) -&gt; str | None:\n        \"\"\"Replay events after specified event ID.\"\"\"\n        await self._ensure_initialized()\n\n        async with aiosqlite.connect(self.db_path) as db:\n            # Find the stream and sequence number for last_event_id\n            cursor = await db.execute(\n                \"SELECT stream_id, sequence_number FROM events WHERE event_id = ?\",\n                (last_event_id,)\n            )\n            result = await cursor.fetchone()\n\n            if not result:\n                return None\n\n            stream_id, last_sequence = result\n\n            # Get events after the last sequence number\n            cursor = await db.execute(\n                \"\"\"SELECT message FROM events\n                   WHERE stream_id = ? AND sequence_number &gt; ?\n                   ORDER BY sequence_number\"\"\",\n                (stream_id, last_sequence)\n            )\n\n            # Replay events via callback\n            async for row in cursor:\n                message_data = json.loads(row[0])\n                await send_callback({\n                    \"message\": message_data,\n                    \"event_id\": None  # Event ID not needed for replay\n                })\n\n            return stream_id\n\n# Usage with custom event store\nsqlite_store = SQLiteEventStore(\"production_events.db\")\nmcp = FastMCP(\"persistent-server\", event_store=sqlite_store)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#redis-based-event-store","title":"Redis-Based Event Store","text":"<pre><code>import json\nimport time\nfrom uuid import uuid4\nimport redis.asyncio as redis\nfrom mcp.server.streamable_http import EventStore\nfrom mcp.types import JSONRPCMessage\n\nclass RedisEventStore(EventStore):\n    \"\"\"Redis-based event store for distributed deployments.\"\"\"\n\n    def __init__(self, redis_url: str = \"redis://localhost:6379\"):\n        self.redis_url = redis_url\n        self._redis = None\n\n    async def _get_redis(self):\n        \"\"\"Get Redis connection with lazy initialization.\"\"\"\n        if self._redis is None:\n            self._redis = redis.from_url(self.redis_url)\n        return self._redis\n\n    async def store_event(\n        self,\n        stream_id: str,\n        message: JSONRPCMessage\n    ) -&gt; str:\n        \"\"\"Store event in Redis stream.\"\"\"\n        redis_client = await self._get_redis()\n\n        # Generate event ID with timestamp and random component\n        event_id = f\"{int(time.time() * 1000)}-{uuid4().hex[:8]}\"\n\n        # Store in Redis stream\n        await redis_client.xadd(\n            f\"mcp_stream:{stream_id}\",\n            {\n                \"message\": json.dumps(message.model_dump()),\n                \"event_id\": event_id\n            },\n            id=event_id\n        )\n\n        # Optional: Set TTL for automatic cleanup\n        await redis_client.expire(f\"mcp_stream:{stream_id}\", 86400)  # 24 hours\n\n        return event_id\n\n    async def replay_events_after(\n        self,\n        last_event_id: str,\n        send_callback,\n    ) -&gt; str | None:\n        \"\"\"Replay events from Redis stream.\"\"\"\n        redis_client = await self._get_redis()\n\n        # Find stream containing the last event ID\n        # In production, you might need a more sophisticated lookup\n        stream_pattern = \"mcp_stream:*\"\n        streams = await redis_client.keys(stream_pattern)\n\n        for stream_key in streams:\n            # Read events after last_event_id\n            try:\n                events = await redis_client.xread(\n                    {stream_key: last_event_id},\n                    count=1000,  # Adjust based on needs\n                    block=0\n                )\n\n                if events:\n                    stream_id = stream_key.decode().replace(\"mcp_stream:\", \"\")\n\n                    for event_id, fields in events[0][1]:\n                        message_data = json.loads(fields[b'message'].decode())\n                        await send_callback({\n                            \"message\": message_data,\n                            \"event_id\": fields[b'event_id'].decode()\n                        })\n\n                    return stream_id\n            except redis.ResponseError:\n                # Event ID not found in this stream\n                continue\n\n        return None\n\n# Usage with Redis event store\nredis_store = RedisEventStore(\"redis://redis-server:6379\")\nmcp = FastMCP(\"distributed-server\", event_store=redis_store)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#connection-resumability-patterns","title":"Connection Resumability Patterns","text":""},{"location":"development/fastmcp/event-store-integration/#client-side-resumption","title":"Client-Side Resumption","text":"<p>Clients can resume connections by sending the last received event ID:</p> <pre><code># Client reconnection with resumption\nfrom mcp.client.session import ClientSession\n\nasync def resumable_client():\n    \"\"\"Client with automatic resumption.\"\"\"\n    last_event_id = None\n\n    # Store event IDs for resumption\n    def handle_event(event):\n        nonlocal last_event_id\n        if hasattr(event, 'event_id'):\n            last_event_id = event.event_id\n\n    # Reconnect with resumption\n    session = ClientSession(\n        read_stream=read_stream,\n        write_stream=write_stream,\n        on_event=handle_event,\n        resumption_token=last_event_id  # Resume from this point\n    )\n\n    await session.initialize()\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#server-side-resumption-handling","title":"Server-Side Resumption Handling","text":"<pre><code>@mcp.tool()\nasync def resumable_operation(\n    operation_id: str,\n    step: int = 0,\n    ctx: Context\n) -&gt; str:\n    \"\"\"Tool that supports resumption from any step.\"\"\"\n\n    # Check if operation was previously started\n    state = await get_operation_state(operation_id)\n\n    if state and step &lt; state.get('current_step', 0):\n        await ctx.info(f\"Resuming operation {operation_id} from step {state['current_step']}\")\n        step = state['current_step']\n\n    # Continue operation from current step\n    total_steps = 10\n    for current_step in range(step, total_steps):\n        await ctx.info(f\"Executing step {current_step + 1}/{total_steps}\")\n\n        # Save progress for resumability\n        await save_operation_state(operation_id, {\n            'current_step': current_step + 1,\n            'status': 'in_progress'\n        })\n\n        # Simulate work\n        await asyncio.sleep(1)\n\n    await save_operation_state(operation_id, {'status': 'completed'})\n    return f\"Operation {operation_id} completed successfully\"\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#advanced-event-store-patterns","title":"Advanced Event Store Patterns","text":""},{"location":"development/fastmcp/event-store-integration/#event-filtering-and-transformation","title":"Event Filtering and Transformation","text":"<pre><code>class FilteredEventStore(EventStore):\n    \"\"\"Event store that filters events before storage.\"\"\"\n\n    def __init__(self, backend_store: EventStore, event_filter=None):\n        self.backend = backend_store\n        self.filter = event_filter or (lambda msg: True)\n\n    async def store_event(\n        self,\n        stream_id: str,\n        message: JSONRPCMessage\n    ) -&gt; str:\n        \"\"\"Store event only if it passes filter.\"\"\"\n        if self.filter(message):\n            return await self.backend.store_event(stream_id, message)\n        else:\n            # Return dummy event ID for filtered events\n            return f\"filtered-{uuid4().hex[:8]}\"\n\n    async def replay_events_after(\n        self,\n        last_event_id: str,\n        send_callback,\n    ) -&gt; str | None:\n        \"\"\"Replay filtered events.\"\"\"\n        return await self.backend.replay_events_after(last_event_id, send_callback)\n\n# Usage with filtering\ndef important_events_only(message: JSONRPCMessage) -&gt; bool:\n    \"\"\"Filter to only store important events.\"\"\"\n    # Only store tool calls and responses\n    return hasattr(message, 'method') and message.method in ['tools/call', 'tools/list']\n\nfiltered_store = FilteredEventStore(\n    backend_store=SQLiteEventStore(),\n    event_filter=important_events_only\n)\n\nmcp = FastMCP(\"filtered-server\", event_store=filtered_store)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#event-store-with-compression","title":"Event Store with Compression","text":"<pre><code>import gzip\nimport base64\nfrom mcp.server.streamable_http import EventStore\n\nclass CompressedEventStore(EventStore):\n    \"\"\"Event store with automatic compression.\"\"\"\n\n    def __init__(self, backend_store: EventStore):\n        self.backend = backend_store\n\n    def _compress_message(self, message: JSONRPCMessage) -&gt; str:\n        \"\"\"Compress message for storage.\"\"\"\n        json_str = json.dumps(message.model_dump())\n        compressed = gzip.compress(json_str.encode())\n        return base64.b64encode(compressed).decode()\n\n    def _decompress_message(self, compressed_data: str) -&gt; dict:\n        \"\"\"Decompress message for replay.\"\"\"\n        compressed_bytes = base64.b64decode(compressed_data.encode())\n        json_str = gzip.decompress(compressed_bytes).decode()\n        return json.loads(json_str)\n\n    async def store_event(\n        self,\n        stream_id: str,\n        message: JSONRPCMessage\n    ) -&gt; str:\n        \"\"\"Store compressed event.\"\"\"\n        # Create wrapper message with compressed payload\n        compressed_message = JSONRPCMessage(\n            jsonrpc=\"2.0\",\n            method=\"compressed_event\",\n            params={\"compressed_data\": self._compress_message(message)}\n        )\n\n        return await self.backend.store_event(stream_id, compressed_message)\n\n    async def replay_events_after(\n        self,\n        last_event_id: str,\n        send_callback,\n    ) -&gt; str | None:\n        \"\"\"Replay and decompress events.\"\"\"\n        async def decompressing_callback(event):\n            if (event.get(\"message\", {}).get(\"method\") == \"compressed_event\"):\n                compressed_data = event[\"message\"][\"params\"][\"compressed_data\"]\n                original_message = self._decompress_message(compressed_data)\n                event[\"message\"] = original_message\n\n            await send_callback(event)\n\n        return await self.backend.replay_events_after(last_event_id, decompressing_callback)\n\n# Usage with compression\ncompressed_store = CompressedEventStore(\n    backend_store=SQLiteEventStore()\n)\n\nmcp = FastMCP(\"compressed-server\", event_store=compressed_store)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#multi-tenant-event-store","title":"Multi-Tenant Event Store","text":"<pre><code>class TenantEventStore(EventStore):\n    \"\"\"Event store with tenant isolation.\"\"\"\n\n    def __init__(self, backend_store: EventStore):\n        self.backend = backend_store\n        self._current_tenant = None\n\n    def set_tenant(self, tenant_id: str):\n        \"\"\"Set current tenant context.\"\"\"\n        self._current_tenant = tenant_id\n\n    def _tenant_stream_id(self, stream_id: str) -&gt; str:\n        \"\"\"Prefix stream ID with tenant.\"\"\"\n        if self._current_tenant:\n            return f\"tenant:{self._current_tenant}:{stream_id}\"\n        return stream_id\n\n    async def store_event(\n        self,\n        stream_id: str,\n        message: JSONRPCMessage\n    ) -&gt; str:\n        \"\"\"Store event with tenant isolation.\"\"\"\n        tenant_stream_id = self._tenant_stream_id(stream_id)\n        return await self.backend.store_event(tenant_stream_id, message)\n\n    async def replay_events_after(\n        self,\n        last_event_id: str,\n        send_callback,\n    ) -&gt; str | None:\n        \"\"\"Replay events for current tenant only.\"\"\"\n        return await self.backend.replay_events_after(last_event_id, send_callback)\n\n# Usage with tenant isolation\ntenant_store = TenantEventStore(RedisEventStore())\n\n@mcp.custom_route(\"/tenant/{tenant_id}/mcp\", methods=[\"GET\", \"POST\"])\nasync def tenant_mcp_endpoint(request: Request) -&gt; Response:\n    \"\"\"Tenant-specific MCP endpoint.\"\"\"\n    tenant_id = request.path_params[\"tenant_id\"]\n\n    # Set tenant context for event store\n    tenant_store.set_tenant(tenant_id)\n\n    # Process request with tenant-isolated events\n    return await mcp.handle_request(request)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#performance-optimization","title":"Performance Optimization","text":""},{"location":"development/fastmcp/event-store-integration/#event-store-caching","title":"Event Store Caching","text":"<pre><code>from collections import OrderedDict\nimport asyncio\n\nclass CachedEventStore(EventStore):\n    \"\"\"Event store with LRU cache for performance.\"\"\"\n\n    def __init__(self, backend_store: EventStore, cache_size: int = 1000):\n        self.backend = backend_store\n        self.cache = OrderedDict()\n        self.cache_size = cache_size\n        self._cache_lock = asyncio.Lock()\n\n    async def _cache_get(self, key: str):\n        \"\"\"Get from cache with LRU update.\"\"\"\n        async with self._cache_lock:\n            if key in self.cache:\n                # Move to end (most recently used)\n                value = self.cache.pop(key)\n                self.cache[key] = value\n                return value\n            return None\n\n    async def _cache_set(self, key: str, value):\n        \"\"\"Set in cache with LRU eviction.\"\"\"\n        async with self._cache_lock:\n            if key in self.cache:\n                self.cache.pop(key)\n            elif len(self.cache) &gt;= self.cache_size:\n                # Remove oldest item\n                self.cache.popitem(last=False)\n\n            self.cache[key] = value\n\n    async def store_event(\n        self,\n        stream_id: str,\n        message: JSONRPCMessage\n    ) -&gt; str:\n        \"\"\"Store event with caching.\"\"\"\n        event_id = await self.backend.store_event(stream_id, message)\n\n        # Cache the event for fast replay\n        cache_key = f\"{stream_id}:{event_id}\"\n        await self._cache_set(cache_key, {\n            \"message\": message.model_dump(),\n            \"event_id\": event_id\n        })\n\n        return event_id\n\n    async def replay_events_after(\n        self,\n        last_event_id: str,\n        send_callback,\n    ) -&gt; str | None:\n        \"\"\"Replay with cache acceleration.\"\"\"\n        # Try cache first for recent events\n        # Fall back to backend for older events\n        return await self.backend.replay_events_after(last_event_id, send_callback)\n\n# Usage with caching\ncached_store = CachedEventStore(\n    backend_store=RedisEventStore(),\n    cache_size=5000\n)\n\nmcp = FastMCP(\"high-performance-server\", event_store=cached_store)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#testing-event-stores","title":"Testing Event Stores","text":""},{"location":"development/fastmcp/event-store-integration/#event-store-testing-utilities","title":"Event Store Testing Utilities","text":"<pre><code>import pytest\nfrom mcp.types import JSONRPCMessage\n\nclass TestEventStore:\n    \"\"\"Test utilities for event store implementations.\"\"\"\n\n    @pytest.fixture\n    async def sample_messages(self):\n        \"\"\"Sample JSONRPCMessage objects for testing.\"\"\"\n        return [\n            JSONRPCMessage(\n                jsonrpc=\"2.0\",\n                method=\"tools/list\",\n                id=\"1\"\n            ),\n            JSONRPCMessage(\n                jsonrpc=\"2.0\",\n                method=\"tools/call\",\n                params={\"name\": \"test_tool\", \"arguments\": {}},\n                id=\"2\"\n            ),\n            JSONRPCMessage(\n                jsonrpc=\"2.0\",\n                result={\"content\": [{\"type\": \"text\", \"text\": \"result\"}]},\n                id=\"2\"\n            )\n        ]\n\n    async def test_basic_storage_and_replay(self, event_store, sample_messages):\n        \"\"\"Test basic event storage and replay functionality.\"\"\"\n        stream_id = \"test-stream-1\"\n        stored_events = []\n\n        # Store events\n        for message in sample_messages:\n            event_id = await event_store.store_event(stream_id, message)\n            stored_events.append(event_id)\n            assert event_id is not None\n\n        # Test replay\n        replayed_events = []\n\n        async def collect_events(event):\n            replayed_events.append(event)\n\n        # Replay from first event\n        result_stream_id = await event_store.replay_events_after(\n            stored_events[0],\n            collect_events\n        )\n\n        assert result_stream_id == stream_id\n        assert len(replayed_events) == len(sample_messages) - 1  # Excludes first event\n\n    async def test_stream_isolation(self, event_store, sample_messages):\n        \"\"\"Test that events are isolated between streams.\"\"\"\n        stream1 = \"stream-1\"\n        stream2 = \"stream-2\"\n\n        # Store events in different streams\n        event1 = await event_store.store_event(stream1, sample_messages[0])\n        event2 = await event_store.store_event(stream2, sample_messages[1])\n\n        # Replay should only return events from same stream\n        replayed = []\n\n        async def collect(event):\n            replayed.append(event)\n\n        await event_store.replay_events_after(event1, collect)\n\n        # Should be empty since no events after event1 in stream1\n        assert len(replayed) == 0\n\n# Usage in tests\n@pytest.mark.asyncio\nasync def test_sqlite_event_store():\n    store = SQLiteEventStore(\":memory:\")  # In-memory for testing\n    test_suite = TestEventStore()\n\n    await test_suite.test_basic_storage_and_replay(store, sample_messages)\n    await test_suite.test_stream_isolation(store, sample_messages)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/event-store-integration/#event-store-design-guidelines","title":"Event Store Design Guidelines","text":"<ol> <li>Choose Appropriate Storage:</li> <li>Use InMemoryEventStore for development and testing</li> <li>Use persistent stores (SQLite, PostgreSQL) for production single-instance deployments</li> <li> <p>Use distributed stores (Redis, Apache Kafka) for multi-instance deployments</p> </li> <li> <p>Implement Proper Cleanup:</p> </li> <li>Set TTL policies for automatic event cleanup</li> <li>Implement archival strategies for long-term storage</li> <li> <p>Monitor storage growth and implement pruning</p> </li> <li> <p>Handle Failures Gracefully:</p> </li> <li>Implement retry logic for storage failures</li> <li>Provide fallback behavior when event store is unavailable</li> <li> <p>Log event store errors appropriately</p> </li> <li> <p>Security Considerations:</p> </li> <li>Encrypt sensitive event data</li> <li>Implement proper access controls</li> <li>Audit event access and modifications</li> </ol>"},{"location":"development/fastmcp/event-store-integration/#performance-optimization_1","title":"Performance Optimization","text":"<pre><code># Optimized event store usage\n@mcp.tool()\nasync def optimized_tool(large_data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool optimized for event store performance.\"\"\"\n\n    # Minimize event payload size\n    await ctx.info(\"Starting large operation\")  # Lightweight event\n\n    # Process data without creating large events\n    result = process_large_data(large_data)\n\n    # Send summary instead of full data\n    await ctx.info(f\"Processed {len(large_data)} bytes -&gt; {len(result)} bytes\")\n\n    return \"Operation completed\"  # Compact result\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#error-handling","title":"Error Handling","text":"<pre><code>class RobustEventStore(EventStore):\n    \"\"\"Event store with comprehensive error handling.\"\"\"\n\n    def __init__(self, backend_store: EventStore, max_retries: int = 3):\n        self.backend = backend_store\n        self.max_retries = max_retries\n\n    async def store_event(\n        self,\n        stream_id: str,\n        message: JSONRPCMessage\n    ) -&gt; str:\n        \"\"\"Store event with retry logic.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                return await self.backend.store_event(stream_id, message)\n            except Exception as e:\n                if attempt == self.max_retries - 1:\n                    # Log final failure\n                    logger.error(f\"Failed to store event after {self.max_retries} attempts: {e}\")\n                    # Return dummy ID to prevent client errors\n                    return f\"failed-{uuid4().hex[:8]}\"\n                else:\n                    # Wait before retry\n                    await asyncio.sleep(2 ** attempt)\n</code></pre>"},{"location":"development/fastmcp/event-store-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/fastmcp/event-store-integration/#common-issues","title":"Common Issues","text":"<ol> <li>Events Not Replaying:</li> <li>Verify event store is configured with stateful sessions</li> <li>Check Last-Event-ID header format</li> <li> <p>Ensure event IDs are properly generated</p> </li> <li> <p>Memory Leaks:</p> </li> <li>Implement event cleanup policies</li> <li>Monitor InMemoryEventStore size limits</li> <li> <p>Use weak references where appropriate</p> </li> <li> <p>Performance Issues:</p> </li> <li>Implement event store caching</li> <li>Optimize database queries with proper indexing</li> <li> <p>Consider event compression for large payloads</p> </li> <li> <p>Connection Issues:</p> </li> <li>Implement connection pooling for database stores</li> <li>Handle Redis connection failures gracefully</li> <li>Add health checks for event store availability</li> </ol> <p>This comprehensive guide provides everything needed to implement robust event store integration in FastMCP servers, from simple in-memory solutions to production-ready distributed systems.</p>"},{"location":"development/fastmcp/examples/","title":"Examples and Best Practices","text":"<p>This guide provides practical examples and best practices for building production-ready FastMCP servers.</p>"},{"location":"development/fastmcp/examples/#complete-server-examples","title":"Complete Server Examples","text":""},{"location":"development/fastmcp/examples/#1-file-management-server","title":"1. File Management Server","text":"<p>A comprehensive server for file operations with proper error handling and security:</p> <pre><code>import os\nimport mimetypes\nfrom pathlib import Path\nfrom typing import Literal, Annotated\nfrom pydantic import Field\n\nfrom mcp.server.fastmcp import FastMCP, Context\n\n# Configuration\nALLOWED_DIRECTORIES = [\n    Path.home() / \"Documents\",\n    Path.home() / \"Projects\"\n]\n\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n\nmcp = FastMCP(\n    name=\"File Manager\",\n    instructions=\"Secure file management with read/write operations\"\n)\n\ndef validate_path(path_str: str) -&gt; Path:\n    \"\"\"Validate and normalize file paths for security.\"\"\"\n    path = Path(path_str).resolve()\n\n    # Check if path is within allowed directories\n    if not any(str(path).startswith(str(allowed)) for allowed in ALLOWED_DIRECTORIES):\n        raise ValueError(f\"Access denied: {path} is outside allowed directories\")\n\n    return path\n\n@mcp.tool()\nasync def list_files(\n    directory: str,\n    pattern: str = \"*\",\n    include_hidden: bool = False,\n    ctx: Context\n) -&gt; list[dict]:\n    \"\"\"List files in a directory with optional filtering.\"\"\"\n\n    await ctx.info(f\"Listing files in {directory}\")\n\n    try:\n        dir_path = validate_path(directory)\n\n        if not dir_path.exists():\n            raise ValueError(f\"Directory does not exist: {directory}\")\n\n        if not dir_path.is_dir():\n            raise ValueError(f\"Path is not a directory: {directory}\")\n\n        files = []\n        for item in dir_path.glob(pattern):\n            # Skip hidden files unless requested\n            if item.name.startswith('.') and not include_hidden:\n                continue\n\n            stat = item.stat()\n            mime_type, _ = mimetypes.guess_type(str(item))\n\n            files.append({\n                \"name\": item.name,\n                \"path\": str(item),\n                \"type\": \"directory\" if item.is_dir() else \"file\",\n                \"size\": stat.st_size if item.is_file() else None,\n                \"modified\": stat.st_mtime,\n                \"mime_type\": mime_type if item.is_file() else None\n            })\n\n        await ctx.info(f\"Found {len(files)} items\")\n        return sorted(files, key=lambda x: (x[\"type\"], x[\"name\"]))\n\n    except Exception as e:\n        await ctx.error(f\"Failed to list files: {e}\")\n        raise\n\n@mcp.tool()\nasync def read_file(file_path: str, ctx: Context) -&gt; str:\n    \"\"\"Read file contents with safety checks.\"\"\"\n\n    await ctx.info(f\"Reading file: {file_path}\")\n\n    try:\n        path = validate_path(file_path)\n\n        if not path.exists():\n            raise ValueError(f\"File does not exist: {file_path}\")\n\n        if not path.is_file():\n            raise ValueError(f\"Path is not a file: {file_path}\")\n\n        # Check file size\n        if path.stat().st_size &gt; MAX_FILE_SIZE:\n            raise ValueError(f\"File too large: {path.stat().st_size} bytes (max: {MAX_FILE_SIZE})\")\n\n        await ctx.debug(f\"File size: {path.stat().st_size} bytes\")\n\n        try:\n            content = path.read_text(encoding='utf-8')\n            await ctx.info(f\"Successfully read {len(content)} characters\")\n            return content\n        except UnicodeDecodeError:\n            # Try binary read for non-text files\n            await ctx.warning(\"File is not UTF-8 text, reading as binary\")\n            binary_content = path.read_bytes()\n            return f\"Binary file ({len(binary_content)} bytes): {path.name}\"\n\n    except Exception as e:\n        await ctx.error(f\"Failed to read file: {e}\")\n        raise\n\n@mcp.tool()\nasync def write_file(\n    file_path: str,\n    content: str,\n    overwrite: bool = False,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Write content to a file with safety checks.\"\"\"\n\n    await ctx.info(f\"Writing to file: {file_path}\")\n\n    try:\n        path = validate_path(file_path)\n\n        # Check if file exists and overwrite policy\n        if path.exists() and not overwrite:\n            raise ValueError(f\"File exists and overwrite=False: {file_path}\")\n\n        # Ensure parent directory exists\n        path.parent.mkdir(parents=True, exist_ok=True)\n        await ctx.debug(f\"Ensured directory exists: {path.parent}\")\n\n        # Write content\n        path.write_text(content, encoding='utf-8')\n\n        # Get file info\n        stat = path.stat()\n\n        await ctx.info(f\"Successfully wrote {len(content)} characters to {file_path}\")\n\n        return {\n            \"path\": str(path),\n            \"size\": stat.st_size,\n            \"created\": not overwrite or not path.existed_before_write,  # Simplified\n            \"modified\": stat.st_mtime\n        }\n\n    except Exception as e:\n        await ctx.error(f\"Failed to write file: {e}\")\n        raise\n\n@mcp.resource(\"file://{path}\")\nasync def file_resource(path: str, ctx: Context) -&gt; str:\n    \"\"\"Expose files as resources.\"\"\"\n    await ctx.debug(f\"Accessing file resource: {path}\")\n    return await read_file(path, ctx)\n\n@mcp.resource(\"directory://{path}\")\nasync def directory_resource(path: str, ctx: Context) -&gt; list[dict]:\n    \"\"\"Expose directory listings as resources.\"\"\"\n    await ctx.debug(f\"Accessing directory resource: {path}\")\n    return await list_files(path, ctx=ctx)\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"development/fastmcp/examples/#2-database-integration-server","title":"2. Database Integration Server","text":"<p>A server that integrates with databases using proper connection management:</p> <pre><code>import sqlite3\nimport json\nfrom contextlib import asynccontextmanager\nfrom typing import Any, Optional\nfrom pydantic import Field\n\nfrom mcp.server.fastmcp import FastMCP, Context\n\n# Database configuration\nDATABASE_PATH = \"app.db\"\n\nmcp = FastMCP(\n    name=\"Database Server\",\n    instructions=\"SQL database operations with query building and data analysis\"\n)\n\nclass DatabaseManager:\n    \"\"\"Manages database connections and operations.\"\"\"\n\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self._init_database()\n\n    def _init_database(self):\n        \"\"\"Initialize database with sample tables.\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS users (\n                    id INTEGER PRIMARY KEY,\n                    name TEXT NOT NULL,\n                    email TEXT UNIQUE,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS orders (\n                    id INTEGER PRIMARY KEY,\n                    user_id INTEGER,\n                    product TEXT NOT NULL,\n                    quantity INTEGER DEFAULT 1,\n                    total_amount DECIMAL(10,2),\n                    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    FOREIGN KEY (user_id) REFERENCES users (id)\n                )\n            \"\"\")\n\n            # Insert sample data if tables are empty\n            if conn.execute(\"SELECT COUNT(*) FROM users\").fetchone()[0] == 0:\n                sample_users = [\n                    (\"Alice Johnson\", \"alice@example.com\"),\n                    (\"Bob Smith\", \"bob@example.com\"),\n                    (\"Carol Davis\", \"carol@example.com\")\n                ]\n                conn.executemany(\n                    \"INSERT INTO users (name, email) VALUES (?, ?)\",\n                    sample_users\n                )\n\n                sample_orders = [\n                    (1, \"Laptop\", 1, 999.99),\n                    (1, \"Mouse\", 2, 49.98),\n                    (2, \"Keyboard\", 1, 79.99),\n                    (3, \"Monitor\", 1, 299.99)\n                ]\n                conn.executemany(\n                    \"INSERT INTO orders (user_id, product, quantity, total_amount) VALUES (?, ?, ?, ?)\",\n                    sample_orders\n                )\n\n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Get database connection with proper cleanup.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row  # Return rows as dictionaries\n        try:\n            yield conn\n        finally:\n            conn.close()\n\n# Initialize database manager\ndb_manager = DatabaseManager(DATABASE_PATH)\n\n@mcp.tool()\nasync def execute_query(\n    query: str,\n    params: list[Any] = Field(default=[], description=\"Query parameters for safety\"),\n    limit: int = Field(default=100, description=\"Maximum rows to return\"),\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Execute a SQL query with safety restrictions.\"\"\"\n\n    await ctx.info(f\"Executing query: {query[:100]}...\")\n\n    # Safety checks\n    query_lower = query.lower().strip()\n\n    # Only allow SELECT statements\n    if not query_lower.startswith('select'):\n        await ctx.error(\"Only SELECT queries are allowed\")\n        raise ValueError(\"Only SELECT queries are allowed for safety\")\n\n    # Prevent dangerous operations\n    dangerous_keywords = ['drop', 'delete', 'insert', 'update', 'alter', 'create']\n    if any(keyword in query_lower for keyword in dangerous_keywords):\n        await ctx.error(\"Query contains dangerous keywords\")\n        raise ValueError(\"Query contains dangerous keywords\")\n\n    try:\n        async with db_manager.get_connection() as conn:\n            await ctx.debug(f\"Executing with params: {params}\")\n\n            cursor = conn.execute(f\"{query} LIMIT {limit}\", params)\n            rows = cursor.fetchall()\n\n            # Convert to list of dictionaries\n            results = [dict(row) for row in rows]\n\n            await ctx.info(f\"Query returned {len(results)} rows\")\n\n            return {\n                \"query\": query,\n                \"row_count\": len(results),\n                \"results\": results,\n                \"columns\": list(rows[0].keys()) if rows else []\n            }\n\n    except sqlite3.Error as e:\n        await ctx.error(f\"Database error: {e}\")\n        raise ValueError(f\"Database error: {e}\")\n\n@mcp.tool()\nasync def get_table_info(table_name: str, ctx: Context) -&gt; dict:\n    \"\"\"Get information about a database table.\"\"\"\n\n    await ctx.info(f\"Getting info for table: {table_name}\")\n\n    # Validate table name (prevent SQL injection)\n    if not table_name.isalnum():\n        raise ValueError(\"Table name must be alphanumeric\")\n\n    try:\n        async with db_manager.get_connection() as conn:\n            # Get table schema\n            schema_cursor = conn.execute(f\"PRAGMA table_info({table_name})\")\n            schema = schema_cursor.fetchall()\n\n            if not schema:\n                raise ValueError(f\"Table {table_name} does not exist\")\n\n            # Get row count\n            count_cursor = conn.execute(f\"SELECT COUNT(*) as count FROM {table_name}\")\n            row_count = count_cursor.fetchone()[\"count\"]\n\n            columns = [\n                {\n                    \"name\": row[\"name\"],\n                    \"type\": row[\"type\"],\n                    \"nullable\": not row[\"notnull\"],\n                    \"primary_key\": bool(row[\"pk\"])\n                }\n                for row in schema\n            ]\n\n            await ctx.info(f\"Table {table_name} has {len(columns)} columns and {row_count} rows\")\n\n            return {\n                \"table_name\": table_name,\n                \"columns\": columns,\n                \"row_count\": row_count\n            }\n\n    except sqlite3.Error as e:\n        await ctx.error(f\"Database error: {e}\")\n        raise ValueError(f\"Database error: {e}\")\n\n@mcp.tool()\nasync def analyze_data(\n    table_name: str,\n    column: str,\n    analysis_type: Literal[\"summary\", \"distribution\", \"nulls\"] = \"summary\",\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Analyze data in a specific table column.\"\"\"\n\n    await ctx.info(f\"Analyzing {column} in {table_name}\")\n\n    # Validate inputs\n    if not table_name.isalnum() or not column.replace('_', '').isalnum():\n        raise ValueError(\"Invalid table or column name\")\n\n    try:\n        async with db_manager.get_connection() as conn:\n            if analysis_type == \"summary\":\n                # Get basic statistics\n                query = f\"\"\"\n                    SELECT\n                        COUNT(*) as total_count,\n                        COUNT({column}) as non_null_count,\n                        MIN({column}) as min_value,\n                        MAX({column}) as max_value,\n                        AVG(CASE WHEN typeof({column}) = 'real' OR typeof({column}) = 'integer'\n                            THEN {column} END) as avg_value\n                    FROM {table_name}\n                \"\"\"\n\n            elif analysis_type == \"distribution\":\n                # Get value distribution\n                query = f\"\"\"\n                    SELECT {column} as value, COUNT(*) as count\n                    FROM {table_name}\n                    GROUP BY {column}\n                    ORDER BY count DESC\n                    LIMIT 20\n                \"\"\"\n\n            elif analysis_type == \"nulls\":\n                # Get null analysis\n                query = f\"\"\"\n                    SELECT\n                        COUNT(*) as total_rows,\n                        COUNT({column}) as non_null_count,\n                        (COUNT(*) - COUNT({column})) as null_count,\n                        ROUND((COUNT(*) - COUNT({column})) * 100.0 / COUNT(*), 2) as null_percentage\n                    FROM {table_name}\n                \"\"\"\n\n            cursor = conn.execute(query)\n            results = [dict(row) for row in cursor.fetchall()]\n\n            await ctx.info(f\"Analysis complete for {column}\")\n\n            return {\n                \"table\": table_name,\n                \"column\": column,\n                \"analysis_type\": analysis_type,\n                \"results\": results\n            }\n\n    except sqlite3.Error as e:\n        await ctx.error(f\"Database error: {e}\")\n        raise ValueError(f\"Database error: {e}\")\n\n@mcp.resource(\"db://tables\")\nasync def list_tables(ctx: Context) -&gt; list[str]:\n    \"\"\"List all database tables.\"\"\"\n    await ctx.debug(\"Listing database tables\")\n\n    async with db_manager.get_connection() as conn:\n        cursor = conn.execute(\"\"\"\n            SELECT name FROM sqlite_master\n            WHERE type='table' AND name NOT LIKE 'sqlite_%'\n        \"\"\")\n        return [row[\"name\"] for row in cursor.fetchall()]\n\n@mcp.resource(\"db://table/{table_name}/schema\")\nasync def table_schema_resource(table_name: str, ctx: Context) -&gt; dict:\n    \"\"\"Get table schema as a resource.\"\"\"\n    await ctx.debug(f\"Getting schema for table: {table_name}\")\n    return await get_table_info(table_name, ctx)\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"development/fastmcp/examples/#3-web-api-integration-server","title":"3. Web API Integration Server","text":"<p>A server that integrates with external APIs and provides caching:</p> <pre><code>import httpx\nimport json\nimport time\nfrom typing import Optional, Dict, Any\nfrom functools import lru_cache\nfrom pydantic import Field, HttpUrl\n\nfrom mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\n    name=\"API Integration Server\",\n    instructions=\"Integrate with external APIs with caching and rate limiting\"\n)\n\n# Configuration\nAPI_TIMEOUT = 30.0\nCACHE_TTL = 300  # 5 minutes\nMAX_RETRIES = 3\n\nclass APIClient:\n    \"\"\"HTTP client with caching and error handling.\"\"\"\n\n    def __init__(self):\n        self.cache: Dict[str, Dict[str, Any]] = {}\n\n    def _get_cache_key(self, url: str, params: Dict[str, Any]) -&gt; str:\n        \"\"\"Generate cache key for request.\"\"\"\n        return f\"{url}?{json.dumps(params, sort_keys=True)}\"\n\n    def _is_cache_valid(self, cache_entry: Dict[str, Any]) -&gt; bool:\n        \"\"\"Check if cache entry is still valid.\"\"\"\n        return time.time() - cache_entry[\"timestamp\"] &lt; CACHE_TTL\n\n    async def get(\n        self,\n        url: str,\n        params: Optional[Dict[str, Any]] = None,\n        use_cache: bool = True,\n        ctx: Optional[Context] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Make HTTP GET request with caching.\"\"\"\n\n        params = params or {}\n        cache_key = self._get_cache_key(url, params)\n\n        # Check cache first\n        if use_cache and cache_key in self.cache:\n            cache_entry = self.cache[cache_key]\n            if self._is_cache_valid(cache_entry):\n                if ctx:\n                    await ctx.debug(\"Returning cached response\")\n                return cache_entry[\"data\"]\n\n        # Make HTTP request with retries\n        for attempt in range(MAX_RETRIES):\n            try:\n                if ctx:\n                    await ctx.debug(f\"Making HTTP request to {url} (attempt {attempt + 1})\")\n\n                async with httpx.AsyncClient(timeout=API_TIMEOUT) as client:\n                    response = await client.get(url, params=params)\n                    response.raise_for_status()\n\n                    data = response.json()\n\n                    # Cache successful response\n                    if use_cache:\n                        self.cache[cache_key] = {\n                            \"data\": data,\n                            \"timestamp\": time.time()\n                        }\n\n                    if ctx:\n                        await ctx.info(f\"Successfully fetched data from {url}\")\n\n                    return data\n\n            except httpx.RequestError as e:\n                if ctx:\n                    await ctx.warning(f\"Request attempt {attempt + 1} failed: {e}\")\n                if attempt == MAX_RETRIES - 1:\n                    raise\n                await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\n            except httpx.HTTPStatusError as e:\n                if ctx:\n                    await ctx.error(f\"HTTP error {e.response.status_code}: {e}\")\n                raise\n\n# Global API client\napi_client = APIClient()\n\n@mcp.tool()\nasync def fetch_weather(\n    city: str,\n    units: Literal[\"metric\", \"imperial\"] = \"metric\",\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Fetch weather data for a city.\"\"\"\n\n    await ctx.info(f\"Fetching weather for {city}\")\n\n    # This is a mock API call - replace with real weather API\n    mock_weather_data = {\n        \"london\": {\"temp\": 15, \"condition\": \"cloudy\", \"humidity\": 80},\n        \"paris\": {\"temp\": 18, \"condition\": \"sunny\", \"humidity\": 60},\n        \"tokyo\": {\"temp\": 22, \"condition\": \"rainy\", \"humidity\": 85},\n        \"new york\": {\"temp\": 12, \"condition\": \"snow\", \"humidity\": 70}\n    }\n\n    city_lower = city.lower()\n    if city_lower not in mock_weather_data:\n        await ctx.error(f\"Weather data not available for {city}\")\n        raise ValueError(f\"Weather data not available for {city}\")\n\n    base_data = mock_weather_data[city_lower]\n\n    # Convert units if needed\n    if units == \"imperial\":\n        temp_f = (base_data[\"temp\"] * 9/5) + 32\n        data = {**base_data, \"temp\": round(temp_f, 1), \"units\": \"\u00b0F\"}\n    else:\n        data = {**base_data, \"temp\": base_data[\"temp\"], \"units\": \"\u00b0C\"}\n\n    await ctx.info(f\"Weather for {city}: {data['temp']}{data['units']}, {data['condition']}\")\n    return {\n        \"city\": city,\n        \"weather\": data,\n        \"timestamp\": time.time()\n    }\n\n@mcp.tool()\nasync def search_news(\n    query: str,\n    limit: int = Field(default=5, ge=1, le=20),\n    language: str = \"en\",\n    ctx: Context\n) -&gt; list[dict]:\n    \"\"\"Search for news articles.\"\"\"\n\n    await ctx.info(f\"Searching news for: {query}\")\n\n    # Mock news data - replace with real news API\n    mock_articles = [\n        {\n            \"title\": f\"Breaking: {query} updates\",\n            \"summary\": f\"Latest developments in {query} situation...\",\n            \"url\": f\"https://example.com/news/{query.replace(' ', '-')}-1\",\n            \"published\": time.time() - 3600,  # 1 hour ago\n            \"source\": \"Example News\"\n        },\n        {\n            \"title\": f\"Analysis: Impact of {query}\",\n            \"summary\": f\"Expert analysis on {query} and its implications...\",\n            \"url\": f\"https://example.com/news/{query.replace(' ', '-')}-2\",\n            \"published\": time.time() - 7200,  # 2 hours ago\n            \"source\": \"Tech Today\"\n        },\n        {\n            \"title\": f\"{query}: What you need to know\",\n            \"summary\": f\"Comprehensive guide to understanding {query}...\",\n            \"url\": f\"https://example.com/news/{query.replace(' ', '-')}-3\",\n            \"published\": time.time() - 10800,  # 3 hours ago\n            \"source\": \"Daily Reporter\"\n        }\n    ]\n\n    # Simulate API delay\n    import asyncio\n    await asyncio.sleep(0.5)\n\n    results = mock_articles[:limit]\n    await ctx.info(f\"Found {len(results)} articles\")\n\n    return results\n\n@mcp.tool()\nasync def translate_text(\n    text: str,\n    target_language: str = \"es\",\n    source_language: str = \"auto\",\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Translate text between languages.\"\"\"\n\n    await ctx.info(f\"Translating text to {target_language}\")\n\n    # Mock translation - replace with real translation API\n    translations = {\n        \"es\": {\n            \"hello\": \"hola\",\n            \"world\": \"mundo\",\n            \"how are you\": \"\u00bfc\u00f3mo est\u00e1s?\",\n            \"thank you\": \"gracias\"\n        },\n        \"fr\": {\n            \"hello\": \"bonjour\",\n            \"world\": \"monde\",\n            \"how are you\": \"comment allez-vous?\",\n            \"thank you\": \"merci\"\n        }\n    }\n\n    text_lower = text.lower()\n    translated = translations.get(target_language, {}).get(text_lower, f\"[{text}]\")\n\n    await ctx.info(f\"Translated '{text}' to '{translated}'\")\n\n    return {\n        \"original_text\": text,\n        \"translated_text\": translated,\n        \"source_language\": source_language,\n        \"target_language\": target_language,\n        \"confidence\": 0.95\n    }\n\n@mcp.resource(\"api://weather/{city}\")\nasync def weather_resource(city: str, ctx: Context) -&gt; dict:\n    \"\"\"Weather data as a resource.\"\"\"\n    return await fetch_weather(city, ctx=ctx)\n\n@mcp.resource(\"api://news/{query}\")\nasync def news_resource(query: str, ctx: Context) -&gt; list[dict]:\n    \"\"\"News search as a resource.\"\"\"\n    return await search_news(query, ctx=ctx)\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"development/fastmcp/examples/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"development/fastmcp/examples/#1-modular-server-design","title":"1. Modular Server Design","text":"<pre><code># server_modules/auth.py\nfrom mcp.server.fastmcp import FastMCP, Context\n\ndef add_auth_tools(mcp: FastMCP):\n    \"\"\"Add authentication-related tools.\"\"\"\n\n    @mcp.tool()\n    async def login(username: str, password: str, ctx: Context) -&gt; dict:\n        # Authentication logic\n        pass\n\n    @mcp.tool()\n    async def logout(ctx: Context) -&gt; bool:\n        # Logout logic\n        pass\n\n# server_modules/data.py\ndef add_data_tools(mcp: FastMCP):\n    \"\"\"Add data management tools.\"\"\"\n\n    @mcp.tool()\n    async def export_data(format: str, ctx: Context) -&gt; str:\n        # Export logic\n        pass\n\n# main.py\nfrom mcp.server.fastmcp import FastMCP\nfrom server_modules import auth, data\n\nmcp = FastMCP(\"Modular Server\")\n\n# Add modules\nauth.add_auth_tools(mcp)\ndata.add_data_tools(mcp)\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"development/fastmcp/examples/#2-plugin-system","title":"2. Plugin System","text":"<pre><code>from abc import ABC, abstractmethod\nfrom typing import Protocol\n\nclass MCPPlugin(Protocol):\n    \"\"\"Protocol for MCP plugins.\"\"\"\n\n    name: str\n    version: str\n\n    def install(self, mcp: FastMCP) -&gt; None:\n        \"\"\"Install plugin into MCP server.\"\"\"\n        ...\n\nclass DatabasePlugin:\n    \"\"\"Database operations plugin.\"\"\"\n\n    name = \"database\"\n    version = \"1.0.0\"\n\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n\n    def install(self, mcp: FastMCP) -&gt; None:\n        \"\"\"Install database tools and resources.\"\"\"\n\n        @mcp.tool()\n        async def query_db(sql: str, ctx: Context) -&gt; list[dict]:\n            # Database query implementation\n            pass\n\n        @mcp.resource(\"db://tables\")\n        async def list_tables(ctx: Context) -&gt; list[str]:\n            # List tables implementation\n            pass\n\n# Usage\nmcp = FastMCP(\"Plugin Server\")\n\n# Install plugins\ndb_plugin = DatabasePlugin(\"sqlite:///app.db\")\ndb_plugin.install(mcp)\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"development/fastmcp/examples/#3-configuration-driven-server","title":"3. Configuration-Driven Server","text":"<pre><code>import yaml\nfrom typing import Dict, Any\nfrom mcp.server.fastmcp import FastMCP, Context\n\nclass ConfigurableServer:\n    \"\"\"Server that configures itself from YAML.\"\"\"\n\n    def __init__(self, config_path: str):\n        with open(config_path) as f:\n            self.config = yaml.safe_load(f)\n\n        self.mcp = FastMCP(\n            name=self.config[\"server\"][\"name\"],\n            **self.config[\"server\"].get(\"settings\", {})\n        )\n\n        self._setup_tools()\n        self._setup_resources()\n\n    def _setup_tools(self):\n        \"\"\"Dynamically create tools from configuration.\"\"\"\n        for tool_config in self.config.get(\"tools\", []):\n            self._create_tool(tool_config)\n\n    def _create_tool(self, config: Dict[str, Any]):\n        \"\"\"Create a tool from configuration.\"\"\"\n\n        async def dynamic_tool(*args, ctx: Context, **kwargs):\n            await ctx.info(f\"Executing {config['name']}\")\n            # Implement tool logic based on config\n            return {\"message\": f\"Executed {config['name']}\"}\n\n        # Set function metadata\n        dynamic_tool.__name__ = config[\"name\"]\n        dynamic_tool.__doc__ = config.get(\"description\", \"\")\n\n        self.mcp.add_tool(dynamic_tool)\n\n    def run(self):\n        \"\"\"Run the configured server.\"\"\"\n        transport = self.config[\"server\"].get(\"transport\", \"stdio\")\n        self.mcp.run(transport)\n\n# config.yaml\n\"\"\"\nserver:\n  name: \"Configurable Server\"\n  transport: \"stdio\"\n  settings:\n    debug: true\n    log_level: \"DEBUG\"\n\ntools:\n  - name: \"hello\"\n    description: \"Say hello\"\n    type: \"simple\"\n  - name: \"analyze\"\n    description: \"Analyze data\"\n    type: \"complex\"\n\nresources:\n  - uri: \"data://config\"\n    type: \"static\"\n    content: \"Configuration data\"\n\"\"\"\n\nif __name__ == \"__main__\":\n    server = ConfigurableServer(\"config.yaml\")\n    server.run()\n</code></pre>"},{"location":"development/fastmcp/examples/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/examples/#1-error-handling-and-logging","title":"1. Error Handling and Logging","text":"<pre><code>from enum import Enum\nfrom typing import Optional\n\nclass ErrorSeverity(Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    CRITICAL = \"critical\"\n\nasync def handle_error(\n    ctx: Context,\n    error: Exception,\n    operation: str,\n    severity: ErrorSeverity = ErrorSeverity.MEDIUM,\n    user_message: Optional[str] = None\n) -&gt; None:\n    \"\"\"Standardized error handling.\"\"\"\n\n    # Log technical details\n    await ctx.error(f\"Operation '{operation}' failed: {error}\")\n\n    # Log user-friendly message if provided\n    if user_message:\n        log_method = {\n            ErrorSeverity.LOW: ctx.info,\n            ErrorSeverity.MEDIUM: ctx.warning,\n            ErrorSeverity.HIGH: ctx.error,\n            ErrorSeverity.CRITICAL: ctx.error\n        }[severity]\n\n        await log_method(user_message)\n\n@mcp.tool()\nasync def robust_operation(data: str, ctx: Context) -&gt; dict:\n    \"\"\"Tool with comprehensive error handling.\"\"\"\n\n    try:\n        await ctx.info(\"Starting operation\")\n\n        # Validate input\n        if not data or not data.strip():\n            await handle_error(\n                ctx,\n                ValueError(\"Empty data\"),\n                \"input_validation\",\n                ErrorSeverity.LOW,\n                \"Please provide non-empty data\"\n            )\n            raise ValueError(\"Data is required\")\n\n        # Process data\n        result = process_data(data)\n\n        await ctx.info(\"Operation completed successfully\")\n        return {\"result\": result, \"status\": \"success\"}\n\n    except ValueError as e:\n        await handle_error(ctx, e, \"data_processing\", ErrorSeverity.MEDIUM)\n        raise\n    except Exception as e:\n        await handle_error(ctx, e, \"data_processing\", ErrorSeverity.HIGH)\n        raise\n\ndef process_data(data: str) -&gt; str:\n    \"\"\"Mock data processing.\"\"\"\n    return data.upper()\n</code></pre>"},{"location":"development/fastmcp/examples/#2-input-validation","title":"2. Input Validation","text":"<pre><code>from pydantic import BaseModel, validator, Field\nfrom typing import List, Optional\nimport re\n\nclass UserInput(BaseModel):\n    \"\"\"Validated user input model.\"\"\"\n\n    username: str = Field(..., min_length=3, max_length=20)\n    email: str = Field(..., regex=r'^[^@]+@[^@]+\\.[^@]+$')\n    age: int = Field(..., ge=13, le=120)\n    tags: List[str] = Field(default=[], max_items=10)\n\n    @validator('username')\n    def username_alphanumeric(cls, v):\n        if not re.match(r'^[a-zA-Z0-9_]+$', v):\n            raise ValueError('Username must be alphanumeric with underscores')\n        return v\n\n    @validator('tags')\n    def validate_tags(cls, v):\n        for tag in v:\n            if len(tag) &gt; 50:\n                raise ValueError('Tags must be 50 characters or less')\n        return v\n\n@mcp.tool()\nasync def create_user(user_data: UserInput, ctx: Context) -&gt; dict:\n    \"\"\"Create user with validated input.\"\"\"\n\n    await ctx.info(f\"Creating user: {user_data.username}\")\n\n    # Pydantic automatically validates the input\n    # Additional business logic validation can go here\n\n    return {\n        \"user_id\": hash(user_data.username) % 10000,\n        \"username\": user_data.username,\n        \"email\": user_data.email,\n        \"created\": True\n    }\n</code></pre>"},{"location":"development/fastmcp/examples/#3-performance-optimization","title":"3. Performance Optimization","text":"<pre><code>import asyncio\nfrom functools import lru_cache\nfrom typing import List, Dict, Any\n\n# Caching expensive operations\n@lru_cache(maxsize=128)\ndef expensive_computation(data: str) -&gt; str:\n    \"\"\"Cached expensive computation.\"\"\"\n    # Simulate expensive operation\n    import time\n    time.sleep(0.1)\n    return data.upper()\n\n@mcp.tool()\nasync def batch_process(\n    items: List[str],\n    batch_size: int = 10,\n    ctx: Context\n) -&gt; List[dict]:\n    \"\"\"Process items in batches for better performance.\"\"\"\n\n    await ctx.info(f\"Processing {len(items)} items in batches of {batch_size}\")\n\n    results = []\n    total_batches = (len(items) + batch_size - 1) // batch_size\n\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        batch_num = i // batch_size + 1\n\n        await ctx.report_progress(\n            progress=batch_num - 1,\n            total=total_batches,\n            message=f\"Processing batch {batch_num}/{total_batches}\"\n        )\n\n        # Process batch concurrently\n        batch_results = await asyncio.gather(*[\n            process_item_async(item) for item in batch\n        ])\n\n        results.extend(batch_results)\n\n        await ctx.debug(f\"Completed batch {batch_num}\")\n\n    await ctx.report_progress(total_batches, total_batches, \"Complete\")\n    return results\n\nasync def process_item_async(item: str) -&gt; dict:\n    \"\"\"Process single item asynchronously.\"\"\"\n    # Simulate async processing\n    await asyncio.sleep(0.01)\n    return {\"item\": item, \"processed\": True}\n</code></pre>"},{"location":"development/fastmcp/examples/#4-testing","title":"4. Testing","text":"<pre><code>import pytest\nimport asyncio\nfrom unittest.mock import AsyncMock, Mock\n\n# Mock context for testing\nclass MockContext:\n    def __init__(self):\n        self.logs = []\n        self.progress_reports = []\n\n    async def info(self, message: str):\n        self.logs.append((\"info\", message))\n\n    async def error(self, message: str):\n        self.logs.append((\"error\", message))\n\n    async def report_progress(self, progress: int, total: int, message: str = \"\"):\n        self.progress_reports.append((progress, total, message))\n\n@pytest.mark.asyncio\nasync def test_file_operations():\n    \"\"\"Test file operations tool.\"\"\"\n    from pathlib import Path\n    import tempfile\n\n    # Create temporary file for testing\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n        f.write(\"test content\")\n        temp_path = f.name\n\n    try:\n        ctx = MockContext()\n\n        # Test reading file\n        content = await read_file(temp_path, ctx)\n        assert content == \"test content\"\n        assert any(\"Successfully read\" in msg for level, msg in ctx.logs if level == \"info\")\n\n    finally:\n        # Clean up\n        Path(temp_path).unlink()\n\n@pytest.mark.asyncio\nasync def test_error_handling():\n    \"\"\"Test error handling in tools.\"\"\"\n    ctx = MockContext()\n\n    # Test with invalid input\n    with pytest.raises(ValueError):\n        await read_file(\"/nonexistent/file\", ctx)\n\n    # Check error was logged\n    assert any(\"Failed to read file\" in msg for level, msg in ctx.logs if level == \"error\")\n\ndef test_input_validation():\n    \"\"\"Test input validation.\"\"\"\n    # Valid input\n    valid_input = UserInput(\n        username=\"testuser\",\n        email=\"test@example.com\",\n        age=25,\n        tags=[\"python\", \"programming\"]\n    )\n    assert valid_input.username == \"testuser\"\n\n    # Invalid input\n    with pytest.raises(ValueError):\n        UserInput(\n            username=\"test user\",  # Spaces not allowed\n            email=\"invalid-email\",\n            age=25\n        )\n</code></pre>"},{"location":"development/fastmcp/examples/#5-documentation-and-type-hints","title":"5. Documentation and Type Hints","text":"<pre><code>from typing import TypeVar, Generic, Union, Optional, Literal\nfrom pydantic import BaseModel, Field\n\nT = TypeVar('T')\n\nclass APIResponse(BaseModel, Generic[T]):\n    \"\"\"Generic API response wrapper.\"\"\"\n\n    success: bool\n    data: Optional[T] = None\n    error: Optional[str] = None\n    timestamp: float = Field(default_factory=time.time)\n\n@mcp.tool()\nasync def comprehensive_tool(\n    input_data: str = Field(\n        ...,\n        description=\"Input data to process\",\n        example=\"sample text\"\n    ),\n    processing_mode: Literal[\"fast\", \"thorough\", \"custom\"] = Field(\n        default=\"fast\",\n        description=\"Processing mode: 'fast' for quick results, 'thorough' for detailed analysis\"\n    ),\n    options: Optional[dict] = Field(\n        default=None,\n        description=\"Additional processing options (mode-specific)\"\n    ),\n    ctx: Context\n) -&gt; APIResponse[dict]:\n    \"\"\"\n    Comprehensive data processing tool with multiple modes.\n\n    This tool processes input data using different algorithms based on the selected mode:\n\n    - 'fast': Quick processing with basic analysis\n    - 'thorough': Detailed processing with comprehensive analysis\n    - 'custom': User-defined processing with custom options\n\n    Args:\n        input_data: The data to process (required)\n        processing_mode: Algorithm to use for processing\n        options: Additional configuration (varies by mode)\n        ctx: MCP context for logging and progress reporting\n\n    Returns:\n        APIResponse containing processed data and metadata\n\n    Raises:\n        ValueError: If input data is invalid or processing fails\n        RuntimeError: If processing mode is not supported\n\n    Example:\n        &gt;&gt;&gt; await comprehensive_tool(\"hello world\", \"fast\")\n        APIResponse(success=True, data={\"result\": \"HELLO WORLD\", \"mode\": \"fast\"})\n    \"\"\"\n\n    await ctx.info(f\"Processing data in {processing_mode} mode\")\n\n    try:\n        # Process based on mode\n        if processing_mode == \"fast\":\n            result = {\"result\": input_data.upper(), \"mode\": processing_mode}\n        elif processing_mode == \"thorough\":\n            result = {\n                \"result\": input_data.upper(),\n                \"length\": len(input_data),\n                \"word_count\": len(input_data.split()),\n                \"mode\": processing_mode\n            }\n        elif processing_mode == \"custom\":\n            # Use custom options\n            custom_options = options or {}\n            result = {\n                \"result\": input_data.upper(),\n                \"mode\": processing_mode,\n                \"options_used\": custom_options\n            }\n        else:\n            raise RuntimeError(f\"Unsupported processing mode: {processing_mode}\")\n\n        await ctx.info(\"Processing completed successfully\")\n\n        return APIResponse(success=True, data=result)\n\n    except Exception as e:\n        await ctx.error(f\"Processing failed: {e}\")\n        return APIResponse(success=False, error=str(e))\n</code></pre>"},{"location":"development/fastmcp/examples/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"development/fastmcp/examples/#1-container-deployment","title":"1. Container Deployment","text":"<pre><code># Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy application\nCOPY . .\n\n# Set environment variables\nENV FASTMCP_HOST=0.0.0.0\nENV FASTMCP_PORT=8000\nENV FASTMCP_LOG_LEVEL=INFO\n\n# Expose port\nEXPOSE 8000\n\n# Run server\nCMD [\"python\", \"server.py\"]\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  mcp-server:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - FASTMCP_DEBUG=false\n      - FASTMCP_LOG_LEVEL=INFO\n      - DATABASE_URL=postgresql://user:pass@db:5432/mcpdb\n    depends_on:\n      - db\n\n  db:\n    image: postgres:15\n    environment:\n      - POSTGRES_DB=mcpdb\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=pass\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"development/fastmcp/examples/#2-production-configuration","title":"2. Production Configuration","text":"<pre><code># production.py\nimport os\nimport logging\nfrom mcp.server.fastmcp import FastMCP\n\n# Production configuration\nmcp = FastMCP(\n    name=\"Production Server\",\n    debug=False,\n    log_level=\"INFO\",\n    host=\"0.0.0.0\",\n    port=int(os.getenv(\"PORT\", \"8000\")),\n    warn_on_duplicate_tools=False,\n    warn_on_duplicate_resources=False,\n    warn_on_duplicate_prompts=False\n)\n\n# Production logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler('/var/log/mcp-server.log')\n    ]\n)\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre> <p>This comprehensive documentation provides a solid foundation for building production-ready FastMCP servers with proper error handling, validation, testing, and deployment strategies.</p>"},{"location":"development/fastmcp/faq/","title":"FAQ","text":"<p>Frequently asked questions about FastMCP development, covering common issues, best practices, and practical solutions for building MCP servers.</p>"},{"location":"development/fastmcp/faq/#getting-started","title":"Getting Started","text":""},{"location":"development/fastmcp/faq/#what-is-fastmcp-and-how-does-it-differ-from-the-raw-mcp-sdk","title":"What is FastMCP and how does it differ from the raw MCP SDK?","text":"<p>FastMCP is a high-level, decorator-based Python framework that simplifies MCP server development. Unlike the raw MCP SDK which requires manual message handling and protocol implementation, FastMCP provides simple <code>@tool</code>, <code>@resource</code>, and <code>@prompt</code> decorators with automatic type validation.</p> <p>Raw MCP SDK approach: <pre><code>@app.list_tools()\nasync def list_tools() -&gt; list[types.Tool]:\n    return [types.Tool(name=\"add\", description=\"Add numbers\", inputSchema={...})]\n\n@app.call_tool()\nasync def call_tool(name: str, arguments: dict) -&gt; list[types.TextContent]:\n    if name == \"add\":\n        return [types.TextContent(type=\"text\", text=str(arguments[\"a\"] + arguments[\"b\"]))]\n</code></pre></p> <p>FastMCP approach: <pre><code>@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n</code></pre></p>"},{"location":"development/fastmcp/faq/#which-transport-should-i-use-stdio-streamable-http-or-sse","title":"Which transport should I use: stdio, streamable-http, or sse?","text":"<ul> <li>stdio: Use for local development, command-line tools, and desktop applications. Best for single-user scenarios with direct process communication.</li> <li>streamable-http: Use for web deployment, cloud services, and when you need authentication, resumable connections, and multiple concurrent clients.</li> <li>sse: Use for web deployment when you need HTTP compatibility but don't need the advanced features of streamable-http.</li> </ul> <pre><code># Local development\nmcp.run(\"stdio\")\n\n# Web deployment with authentication\nmcp.run(\"streamable-http\")\n\n# Simple web deployment\nmcp.run(\"sse\")\n</code></pre>"},{"location":"development/fastmcp/faq/#how-do-i-install-and-run-my-first-fastmcp-server","title":"How do I install and run my first FastMCP server?","text":"<ol> <li> <p>Install FastMCP: <pre><code>uv add mcp\n</code></pre></p> </li> <li> <p>Create a simple server: <pre><code>from mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"My First Server\")\n\n@mcp.tool()\ndef greet(name: str) -&gt; str:\n    \"\"\"Greet someone by name.\"\"\"\n    return f\"Hello, {name}!\"\n\n@mcp.tool()\nasync def async_example(message: str, ctx: Context) -&gt; str:\n    \"\"\"Example with logging and context.\"\"\"\n    await ctx.info(f\"Processing message: {message}\")\n    return f\"Processed: {message}\"\n\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")  # Change to \"streamable-http\" for web\n</code></pre></p> </li> <li> <p>Run the server: <pre><code>python server.py\n</code></pre></p> </li> </ol>"},{"location":"development/fastmcp/faq/#tools-development","title":"Tools Development","text":""},{"location":"development/fastmcp/faq/#how-do-i-validate-tool-inputs-and-handle-errors-properly","title":"How do I validate tool inputs and handle errors properly?","text":"<p>Use Pydantic <code>Field</code> for validation and proper error handling:</p> <pre><code>from pydantic import BaseModel, Field\nfrom mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"Validation Server\")\n\nclass UserInput(BaseModel):\n    name: str = Field(min_length=1, max_length=50, description=\"User's name\")\n    age: int = Field(ge=0, le=150, description=\"User's age\")\n    email: str = Field(regex=r'^[^@]+@[^@]+\\.[^@]+$', description=\"Valid email\")\n\n@mcp.tool()\nasync def create_user(user: UserInput, ctx: Context) -&gt; dict:\n    \"\"\"Create user with validation.\"\"\"\n    try:\n        await ctx.info(f\"Creating user: {user.name}\")\n\n        # Simulate user creation\n        user_id = hash(user.email) % 10000\n\n        await ctx.info(\"User created successfully\")\n        return {\n            \"user_id\": user_id,\n            \"name\": user.name,\n            \"age\": user.age,\n            \"email\": user.email\n        }\n    except Exception as e:\n        await ctx.error(f\"Failed to create user: {e}\")\n        raise\n</code></pre>"},{"location":"development/fastmcp/faq/#when-should-i-use-async-vs-sync-tools","title":"When should I use async vs sync tools?","text":"<p>Use async tools when you need to: - Make HTTP requests - Access databases - Perform file I/O operations - Use context capabilities (logging, progress reporting) - Call other async functions</p> <p>Use sync tools for simple computational tasks that don't involve I/O.</p> <pre><code># Sync tool for computation\n@mcp.tool()\ndef calculate_fibonacci(n: int) -&gt; int:\n    \"\"\"Calculate fibonacci number (sync).\"\"\"\n    if n &lt;= 1:\n        return n\n    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n\n# Async tool for I/O operations\n@mcp.tool()\nasync def fetch_data(url: str, ctx: Context) -&gt; dict:\n    \"\"\"Fetch data from URL (async).\"\"\"\n    import httpx\n\n    await ctx.info(f\"Fetching data from {url}\")\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        response.raise_for_status()\n\n    await ctx.info(\"Data fetched successfully\")\n    return response.json()\n</code></pre>"},{"location":"development/fastmcp/faq/#how-do-i-access-mcp-capabilities-like-logging-and-progress-reporting","title":"How do I access MCP capabilities like logging and progress reporting?","text":"<p>Inject a <code>Context</code> parameter into your tool:</p> <pre><code>@mcp.tool()\nasync def long_running_task(\n    items: list[str],\n    ctx: Context\n) -&gt; list[str]:\n    \"\"\"Process items with progress reporting.\"\"\"\n    results = []\n\n    await ctx.info(f\"Starting to process {len(items)} items\")\n\n    for i, item in enumerate(items):\n        # Log progress\n        await ctx.debug(f\"Processing item {i+1}: {item}\")\n\n        # Report progress\n        await ctx.report_progress(\n            progress=i / len(items),\n            total=1.0,\n            message=f\"Processing {item}\"\n        )\n\n        # Simulate work\n        import asyncio\n        await asyncio.sleep(0.5)\n\n        result = f\"processed_{item}\"\n        results.append(result)\n\n    await ctx.info(\"All items processed successfully\")\n    return results\n</code></pre>"},{"location":"development/fastmcp/faq/#can-i-return-rich-content-like-images-from-tools","title":"Can I return rich content like images from tools?","text":"<p>Yes, use the appropriate content types:</p> <pre><code>from mcp.server.fastmcp import Image\nimport base64\n\n@mcp.tool()\ndef generate_chart(data: list[int]) -&gt; Image:\n    \"\"\"Generate a chart image.\"\"\"\n    # Generate chart (example using matplotlib)\n    import matplotlib.pyplot as plt\n    import io\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(data)\n    plt.title(\"Data Chart\")\n\n    # Save to bytes\n    img_buffer = io.BytesIO()\n    plt.savefig(img_buffer, format='png')\n    img_buffer.seek(0)\n\n    return Image(\n        data=img_buffer.read(),\n        mime_type=\"image/png\"\n    )\n\n@mcp.tool()\ndef get_file_content(filename: str) -&gt; str:\n    \"\"\"Return file content as text.\"\"\"\n    with open(filename, 'r') as f:\n        return f.read()\n</code></pre>"},{"location":"development/fastmcp/faq/#resources","title":"Resources","text":""},{"location":"development/fastmcp/faq/#whats-the-difference-between-static-resources-and-resource-templates","title":"What's the difference between static resources and resource templates?","text":"<ul> <li>Static resources: Fixed URIs that return specific data</li> <li>Resource templates: URI patterns with parameters for dynamic resources</li> </ul> <pre><code># Static resource - fixed URI\n@mcp.resource(\"config://app-settings\")\ndef app_config() -&gt; dict:\n    \"\"\"Application configuration.\"\"\"\n    return {\n        \"version\": \"1.0\",\n        \"debug\": False,\n        \"max_connections\": 100\n    }\n\n# Resource template - dynamic URI with parameters\n@mcp.resource(\"user://{user_id}/profile\")\ndef user_profile(user_id: str) -&gt; dict:\n    \"\"\"Get user profile by ID.\"\"\"\n    return {\n        \"user_id\": user_id,\n        \"name\": f\"User {user_id}\",\n        \"profile_data\": \"...\"\n    }\n\n# Multiple parameters\n@mcp.resource(\"files://{category}/{filename}\")\ndef get_file(category: str, filename: str) -&gt; str:\n    \"\"\"Get file content by category and name.\"\"\"\n    file_path = f\"/data/{category}/{filename}\"\n    with open(file_path, 'r') as f:\n        return f.read()\n</code></pre>"},{"location":"development/fastmcp/faq/#how-do-i-implement-secure-file-access-with-resources","title":"How do I implement secure file access with resources?","text":"<p>Validate paths and restrict access to allowed directories:</p> <pre><code>from pathlib import Path\nimport os\n\n# Define allowed base directories\nALLOWED_DIRECTORIES = [\n    \"/app/public\",\n    \"/app/data\",\n    \"/tmp/uploads\"\n]\n\n@mcp.resource(\"file://{path}\")\ndef secure_file_access(path: str) -&gt; str:\n    \"\"\"Securely access files within allowed directories.\"\"\"\n    try:\n        # Resolve the full path\n        requested_path = Path(path).resolve()\n\n        # Check if path is within allowed directories\n        is_allowed = any(\n            str(requested_path).startswith(allowed_dir)\n            for allowed_dir in ALLOWED_DIRECTORIES\n        )\n\n        if not is_allowed:\n            raise ValueError(f\"Access denied: {path} is outside allowed directories\")\n\n        # Check if file exists\n        if not requested_path.exists():\n            raise ValueError(f\"File not found: {path}\")\n\n        # Check if it's actually a file (not a directory)\n        if not requested_path.is_file():\n            raise ValueError(f\"Path is not a file: {path}\")\n\n        # Read and return file content\n        return requested_path.read_text()\n\n    except Exception as e:\n        raise ValueError(f\"Error accessing file {path}: {e}\")\n\n@mcp.resource(\"safe-image://{image_name}\")\ndef get_safe_image(image_name: str) -&gt; bytes:\n    \"\"\"Get images from safe directory only.\"\"\"\n    # Validate image name (no path traversal)\n    if '..' in image_name or '/' in image_name or '\\\\' in image_name:\n        raise ValueError(\"Invalid image name\")\n\n    image_path = Path(\"/app/images\") / f\"{image_name}.png\"\n\n    if not image_path.exists():\n        raise ValueError(f\"Image not found: {image_name}\")\n\n    return image_path.read_bytes()\n</code></pre>"},{"location":"development/fastmcp/faq/#how-do-i-handle-different-file-types-and-mime-types-in-resources","title":"How do I handle different file types and MIME types in resources?","text":"<p>Specify appropriate MIME types and handle different content types:</p> <pre><code>import mimetypes\nfrom pathlib import Path\n\n@mcp.resource(\"image://{name}\", mime_type=\"image/png\")\ndef get_png_image(name: str) -&gt; bytes:\n    \"\"\"Get PNG image.\"\"\"\n    return Path(f\"/images/{name}.png\").read_bytes()\n\n@mcp.resource(\"data://{name}\", mime_type=\"application/json\")\ndef get_json_data(name: str) -&gt; dict:\n    \"\"\"Get JSON data.\"\"\"\n    return {\"name\": name, \"type\": \"json\", \"data\": \"...\"}\n\n@mcp.resource(\"document://{filename}\")\ndef get_document(filename: str) -&gt; str:\n    \"\"\"Get document with auto-detected MIME type.\"\"\"\n    file_path = Path(f\"/documents/{filename}\")\n\n    if not file_path.exists():\n        raise ValueError(f\"Document not found: {filename}\")\n\n    # Auto-detect MIME type\n    mime_type, _ = mimetypes.guess_type(str(file_path))\n\n    if mime_type and mime_type.startswith('text/'):\n        return file_path.read_text()\n    else:\n        # Return base64 encoded binary data\n        import base64\n        return base64.b64encode(file_path.read_bytes()).decode()\n</code></pre>"},{"location":"development/fastmcp/faq/#authentication-security","title":"Authentication &amp; Security","text":""},{"location":"development/fastmcp/faq/#how-do-i-add-authentication-to-my-fastmcp-server","title":"How do I add authentication to my FastMCP server?","text":"<p>Authentication is only available for HTTP transports. Implement an OAuth provider:</p> <pre><code>from mcp.server.fastmcp import FastMCP\nfrom mcp.server.auth.provider import OAuthAuthorizationServerProvider\nfrom mcp.server.auth.settings import AuthSettings, ClientRegistrationOptions\nfrom mcp.server.auth.middleware import get_access_token\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\n\n# Create custom OAuth provider\nclass MyOAuthProvider(OAuthAuthorizationServerProvider):\n    def __init__(self):\n        super().__init__(provider_id=\"my-provider\")\n\n    async def authorize(self, request) -&gt; str:\n        # Implement authorization logic\n        return \"auth-code-123\"\n\n    async def token(self, request) -&gt; dict:\n        # Implement token exchange\n        return {\n            \"access_token\": \"token-123\",\n            \"token_type\": \"bearer\",\n            \"scope\": \"read write\"\n        }\n\n# Configure authentication\noauth_provider = MyOAuthProvider()\nauth_settings = AuthSettings(\n    issuer_url=\"https://myserver.com\",\n    client_registration_options=ClientRegistrationOptions(\n        valid_scopes=[\"read\", \"write\", \"admin\"],\n        default_scopes=[\"read\"]\n    )\n)\n\nmcp = FastMCP(\n    \"Secure Server\",\n    auth_server_provider=oauth_provider,\n    auth=auth_settings\n)\n\n@mcp.tool()\nasync def protected_tool(data: str) -&gt; str:\n    \"\"\"Tool that requires authentication.\"\"\"\n    access_token = get_access_token()\n\n    if not access_token:\n        raise ValueError(\"Authentication required\")\n\n    if \"write\" not in access_token.scopes:\n        raise ValueError(\"Insufficient permissions\")\n\n    return f\"Processed: {data} for user {access_token.user_id}\"\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")  # Authentication requires HTTP transport\n</code></pre>"},{"location":"development/fastmcp/faq/#can-i-use-authentication-with-stdio-transport","title":"Can I use authentication with stdio transport?","text":"<p>No, authentication is only available with HTTP transports (streamable-http and sse). The stdio transport runs locally and doesn't require authentication since it communicates directly through stdin/stdout.</p>"},{"location":"development/fastmcp/faq/#how-do-i-implement-scope-based-access-control","title":"How do I implement scope-based access control?","text":"<p>Configure scopes in your auth settings and validate them in tools:</p> <pre><code>from mcp.server.auth.middleware import get_access_token\n\n# Configure scopes\nauth_settings = AuthSettings(\n    client_registration_options=ClientRegistrationOptions(\n        valid_scopes=[\"read\", \"write\", \"admin\", \"user:profile\", \"data:modify\"],\n        default_scopes=[\"read\"]\n    )\n)\n\n@mcp.tool()\nasync def read_data(ctx: Context) -&gt; dict:\n    \"\"\"Tool requiring read scope.\"\"\"\n    access_token = get_access_token()\n\n    if not access_token or \"read\" not in access_token.scopes:\n        raise ValueError(\"Read permission required\")\n\n    await ctx.info(f\"Data accessed by user: {access_token.user_id}\")\n    return {\"data\": \"sensitive information\"}\n\n@mcp.tool()\nasync def modify_user_profile(user_id: str, data: dict, ctx: Context) -&gt; dict:\n    \"\"\"Tool requiring specific scopes.\"\"\"\n    access_token = get_access_token()\n\n    if not access_token:\n        raise ValueError(\"Authentication required\")\n\n    required_scopes = [\"write\", \"user:profile\"]\n    if not all(scope in access_token.scopes for scope in required_scopes):\n        raise ValueError(f\"Required scopes: {required_scopes}\")\n\n    await ctx.info(f\"Profile modified for user {user_id}\")\n    return {\"status\": \"updated\", \"user_id\": user_id}\n</code></pre>"},{"location":"development/fastmcp/faq/#configuration-environment","title":"Configuration &amp; Environment","text":""},{"location":"development/fastmcp/faq/#how-do-i-configure-my-server-for-different-environments","title":"How do I configure my server for different environments?","text":"<p>Use environment variables and conditional configuration:</p> <pre><code>import os\n\n# Environment-based configuration\ndef get_config():\n    env = os.getenv(\"ENVIRONMENT\", \"development\")\n\n    if env == \"production\":\n        return {\n            \"debug\": False,\n            \"log_level\": \"INFO\",\n            \"host\": \"0.0.0.0\",\n            \"port\": 8000\n        }\n    elif env == \"staging\":\n        return {\n            \"debug\": True,\n            \"log_level\": \"DEBUG\",\n            \"host\": \"0.0.0.0\",\n            \"port\": 8080\n        }\n    else:  # development\n        return {\n            \"debug\": True,\n            \"log_level\": \"DEBUG\",\n            \"host\": \"127.0.0.1\",\n            \"port\": 3000\n        }\n\nconfig = get_config()\nmcp = FastMCP(\"Environment Server\", **config)\n\n# Or use environment variables directly\nmcp = FastMCP(\n    \"My Server\",\n    debug=os.getenv(\"DEBUG\", \"false\").lower() == \"true\",\n    log_level=os.getenv(\"LOG_LEVEL\", \"INFO\"),\n    host=os.getenv(\"HOST\", \"127.0.0.1\"),\n    port=int(os.getenv(\"PORT\", \"8000\"))\n)\n</code></pre>"},{"location":"development/fastmcp/faq/#what-environment-variables-does-fastmcp-support","title":"What environment variables does FastMCP support?","text":"<p>Key environment variables with the <code>FASTMCP_</code> prefix:</p> <pre><code># Server configuration\nFASTMCP_HOST=0.0.0.0\nFASTMCP_PORT=8000\nFASTMCP_DEBUG=true\nFASTMCP_LOG_LEVEL=DEBUG\n\n# HTTP transport settings\nFASTMCP_MOUNT_PATH=/api\nFASTMCP_STREAMABLE_HTTP_PATH=/mcp\n\n# Behavior settings\nFASTMCP_WARN_ON_DUPLICATE_TOOLS=false\nFASTMCP_WARN_ON_DUPLICATE_RESOURCES=false\nFASTMCP_WARN_ON_DUPLICATE_PROMPTS=false\n\n# Authentication (when using auth)\nFASTMCP_AUTH_ISSUER_URL=https://myserver.com\nFASTMCP_AUTH_CLIENT_ID=my-client-id\n</code></pre> <p>Example usage: <pre><code># All settings can be configured via environment variables\nmcp = FastMCP(\"Env Server\")  # Will use FASTMCP_* environment variables\n\n# Or override specific settings\nmcp = FastMCP(\"Mixed Server\", debug=True)  # debug=True overrides FASTMCP_DEBUG\n</code></pre></p>"},{"location":"development/fastmcp/faq/#performance-optimization","title":"Performance &amp; Optimization","text":""},{"location":"development/fastmcp/faq/#how-do-i-optimize-my-fastmcp-server-for-production","title":"How do I optimize my FastMCP server for production?","text":"<p>Key optimization strategies:</p> <pre><code>import asyncio\nfrom functools import lru_cache\nimport asyncpg\n\n# 1. Use async tools for I/O operations\n@mcp.tool()\nasync def optimized_database_query(query: str, ctx: Context) -&gt; list[dict]:\n    \"\"\"Async database query with connection pooling.\"\"\"\n    pool = await get_db_pool()  # Reuse connection pool\n\n    async with pool.acquire() as conn:\n        rows = await conn.fetch(query)\n        return [dict(row) for row in rows]\n\n# 2. Implement caching for expensive operations\n@lru_cache(maxsize=128)\ndef expensive_computation(data: str) -&gt; str:\n    \"\"\"Cache expensive computations.\"\"\"\n    # Simulate expensive operation\n    import time\n    time.sleep(1)\n    return f\"processed_{data}\"\n\n@mcp.tool()\nasync def cached_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with caching.\"\"\"\n    await ctx.info(\"Using cached computation\")\n    return expensive_computation(data)\n\n# 3. Process items in batches\n@mcp.tool()\nasync def batch_processor(\n    items: list[str],\n    batch_size: int = 10,\n    ctx: Context\n) -&gt; list[str]:\n    \"\"\"Process items in batches for better performance.\"\"\"\n    results = []\n\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        await ctx.info(f\"Processing batch {i//batch_size + 1}\")\n\n        # Process batch concurrently\n        batch_tasks = [process_item(item) for item in batch]\n        batch_results = await asyncio.gather(*batch_tasks)\n\n        results.extend(batch_results)\n\n        # Report progress\n        await ctx.report_progress(\n            progress=min(i + batch_size, len(items)) / len(items),\n            total=1.0,\n            message=f\"Processed {min(i + batch_size, len(items))} of {len(items)} items\"\n        )\n\n    return results\n\n# 4. Use proper timeout and error handling\n@mcp.tool()\nasync def robust_api_call(url: str, timeout: int = 30, ctx: Context) -&gt; dict:\n    \"\"\"API call with timeout and retry logic.\"\"\"\n    import httpx\n\n    for attempt in range(3):  # Retry up to 3 times\n        try:\n            async with httpx.AsyncClient(timeout=timeout) as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                return response.json()\n        except httpx.TimeoutException:\n            await ctx.warning(f\"Timeout on attempt {attempt + 1}\")\n            if attempt == 2:  # Last attempt\n                raise\n        except httpx.HTTPStatusError as e:\n            await ctx.error(f\"HTTP error {e.response.status_code}\")\n            raise\n</code></pre>"},{"location":"development/fastmcp/faq/#how-do-i-handle-long-running-operations-and-progress-reporting","title":"How do I handle long-running operations and progress reporting?","text":"<p>Use context progress reporting and proper async patterns:</p> <pre><code>@mcp.tool()\nasync def long_data_processing(\n    data_source: str,\n    processing_options: dict,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Long-running data processing with progress updates.\"\"\"\n\n    # Phase 1: Load data\n    await ctx.info(\"Loading data...\")\n    await ctx.report_progress(0.1, 1.0, \"Loading data\")\n\n    data = await load_data(data_source)\n    data_size = len(data)\n\n    # Phase 2: Process data\n    await ctx.info(f\"Processing {data_size} items...\")\n    processed_items = []\n\n    for i, item in enumerate(data):\n        # Update progress every 10 items or every 5%\n        if i % 10 == 0 or (i / data_size) % 0.05 == 0:\n            progress = 0.1 + (i / data_size) * 0.8  # 10% to 90%\n            await ctx.report_progress(\n                progress=progress,\n                total=1.0,\n                message=f\"Processing item {i+1} of {data_size}\"\n            )\n\n        # Process individual item\n        processed_item = await process_single_item(item, processing_options)\n        processed_items.append(processed_item)\n\n        # Yield control to allow cancellation\n        await asyncio.sleep(0)\n\n    # Phase 3: Save results\n    await ctx.info(\"Saving results...\")\n    await ctx.report_progress(0.95, 1.0, \"Saving results\")\n\n    result_file = await save_results(processed_items)\n\n    # Complete\n    await ctx.report_progress(1.0, 1.0, \"Processing complete\")\n    await ctx.info(f\"Processing complete. Results saved to {result_file}\")\n\n    return {\n        \"status\": \"completed\",\n        \"items_processed\": len(processed_items),\n        \"result_file\": result_file,\n        \"processing_options\": processing_options\n    }\n</code></pre>"},{"location":"development/fastmcp/faq/#debugging-troubleshooting","title":"Debugging &amp; Troubleshooting","text":""},{"location":"development/fastmcp/faq/#how-do-i-debug-my-fastmcp-server","title":"How do I debug my FastMCP server?","text":"<p>Enable debug mode and use comprehensive logging:</p> <pre><code># Enable debug mode\nmcp = FastMCP(\n    \"Debug Server\",\n    debug=True,\n    log_level=\"DEBUG\"\n)\n\n@mcp.tool()\nasync def debug_tool(data: dict, ctx: Context) -&gt; dict:\n    \"\"\"Tool with comprehensive debugging.\"\"\"\n\n    # Debug logging\n    await ctx.debug(f\"Tool called with data: {data}\")\n    await ctx.debug(f\"Request ID: {ctx.request_id}\")\n    await ctx.debug(f\"Client ID: {ctx.client_id}\")\n\n    # Info logging\n    await ctx.info(\"Starting data processing\")\n\n    try:\n        # Process data\n        result = process_complex_data(data)\n\n        await ctx.info(\"Processing completed successfully\")\n        await ctx.debug(f\"Result keys: {list(result.keys())}\")\n\n        return result\n\n    except Exception as e:\n        # Error logging with context\n        await ctx.error(f\"Processing failed: {type(e).__name__}: {e}\")\n\n        # In debug mode, log full traceback\n        import traceback\n        await ctx.debug(f\"Traceback: {traceback.format_exc()}\")\n\n        raise\n\n# Custom route for debugging\n@mcp.custom_route(\"/debug/info\", methods=[\"GET\"])\nasync def debug_info(request):\n    \"\"\"Debug endpoint showing server state.\"\"\"\n    return JSONResponse({\n        \"server_name\": mcp.name,\n        \"debug_mode\": mcp.settings.debug,\n        \"log_level\": mcp.settings.log_level,\n        \"tools_count\": len(mcp._tool_manager.tools),\n        \"resources_count\": len(mcp._resource_manager.resources)\n    })\n</code></pre>"},{"location":"development/fastmcp/faq/#my-tools-arent-being-discovered-whats-wrong","title":"My tools aren't being discovered. What's wrong?","text":"<p>Common issues and solutions:</p> <pre><code># \u274c Wrong: Tool defined after mcp.run()\nmcp = FastMCP(\"My Server\")\n\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")\n\n@mcp.tool()  # This won't work - defined after run()\ndef late_tool() -&gt; str:\n    return \"This won't be registered\"\n\n# \u2705 Correct: Tools defined before mcp.run()\nmcp = FastMCP(\"My Server\")\n\n@mcp.tool()\ndef early_tool() -&gt; str:\n    return \"This will work\"\n\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")\n\n# \u274c Wrong: Missing type annotations\n@mcp.tool()\ndef bad_tool(x, y):  # No type annotations\n    return x + y\n\n# \u2705 Correct: Proper type annotations\n@mcp.tool()\ndef good_tool(x: int, y: int) -&gt; int:\n    return x + y\n\n# \u274c Wrong: Invalid function signature\n@mcp.tool()\ndef invalid_tool(*args, **kwargs) -&gt; str:  # Can't use *args/**kwargs\n    return \"Invalid\"\n\n# \u2705 Correct: Explicit parameters\n@mcp.tool()\ndef valid_tool(a: str, b: int = 10) -&gt; str:\n    return f\"{a}: {b}\"\n\n# Check if tools are registered\nprint(f\"Registered tools: {list(mcp._tool_manager.tools.keys())}\")\n</code></pre>"},{"location":"development/fastmcp/faq/#how-do-i-handle-errors-gracefully-in-my-tools","title":"How do I handle errors gracefully in my tools?","text":"<p>Use structured error handling with context logging:</p> <pre><code>from mcp.server.fastmcp.exceptions import ValidationError, ToolError\n\n@mcp.tool()\nasync def robust_tool(\n    data: str,\n    validate: bool = True,\n    ctx: Context\n) -&gt; dict:\n    \"\"\"Tool with comprehensive error handling.\"\"\"\n\n    try:\n        await ctx.info(f\"Processing data: {data}\")\n\n        # Input validation\n        if validate and not data.strip():\n            raise ValidationError(\"Data cannot be empty\")\n\n        # Simulate processing that might fail\n        if data == \"error\":\n            raise ValueError(\"Simulated processing error\")\n\n        if data == \"timeout\":\n            import asyncio\n            await asyncio.sleep(100)  # Simulate timeout\n\n        # Success case\n        result = {\n            \"original\": data,\n            \"processed\": data.upper(),\n            \"length\": len(data),\n            \"timestamp\": time.time()\n        }\n\n        await ctx.info(\"Processing completed successfully\")\n        return result\n\n    except ValidationError as e:\n        await ctx.error(f\"Validation error: {e}\")\n        raise\n\n    except ValueError as e:\n        await ctx.error(f\"Processing error: {e}\")\n        # Could return error response instead of raising\n        return {\n            \"error\": \"processing_failed\",\n            \"message\": str(e),\n            \"data\": data\n        }\n\n    except asyncio.TimeoutError:\n        await ctx.error(\"Operation timed out\")\n        raise ToolError(\"Processing timed out\")\n\n    except Exception as e:\n        await ctx.error(f\"Unexpected error: {type(e).__name__}: {e}\")\n\n        # Log full context in debug mode\n        if mcp.settings.debug:\n            import traceback\n            await ctx.debug(f\"Full traceback: {traceback.format_exc()}\")\n\n        raise ToolError(f\"Tool execution failed: {e}\") from e\n</code></pre>"},{"location":"development/fastmcp/faq/#advanced-usage","title":"Advanced Usage","text":""},{"location":"development/fastmcp/faq/#how-do-i-create-a-modular-server-with-multiple-components","title":"How do I create a modular server with multiple components?","text":"<p>Organize tools into modules and compose them:</p> <pre><code># modules/auth.py\nfrom mcp.server.fastmcp import FastMCP, Context\n\ndef add_auth_tools(mcp: FastMCP):\n    \"\"\"Add authentication-related tools.\"\"\"\n\n    @mcp.tool()\n    async def login(username: str, password: str, ctx: Context) -&gt; dict:\n        \"\"\"User login.\"\"\"\n        await ctx.info(f\"Login attempt for user: {username}\")\n        # Implement authentication logic\n        return {\"token\": \"auth-token\", \"user\": username}\n\n    @mcp.tool()\n    async def logout(token: str, ctx: Context) -&gt; bool:\n        \"\"\"User logout.\"\"\"\n        await ctx.info(\"User logged out\")\n        return True\n\n# modules/data.py\ndef add_data_tools(mcp: FastMCP):\n    \"\"\"Add data management tools.\"\"\"\n\n    @mcp.tool()\n    async def get_user_data(user_id: str, ctx: Context) -&gt; dict:\n        \"\"\"Get user data.\"\"\"\n        await ctx.info(f\"Fetching data for user: {user_id}\")\n        return {\"user_id\": user_id, \"data\": \"user data\"}\n\n    @mcp.resource(\"user://{user_id}/profile\")\n    def user_profile(user_id: str) -&gt; dict:\n        \"\"\"User profile resource.\"\"\"\n        return {\"user_id\": user_id, \"profile\": \"profile data\"}\n\n# main.py\nfrom modules.auth import add_auth_tools\nfrom modules.data import add_data_tools\n\nmcp = FastMCP(\"Modular Server\")\n\n# Add modules\nadd_auth_tools(mcp)\nadd_data_tools(mcp)\n\n# Add main tools\n@mcp.tool()\nasync def server_status(ctx: Context) -&gt; dict:\n    \"\"\"Get server status.\"\"\"\n    return {\n        \"status\": \"running\",\n        \"tools\": len(mcp._tool_manager.tools),\n        \"resources\": len(mcp._resource_manager.resources)\n    }\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/faq/#can-i-create-custom-http-routes-alongside-my-mcp-server","title":"Can I create custom HTTP routes alongside my MCP server?","text":"<p>Yes, for HTTP transports you can add custom routes:</p> <pre><code>from starlette.requests import Request\nfrom starlette.responses import JSONResponse, FileResponse\nimport time\n\n@mcp.custom_route(\"/health\", methods=[\"GET\"])\nasync def health_check(request: Request) -&gt; JSONResponse:\n    \"\"\"Health check endpoint.\"\"\"\n    return JSONResponse({\n        \"status\": \"healthy\",\n        \"timestamp\": time.time(),\n        \"server\": mcp.name\n    })\n\n@mcp.custom_route(\"/api/stats\", methods=[\"GET\"])\nasync def server_stats(request: Request) -&gt; JSONResponse:\n    \"\"\"Server statistics API.\"\"\"\n    return JSONResponse({\n        \"tools\": len(mcp._tool_manager.tools),\n        \"resources\": len(mcp._resource_manager.resources),\n        \"uptime\": time.time() - start_time\n    })\n\n@mcp.custom_route(\"/download/{filename}\", methods=[\"GET\"])\nasync def download_file(request: Request) -&gt; FileResponse:\n    \"\"\"File download endpoint.\"\"\"\n    filename = request.path_params[\"filename\"]\n    file_path = f\"/downloads/{filename}\"\n\n    if not os.path.exists(file_path):\n        return JSONResponse({\"error\": \"File not found\"}, status_code=404)\n\n    return FileResponse(file_path, filename=filename)\n\n@mcp.custom_route(\"/webhook\", methods=[\"POST\"])\nasync def webhook_handler(request: Request) -&gt; JSONResponse:\n    \"\"\"Webhook endpoint for external integrations.\"\"\"\n    try:\n        payload = await request.json()\n\n        # Process webhook\n        event_type = payload.get(\"type\")\n        if event_type == \"user_update\":\n            await handle_user_update(payload[\"data\"])\n\n        return JSONResponse({\"status\": \"processed\"})\n\n    except Exception as e:\n        return JSONResponse(\n            {\"error\": str(e)},\n            status_code=400\n        )\n</code></pre>"},{"location":"development/fastmcp/faq/#deployment-production","title":"Deployment &amp; Production","text":""},{"location":"development/fastmcp/faq/#how-do-i-deploy-my-fastmcp-server-to-production","title":"How do I deploy my FastMCP server to production?","text":"<p>Use containers and proper configuration management:</p> <p>Dockerfile: <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy application\nCOPY . .\n\n# Set environment variables\nENV FASTMCP_HOST=0.0.0.0\nENV FASTMCP_PORT=8000\nENV FASTMCP_LOG_LEVEL=INFO\nENV FASTMCP_DEBUG=false\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Run server\nCMD [\"python\", \"server.py\"]\n</code></pre></p> <p>docker-compose.yml: <pre><code>version: '3.8'\nservices:\n  fastmcp-server:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - FASTMCP_HOST=0.0.0.0\n      - FASTMCP_PORT=8000\n      - FASTMCP_LOG_LEVEL=INFO\n      - DATABASE_URL=postgresql://user:pass@db:5432/mydb\n    depends_on:\n      - db\n    restart: unless-stopped\n\n  db:\n    image: postgres:13\n    environment:\n      - POSTGRES_DB=mydb\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=pass\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre></p> <p>Production server setup: <pre><code>import os\nimport logging\nfrom mcp.server.fastmcp import FastMCP, Context\n\n# Configure logging for production\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Production configuration\nmcp = FastMCP(\n    \"Production Server\",\n    host=os.getenv(\"FASTMCP_HOST\", \"0.0.0.0\"),\n    port=int(os.getenv(\"FASTMCP_PORT\", \"8000\")),\n    debug=os.getenv(\"FASTMCP_DEBUG\", \"false\").lower() == \"true\",\n    log_level=os.getenv(\"FASTMCP_LOG_LEVEL\", \"INFO\")\n)\n\n# Health check endpoint\n@mcp.custom_route(\"/health\", methods=[\"GET\"])\nasync def health_check(request):\n    \"\"\"Production health check.\"\"\"\n    return JSONResponse({\n        \"status\": \"healthy\",\n        \"timestamp\": time.time(),\n        \"version\": os.getenv(\"APP_VERSION\", \"unknown\")\n    })\n\n# Ready check endpoint\n@mcp.custom_route(\"/ready\", methods=[\"GET\"])\nasync def readiness_check(request):\n    \"\"\"Readiness probe for Kubernetes.\"\"\"\n    try:\n        # Check database connection\n        await check_database_connection()\n\n        # Check other dependencies\n        await check_external_services()\n\n        return JSONResponse({\"status\": \"ready\"})\n    except Exception as e:\n        return JSONResponse(\n            {\"status\": \"not ready\", \"error\": str(e)},\n            status_code=503\n        )\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre></p> <p>This comprehensive FAQ covers the most common questions and scenarios developers encounter when building FastMCP servers, providing practical solutions and best practices for each area.</p>"},{"location":"development/fastmcp/lifespan-management/","title":"Lifespan Management","text":"<p>Comprehensive guide to managing server lifecycle, resource initialization, dependency injection, and graceful startup/shutdown patterns in FastMCP servers.</p>"},{"location":"development/fastmcp/lifespan-management/#overview","title":"Overview","text":"<p>Lifespan management in FastMCP enables proper resource lifecycle control, allowing you to initialize resources during server startup, make them available throughout the server's lifetime, and clean them up during shutdown. This is essential for database connections, HTTP clients, caches, and other long-lived resources.</p>"},{"location":"development/fastmcp/lifespan-management/#basic-lifespan-patterns","title":"Basic Lifespan Patterns","text":""},{"location":"development/fastmcp/lifespan-management/#simple-lifespan-context-manager","title":"Simple Lifespan Context Manager","text":"<pre><code>from contextlib import asynccontextmanager\nfrom mcp.server.fastmcp import FastMCP, Context\nimport asyncio\n\n@asynccontextmanager\nasync def basic_lifespan(server: FastMCP):\n    \"\"\"Basic lifespan with startup and shutdown hooks.\"\"\"\n    # Startup phase\n    print(\"Server starting up...\")\n\n    # Initialize application state\n    app_state = {\n        \"startup_time\": time.time(),\n        \"request_count\": 0,\n        \"status\": \"running\"\n    }\n\n    try:\n        # Yield control to the server\n        yield app_state\n    finally:\n        # Shutdown phase\n        print(\"Server shutting down...\")\n        app_state[\"status\"] = \"stopped\"\n\n# Create server with lifespan\nmcp = FastMCP(\"basic-server\", lifespan=basic_lifespan)\n\n@mcp.tool()\nasync def get_server_stats(ctx: Context) -&gt; dict:\n    \"\"\"Get server statistics from lifespan context.\"\"\"\n    # Access lifespan context\n    app_state = ctx.request_context.lifespan_context\n\n    # Update request count\n    app_state[\"request_count\"] += 1\n\n    return {\n        \"startup_time\": app_state[\"startup_time\"],\n        \"request_count\": app_state[\"request_count\"],\n        \"status\": app_state[\"status\"],\n        \"uptime_seconds\": time.time() - app_state[\"startup_time\"]\n    }\n\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#error-handling-in-lifespan","title":"Error Handling in Lifespan","text":"<pre><code>import logging\nfrom contextlib import asynccontextmanager\n\nlogger = logging.getLogger(__name__)\n\n@asynccontextmanager\nasync def robust_lifespan(server: FastMCP):\n    \"\"\"Lifespan with comprehensive error handling.\"\"\"\n    context = {\"initialized\": False}\n\n    try:\n        # Startup with error handling\n        logger.info(\"Starting server initialization...\")\n\n        # Initialize critical resources\n        try:\n            # Simulate resource initialization\n            await asyncio.sleep(0.1)  # Database connection setup\n            context[\"database\"] = \"connected\"\n            context[\"initialized\"] = True\n            logger.info(\"Database connection established\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize database: {e}\")\n            raise\n\n        logger.info(\"Server initialization completed successfully\")\n        yield context\n\n    except Exception as e:\n        logger.error(f\"Error during server lifecycle: {e}\")\n        context[\"initialized\"] = False\n        raise\n    finally:\n        # Cleanup with error handling\n        logger.info(\"Starting server cleanup...\")\n\n        if context.get(\"database\") == \"connected\":\n            try:\n                # Cleanup database connection\n                await asyncio.sleep(0.1)  # Database cleanup\n                logger.info(\"Database connection closed\")\n            except Exception as e:\n                logger.error(f\"Error during database cleanup: {e}\")\n\n        logger.info(\"Server cleanup completed\")\n\nmcp = FastMCP(\"robust-server\", lifespan=robust_lifespan)\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#database-integration-patterns","title":"Database Integration Patterns","text":""},{"location":"development/fastmcp/lifespan-management/#database-connection-pool-management","title":"Database Connection Pool Management","text":"<pre><code>import asyncpg\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def database_lifespan(server: FastMCP):\n    \"\"\"Lifespan managing PostgreSQL connection pool.\"\"\"\n    # Database configuration\n    DATABASE_URL = \"postgresql://user:password@localhost/dbname\"\n\n    # Initialize connection pool\n    pool = await asyncpg.create_pool(\n        DATABASE_URL,\n        min_size=1,\n        max_size=10,\n        command_timeout=60\n    )\n\n    # Test connection\n    async with pool.acquire() as conn:\n        await conn.fetchval(\"SELECT 1\")\n\n    logger.info(f\"Database pool created with {pool.get_size()} connections\")\n\n    context = {\n        \"db_pool\": pool,\n        \"database_url\": DATABASE_URL\n    }\n\n    try:\n        yield context\n    finally:\n        # Close pool gracefully\n        await pool.close()\n        logger.info(\"Database pool closed\")\n\nmcp = FastMCP(\"database-server\", lifespan=database_lifespan)\n\n@mcp.tool()\nasync def create_user(name: str, email: str, ctx: Context) -&gt; dict:\n    \"\"\"Create user using database pool from lifespan.\"\"\"\n    pool = ctx.request_context.lifespan_context[\"db_pool\"]\n\n    async with pool.acquire() as conn:\n        # Insert user\n        user_id = await conn.fetchval(\n            \"INSERT INTO users (name, email) VALUES ($1, $2) RETURNING id\",\n            name, email\n        )\n\n        # Fetch created user\n        user = await conn.fetchrow(\n            \"SELECT id, name, email, created_at FROM users WHERE id = $1\",\n            user_id\n        )\n\n        return dict(user)\n\n@mcp.tool()\nasync def get_user(user_id: int, ctx: Context) -&gt; dict | None:\n    \"\"\"Get user by ID using database pool.\"\"\"\n    pool = ctx.request_context.lifespan_context[\"db_pool\"]\n\n    async with pool.acquire() as conn:\n        user = await conn.fetchrow(\n            \"SELECT id, name, email, created_at FROM users WHERE id = $1\",\n            user_id\n        )\n\n        return dict(user) if user else None\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#sqlite-with-connection-management","title":"SQLite with Connection Management","text":"<pre><code>import aiosqlite\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def sqlite_lifespan(server: FastMCP):\n    \"\"\"Lifespan managing SQLite database.\"\"\"\n    db_path = \"app_data.db\"\n\n    # Initialize database\n    async with aiosqlite.connect(db_path) as db:\n        # Create tables\n        await db.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS tasks (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                title TEXT NOT NULL,\n                description TEXT,\n                completed BOOLEAN DEFAULT FALSE,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\")\n        await db.commit()\n\n    logger.info(f\"SQLite database initialized: {db_path}\")\n\n    context = {\"db_path\": db_path}\n\n    try:\n        yield context\n    finally:\n        logger.info(\"SQLite database lifespan completed\")\n\nmcp = FastMCP(\"sqlite-server\", lifespan=sqlite_lifespan)\n\n@mcp.tool()\nasync def add_task(title: str, description: str = \"\", ctx: Context) -&gt; dict:\n    \"\"\"Add task to SQLite database.\"\"\"\n    db_path = ctx.request_context.lifespan_context[\"db_path\"]\n\n    async with aiosqlite.connect(db_path) as db:\n        cursor = await db.execute(\n            \"INSERT INTO tasks (title, description) VALUES (?, ?) RETURNING id\",\n            (title, description)\n        )\n        task_id = (await cursor.fetchone())[0]\n        await db.commit()\n\n        # Fetch created task\n        cursor = await db.execute(\n            \"SELECT id, title, description, completed, created_at FROM tasks WHERE id = ?\",\n            (task_id,)\n        )\n        task = await cursor.fetchone()\n\n        return {\n            \"id\": task[0],\n            \"title\": task[1],\n            \"description\": task[2],\n            \"completed\": bool(task[3]),\n            \"created_at\": task[4]\n        }\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#http-client-and-external-service-management","title":"HTTP Client and External Service Management","text":""},{"location":"development/fastmcp/lifespan-management/#http-client-pool","title":"HTTP Client Pool","text":"<pre><code>import httpx\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def http_client_lifespan(server: FastMCP):\n    \"\"\"Lifespan managing HTTP client with connection pooling.\"\"\"\n    # Configure HTTP client with connection pooling\n    limits = httpx.Limits(max_keepalive_connections=20, max_connections=100)\n    timeout = httpx.Timeout(10.0, read=30.0)\n\n    async with httpx.AsyncClient(\n        limits=limits,\n        timeout=timeout,\n        headers={\"User-Agent\": \"FastMCP Server/1.0\"}\n    ) as client:\n\n        # Test external connectivity\n        try:\n            response = await client.get(\"https://httpbin.org/status/200\")\n            response.raise_for_status()\n            logger.info(\"HTTP client connectivity verified\")\n        except Exception as e:\n            logger.warning(f\"HTTP client connectivity test failed: {e}\")\n\n        context = {\n            \"http_client\": client,\n            \"external_apis\": {\n                \"weather\": \"https://api.weather.com\",\n                \"geocoding\": \"https://api.geocoding.com\"\n            }\n        }\n\n        yield context\n\nmcp = FastMCP(\"http-client-server\", lifespan=http_client_lifespan)\n\n@mcp.tool()\nasync def fetch_weather(city: str, ctx: Context) -&gt; dict:\n    \"\"\"Fetch weather data using managed HTTP client.\"\"\"\n    lifespan_ctx = ctx.request_context.lifespan_context\n    client = lifespan_ctx[\"http_client\"]\n    weather_api = lifespan_ctx[\"external_apis\"][\"weather\"]\n\n    try:\n        response = await client.get(f\"{weather_api}/current\", params={\"city\": city})\n        response.raise_for_status()\n        return response.json()\n    except httpx.RequestError as e:\n        await ctx.error(f\"Weather API request failed: {e}\")\n        raise\n    except httpx.HTTPStatusError as e:\n        await ctx.error(f\"Weather API returned {e.response.status_code}\")\n        raise\n\n@mcp.tool()\nasync def fetch_url(url: str, ctx: Context) -&gt; dict:\n    \"\"\"Generic URL fetcher using managed HTTP client.\"\"\"\n    client = ctx.request_context.lifespan_context[\"http_client\"]\n\n    try:\n        response = await client.get(url)\n        response.raise_for_status()\n\n        return {\n            \"status_code\": response.status_code,\n            \"headers\": dict(response.headers),\n            \"content_type\": response.headers.get(\"content-type\"),\n            \"content\": response.text[:1000]  # Limit content size\n        }\n    except Exception as e:\n        await ctx.error(f\"Failed to fetch URL {url}: {e}\")\n        raise\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#caching-and-state-management","title":"Caching and State Management","text":""},{"location":"development/fastmcp/lifespan-management/#redis-cache-integration","title":"Redis Cache Integration","text":"<pre><code>import redis.asyncio as redis\nfrom contextlib import asynccontextmanager\nimport json\n\n@asynccontextmanager\nasync def redis_cache_lifespan(server: FastMCP):\n    \"\"\"Lifespan managing Redis cache.\"\"\"\n    redis_url = \"redis://localhost:6379\"\n\n    # Initialize Redis connection\n    redis_client = redis.from_url(redis_url)\n\n    # Test Redis connectivity\n    try:\n        await redis_client.ping()\n        logger.info(\"Redis cache connection established\")\n    except Exception as e:\n        logger.error(f\"Failed to connect to Redis: {e}\")\n        raise\n\n    context = {\n        \"cache\": redis_client,\n        \"cache_ttl\": 3600  # 1 hour default TTL\n    }\n\n    try:\n        yield context\n    finally:\n        await redis_client.close()\n        logger.info(\"Redis cache connection closed\")\n\nmcp = FastMCP(\"redis-cache-server\", lifespan=redis_cache_lifespan)\n\n@mcp.tool()\nasync def cached_computation(input_data: str, ctx: Context) -&gt; dict:\n    \"\"\"Perform computation with Redis caching.\"\"\"\n    lifespan_ctx = ctx.request_context.lifespan_context\n    cache = lifespan_ctx[\"cache\"]\n    ttl = lifespan_ctx[\"cache_ttl\"]\n\n    # Generate cache key\n    cache_key = f\"computation:{hash(input_data)}\"\n\n    # Check cache\n    cached_result = await cache.get(cache_key)\n    if cached_result:\n        await ctx.info(\"Cache hit\")\n        return json.loads(cached_result)\n\n    # Perform computation\n    await ctx.info(\"Cache miss - performing computation\")\n    result = {\n        \"input\": input_data,\n        \"output\": f\"processed_{input_data}\",\n        \"timestamp\": time.time()\n    }\n\n    # Cache result\n    await cache.setex(cache_key, ttl, json.dumps(result))\n\n    return result\n\n@mcp.tool()\nasync def clear_cache(pattern: str = \"*\", ctx: Context) -&gt; dict:\n    \"\"\"Clear cache entries matching pattern.\"\"\"\n    cache = ctx.request_context.lifespan_context[\"cache\"]\n\n    keys = await cache.keys(f\"computation:{pattern}\")\n    if keys:\n        deleted_count = await cache.delete(*keys)\n        await ctx.info(f\"Cleared {deleted_count} cache entries\")\n        return {\"cleared\": deleted_count, \"pattern\": pattern}\n    else:\n        return {\"cleared\": 0, \"pattern\": pattern}\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#in-memory-cache","title":"In-Memory Cache","text":"<pre><code>from contextlib import asynccontextmanager\nfrom collections import OrderedDict\nimport time\n\nclass LRUCache:\n    \"\"\"Simple LRU cache implementation.\"\"\"\n\n    def __init__(self, max_size: int = 1000, ttl: int = 3600):\n        self.max_size = max_size\n        self.ttl = ttl\n        self._cache = OrderedDict()\n\n    def get(self, key: str):\n        \"\"\"Get item from cache.\"\"\"\n        if key in self._cache:\n            value, timestamp = self._cache.pop(key)\n            if time.time() - timestamp &lt; self.ttl:\n                # Move to end (most recently used)\n                self._cache[key] = (value, timestamp)\n                return value\n            # Expired\n        return None\n\n    def set(self, key: str, value):\n        \"\"\"Set item in cache.\"\"\"\n        if key in self._cache:\n            self._cache.pop(key)\n        elif len(self._cache) &gt;= self.max_size:\n            # Remove oldest item\n            self._cache.popitem(last=False)\n\n        self._cache[key] = (value, time.time())\n\n    def clear(self, pattern: str = None):\n        \"\"\"Clear cache entries.\"\"\"\n        if pattern is None:\n            count = len(self._cache)\n            self._cache.clear()\n            return count\n        else:\n            # Simple pattern matching\n            keys_to_remove = [k for k in self._cache.keys() if pattern in k]\n            for key in keys_to_remove:\n                del self._cache[key]\n            return len(keys_to_remove)\n\n@asynccontextmanager\nasync def memory_cache_lifespan(server: FastMCP):\n    \"\"\"Lifespan managing in-memory cache.\"\"\"\n    cache = LRUCache(max_size=5000, ttl=1800)  # 30 minutes TTL\n\n    logger.info(\"In-memory cache initialized\")\n\n    context = {\"cache\": cache}\n\n    try:\n        yield context\n    finally:\n        cache.clear()\n        logger.info(\"In-memory cache cleared\")\n\nmcp = FastMCP(\"memory-cache-server\", lifespan=memory_cache_lifespan)\n\n@mcp.tool()\nasync def cached_operation(key: str, value: str, ctx: Context) -&gt; dict:\n    \"\"\"Operation with in-memory caching.\"\"\"\n    cache = ctx.request_context.lifespan_context[\"cache\"]\n\n    # Check cache\n    cached_value = cache.get(key)\n    if cached_value:\n        await ctx.info(f\"Cache hit for key: {key}\")\n        return {\"key\": key, \"value\": cached_value, \"cached\": True}\n\n    # Process and cache\n    processed_value = f\"processed_{value}\"\n    cache.set(key, processed_value)\n\n    await ctx.info(f\"Cached new value for key: {key}\")\n    return {\"key\": key, \"value\": processed_value, \"cached\": False}\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#dependency-injection-patterns","title":"Dependency Injection Patterns","text":""},{"location":"development/fastmcp/lifespan-management/#service-container-pattern","title":"Service Container Pattern","text":"<pre><code>from dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nimport asyncpg\nimport httpx\n\n@dataclass\nclass ServiceContainer:\n    \"\"\"Container for all application services.\"\"\"\n    db_pool: asyncpg.Pool\n    http_client: httpx.AsyncClient\n    cache: dict\n    config: dict\n\n    async def close(self):\n        \"\"\"Close all managed resources.\"\"\"\n        await self.db_pool.close()\n        await self.http_client.aclose()\n\n@asynccontextmanager\nasync def service_container_lifespan(server: FastMCP):\n    \"\"\"Lifespan providing dependency injection via service container.\"\"\"\n    # Initialize all services\n    db_pool = await asyncpg.create_pool(\"postgresql://user:password@localhost/db\")\n    http_client = httpx.AsyncClient(timeout=30.0)\n    cache = {}\n    config = {\n        \"api_key\": \"secret-key\",\n        \"debug\": True,\n        \"max_requests_per_minute\": 100\n    }\n\n    # Create service container\n    services = ServiceContainer(\n        db_pool=db_pool,\n        http_client=http_client,\n        cache=cache,\n        config=config\n    )\n\n    logger.info(\"Service container initialized\")\n\n    context = {\"services\": services}\n\n    try:\n        yield context\n    finally:\n        await services.close()\n        logger.info(\"Service container closed\")\n\nmcp = FastMCP(\"service-container-server\", lifespan=service_container_lifespan)\n\n@mcp.tool()\nasync def create_and_notify_user(name: str, email: str, ctx: Context) -&gt; dict:\n    \"\"\"Tool using multiple injected services.\"\"\"\n    services = ctx.request_context.lifespan_context[\"services\"]\n\n    # Use database\n    async with services.db_pool.acquire() as conn:\n        user_id = await conn.fetchval(\n            \"INSERT INTO users (name, email) VALUES ($1, $2) RETURNING id\",\n            name, email\n        )\n\n    # Cache user data\n    services.cache[f\"user:{user_id}\"] = {\"name\": name, \"email\": email}\n\n    # Send notification via HTTP API\n    if services.config[\"debug\"]:\n        await ctx.info(f\"Would send notification to {email}\")\n    else:\n        response = await services.http_client.post(\n            \"https://api.notifications.com/send\",\n            json={\"email\": email, \"message\": f\"Welcome {name}!\"},\n            headers={\"Authorization\": f\"Bearer {services.config['api_key']}\"}\n        )\n        response.raise_for_status()\n\n    return {\"user_id\": user_id, \"name\": name, \"email\": email}\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#transport-specific-lifespan-patterns","title":"Transport-Specific Lifespan Patterns","text":""},{"location":"development/fastmcp/lifespan-management/#streamablehttp-with-session-manager","title":"StreamableHTTP with Session Manager","text":"<pre><code>from contextlib import asynccontextmanager\nfrom mcp.server.streamable_http_manager import StreamableHTTPSessionManager\n\n@asynccontextmanager\nasync def streamable_http_lifespan(server: FastMCP):\n    \"\"\"Lifespan for StreamableHTTP with session management.\"\"\"\n    # Initialize application state\n    app_state = {\n        \"active_sessions\": {},\n        \"session_count\": 0,\n        \"startup_time\": time.time()\n    }\n\n    logger.info(\"StreamableHTTP server starting\")\n\n    try:\n        yield app_state\n    finally:\n        logger.info(f\"StreamableHTTP server shutting down. \"\n                   f\"Total sessions handled: {app_state['session_count']}\")\n\nmcp = FastMCP(\"streamable-http-server\", lifespan=streamable_http_lifespan)\n\n@mcp.tool()\nasync def get_session_stats(ctx: Context) -&gt; dict:\n    \"\"\"Get StreamableHTTP session statistics.\"\"\"\n    app_state = ctx.request_context.lifespan_context\n\n    return {\n        \"active_sessions\": len(app_state[\"active_sessions\"]),\n        \"total_sessions\": app_state[\"session_count\"],\n        \"uptime_seconds\": time.time() - app_state[\"startup_time\"]\n    }\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#multi-transport-configuration","title":"Multi-Transport Configuration","text":"<pre><code>@asynccontextmanager\nasync def multi_transport_lifespan(server: FastMCP):\n    \"\"\"Lifespan supporting multiple transport types.\"\"\"\n    # Initialize common resources\n    shared_resources = {\n        \"cache\": {},\n        \"stats\": {\"requests\": 0, \"errors\": 0}\n    }\n\n    # Transport-specific initialization could go here\n    # (though FastMCP handles transport setup automatically)\n\n    logger.info(\"Multi-transport server initialized\")\n\n    try:\n        yield shared_resources\n    finally:\n        logger.info(f\"Server shutdown. Final stats: {shared_resources['stats']}\")\n\nmcp = FastMCP(\"multi-transport-server\", lifespan=multi_transport_lifespan)\n\n@mcp.tool()\nasync def increment_stats(stat_name: str, ctx: Context) -&gt; dict:\n    \"\"\"Increment a statistic counter.\"\"\"\n    stats = ctx.request_context.lifespan_context[\"stats\"]\n    stats[stat_name] = stats.get(stat_name, 0) + 1\n\n    return {\"stat\": stat_name, \"value\": stats[stat_name]}\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#advanced-lifespan-patterns","title":"Advanced Lifespan Patterns","text":""},{"location":"development/fastmcp/lifespan-management/#health-check-integration","title":"Health Check Integration","text":"<pre><code>from enum import Enum\nfrom dataclasses import dataclass\nimport asyncio\n\nclass HealthStatus(Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n\n@dataclass\nclass HealthCheck:\n    name: str\n    status: HealthStatus\n    last_check: float\n    message: str = \"\"\n\nclass HealthMonitor:\n    \"\"\"Health monitoring service.\"\"\"\n\n    def __init__(self):\n        self.checks = {}\n        self._running = False\n        self._task = None\n\n    def add_check(self, name: str, check_func):\n        \"\"\"Add a health check.\"\"\"\n        self.checks[name] = check_func\n\n    async def start(self):\n        \"\"\"Start health monitoring.\"\"\"\n        self._running = True\n        self._task = asyncio.create_task(self._monitor_loop())\n\n    async def stop(self):\n        \"\"\"Stop health monitoring.\"\"\"\n        self._running = False\n        if self._task:\n            self._task.cancel()\n            try:\n                await self._task\n            except asyncio.CancelledError:\n                pass\n\n    async def _monitor_loop(self):\n        \"\"\"Health monitoring loop.\"\"\"\n        while self._running:\n            await asyncio.sleep(30)  # Check every 30 seconds\n            for name, check_func in self.checks.items():\n                try:\n                    await check_func()\n                except Exception as e:\n                    logger.warning(f\"Health check {name} failed: {e}\")\n\n@asynccontextmanager\nasync def health_monitored_lifespan(server: FastMCP):\n    \"\"\"Lifespan with integrated health monitoring.\"\"\"\n    # Initialize resources\n    db_pool = await asyncpg.create_pool(\"postgresql://user:password@localhost/db\")\n    health_monitor = HealthMonitor()\n\n    # Add health checks\n    async def db_health_check():\n        async with db_pool.acquire() as conn:\n            await conn.fetchval(\"SELECT 1\")\n\n    health_monitor.add_check(\"database\", db_health_check)\n\n    # Start health monitoring\n    await health_monitor.start()\n\n    context = {\n        \"db_pool\": db_pool,\n        \"health_monitor\": health_monitor\n    }\n\n    logger.info(\"Server with health monitoring started\")\n\n    try:\n        yield context\n    finally:\n        await health_monitor.stop()\n        await db_pool.close()\n        logger.info(\"Health monitoring stopped\")\n\nmcp = FastMCP(\"health-monitored-server\", lifespan=health_monitored_lifespan)\n\n@mcp.tool()\nasync def health_status(ctx: Context) -&gt; dict:\n    \"\"\"Get server health status.\"\"\"\n    # Implementation would access health monitor from context\n    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#configuration-validation","title":"Configuration Validation","text":"<pre><code>from pydantic import BaseSettings, validator\nfrom contextlib import asynccontextmanager\n\nclass AppSettings(BaseSettings):\n    \"\"\"Application settings with validation.\"\"\"\n    database_url: str\n    redis_url: str = \"redis://localhost:6379\"\n    api_key: str\n    debug: bool = False\n    max_connections: int = 100\n\n    @validator('max_connections')\n    def validate_max_connections(cls, v):\n        if v &lt; 1 or v &gt; 1000:\n            raise ValueError('max_connections must be between 1 and 1000')\n        return v\n\n    class Config:\n        env_prefix = \"APP_\"\n\n@asynccontextmanager\nasync def validated_config_lifespan(server: FastMCP):\n    \"\"\"Lifespan with configuration validation.\"\"\"\n    try:\n        # Load and validate configuration\n        settings = AppSettings()\n        logger.info(\"Configuration validated successfully\")\n    except Exception as e:\n        logger.error(f\"Configuration validation failed: {e}\")\n        raise\n\n    # Initialize resources with validated config\n    resources = {}\n\n    try:\n        # Database\n        resources[\"db_pool\"] = await asyncpg.create_pool(\n            settings.database_url,\n            max_size=settings.max_connections\n        )\n\n        # Redis\n        resources[\"redis\"] = redis.from_url(settings.redis_url)\n\n        context = {\n            \"settings\": settings,\n            \"resources\": resources\n        }\n\n        yield context\n\n    finally:\n        # Cleanup resources\n        for resource in resources.values():\n            if hasattr(resource, 'close'):\n                await resource.close()\n\nmcp = FastMCP(\"validated-config-server\", lifespan=validated_config_lifespan)\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/lifespan-management/#lifespan-design-guidelines","title":"Lifespan Design Guidelines","text":"<ol> <li>Keep It Simple: Start with basic lifespan patterns and add complexity as needed</li> <li>Error Handling: Always implement proper exception handling in startup and cleanup</li> <li>Resource Cleanup: Ensure all resources are properly closed in the finally block</li> <li>Logging: Add comprehensive logging for debugging lifecycle issues</li> <li>Testing: Test both successful and failed initialization scenarios</li> </ol>"},{"location":"development/fastmcp/lifespan-management/#performance-considerations","title":"Performance Considerations","text":"<pre><code>@asynccontextmanager\nasync def optimized_lifespan(server: FastMCP):\n    \"\"\"Performance-optimized lifespan.\"\"\"\n    # Initialize resources concurrently\n    async with asyncio.TaskGroup() as tg:\n        db_task = tg.create_task(asyncpg.create_pool(\"postgresql://...\"))\n        redis_task = tg.create_task(redis.from_url(\"redis://...\"))\n        http_task = tg.create_task(httpx.AsyncClient().__aenter__())\n\n    context = {\n        \"db_pool\": db_task.result(),\n        \"redis\": redis_task.result(),\n        \"http_client\": http_task.result()\n    }\n\n    try:\n        yield context\n    finally:\n        # Cleanup concurrently\n        async with asyncio.TaskGroup() as tg:\n            tg.create_task(context[\"db_pool\"].close())\n            tg.create_task(context[\"redis\"].close())\n            tg.create_task(context[\"http_client\"].__aexit__(None, None, None))\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#security-considerations","title":"Security Considerations","text":"<pre><code>@asynccontextmanager\nasync def secure_lifespan(server: FastMCP):\n    \"\"\"Security-focused lifespan implementation.\"\"\"\n    # Load secrets securely\n    import os\n    database_url = os.environ.get(\"DATABASE_URL\")\n    api_key = os.environ.get(\"API_KEY\")\n\n    if not database_url or not api_key:\n        raise ValueError(\"Required environment variables not set\")\n\n    # Initialize with security headers\n    http_client = httpx.AsyncClient(\n        headers={\n            \"User-Agent\": \"FastMCP-Server/1.0\",\n            \"X-Request-ID\": str(uuid.uuid4())\n        },\n        verify=True  # Verify SSL certificates\n    )\n\n    context = {\n        \"http_client\": http_client,\n        # Don't store secrets in context\n        \"api_key_set\": bool(api_key)\n    }\n\n    try:\n        yield context\n    finally:\n        await http_client.aclose()\n        # Clear any sensitive data\n        context.clear()\n</code></pre>"},{"location":"development/fastmcp/lifespan-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/fastmcp/lifespan-management/#common-issues","title":"Common Issues","text":"<ol> <li>Lifespan Errors: Check exception handling in both startup and cleanup phases</li> <li>Resource Leaks: Ensure all resources are properly closed in finally blocks</li> <li>Timeout Issues: Set appropriate timeouts for resource initialization</li> <li>Context Access: Verify lifespan context is accessed correctly in tools</li> </ol>"},{"location":"development/fastmcp/lifespan-management/#debugging-lifespan-issues","title":"Debugging Lifespan Issues","text":"<pre><code>import traceback\n\n@asynccontextmanager\nasync def debug_lifespan(server: FastMCP):\n    \"\"\"Lifespan with comprehensive debugging.\"\"\"\n    logger.info(\"=== LIFESPAN DEBUG: Starting initialization ===\")\n\n    context = {}\n    initialization_steps = []\n\n    try:\n        # Step 1: Database\n        logger.info(\"Step 1: Initializing database...\")\n        initialization_steps.append(\"database\")\n        context[\"db_pool\"] = await asyncpg.create_pool(\"postgresql://...\")\n        logger.info(\"\u2713 Database initialized\")\n\n        # Step 2: Cache\n        logger.info(\"Step 2: Initializing cache...\")\n        initialization_steps.append(\"cache\")\n        context[\"cache\"] = {}\n        logger.info(\"\u2713 Cache initialized\")\n\n        logger.info(\"=== LIFESPAN DEBUG: Initialization complete ===\")\n        yield context\n\n    except Exception as e:\n        logger.error(f\"=== LIFESPAN DEBUG: Initialization failed at step: {initialization_steps[-1] if initialization_steps else 'unknown'} ===\")\n        logger.error(f\"Error: {e}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        raise\n    finally:\n        logger.info(\"=== LIFESPAN DEBUG: Starting cleanup ===\")\n\n        # Cleanup in reverse order\n        for step in reversed(initialization_steps):\n            try:\n                if step == \"database\" and \"db_pool\" in context:\n                    await context[\"db_pool\"].close()\n                    logger.info(\"\u2713 Database cleanup complete\")\n                elif step == \"cache\":\n                    context.get(\"cache\", {}).clear()\n                    logger.info(\"\u2713 Cache cleanup complete\")\n            except Exception as e:\n                logger.error(f\"Cleanup error for {step}: {e}\")\n\n        logger.info(\"=== LIFESPAN DEBUG: Cleanup complete ===\")\n</code></pre> <p>This comprehensive guide provides everything needed to implement robust lifespan management in FastMCP servers, from basic patterns to production-ready dependency injection and resource management systems.</p>"},{"location":"development/fastmcp/migration-guide/","title":"Migration Guide","text":"<p>Complete guide for migrating from the raw MCP SDK to FastMCP, including step-by-step migration examples, feature comparisons, and best practices.</p>"},{"location":"development/fastmcp/migration-guide/#overview","title":"Overview","text":"<p>FastMCP provides a high-level, decorator-based API that significantly simplifies MCP server development compared to the raw MCP SDK. This guide helps you migrate existing servers to take advantage of FastMCP's ergonomic features.</p>"},{"location":"development/fastmcp/migration-guide/#key-benefits-of-migration","title":"Key Benefits of Migration","text":"<ul> <li>Simpler API: Decorator-based tool, resource, and prompt registration</li> <li>Automatic Type Validation: Built-in Pydantic validation</li> <li>Context Injection: Easy access to MCP capabilities via Context object</li> <li>Better Error Handling: Automatic error wrapping and reporting</li> <li>Reduced Boilerplate: Less code required for common patterns</li> <li>Enhanced Development Experience: Better debugging, testing, and monitoring</li> </ul>"},{"location":"development/fastmcp/migration-guide/#migration-process","title":"Migration Process","text":""},{"location":"development/fastmcp/migration-guide/#1-dependency-updates","title":"1. Dependency Updates","text":"<p>Update your <code>pyproject.toml</code>: <pre><code># Before (raw MCP SDK)\ndependencies = [\"mcp\"]\n\n# After (FastMCP)\ndependencies = [\"mcp\"]  # Same package, FastMCP is included\n</code></pre></p> <p>Import changes: <pre><code># Before (raw MCP SDK)\nfrom mcp.server.lowlevel import Server\nimport mcp.types as types\nfrom mcp.server.stdio import stdio_server\n\n# After (FastMCP)\nfrom mcp.server.fastmcp import FastMCP, Context\nfrom mcp.server.fastmcp.prompts.base import UserMessage, AssistantMessage\n# Note: mcp.types still available for compatibility\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#2-server-initialization","title":"2. Server Initialization","text":"<p>Before (Raw MCP SDK): <pre><code>import anyio\nfrom mcp.server.lowlevel import Server\nfrom mcp.server.stdio import stdio_server\n\ndef main():\n    app = Server(\"my-mcp-server\")\n\n    # Register handlers (shown below)\n\n    async def arun():\n        async with stdio_server() as streams:\n            await app.run(\n                streams[0],\n                streams[1],\n                app.create_initialization_options()\n            )\n\n    anyio.run(arun)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> <p>After (FastMCP): <pre><code>from mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"my-mcp-server\")\n\n# Register handlers with decorators (shown below)\n\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")  # Much simpler!\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#tool-migration","title":"Tool Migration","text":""},{"location":"development/fastmcp/migration-guide/#simple-tools","title":"Simple Tools","text":"<p>Before (Raw MCP SDK): <pre><code>@app.list_tools()\nasync def list_tools() -&gt; list[types.Tool]:\n    return [\n        types.Tool(\n            name=\"add_numbers\",\n            description=\"Add two numbers together\",\n            inputSchema={\n                \"type\": \"object\",\n                \"required\": [\"a\", \"b\"],\n                \"properties\": {\n                    \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n                    \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n                }\n            }\n        )\n    ]\n\n@app.call_tool()\nasync def call_tool(\n    name: str, arguments: dict\n) -&gt; list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n    if name == \"add_numbers\":\n        if \"a\" not in arguments or \"b\" not in arguments:\n            raise ValueError(\"Missing required arguments\")\n\n        try:\n            a = float(arguments[\"a\"])\n            b = float(arguments[\"b\"])\n            result = a + b\n            return [types.TextContent(type=\"text\", text=str(result))]\n        except (ValueError, TypeError):\n            raise ValueError(\"Arguments must be numbers\")\n\n    raise ValueError(f\"Unknown tool: {name}\")\n</code></pre></p> <p>After (FastMCP): <pre><code>@mcp.tool()\ndef add_numbers(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\n# That's it! FastMCP handles:\n# - Schema generation from type hints\n# - Input validation\n# - Error handling\n# - Return value conversion\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#tools-with-context","title":"Tools with Context","text":"<p>Before (Raw MCP SDK): <pre><code>from mcp.shared.context import RequestContext\n\n@app.call_tool()\nasync def call_tool(name: str, arguments: dict) -&gt; list[types.TextContent]:\n    if name == \"logged_operation\":\n        # Manual context access (complex)\n        request_ctx = app.request_context\n\n        # Manual logging to client\n        await request_ctx.session.send_log_message(\n            level=\"info\",\n            data=\"Processing operation\",\n            related_request_id=request_ctx.request_id\n        )\n\n        result = process_data(arguments.get(\"data\", \"\"))\n        return [types.TextContent(type=\"text\", text=result)]\n</code></pre></p> <p>After (FastMCP): <pre><code>@mcp.tool()\nasync def logged_operation(data: str, ctx: Context) -&gt; str:\n    \"\"\"Operation with logging.\"\"\"\n    await ctx.info(\"Processing operation\")\n    result = process_data(data)\n    return result\n\n# FastMCP automatically:\n# - Injects Context object\n# - Handles logging to client\n# - Manages request context\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#complex-tools-with-validation","title":"Complex Tools with Validation","text":"<p>Before (Raw MCP SDK): <pre><code>@app.list_tools()\nasync def list_tools() -&gt; list[types.Tool]:\n    return [\n        types.Tool(\n            name=\"create_user\",\n            description=\"Create a new user\",\n            inputSchema={\n                \"type\": \"object\",\n                \"required\": [\"name\", \"email\"],\n                \"properties\": {\n                    \"name\": {\n                        \"type\": \"string\",\n                        \"minLength\": 1,\n                        \"maxLength\": 100\n                    },\n                    \"email\": {\n                        \"type\": \"string\",\n                        \"format\": \"email\"\n                    },\n                    \"age\": {\n                        \"type\": \"integer\",\n                        \"minimum\": 0,\n                        \"maximum\": 150\n                    }\n                }\n            }\n        )\n    ]\n\n@app.call_tool()\nasync def call_tool(name: str, arguments: dict) -&gt; list[types.TextContent]:\n    if name == \"create_user\":\n        # Manual validation\n        if not arguments.get(\"name\") or len(arguments[\"name\"]) &gt; 100:\n            raise ValueError(\"Invalid name\")\n\n        # Manual email validation\n        email = arguments.get(\"email\", \"\")\n        if \"@\" not in email:\n            raise ValueError(\"Invalid email\")\n\n        # Create user logic\n        user = create_user(arguments[\"name\"], email, arguments.get(\"age\"))\n        return [types.TextContent(type=\"text\", text=f\"Created user: {user.id}\")]\n</code></pre></p> <p>After (FastMCP): <pre><code>from pydantic import BaseModel, Field, EmailStr\n\nclass UserInput(BaseModel):\n    name: str = Field(min_length=1, max_length=100)\n    email: EmailStr\n    age: int | None = Field(None, ge=0, le=150)\n\n@mcp.tool()\ndef create_user(user_data: UserInput) -&gt; str:\n    \"\"\"Create a new user with validation.\"\"\"\n    user = create_user(user_data.name, user_data.email, user_data.age)\n    return f\"Created user: {user.id}\"\n\n# FastMCP automatically:\n# - Generates JSON schema from Pydantic model\n# - Validates all inputs\n# - Provides helpful error messages\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#resource-migration","title":"Resource Migration","text":""},{"location":"development/fastmcp/migration-guide/#static-resources","title":"Static Resources","text":"<p>Before (Raw MCP SDK): <pre><code>from pydantic import AnyUrl\n\nSAMPLE_RESOURCES = {\n    \"greeting\": \"Hello! This is a sample resource.\",\n    \"help\": \"Help documentation here.\",\n}\n\n@app.list_resources()\nasync def list_resources() -&gt; list[types.Resource]:\n    return [\n        types.Resource(\n            uri=AnyUrl(f\"file:///{name}.txt\"),\n            name=name,\n            description=f\"A sample text resource named {name}\",\n            mimeType=\"text/plain\",\n        )\n        for name in SAMPLE_RESOURCES.keys()\n    ]\n\n@app.read_resource()\nasync def read_resource(uri: AnyUrl) -&gt; str | bytes:\n    if uri.path is None:\n        raise ValueError(f\"Invalid resource path: {uri}\")\n    name = uri.path.replace(\".txt\", \"\").lstrip(\"/\")\n\n    if name not in SAMPLE_RESOURCES:\n        raise ValueError(f\"Unknown resource: {uri}\")\n\n    return SAMPLE_RESOURCES[name]\n</code></pre></p> <p>After (FastMCP): <pre><code>SAMPLE_RESOURCES = {\n    \"greeting\": \"Hello! This is a sample resource.\",\n    \"help\": \"Help documentation here.\",\n}\n\n@mcp.resource(\"file:///greeting.txt\")\ndef greeting_resource() -&gt; str:\n    \"\"\"Sample greeting resource.\"\"\"\n    return SAMPLE_RESOURCES[\"greeting\"]\n\n@mcp.resource(\"file:///help.txt\")\ndef help_resource() -&gt; str:\n    \"\"\"Help documentation resource.\"\"\"\n    return SAMPLE_RESOURCES[\"help\"]\n\n# FastMCP handles:\n# - Resource registration and listing\n# - URI matching\n# - Error handling\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#template-resources","title":"Template Resources","text":"<p>Before (Raw MCP SDK): <pre><code>@app.list_resource_templates()\nasync def list_resource_templates() -&gt; list[types.ResourceTemplate]:\n    return [\n        types.ResourceTemplate(\n            uriTemplate=\"file:///{name}.txt\",\n            name=\"Dynamic file resource\",\n            description=\"Access files by name\"\n        )\n    ]\n\n@app.read_resource()\nasync def read_resource(uri: AnyUrl) -&gt; str | bytes:\n    # Manual template parameter extraction\n    if uri.path is None:\n        raise ValueError(\"Invalid URI path\")\n\n    name = uri.path.replace(\".txt\", \"\").lstrip(\"/\")\n\n    # Manual parameter validation\n    if not name or \"..\" in name or \"/\" in name:\n        raise ValueError(\"Invalid file name\")\n\n    return load_file_content(name)\n</code></pre></p> <p>After (FastMCP): <pre><code>@mcp.resource(\"file:///{name}.txt\")\ndef dynamic_file_resource(name: str) -&gt; str:\n    \"\"\"Access files by name.\"\"\"\n    # FastMCP validates parameters automatically\n    return load_file_content(name)\n\n# FastMCP automatically:\n# - Extracts template parameters\n# - Validates parameter types\n# - Handles template registration\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#prompt-migration","title":"Prompt Migration","text":"<p>Before (Raw MCP SDK): <pre><code>@app.list_prompts()\nasync def list_prompts() -&gt; list[types.Prompt]:\n    return [\n        types.Prompt(\n            name=\"analyze_data\",\n            description=\"Generate data analysis prompt\",\n            arguments=[\n                types.PromptArgument(\n                    name=\"dataset\",\n                    description=\"Dataset to analyze\",\n                    required=True\n                ),\n                types.PromptArgument(\n                    name=\"focus\",\n                    description=\"Analysis focus area\",\n                    required=False\n                )\n            ]\n        )\n    ]\n\n@app.get_prompt()\nasync def get_prompt(\n    name: str, arguments: dict[str, str] | None = None\n) -&gt; types.GetPromptResult:\n    if name != \"analyze_data\":\n        raise ValueError(f\"Unknown prompt: {name}\")\n\n    if not arguments or \"dataset\" not in arguments:\n        raise ValueError(\"Missing required argument: dataset\")\n\n    dataset = arguments[\"dataset\"]\n    focus = arguments.get(\"focus\", \"general trends\")\n\n    messages = [\n        types.PromptMessage(\n            role=\"user\",\n            content=types.TextContent(\n                type=\"text\",\n                text=f\"Analyze the {dataset} dataset, focusing on {focus}\"\n            )\n        )\n    ]\n\n    return types.GetPromptResult(\n        messages=messages,\n        description=\"Data analysis prompt\"\n    )\n</code></pre></p> <p>After (FastMCP): <pre><code>from mcp.server.fastmcp.prompts.base import UserMessage\n\n@mcp.prompt()\ndef analyze_data(dataset: str, focus: str = \"general trends\") -&gt; list[UserMessage]:\n    \"\"\"Generate data analysis prompt.\"\"\"\n    return [\n        UserMessage(f\"Analyze the {dataset} dataset, focusing on {focus}\")\n    ]\n\n# FastMCP automatically:\n# - Generates argument schema from function signature\n# - Validates required/optional parameters\n# - Handles prompt registration and rendering\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#transport-migration","title":"Transport Migration","text":""},{"location":"development/fastmcp/migration-guide/#stdio-transport","title":"Stdio Transport","text":"<p>Before (Raw MCP SDK): <pre><code>import anyio\nfrom mcp.server.stdio import stdio_server\n\nasync def arun():\n    async with stdio_server() as streams:\n        await app.run(\n            streams[0],\n            streams[1],\n            app.create_initialization_options()\n        )\n\nanyio.run(arun)\n</code></pre></p> <p>After (FastMCP): <pre><code>mcp.run(\"stdio\")  # That's it!\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#http-transports","title":"HTTP Transports","text":"<p>Before (Raw MCP SDK): <pre><code>from mcp.server.sse import SseServerTransport\nfrom starlette.applications import Starlette\nfrom starlette.responses import Response\nfrom starlette.routing import Mount, Route\nimport uvicorn\n\nsse = SseServerTransport(\"/messages/\")\n\nasync def handle_sse(request):\n    async with sse.connect_sse(\n        request.scope, request.receive, request._send\n    ) as streams:\n        await app.run(\n            streams[0],\n            streams[1],\n            app.create_initialization_options()\n        )\n    return Response()\n\nstarlette_app = Starlette(\n    debug=True,\n    routes=[\n        Route(\"/sse\", endpoint=handle_sse, methods=[\"GET\"]),\n        Mount(\"/messages/\", app=sse.handle_post_message),\n    ],\n)\n\nuvicorn.run(starlette_app, host=\"127.0.0.1\", port=8000)\n</code></pre></p> <p>After (FastMCP): <pre><code>mcp.run(\"sse\")  # Or \"streamable-http\"\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#advanced-migration-patterns","title":"Advanced Migration Patterns","text":""},{"location":"development/fastmcp/migration-guide/#error-handling","title":"Error Handling","text":"<p>Before (Raw MCP SDK): <pre><code>@app.call_tool()\nasync def call_tool(name: str, arguments: dict) -&gt; list[types.TextContent]:\n    try:\n        if name == \"risky_operation\":\n            result = perform_risky_operation(arguments.get(\"data\"))\n            return [types.TextContent(type=\"text\", text=result)]\n        else:\n            raise ValueError(f\"Unknown tool: {name}\")\n    except Exception as e:\n        # Manual error handling and logging\n        logger.error(f\"Tool {name} failed: {e}\")\n        raise  # Re-raise for client\n</code></pre></p> <p>After (FastMCP): <pre><code>@mcp.tool()\nasync def risky_operation(data: str, ctx: Context) -&gt; str:\n    \"\"\"Operation with automatic error handling.\"\"\"\n    try:\n        result = perform_risky_operation(data)\n        await ctx.info(\"Operation completed successfully\")\n        return result\n    except Exception as e:\n        await ctx.error(f\"Operation failed: {e}\")\n        raise  # FastMCP handles error formatting for client\n\n# FastMCP automatically:\n# - Wraps exceptions appropriately\n# - Provides context for logging\n# - Formats errors for client\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#custom-http-routes","title":"Custom HTTP Routes","text":"<p>Before (Raw MCP SDK): <pre><code>from starlette.routing import Route\nfrom starlette.responses import JSONResponse\n\nasync def health_check(request):\n    return JSONResponse({\"status\": \"healthy\"})\n\n# Manual route setup in Starlette app\nadditional_routes = [Route(\"/health\", endpoint=health_check)]\n</code></pre></p> <p>After (FastMCP): <pre><code>from starlette.requests import Request\nfrom starlette.responses import JSONResponse\n\n@mcp.custom_route(\"/health\", methods=[\"GET\"])\nasync def health_check(request: Request) -&gt; JSONResponse:\n    \"\"\"Health check endpoint.\"\"\"\n    return JSONResponse({\"status\": \"healthy\"})\n\n# FastMCP automatically integrates custom routes\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#lifespan-management","title":"Lifespan Management","text":"<p>Before (Raw MCP SDK): <pre><code># Complex manual setup required\nimport asyncio\nfrom contextlib import asynccontextmanager\n\ndatabase_pool = None\n\nasync def initialize():\n    global database_pool\n    database_pool = await create_database_pool()\n\nasync def cleanup():\n    global database_pool\n    if database_pool:\n        await database_pool.close()\n\n# Manual lifecycle management in main()\n</code></pre></p> <p>After (FastMCP): <pre><code>from contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(app: FastMCP):\n    \"\"\"Manage application lifecycle.\"\"\"\n    # Startup\n    database_pool = await create_database_pool()\n    app.database_pool = database_pool\n\n    try:\n        yield\n    finally:\n        # Cleanup\n        await database_pool.close()\n\nmcp = FastMCP(\"My Server\", lifespan=lifespan)\n\n@mcp.tool()\nasync def database_tool(query: str, ctx: Context) -&gt; str:\n    \"\"\"Tool that uses managed database connection.\"\"\"\n    pool = ctx.fastmcp.database_pool\n    async with pool.acquire() as conn:\n        result = await conn.fetchval(query)\n        return str(result)\n</code></pre></p>"},{"location":"development/fastmcp/migration-guide/#complete-migration-example","title":"Complete Migration Example","text":"<p>Here's a complete before/after example showing a full server migration:</p>"},{"location":"development/fastmcp/migration-guide/#before-raw-mcp-sdk","title":"Before (Raw MCP SDK)","text":"<pre><code>import anyio\nimport click\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.shared._httpx_utils import create_mcp_http_client\n\n# Data storage\nUSERS = {}\nuser_counter = 0\n\nasync def fetch_weather(city: str) -&gt; str:\n    \"\"\"Fetch weather data for a city.\"\"\"\n    async with create_mcp_http_client() as client:\n        response = await client.get(f\"https://api.weather.com/{city}\")\n        response.raise_for_status()\n        return response.json()[\"temperature\"]\n\n@click.command()\ndef main():\n    app = Server(\"user-weather-server\")\n\n    @app.list_tools()\n    async def list_tools() -&gt; list[types.Tool]:\n        return [\n            types.Tool(\n                name=\"create_user\",\n                description=\"Create a new user\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"required\": [\"name\", \"email\"],\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"email\": {\"type\": \"string\"}\n                    }\n                }\n            ),\n            types.Tool(\n                name=\"get_weather\",\n                description=\"Get weather for a city\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"required\": [\"city\"],\n                    \"properties\": {\n                        \"city\": {\"type\": \"string\"}\n                    }\n                }\n            )\n        ]\n\n    @app.call_tool()\n    async def call_tool(name: str, arguments: dict) -&gt; list[types.TextContent]:\n        global user_counter\n\n        if name == \"create_user\":\n            if \"name\" not in arguments or \"email\" not in arguments:\n                raise ValueError(\"Missing required arguments\")\n\n            user_counter += 1\n            user_id = user_counter\n            USERS[user_id] = {\n                \"name\": arguments[\"name\"],\n                \"email\": arguments[\"email\"]\n            }\n\n            return [types.TextContent(\n                type=\"text\",\n                text=f\"Created user {user_id}: {arguments['name']}\"\n            )]\n\n        elif name == \"get_weather\":\n            if \"city\" not in arguments:\n                raise ValueError(\"Missing city argument\")\n\n            try:\n                temp = await fetch_weather(arguments[\"city\"])\n                return [types.TextContent(\n                    type=\"text\",\n                    text=f\"Temperature in {arguments['city']}: {temp}\u00b0C\"\n                )]\n            except Exception as e:\n                raise ValueError(f\"Failed to fetch weather: {e}\")\n\n        else:\n            raise ValueError(f\"Unknown tool: {name}\")\n\n    @app.list_resources()\n    async def list_resources() -&gt; list[types.Resource]:\n        return [\n            types.Resource(\n                uri=f\"users://user/{user_id}\",\n                name=f\"User {user_id}\",\n                description=f\"User data for {data['name']}\",\n                mimeType=\"application/json\"\n            )\n            for user_id, data in USERS.items()\n        ]\n\n    @app.read_resource()\n    async def read_resource(uri: str) -&gt; str:\n        if not uri.startswith(\"users://user/\"):\n            raise ValueError(\"Invalid URI\")\n\n        user_id = int(uri.split(\"/\")[-1])\n        if user_id not in USERS:\n            raise ValueError(\"User not found\")\n\n        import json\n        return json.dumps(USERS[user_id])\n\n    async def arun():\n        async with stdio_server() as streams:\n            await app.run(\n                streams[0],\n                streams[1],\n                app.create_initialization_options()\n            )\n\n    anyio.run(arun)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"development/fastmcp/migration-guide/#after-fastmcp","title":"After (FastMCP)","text":"<pre><code>from mcp.server.fastmcp import FastMCP, Context\nfrom mcp.shared._httpx_utils import create_mcp_http_client\nfrom pydantic import BaseModel, EmailStr\n\n# Create server\nmcp = FastMCP(\"user-weather-server\")\n\n# Data storage\nUSERS = {}\nuser_counter = 0\n\n# Pydantic models for validation\nclass UserInput(BaseModel):\n    name: str\n    email: EmailStr\n\n@mcp.tool()\nasync def create_user(user_data: UserInput, ctx: Context) -&gt; str:\n    \"\"\"Create a new user with validation.\"\"\"\n    global user_counter\n\n    user_counter += 1\n    user_id = user_counter\n\n    USERS[user_id] = {\n        \"name\": user_data.name,\n        \"email\": user_data.email\n    }\n\n    await ctx.info(f\"Created user {user_id}\")\n    return f\"Created user {user_id}: {user_data.name}\"\n\n@mcp.tool()\nasync def get_weather(city: str, ctx: Context) -&gt; str:\n    \"\"\"Get weather for a city.\"\"\"\n    await ctx.info(f\"Fetching weather for {city}\")\n\n    try:\n        async with create_mcp_http_client() as client:\n            response = await client.get(f\"https://api.weather.com/{city}\")\n            response.raise_for_status()\n            temp = response.json()[\"temperature\"]\n\n        await ctx.info(\"Weather data retrieved successfully\")\n        return f\"Temperature in {city}: {temp}\u00b0C\"\n\n    except Exception as e:\n        await ctx.error(f\"Failed to fetch weather: {e}\")\n        raise\n\n@mcp.resource(\"users://user/{user_id}\")\ndef get_user(user_id: int) -&gt; dict:\n    \"\"\"Get user data by ID.\"\"\"\n    if user_id not in USERS:\n        raise ValueError(\"User not found\")\n\n    return USERS[user_id]\n\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")\n</code></pre>"},{"location":"development/fastmcp/migration-guide/#migration-checklist","title":"Migration Checklist","text":""},{"location":"development/fastmcp/migration-guide/#pre-migration","title":"Pre-Migration","text":"<ul> <li> Audit current server: List all tools, resources, and prompts</li> <li> Review error handling: Note custom error handling patterns</li> <li> Check dependencies: Identify external libraries and integrations</li> <li> Document custom features: Note any custom transport or middleware</li> </ul>"},{"location":"development/fastmcp/migration-guide/#during-migration","title":"During Migration","text":"<ul> <li> Update imports: Switch to FastMCP imports</li> <li> Convert server creation: Replace Server with FastMCP</li> <li> Migrate tools: Convert to decorator-based registration</li> <li> Migrate resources: Use @resource decorator</li> <li> Migrate prompts: Use @prompt decorator</li> <li> Add type hints: Enable automatic validation</li> <li> Update transport: Simplify transport setup</li> <li> Add context usage: Leverage Context for logging and progress</li> </ul>"},{"location":"development/fastmcp/migration-guide/#post-migration-testing","title":"Post-Migration Testing","text":"<ul> <li> Test all tools: Verify functionality and validation</li> <li> Test all resources: Check URI resolution and content</li> <li> Test all prompts: Verify argument handling</li> <li> Test error cases: Ensure proper error handling</li> <li> Test transports: Verify stdio and HTTP transports work</li> <li> Performance testing: Compare performance (should be similar or better)</li> </ul>"},{"location":"development/fastmcp/migration-guide/#validation","title":"Validation","text":"<ul> <li> Code reduction: Confirm significant reduction in boilerplate</li> <li> Type safety: Verify automatic validation works</li> <li> Error handling: Check improved error messages</li> <li> Development experience: Confirm easier debugging and testing</li> </ul>"},{"location":"development/fastmcp/migration-guide/#common-migration-issues","title":"Common Migration Issues","text":""},{"location":"development/fastmcp/migration-guide/#1-type-annotation-requirements","title":"1. Type Annotation Requirements","text":"<p>Issue: FastMCP requires type annotations for automatic validation.</p> <pre><code># \u274c Won't work - no type annotations\n@mcp.tool()\ndef bad_tool(x, y):\n    return x + y\n\n# \u2705 Works - proper type annotations\n@mcp.tool()\ndef good_tool(x: int, y: int) -&gt; int:\n    return x + y\n</code></pre>"},{"location":"development/fastmcp/migration-guide/#2-context-parameter-detection","title":"2. Context Parameter Detection","text":"<p>Issue: Context must be properly typed to be detected.</p> <pre><code># \u274c Won't be detected as context\n@mcp.tool()\ndef bad_context_tool(x: int, ctx) -&gt; str:\n    return str(x)\n\n# \u2705 Properly detected\n@mcp.tool()\ndef good_context_tool(x: int, ctx: Context) -&gt; str:\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/migration-guide/#3-resource-uri-template-matching","title":"3. Resource URI Template Matching","text":"<p>Issue: Template parameters must match function parameters exactly.</p> <pre><code># \u274c Parameter name mismatch\n@mcp.resource(\"data://{user_id}\")\ndef bad_resource(id: str) -&gt; str:  # 'id' != 'user_id'\n    return f\"User {id}\"\n\n# \u2705 Parameter names match\n@mcp.resource(\"data://{user_id}\")\ndef good_resource(user_id: str) -&gt; str:\n    return f\"User {user_id}\"\n</code></pre> <p>This migration guide provides everything needed to successfully transition from the raw MCP SDK to FastMCP's more ergonomic approach.</p>"},{"location":"development/fastmcp/monitoring-observability/","title":"Macro Syntax Error","text":"<p>File: <code>development/fastmcp/monitoring-observability.md</code></p> <p>Line 300 in Markdown file: expected token ':', got '}' <pre><code>        return f\"{name}{{{label_str}}}\"\n</code></pre></p>"},{"location":"development/fastmcp/overview/","title":"FastMCP Overview","text":"<p>FastMCP is a high-level, ergonomic Python framework for building MCP (Model Context Protocol) servers. It provides a simple decorator-based API that abstracts away the complexity of the underlying MCP protocol while giving you full access to its capabilities.</p>"},{"location":"development/fastmcp/overview/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) enables seamless integration between LLM applications and external data sources and tools. MCP servers expose three main types of capabilities to clients:</p> <ul> <li>Tools: Functions that clients can call to perform actions</li> <li>Resources: Data sources that clients can read from</li> <li>Prompts: Templates that help clients interact with your server</li> </ul>"},{"location":"development/fastmcp/overview/#why-fastmcp","title":"Why FastMCP?","text":"<p>FastMCP simplifies MCP server development by providing:</p> <ul> <li>Decorator-based API: Simple <code>@tool</code>, <code>@resource</code>, and <code>@prompt</code> decorators</li> <li>Automatic type validation: Built on Pydantic for robust input validation</li> <li>Multiple transport protocols: Support for stdio, HTTP+SSE, and StreamableHTTP</li> <li>Context injection: Easy access to MCP capabilities like logging and progress reporting</li> <li>Authentication: Built-in OAuth2 support for secure servers</li> <li>Development-friendly: Hot reload, detailed error messages, and comprehensive logging</li> </ul>"},{"location":"development/fastmcp/overview/#quick-start","title":"Quick Start","text":"<p>Here's a minimal FastMCP server:</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"My Server\")\n\n@mcp.tool()\ndef add_numbers(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\n@mcp.resource(\"data://example\")\ndef get_data() -&gt; str:\n    \"\"\"Provide example data.\"\"\"\n    return \"Hello from FastMCP!\"\n\n# Run with stdio transport (most common)\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")\n</code></pre>"},{"location":"development/fastmcp/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"development/fastmcp/overview/#server-creation","title":"Server Creation","text":"<p>Every FastMCP server starts with creating a <code>FastMCP</code> instance:</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\n# Basic server\nmcp = FastMCP(\"Server Name\")\n\n# Server with instructions for the client\nmcp = FastMCP(\n    name=\"Calculator Server\",\n    instructions=\"A server that provides mathematical operations\"\n)\n</code></pre>"},{"location":"development/fastmcp/overview/#decorators","title":"Decorators","text":"<p>FastMCP uses decorators to register capabilities:</p> <ul> <li><code>@mcp.tool()</code> - Register a function as a callable tool</li> <li><code>@mcp.resource()</code> - Register a function as a readable resource</li> <li><code>@mcp.prompt()</code> - Register a function as a prompt template</li> </ul>"},{"location":"development/fastmcp/overview/#transport-protocols","title":"Transport Protocols","text":"<p>FastMCP supports three transport protocols:</p> <ol> <li>stdio (recommended): Uses standard input/output, ideal for local processes</li> <li>streamable-http: HTTP-based protocol for web deployments</li> <li>sse: Server-Sent Events, compatible with HTTP infrastructure</li> </ol> <pre><code># Run with different transports\nmcp.run(\"stdio\")              # Most common\nmcp.run(\"streamable-http\")    # For web deployment\nmcp.run(\"sse\")               # For HTTP compatibility\n</code></pre>"},{"location":"development/fastmcp/overview/#context-and-capabilities","title":"Context and Capabilities","text":"<p>Tools and resources can access MCP capabilities through context injection:</p> <pre><code>from mcp.server.fastmcp import FastMCP, Context\n\n@mcp.tool()\nasync def advanced_tool(data: str, ctx: Context) -&gt; str:\n    # Log messages to the client\n    await ctx.info(f\"Processing: {data}\")\n\n    # Report progress\n    await ctx.report_progress(50, 100, \"Half done\")\n\n    # Read other resources\n    resource_content = await ctx.read_resource(\"data://other\")\n\n    return f\"Processed: {data}\"\n</code></pre>"},{"location":"development/fastmcp/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Transport Protocols - Learn about stdio vs HTTP transports</li> <li>Tools - Build interactive tools for clients to call</li> <li>Resources - Expose data sources and templates</li> <li>Context - Use logging, progress, and other MCP capabilities</li> <li>Configuration - Environment variables and settings</li> <li>Examples - Common patterns and best practices</li> </ul>"},{"location":"development/fastmcp/overview/#installation","title":"Installation","text":"<p>FastMCP is included with the MCP Python SDK:</p> <pre><code>uv add mcp\n</code></pre> <p>Or with pip:</p> <pre><code>pip install mcp\n</code></pre>"},{"location":"development/fastmcp/performance-guide/","title":"Performance Guide","text":"<p>Comprehensive guide for optimizing FastMCP servers, including benchmarking, scaling strategies, resource management, and performance monitoring.</p>"},{"location":"development/fastmcp/performance-guide/#quick-start","title":"Quick Start","text":""},{"location":"development/fastmcp/performance-guide/#performance-basics","title":"Performance Basics","text":"<p>FastMCP is built on async Python and designed for high performance. Key principles:</p> <ul> <li>Async by default: Use <code>async/await</code> for I/O operations</li> <li>Concurrent execution: Multiple requests are handled concurrently</li> <li>Resource pooling: Share expensive resources like database connections</li> <li>Caching: Cache expensive computations and data</li> </ul>"},{"location":"development/fastmcp/performance-guide/#performance-checklist","title":"Performance Checklist","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\n# \u2705 Good: Async operations\n@mcp.tool()\nasync def fast_operation(data: str) -&gt; str:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.example.com/{data}\")\n        return response.text\n\n# \u274c Bad: Blocking operations\n@mcp.tool()\ndef slow_operation(data: str) -&gt; str:\n    import time\n    time.sleep(1)  # Blocks entire server\n    return \"done\"\n</code></pre>"},{"location":"development/fastmcp/performance-guide/#async-programming","title":"Async Programming","text":""},{"location":"development/fastmcp/performance-guide/#1-async-tools-and-resources","title":"1. Async Tools and Resources","text":"<p>Use async for I/O operations: <pre><code>import asyncio\nimport httpx\nfrom mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"High Performance Server\")\n\n@mcp.tool()\nasync def fetch_multiple_apis(urls: list[str], ctx: Context) -&gt; list[str]:\n    \"\"\"Fetch multiple APIs concurrently.\"\"\"\n    async with httpx.AsyncClient() as client:\n        tasks = [client.get(url) for url in urls]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n\n        results = []\n        for i, response in enumerate(responses):\n            if isinstance(response, Exception):\n                ctx.warning(f\"Failed to fetch {urls[i]}: {response}\")\n                results.append(f\"Error: {response}\")\n            else:\n                results.append(response.text)\n\n        return results\n\n@mcp.resource(\"data://batch/{batch_id}\")\nasync def get_batch_data(batch_id: str) -&gt; dict:\n    \"\"\"Process batch data asynchronously.\"\"\"\n    # Simulate async database query\n    await asyncio.sleep(0.1)\n    return {\"batch_id\": batch_id, \"status\": \"processed\"}\n\n@mcp.tool()\nasync def parallel_processing(items: list[str], ctx: Context) -&gt; dict:\n    \"\"\"Process multiple items in parallel.\"\"\"\n    async def process_item(item: str) -&gt; str:\n        # Simulate async processing\n        await asyncio.sleep(0.01)\n        return f\"processed_{item}\"\n\n    await ctx.report_progress(0, len(items), \"Starting batch processing\")\n\n    # Process in batches to avoid overwhelming the system\n    batch_size = 10\n    results = []\n\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        batch_tasks = [process_item(item) for item in batch]\n        batch_results = await asyncio.gather(*batch_tasks)\n        results.extend(batch_results)\n\n        progress = min(i + batch_size, len(items))\n        await ctx.report_progress(progress, len(items), f\"Processed {progress} items\")\n\n    return {\"total_processed\": len(results), \"results\": results}\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#2-async-context-managers","title":"2. Async Context Managers","text":"<p>Proper resource management: <pre><code>from contextlib import asynccontextmanager\nimport asyncpg\n\nclass DatabaseManager:\n    def __init__(self):\n        self.pool = None\n\n    async def initialize(self):\n        \"\"\"Initialize connection pool.\"\"\"\n        self.pool = await asyncpg.create_pool(\n            \"postgresql://user:pass@localhost/db\",\n            min_size=5,\n            max_size=20,\n            command_timeout=30\n        )\n\n    async def close(self):\n        \"\"\"Close connection pool.\"\"\"\n        if self.pool:\n            await self.pool.close()\n\n# Global database manager\ndb = DatabaseManager()\n\n@asynccontextmanager\nasync def lifespan(app: FastMCP):\n    \"\"\"Manage application lifespan with connection pooling.\"\"\"\n    await db.initialize()\n    try:\n        yield\n    finally:\n        await db.close()\n\nmcp = FastMCP(\"Database Server\", lifespan=lifespan)\n\n@mcp.tool()\nasync def efficient_query(user_id: int) -&gt; dict:\n    \"\"\"Efficient database query using connection pool.\"\"\"\n    async with db.pool.acquire() as conn:\n        result = await conn.fetchrow(\n            \"SELECT * FROM users WHERE id = $1\", user_id\n        )\n        return dict(result) if result else {}\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#caching-strategies","title":"Caching Strategies","text":""},{"location":"development/fastmcp/performance-guide/#1-in-memory-caching","title":"1. In-Memory Caching","text":"<p>Simple LRU cache: <pre><code>from functools import lru_cache\nimport asyncio\nfrom typing import Dict, Any\nimport time\n\n# Sync caching\n@lru_cache(maxsize=1000)\ndef expensive_computation(data: str) -&gt; str:\n    \"\"\"Cache expensive synchronous computations.\"\"\"\n    # Simulate expensive operation\n    time.sleep(0.1)\n    return f\"processed_{data}\"\n\n@mcp.tool()\ndef cached_tool(data: str) -&gt; str:\n    \"\"\"Tool using cached computation.\"\"\"\n    return expensive_computation(data)\n\n# Async caching with manual cache\n_async_cache: Dict[str, Any] = {}\n_cache_timestamps: Dict[str, float] = {}\nCACHE_TTL = 300  # 5 minutes\n\nasync def cached_async_operation(key: str) -&gt; str:\n    \"\"\"Async operation with TTL cache.\"\"\"\n    current_time = time.time()\n\n    # Check cache\n    if (key in _async_cache and\n        key in _cache_timestamps and\n        current_time - _cache_timestamps[key] &lt; CACHE_TTL):\n        return _async_cache[key]\n\n    # Compute result\n    await asyncio.sleep(0.1)  # Simulate async work\n    result = f\"computed_{key}_{current_time}\"\n\n    # Store in cache\n    _async_cache[key] = result\n    _cache_timestamps[key] = current_time\n\n    return result\n\n@mcp.tool()\nasync def cached_async_tool(key: str) -&gt; str:\n    \"\"\"Tool using async cache.\"\"\"\n    return await cached_async_operation(key)\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#2-advanced-caching","title":"2. Advanced Caching","text":"<p>Redis-based caching: <pre><code>import json\nimport redis.asyncio as redis\nfrom typing import Optional\n\nclass RedisCache:\n    def __init__(self, url: str = \"redis://localhost:6379\"):\n        self.redis = redis.from_url(url)\n\n    async def get(self, key: str) -&gt; Optional[Any]:\n        \"\"\"Get value from cache.\"\"\"\n        try:\n            value = await self.redis.get(key)\n            return json.loads(value) if value else None\n        except Exception:\n            return None\n\n    async def set(self, key: str, value: Any, ttl: int = 300):\n        \"\"\"Set value in cache with TTL.\"\"\"\n        try:\n            await self.redis.setex(key, ttl, json.dumps(value))\n        except Exception:\n            pass  # Fail silently for cache errors\n\n    async def close(self):\n        \"\"\"Close Redis connection.\"\"\"\n        await self.redis.close()\n\n# Global cache instance\ncache = RedisCache()\n\n@asynccontextmanager\nasync def lifespan_with_cache(app: FastMCP):\n    \"\"\"Lifespan with Redis cache.\"\"\"\n    try:\n        yield\n    finally:\n        await cache.close()\n\n@mcp.tool()\nasync def cached_api_call(endpoint: str, ctx: Context) -&gt; dict:\n    \"\"\"API call with Redis caching.\"\"\"\n    cache_key = f\"api_call:{endpoint}\"\n\n    # Try cache first\n    cached_result = await cache.get(cache_key)\n    if cached_result:\n        ctx.info(\"Cache hit\")\n        return cached_result\n\n    # Make API call\n    ctx.info(\"Cache miss, fetching from API\")\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.example.com/{endpoint}\")\n        result = response.json()\n\n    # Cache the result\n    await cache.set(cache_key, result, ttl=600)  # 10 minutes\n\n    return result\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#resource-management","title":"Resource Management","text":""},{"location":"development/fastmcp/performance-guide/#1-connection-pooling","title":"1. Connection Pooling","text":"<p>Database connection pooling: <pre><code>import asyncpg\nfrom contextlib import asynccontextmanager\n\nclass OptimizedDatabase:\n    def __init__(self):\n        self.pool = None\n\n    async def initialize(self):\n        \"\"\"Initialize optimized connection pool.\"\"\"\n        self.pool = await asyncpg.create_pool(\n            \"postgresql://user:pass@localhost/db\",\n            min_size=10,          # Minimum connections\n            max_size=50,          # Maximum connections\n            max_queries=50000,    # Max queries per connection\n            max_inactive_connection_lifetime=300,  # 5 minutes\n            command_timeout=60,   # Command timeout\n            server_settings={\n                \"application_name\": \"fastmcp_server\",\n                \"jit\": \"off\",     # Disable JIT for predictable performance\n            }\n        )\n\n    async def execute_query(self, query: str, *args) -&gt; list:\n        \"\"\"Execute query with connection from pool.\"\"\"\n        async with self.pool.acquire() as conn:\n            # Use prepared statements for better performance\n            stmt = await conn.prepare(query)\n            return await stmt.fetch(*args)\n\ndb = OptimizedDatabase()\n\n@mcp.tool()\nasync def bulk_user_query(user_ids: list[int]) -&gt; list[dict]:\n    \"\"\"Efficient bulk query using connection pooling.\"\"\"\n    if not user_ids:\n        return []\n\n    # Use ANY() for efficient bulk queries\n    query = \"SELECT * FROM users WHERE id = ANY($1)\"\n    results = await db.execute_query(query, user_ids)\n\n    return [dict(row) for row in results]\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#2-http-client-pooling","title":"2. HTTP Client Pooling","text":"<p>Efficient HTTP client management: <pre><code>import httpx\nfrom contextlib import asynccontextmanager\n\nclass HTTPClientManager:\n    def __init__(self):\n        self.client = None\n\n    async def initialize(self):\n        \"\"\"Initialize HTTP client with optimal settings.\"\"\"\n        timeout = httpx.Timeout(\n            connect=5.0,    # Connection timeout\n            read=30.0,      # Read timeout\n            write=10.0,     # Write timeout\n            pool=5.0        # Pool timeout\n        )\n\n        limits = httpx.Limits(\n            max_keepalive_connections=20,\n            max_connections=100,\n            keepalive_expiry=30.0\n        )\n\n        self.client = httpx.AsyncClient(\n            timeout=timeout,\n            limits=limits,\n            http2=True,  # Enable HTTP/2\n            follow_redirects=True\n        )\n\n    async def close(self):\n        \"\"\"Close HTTP client.\"\"\"\n        if self.client:\n            await self.client.aclose()\n\nhttp_client = HTTPClientManager()\n\n@asynccontextmanager\nasync def lifespan_with_http(app: FastMCP):\n    \"\"\"Lifespan with HTTP client management.\"\"\"\n    await http_client.initialize()\n    try:\n        yield\n    finally:\n        await http_client.close()\n\n@mcp.tool()\nasync def efficient_api_calls(urls: list[str]) -&gt; list[dict]:\n    \"\"\"Efficient API calls using shared client.\"\"\"\n    async def fetch_url(url: str) -&gt; dict:\n        try:\n            response = await http_client.client.get(url)\n            response.raise_for_status()\n            return {\"url\": url, \"status\": \"success\", \"data\": response.json()}\n        except Exception as e:\n            return {\"url\": url, \"status\": \"error\", \"error\": str(e)}\n\n    # Use semaphore to limit concurrent requests\n    semaphore = asyncio.Semaphore(10)  # Max 10 concurrent requests\n\n    async def bounded_fetch(url: str) -&gt; dict:\n        async with semaphore:\n            return await fetch_url(url)\n\n    tasks = [bounded_fetch(url) for url in urls]\n    return await asyncio.gather(*tasks)\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"development/fastmcp/performance-guide/#1-built-in-metrics","title":"1. Built-in Metrics","text":"<p>Monitor server performance: <pre><code>import time\nimport psutil\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, Any\nimport threading\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = {\n            \"requests_total\": 0,\n            \"requests_in_progress\": 0,\n            \"errors_total\": 0,\n            \"start_time\": time.time(),\n        }\n        self._lock = threading.Lock()\n\n    def increment_request(self):\n        with self._lock:\n            self.metrics[\"requests_total\"] += 1\n            self.metrics[\"requests_in_progress\"] += 1\n\n    def decrement_request(self):\n        with self._lock:\n            self.metrics[\"requests_in_progress\"] -= 1\n\n    def increment_error(self):\n        with self._lock:\n            self.metrics[\"errors_total\"] += 1\n\n    def get_metrics(self) -&gt; Dict[str, Any]:\n        with self._lock:\n            process = psutil.Process()\n            memory_info = process.memory_info()\n\n            uptime = time.time() - self.metrics[\"start_time\"]\n\n            return {\n                **self.metrics,\n                \"uptime_seconds\": uptime,\n                \"memory_rss_mb\": memory_info.rss / 1024 / 1024,\n                \"memory_vms_mb\": memory_info.vms / 1024 / 1024,\n                \"cpu_percent\": process.cpu_percent(),\n                \"thread_count\": process.num_threads(),\n                \"fd_count\": process.num_fds() if hasattr(process, 'num_fds') else 0,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\nmonitor = PerformanceMonitor()\n\n@mcp.tool()\ndef get_server_metrics() -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive server performance metrics.\"\"\"\n    return monitor.get_metrics()\n\n# Decorator for monitoring tool performance\ndef monitored_tool(func):\n    \"\"\"Decorator to monitor tool performance.\"\"\"\n    async def wrapper(*args, **kwargs):\n        monitor.increment_request()\n        start_time = time.time()\n\n        try:\n            result = await func(*args, **kwargs)\n            return result\n        except Exception as e:\n            monitor.increment_error()\n            raise\n        finally:\n            monitor.decrement_request()\n            duration = time.time() - start_time\n            # Log slow operations\n            if duration &gt; 1.0:  # 1 second threshold\n                print(f\"Slow operation: {func.__name__} took {duration:.2f}s\")\n\n    return wrapper\n\n@mcp.tool()\n@monitored_tool\nasync def monitored_operation(data: str) -&gt; str:\n    \"\"\"Tool with performance monitoring.\"\"\"\n    await asyncio.sleep(0.1)  # Simulate work\n    return f\"Processed: {data}\"\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#2-health-checks","title":"2. Health Checks","text":"<p>Implement health check endpoints: <pre><code>from starlette.responses import JSONResponse\nfrom starlette.requests import Request\n\n@mcp.custom_route(\"/health\", methods=[\"GET\"])\nasync def health_check(request: Request) -&gt; JSONResponse:\n    \"\"\"Basic health check endpoint.\"\"\"\n    return JSONResponse({\n        \"status\": \"healthy\",\n        \"timestamp\": datetime.now().isoformat(),\n        \"version\": \"1.0.0\"\n    })\n\n@mcp.custom_route(\"/metrics\", methods=[\"GET\"])\nasync def metrics_endpoint(request: Request) -&gt; JSONResponse:\n    \"\"\"Detailed metrics endpoint.\"\"\"\n    return JSONResponse(monitor.get_metrics())\n\n@mcp.custom_route(\"/health/detailed\", methods=[\"GET\"])\nasync def detailed_health_check(request: Request) -&gt; JSONResponse:\n    \"\"\"Detailed health check with dependency checks.\"\"\"\n    health_status = {\n        \"status\": \"healthy\",\n        \"timestamp\": datetime.now().isoformat(),\n        \"checks\": {}\n    }\n\n    # Check database connectivity\n    try:\n        if hasattr(db, 'pool') and db.pool:\n            async with db.pool.acquire() as conn:\n                await conn.fetchval(\"SELECT 1\")\n            health_status[\"checks\"][\"database\"] = \"healthy\"\n        else:\n            health_status[\"checks\"][\"database\"] = \"not_configured\"\n    except Exception as e:\n        health_status[\"checks\"][\"database\"] = f\"unhealthy: {e}\"\n        health_status[\"status\"] = \"degraded\"\n\n    # Check Redis cache\n    try:\n        if hasattr(cache, 'redis'):\n            await cache.redis.ping()\n            health_status[\"checks\"][\"cache\"] = \"healthy\"\n        else:\n            health_status[\"checks\"][\"cache\"] = \"not_configured\"\n    except Exception as e:\n        health_status[\"checks\"][\"cache\"] = f\"unhealthy: {e}\"\n        health_status[\"status\"] = \"degraded\"\n\n    return JSONResponse(health_status)\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"development/fastmcp/performance-guide/#1-request-batching","title":"1. Request Batching","text":"<p>Batch multiple operations: <pre><code>from collections import defaultdict\nimport asyncio\n\nclass RequestBatcher:\n    def __init__(self, batch_size: int = 10, max_wait: float = 0.1):\n        self.batch_size = batch_size\n        self.max_wait = max_wait\n        self.pending_requests = defaultdict(list)\n        self._locks = defaultdict(asyncio.Lock)\n\n    async def add_request(self, operation: str, request_data: Any) -&gt; Any:\n        \"\"\"Add request to batch and wait for result.\"\"\"\n        async with self._locks[operation]:\n            future = asyncio.get_event_loop().create_future()\n            self.pending_requests[operation].append((request_data, future))\n\n            # Process batch if full or start timer\n            if len(self.pending_requests[operation]) &gt;= self.batch_size:\n                await self._process_batch(operation)\n            else:\n                asyncio.create_task(self._delayed_process(operation))\n\n            return await future\n\n    async def _delayed_process(self, operation: str):\n        \"\"\"Process batch after delay.\"\"\"\n        await asyncio.sleep(self.max_wait)\n        async with self._locks[operation]:\n            if self.pending_requests[operation]:\n                await self._process_batch(operation)\n\n    async def _process_batch(self, operation: str):\n        \"\"\"Process batch of requests.\"\"\"\n        requests = self.pending_requests[operation]\n        self.pending_requests[operation] = []\n\n        if not requests:\n            return\n\n        try:\n            # Batch process all requests\n            if operation == \"database_query\":\n                results = await self._batch_database_query(\n                    [req[0] for req in requests]\n                )\n            else:\n                results = [f\"processed_{req[0]}\" for req in requests]\n\n            # Resolve futures\n            for (request_data, future), result in zip(requests, results):\n                future.set_result(result)\n\n        except Exception as e:\n            # Reject all futures\n            for request_data, future in requests:\n                future.set_exception(e)\n\n    async def _batch_database_query(self, user_ids: list[int]) -&gt; list[dict]:\n        \"\"\"Batch database query.\"\"\"\n        async with db.pool.acquire() as conn:\n            results = await conn.fetch(\n                \"SELECT * FROM users WHERE id = ANY($1)\", user_ids\n            )\n            # Map results back to original order\n            result_map = {row['id']: dict(row) for row in results}\n            return [result_map.get(uid, {}) for uid in user_ids]\n\nbatcher = RequestBatcher()\n\n@mcp.tool()\nasync def batched_user_lookup(user_id: int) -&gt; dict:\n    \"\"\"Look up user with automatic batching.\"\"\"\n    return await batcher.add_request(\"database_query\", user_id)\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#2-streaming-and-pagination","title":"2. Streaming and Pagination","text":"<p>Handle large datasets efficiently: <pre><code>from typing import AsyncGenerator\n\n@mcp.tool()\nasync def stream_large_dataset(limit: int = 1000, ctx: Context) -&gt; str:\n    \"\"\"Stream large dataset with progress reporting.\"\"\"\n    async def data_generator() -&gt; AsyncGenerator[dict, None]:\n        \"\"\"Generate data in chunks.\"\"\"\n        for i in range(0, limit, 100):  # Process in chunks of 100\n            chunk_size = min(100, limit - i)\n\n            # Simulate database query\n            await asyncio.sleep(0.01)\n\n            for j in range(chunk_size):\n                yield {\"id\": i + j, \"data\": f\"item_{i + j}\"}\n\n            # Report progress\n            await ctx.report_progress(i + chunk_size, limit, f\"Processed {i + chunk_size} items\")\n\n    results = []\n    async for item in data_generator():\n        results.append(item)\n\n    return f\"Streamed {len(results)} items\"\n\n@mcp.resource(\"data://paginated/{page}\")\nasync def paginated_data(page: int) -&gt; dict:\n    \"\"\"Paginated resource for large datasets.\"\"\"\n    page_size = 50\n    offset = (page - 1) * page_size\n\n    # Simulate database query with pagination\n    await asyncio.sleep(0.01)\n\n    items = [\n        {\"id\": i, \"name\": f\"Item {i}\"}\n        for i in range(offset, offset + page_size)\n    ]\n\n    return {\n        \"page\": page,\n        \"page_size\": page_size,\n        \"items\": items,\n        \"total_pages\": 100,  # Example total\n        \"has_next\": page &lt; 100,\n        \"has_previous\": page &gt; 1\n    }\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#benchmarking","title":"Benchmarking","text":""},{"location":"development/fastmcp/performance-guide/#1-load-testing","title":"1. Load Testing","text":"<p>Test server performance under load: <pre><code>import asyncio\nimport time\nimport statistics\nfrom concurrent.futures import ThreadPoolExecutor\n\nasync def benchmark_tool_performance():\n    \"\"\"Benchmark tool performance.\"\"\"\n    from mcp.shared.memory import create_connected_server_and_client_session\n\n    # Create test server\n    test_server = FastMCP(\"Benchmark Server\")\n\n    @test_server.tool()\n    async def benchmark_tool(data: str) -&gt; str:\n        \"\"\"Simple tool for benchmarking.\"\"\"\n        await asyncio.sleep(0.001)  # Simulate minimal work\n        return f\"processed_{data}\"\n\n    # Run benchmark\n    async with create_connected_server_and_client_session(test_server._mcp_server) as (\n        server_session,\n        client_session,\n    ):\n        await client_session.initialize()\n\n        # Warmup\n        for _ in range(10):\n            await client_session.call_tool(\"benchmark_tool\", {\"data\": \"warmup\"})\n\n        # Benchmark\n        num_requests = 100\n        start_time = time.time()\n\n        tasks = [\n            client_session.call_tool(\"benchmark_tool\", {\"data\": f\"test_{i}\"})\n            for i in range(num_requests)\n        ]\n\n        results = await asyncio.gather(*tasks)\n        end_time = time.time()\n\n        duration = end_time - start_time\n        rps = num_requests / duration\n\n        print(f\"Benchmark Results:\")\n        print(f\"  Requests: {num_requests}\")\n        print(f\"  Duration: {duration:.2f}s\")\n        print(f\"  Requests/sec: {rps:.2f}\")\n        print(f\"  Avg latency: {duration/num_requests*1000:.2f}ms\")\n\n# Run benchmark\nif __name__ == \"__main__\":\n    asyncio.run(benchmark_tool_performance())\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#2-performance-profiling","title":"2. Performance Profiling","text":"<p>Profile your FastMCP server: <pre><code>import cProfile\nimport pstats\nimport io\nfrom contextlib import contextmanager\n\n@contextmanager\ndef profile_tool():\n    \"\"\"Context manager for profiling tool execution.\"\"\"\n    pr = cProfile.Profile()\n    pr.enable()\n    try:\n        yield\n    finally:\n        pr.disable()\n        s = io.StringIO()\n        ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n        ps.print_stats(20)  # Top 20 functions\n        print(s.getvalue())\n\n@mcp.tool()\nasync def profiled_tool(data: str) -&gt; str:\n    \"\"\"Tool with performance profiling.\"\"\"\n    with profile_tool():\n        # Your tool logic here\n        result = expensive_computation(data)\n        return result\n\n# Memory profiling with tracemalloc\nimport tracemalloc\n\n@mcp.tool()\nasync def memory_profiled_tool(data: str) -&gt; str:\n    \"\"\"Tool with memory profiling.\"\"\"\n    tracemalloc.start()\n\n    # Your tool logic\n    result = process_large_data(data)\n\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n\n    print(f\"Memory usage: current={current/1024/1024:.1f}MB, peak={peak/1024/1024:.1f}MB\")\n    return result\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"development/fastmcp/performance-guide/#1-horizontal-scaling","title":"1. Horizontal Scaling","text":"<p>Deploy multiple server instances: <pre><code># Load balancer configuration (nginx example)\n\"\"\"\nupstream fastmcp_backend {\n    server 127.0.0.1:8000;\n    server 127.0.0.1:8001;\n    server 127.0.0.1:8002;\n    server 127.0.0.1:8003;\n}\n\nserver {\n    listen 80;\n    location / {\n        proxy_pass http://fastmcp_backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n\"\"\"\n\n# Server configuration for horizontal scaling\nimport os\n\ndef create_scaled_server():\n    \"\"\"Create server instance for horizontal scaling.\"\"\"\n    # Use different ports for each instance\n    port = int(os.environ.get(\"SERVER_PORT\", \"8000\"))\n    worker_id = os.environ.get(\"WORKER_ID\", \"0\")\n\n    mcp = FastMCP(\n        f\"FastMCP Worker {worker_id}\",\n        port=port,\n        # Use shared cache for consistency\n        # Configure database connection pooling\n    )\n\n    return mcp\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#2-vertical-scaling","title":"2. Vertical Scaling","text":"<p>Optimize single server performance: <pre><code>import multiprocessing\n\ndef configure_for_vertical_scaling():\n    \"\"\"Configure server for maximum single-machine performance.\"\"\"\n    # Calculate optimal settings based on system resources\n    cpu_count = multiprocessing.cpu_count()\n\n    return FastMCP(\n        \"High Performance Server\",\n        # Configure based on available resources\n        debug=False,  # Disable debug in production\n        log_level=\"WARNING\",  # Reduce logging overhead\n\n        # HTTP settings optimized for performance\n        host=\"0.0.0.0\",\n        port=8000,\n\n        # Database connection pool sizing\n        # Rule of thumb: (CPU cores * 2) + effective spindle count\n        # For SSD: CPU cores * 2\n        # For network storage: depends on network latency\n    )\n</code></pre></p>"},{"location":"development/fastmcp/performance-guide/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"development/fastmcp/performance-guide/#1-dos","title":"1. Do's","text":"<pre><code># \u2705 Use async/await for I/O operations\n@mcp.tool()\nasync def good_api_call() -&gt; str:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(\"https://api.example.com\")\n        return response.text\n\n# \u2705 Use connection pooling\nasync with db.pool.acquire() as conn:\n    result = await conn.fetch(\"SELECT * FROM table\")\n\n# \u2705 Implement caching\n@lru_cache(maxsize=1000)\ndef cached_operation(data: str) -&gt; str:\n    return expensive_computation(data)\n\n# \u2705 Batch similar operations\nasync def batch_process(items: list[str]) -&gt; list[str]:\n    return await asyncio.gather(*[process_item(item) for item in items])\n\n# \u2705 Use progress reporting for long operations\nasync def long_operation(ctx: Context) -&gt; str:\n    for i in range(100):\n        await asyncio.sleep(0.01)\n        await ctx.report_progress(i, 100, f\"Step {i}\")\n    return \"complete\"\n</code></pre>"},{"location":"development/fastmcp/performance-guide/#2-donts","title":"2. Don'ts","text":"<pre><code># \u274c Don't block the event loop\ndef bad_blocking_operation() -&gt; str:\n    time.sleep(1)  # Blocks everything\n    return \"done\"\n\n# \u274c Don't create new connections for each request\ndef bad_database_query() -&gt; dict:\n    conn = psycopg2.connect(\"postgresql://...\")  # New connection each time\n    result = conn.execute(\"SELECT * FROM table\")\n    return result\n\n# \u274c Don't ignore resource cleanup\ndef bad_resource_handling() -&gt; str:\n    file = open(\"large_file.txt\")  # Never closed\n    return file.read()\n\n# \u274c Don't process large datasets without streaming\ndef bad_large_dataset() -&gt; list:\n    return [expensive_operation(i) for i in range(1000000)]  # Memory explosion\n</code></pre> <p>This performance guide provides comprehensive strategies for building high-performance FastMCP servers that can handle production workloads efficiently.</p>"},{"location":"development/fastmcp/prompts/","title":"Prompts","text":"<p>Prompts in FastMCP are templates that help structure conversations with language models. They allow you to create reusable message templates with parameters, making it easy to generate consistent, well-formatted prompts for various tasks.</p>"},{"location":"development/fastmcp/prompts/#overview","title":"Overview","text":"<p>Prompts are functions that return structured messages for language model conversations. They can:</p> <ul> <li>Accept parameters to customize the generated messages</li> <li>Return single messages or conversation threads</li> <li>Include various content types (text, images, resources)</li> <li>Be used by MCP clients to enhance AI interactions</li> </ul>"},{"location":"development/fastmcp/prompts/#basic-prompts","title":"Basic Prompts","text":""},{"location":"development/fastmcp/prompts/#simple-string-prompt","title":"Simple String Prompt","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Prompt Server\")\n\n@mcp.prompt()\ndef greeting_prompt() -&gt; str:\n    \"\"\"Generate a friendly greeting.\"\"\"\n    return \"Hello! I'm ready to help you with any questions you might have.\"\n\n@mcp.prompt()\ndef analysis_prompt() -&gt; str:\n    \"\"\"Prompt for data analysis tasks.\"\"\"\n    return \"Please analyze the following data and provide insights, trends, and recommendations.\"\n</code></pre>"},{"location":"development/fastmcp/prompts/#parameterized-prompts","title":"Parameterized Prompts","text":"<pre><code>@mcp.prompt()\ndef topic_prompt(topic: str, context: str = \"\") -&gt; str:\n    \"\"\"Generate a prompt for discussing a specific topic.\"\"\"\n    prompt = f\"Let's discuss {topic}.\"\n    if context:\n        prompt += f\" Here's some relevant context: {context}\"\n    return prompt\n\n@mcp.prompt()\ndef writing_prompt(\n    style: str,\n    audience: str,\n    length: int = 500\n) -&gt; str:\n    \"\"\"Generate a writing prompt with specific requirements.\"\"\"\n    return f\"\"\"\n    Please write content in a {style} style for {audience}.\n    Target length: approximately {length} words.\n    Focus on clarity, engagement, and appropriateness for the target audience.\n    \"\"\"\n</code></pre>"},{"location":"development/fastmcp/prompts/#message-objects","title":"Message Objects","text":""},{"location":"development/fastmcp/prompts/#using-message-classes","title":"Using Message Classes","text":"<pre><code>from mcp.server.fastmcp.prompts import UserMessage, AssistantMessage\n\n@mcp.prompt()\ndef conversation_starter() -&gt; list[UserMessage]:\n    \"\"\"Create a conversation starter.\"\"\"\n    return [\n        UserMessage(\"Hi! I'd like to brainstorm some ideas.\"),\n        UserMessage(\"What's the best way to approach creative problem solving?\")\n    ]\n\n@mcp.prompt()\ndef example_conversation() -&gt; list[UserMessage | AssistantMessage]:\n    \"\"\"Show an example conversation flow.\"\"\"\n    return [\n        UserMessage(\"What are the benefits of renewable energy?\"),\n        AssistantMessage(\"Renewable energy offers several key benefits: environmental sustainability, energy independence, cost savings over time, and job creation in green industries.\"),\n        UserMessage(\"Can you elaborate on the environmental benefits?\")\n    ]\n</code></pre>"},{"location":"development/fastmcp/prompts/#mixed-content-types","title":"Mixed Content Types","text":"<pre><code>from mcp.types import TextContent, EmbeddedResource\n\n@mcp.prompt()\ndef analysis_with_data(dataset_name: str) -&gt; list[dict]:\n    \"\"\"Create prompt with embedded resource data.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": TextContent(\n                type=\"text\",\n                text=\"Please analyze the following dataset and provide insights:\"\n            )\n        },\n        {\n            \"role\": \"user\",\n            \"content\": EmbeddedResource(\n                type=\"resource\",\n                resource={\"uri\": f\"data://{dataset_name}\", \"text\": \"Dataset content here\"}\n            )\n        }\n    ]\n</code></pre>"},{"location":"development/fastmcp/prompts/#parameter-validation","title":"Parameter Validation","text":""},{"location":"development/fastmcp/prompts/#type-hints-and-defaults","title":"Type Hints and Defaults","text":"<pre><code>from typing import Literal\nfrom pydantic import Field\n\n@mcp.prompt()\ndef code_review_prompt(\n    language: Literal[\"python\", \"javascript\", \"java\", \"go\"],\n    focus: str = Field(description=\"What aspect to focus on (e.g., performance, security)\"),\n    severity: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n) -&gt; str:\n    \"\"\"Generate a code review prompt.\"\"\"\n    return f\"\"\"\n    Please review this {language} code with a focus on {focus}.\n    Review severity level: {severity}\n\n    Look for:\n    - Code quality and best practices\n    - Potential bugs or issues\n    - Performance considerations\n    - Security vulnerabilities\n\n    Provide specific, actionable feedback.\n    \"\"\"\n\n@mcp.prompt()\ndef learning_prompt(\n    subject: str = Field(description=\"Subject to learn about\"),\n    level: Literal[\"beginner\", \"intermediate\", \"advanced\"] = \"beginner\",\n    format: Literal[\"tutorial\", \"quiz\", \"exercise\"] = \"tutorial\"\n) -&gt; str:\n    \"\"\"Generate personalized learning content.\"\"\"\n    return f\"\"\"\n    Create a {format} for learning {subject} at {level} level.\n\n    Requirements:\n    - Match the learner's {level} skill level\n    - Use clear, engaging explanations\n    - Include practical examples\n    - Provide progression steps\n    \"\"\"\n</code></pre>"},{"location":"development/fastmcp/prompts/#complex-validation","title":"Complex Validation","text":"<pre><code>from typing import Annotated\nfrom pydantic import Field\n\n@mcp.prompt()\ndef article_prompt(\n    title: Annotated[str, Field(min_length=5, max_length=100)],\n    keywords: Annotated[list[str], Field(min_length=1, max_length=10)],\n    word_count: Annotated[int, Field(ge=100, le=5000)],\n    tone: Literal[\"formal\", \"casual\", \"academic\", \"creative\"] = \"casual\"\n) -&gt; str:\n    \"\"\"Generate an article writing prompt with validation.\"\"\"\n    keywords_str = \", \".join(keywords)\n\n    return f\"\"\"\n    Write an article titled \"{title}\" with the following specifications:\n\n    - Word count: {word_count} words\n    - Tone: {tone}\n    - Keywords to include: {keywords_str}\n    - Structure: Introduction, main content, conclusion\n    - Include relevant examples and evidence\n    \"\"\"\n</code></pre>"},{"location":"development/fastmcp/prompts/#async-prompts","title":"Async Prompts","text":"<p>Prompts can be async for dynamic content generation:</p> <pre><code>import httpx\nfrom datetime import datetime\n\n@mcp.prompt()\nasync def news_prompt(topic: str) -&gt; str:\n    \"\"\"Generate a prompt with current news context.\"\"\"\n    async with httpx.AsyncClient() as client:\n        # Fetch recent news (example)\n        response = await client.get(f\"https://api.news.com/search?q={topic}\")\n        news_data = response.json()\n\n    context = f\"Recent news about {topic}:\\n\"\n    for article in news_data.get(\"articles\", [])[:3]:\n        context += f\"- {article['title']}\\n\"\n\n    return f\"\"\"\n    {context}\n\n    Based on this recent news context, please provide analysis and insights about {topic}.\n    Consider current trends, implications, and potential future developments.\n    \"\"\"\n\n@mcp.prompt()\nasync def personalized_prompt(user_id: str) -&gt; str:\n    \"\"\"Generate personalized prompt based on user data.\"\"\"\n    # Fetch user preferences (example)\n    user_data = await get_user_preferences(user_id)\n\n    interests = \", \".join(user_data.get(\"interests\", []))\n    experience_level = user_data.get(\"experience_level\", \"beginner\")\n\n    return f\"\"\"\n    Welcome back! Based on your interests in {interests} and your {experience_level}\n    experience level, I'm ready to provide tailored assistance.\n\n    What would you like to explore today?\n    \"\"\"\n\nasync def get_user_preferences(user_id: str) -&gt; dict:\n    \"\"\"Mock function to fetch user preferences.\"\"\"\n    return {\n        \"interests\": [\"technology\", \"science\", \"programming\"],\n        \"experience_level\": \"intermediate\"\n    }\n</code></pre>"},{"location":"development/fastmcp/prompts/#prompt-templates","title":"Prompt Templates","text":""},{"location":"development/fastmcp/prompts/#research-templates","title":"Research Templates","text":"<pre><code>@mcp.prompt()\ndef research_prompt(\n    topic: str,\n    research_type: Literal[\"academic\", \"market\", \"technical\"] = \"academic\",\n    depth: Literal[\"overview\", \"detailed\", \"comprehensive\"] = \"overview\"\n) -&gt; list[UserMessage]:\n    \"\"\"Generate research-focused prompts.\"\"\"\n\n    base_prompt = f\"I need to research {topic}.\"\n\n    if research_type == \"academic\":\n        methodology = \"peer-reviewed sources, academic papers, and scholarly articles\"\n    elif research_type == \"market\":\n        methodology = \"market reports, industry analysis, and business data\"\n    else:  # technical\n        methodology = \"technical documentation, specifications, and expert resources\"\n\n    depth_instructions = {\n        \"overview\": \"Provide a high-level summary with key points\",\n        \"detailed\": \"Include detailed analysis with supporting evidence\",\n        \"comprehensive\": \"Provide exhaustive coverage with multiple perspectives\"\n    }\n\n    return [\n        UserMessage(base_prompt),\n        UserMessage(f\"Focus on {methodology}.\"),\n        UserMessage(depth_instructions[depth] + \".\"),\n        UserMessage(\"Structure the research with clear sections and cite sources.\")\n    ]\n\n@mcp.prompt()\ndef problem_solving_prompt(\n    problem: str,\n    domain: str,\n    constraints: list[str] = [],\n    approach: Literal[\"analytical\", \"creative\", \"systematic\"] = \"analytical\"\n) -&gt; str:\n    \"\"\"Generate problem-solving prompts.\"\"\"\n\n    constraints_text = \"\"\n    if constraints:\n        constraints_text = f\"\\nConstraints to consider: {', '.join(constraints)}\"\n\n    approach_instructions = {\n        \"analytical\": \"Break down the problem systematically and use logical reasoning\",\n        \"creative\": \"Think outside the box and consider unconventional solutions\",\n        \"systematic\": \"Follow a structured methodology and evaluate all options\"\n    }\n\n    return f\"\"\"\n    Problem to solve: {problem}\n    Domain: {domain}{constraints_text}\n\n    Approach: {approach_instructions[approach]}\n\n    Please:\n    1. Analyze the problem clearly\n    2. Identify key factors and variables\n    3. Generate potential solutions\n    4. Evaluate pros and cons\n    5. Recommend the best approach\n    \"\"\"\n</code></pre>"},{"location":"development/fastmcp/prompts/#content-generation-templates","title":"Content Generation Templates","text":"<pre><code>@mcp.prompt()\ndef content_brief_prompt(\n    content_type: Literal[\"blog\", \"email\", \"social\", \"documentation\"],\n    target_audience: str,\n    key_message: str,\n    call_to_action: str = \"\"\n) -&gt; str:\n    \"\"\"Generate content creation briefs.\"\"\"\n\n    format_guidelines = {\n        \"blog\": \"engaging introduction, structured body with headers, conclusion\",\n        \"email\": \"clear subject line, personal greeting, concise message, strong CTA\",\n        \"social\": \"attention-grabbing hook, concise message, relevant hashtags\",\n        \"documentation\": \"clear structure, step-by-step instructions, examples\"\n    }\n\n    cta_text = f\"\\nCall to action: {call_to_action}\" if call_to_action else \"\"\n\n    return f\"\"\"\n    Create {content_type} content with the following specifications:\n\n    Target audience: {target_audience}\n    Key message: {key_message}{cta_text}\n\n    Format requirements: {format_guidelines[content_type]}\n\n    Ensure the content is:\n    - Relevant and valuable to the target audience\n    - Clear and easy to understand\n    - Engaging and actionable\n    - Appropriate for the {content_type} format\n    \"\"\"\n\n@mcp.prompt()\ndef educational_content_prompt(\n    subject: str,\n    learning_objective: str,\n    student_level: Literal[\"elementary\", \"middle\", \"high\", \"college\", \"adult\"],\n    format: Literal[\"lesson\", \"quiz\", \"exercise\", \"project\"] = \"lesson\"\n) -&gt; list[UserMessage]:\n    \"\"\"Generate educational content prompts.\"\"\"\n\n    level_adaptations = {\n        \"elementary\": \"simple vocabulary, visual examples, short attention spans\",\n        \"middle\": \"age-appropriate complexity, interactive elements, peer learning\",\n        \"high\": \"critical thinking, real-world applications, independent work\",\n        \"college\": \"advanced concepts, research skills, analytical thinking\",\n        \"adult\": \"practical applications, professional relevance, flexible pacing\"\n    }\n\n    return [\n        UserMessage(f\"Create a {format} for teaching {subject}.\"),\n        UserMessage(f\"Learning objective: {learning_objective}\"),\n        UserMessage(f\"Student level: {student_level}\"),\n        UserMessage(f\"Adapt content for: {level_adaptations[student_level]}\"),\n        UserMessage(\"Include assessment methods and follow-up activities.\")\n    ]\n</code></pre>"},{"location":"development/fastmcp/prompts/#dynamic-prompts","title":"Dynamic Prompts","text":""},{"location":"development/fastmcp/prompts/#context-aware-prompts","title":"Context-Aware Prompts","text":"<pre><code>from datetime import datetime, date\n\n@mcp.prompt()\ndef daily_briefing_prompt(\n    focus_areas: list[str],\n    include_weather: bool = True,\n    include_calendar: bool = True\n) -&gt; str:\n    \"\"\"Generate personalized daily briefing prompt.\"\"\"\n\n    today = date.today().strftime(\"%A, %B %d, %Y\")\n\n    sections = [f\"Good morning! Here's your briefing for {today}:\"]\n\n    if include_weather:\n        sections.append(\"- Current weather and forecast\")\n\n    if include_calendar:\n        sections.append(\"- Today's schedule and important events\")\n\n    for area in focus_areas:\n        sections.append(f\"- Updates and insights about {area}\")\n\n    sections.append(\"\\nPrioritize the most important information and actionable items.\")\n\n    return \"\\n\".join(sections)\n\n@mcp.prompt()\ndef project_status_prompt(\n    project_name: str,\n    team_size: int,\n    deadline: str,\n    current_phase: str\n) -&gt; list[UserMessage]:\n    \"\"\"Generate project status review prompt.\"\"\"\n\n    return [\n        UserMessage(f\"Project Status Review: {project_name}\"),\n        UserMessage(f\"Team size: {team_size} members\"),\n        UserMessage(f\"Deadline: {deadline}\"),\n        UserMessage(f\"Current phase: {current_phase}\"),\n        UserMessage(\"\"\"\n        Please provide a comprehensive status update including:\n        1. Progress against timeline\n        2. Key accomplishments this period\n        3. Current blockers or risks\n        4. Resource needs\n        5. Next steps and priorities\n        6. Team performance insights\n        \"\"\")\n    ]\n</code></pre>"},{"location":"development/fastmcp/prompts/#multi-modal-prompts","title":"Multi-Modal Prompts","text":"<pre><code>from mcp.types import EmbeddedResource, ImageContent\n\n@mcp.prompt()\ndef image_analysis_prompt(\n    image_uri: str,\n    analysis_type: Literal[\"description\", \"technical\", \"artistic\", \"business\"] = \"description\"\n) -&gt; list[dict]:\n    \"\"\"Generate prompts for image analysis.\"\"\"\n\n    analysis_instructions = {\n        \"description\": \"Describe what you see in detail, including objects, people, setting, and mood\",\n        \"technical\": \"Analyze composition, lighting, color theory, and photographic techniques\",\n        \"artistic\": \"Discuss artistic style, aesthetic elements, and creative expression\",\n        \"business\": \"Evaluate commercial potential, target audience, and marketing applications\"\n    }\n\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": EmbeddedResource(\n                type=\"resource\",\n                resource={\"uri\": image_uri}\n            )\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please analyze this image with a {analysis_type} focus: {analysis_instructions[analysis_type]}\"\n        }\n    ]\n\n@mcp.prompt()\ndef document_review_prompt(\n    document_uri: str,\n    review_type: Literal[\"content\", \"style\", \"accuracy\", \"compliance\"]\n) -&gt; list[dict]:\n    \"\"\"Generate document review prompts.\"\"\"\n\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": \"Please review the following document:\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": EmbeddedResource(\n                type=\"resource\",\n                resource={\"uri\": document_uri}\n            )\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Focus on {review_type} review with specific feedback and recommendations.\"\n        }\n    ]\n</code></pre>"},{"location":"development/fastmcp/prompts/#error-handling","title":"Error Handling","text":""},{"location":"development/fastmcp/prompts/#validation-and-error-messages","title":"Validation and Error Messages","text":"<pre><code>@mcp.prompt()\ndef validated_prompt(\n    task_type: str,\n    priority: Literal[\"low\", \"medium\", \"high\"],\n    deadline: str\n) -&gt; str:\n    \"\"\"Prompt with validation and error handling.\"\"\"\n\n    # Validate inputs\n    valid_tasks = [\"research\", \"analysis\", \"writing\", \"coding\", \"design\"]\n    if task_type not in valid_tasks:\n        raise ValueError(f\"Invalid task type. Must be one of: {', '.join(valid_tasks)}\")\n\n    try:\n        # Validate date format\n        datetime.strptime(deadline, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"Deadline must be in YYYY-MM-DD format\")\n\n    urgency = {\"low\": \"when convenient\", \"medium\": \"with moderate urgency\", \"high\": \"as a priority\"}\n\n    return f\"\"\"\n    Task: {task_type}\n    Priority: {priority} - handle this {urgency[priority]}\n    Deadline: {deadline}\n\n    Please approach this systematically and provide regular updates on progress.\n    \"\"\"\n</code></pre>"},{"location":"development/fastmcp/prompts/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/prompts/#1-clear-parameter-documentation","title":"1. Clear Parameter Documentation","text":"<pre><code>@mcp.prompt()\ndef well_documented_prompt(\n    objective: str = Field(description=\"The main goal or outcome you want to achieve\"),\n    context: str = Field(description=\"Background information or situation details\"),\n    constraints: list[str] = Field(\n        description=\"Any limitations, requirements, or boundaries to consider\",\n        default=[]\n    ),\n    format: Literal[\"bullet\", \"paragraph\", \"outline\"] = Field(\n        description=\"Preferred response format\",\n        default=\"paragraph\"\n    )\n) -&gt; str:\n    \"\"\"Generate a well-structured prompt with clear documentation.\"\"\"\n\n    prompt_parts = [f\"Objective: {objective}\", f\"Context: {context}\"]\n\n    if constraints:\n        prompt_parts.append(f\"Constraints: {', '.join(constraints)}\")\n\n    format_instructions = {\n        \"bullet\": \"Please respond in bullet point format\",\n        \"paragraph\": \"Please respond in paragraph format\",\n        \"outline\": \"Please respond as a structured outline\"\n    }\n    prompt_parts.append(format_instructions[format])\n\n    return \"\\n\\n\".join(prompt_parts)\n</code></pre>"},{"location":"development/fastmcp/prompts/#2-reusable-prompt-components","title":"2. Reusable Prompt Components","text":"<pre><code>def create_role_context(role: str, expertise: str) -&gt; str:\n    \"\"\"Create consistent role context for prompts.\"\"\"\n    return f\"You are a {role} with expertise in {expertise}.\"\n\ndef create_output_format(format_type: str) -&gt; str:\n    \"\"\"Create consistent output format instructions.\"\"\"\n    formats = {\n        \"executive\": \"Provide an executive summary with key points and recommendations\",\n        \"detailed\": \"Provide comprehensive analysis with supporting details\",\n        \"actionable\": \"Focus on specific, actionable steps and next actions\"\n    }\n    return formats.get(format_type, \"Provide a clear and helpful response\")\n\n@mcp.prompt()\ndef consulting_prompt(\n    domain: str,\n    problem: str,\n    output_format: Literal[\"executive\", \"detailed\", \"actionable\"] = \"detailed\"\n) -&gt; str:\n    \"\"\"Generate consulting-style prompts with reusable components.\"\"\"\n\n    role = create_role_context(\"senior consultant\", domain)\n    format_instruction = create_output_format(output_format)\n\n    return f\"\"\"\n    {role}\n\n    Problem: {problem}\n\n    {format_instruction}\n\n    Structure your response with:\n    1. Problem analysis\n    2. Root cause identification\n    3. Solution recommendations\n    4. Implementation roadmap\n    5. Success metrics\n    \"\"\"\n</code></pre>"},{"location":"development/fastmcp/prompts/#3-conditional-prompt-logic","title":"3. Conditional Prompt Logic","text":"<pre><code>@mcp.prompt()\ndef adaptive_learning_prompt(\n    topic: str,\n    current_knowledge: Literal[\"none\", \"basic\", \"intermediate\", \"advanced\"],\n    learning_style: Literal[\"visual\", \"auditory\", \"hands-on\", \"reading\"] = \"reading\",\n    time_available: int = 30  # minutes\n) -&gt; str:\n    \"\"\"Generate adaptive learning prompts based on user profile.\"\"\"\n\n    # Adjust complexity based on knowledge level\n    complexity_map = {\n        \"none\": \"Start with fundamental concepts and basic definitions\",\n        \"basic\": \"Build on basic knowledge with practical examples\",\n        \"intermediate\": \"Dive deeper into advanced concepts and applications\",\n        \"advanced\": \"Explore cutting-edge developments and expert perspectives\"\n    }\n\n    # Adapt content format to learning style\n    style_instructions = {\n        \"visual\": \"Include diagrams, charts, and visual examples\",\n        \"auditory\": \"Use storytelling and verbal explanations\",\n        \"hands-on\": \"Provide practical exercises and interactive examples\",\n        \"reading\": \"Use detailed written explanations and references\"\n    }\n\n    # Adjust scope based on time available\n    if time_available &lt;= 15:\n        scope = \"Provide a concise overview focusing on the most essential points\"\n    elif time_available &lt;= 45:\n        scope = \"Provide a balanced explanation with key details and examples\"\n    else:\n        scope = \"Provide comprehensive coverage with multiple examples and deeper exploration\"\n\n    return f\"\"\"\n    Topic: {topic}\n    Learning level: {complexity_map[current_knowledge]}\n    Learning style: {style_instructions[learning_style]}\n    Time available: {time_available} minutes\n\n    {scope}\n\n    Make the content engaging and appropriate for the learner's profile.\n    \"\"\"\n</code></pre>"},{"location":"development/fastmcp/resources/","title":"Resources","text":"<p>Resources in FastMCP are data sources that MCP clients can read from. They provide a standardized way to expose files, databases, APIs, or any other data to LLM applications.</p>"},{"location":"development/fastmcp/resources/#overview","title":"Overview","text":"<p>Resources come in two types:</p> <ul> <li>Static Resources: Fixed URIs that return data (e.g., <code>data://config</code>, <code>file:///etc/hosts</code>)</li> <li>Resource Templates: URI patterns with parameters (e.g., <code>weather://{city}/current</code>, <code>user://{id}/profile</code>)</li> </ul> <p>Both types are created using the <code>@mcp.resource()</code> decorator or by adding Resource objects directly.</p>"},{"location":"development/fastmcp/resources/#static-resources","title":"Static Resources","text":"<p>Static resources have fixed URIs and are ideal for configuration files, static data, or computed values.</p>"},{"location":"development/fastmcp/resources/#basic-static-resource","title":"Basic Static Resource","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Data Server\")\n\n@mcp.resource(\"data://config\")\ndef get_config() -&gt; str:\n    \"\"\"Server configuration data.\"\"\"\n    return \"debug=true\\nport=8080\\nhost=localhost\"\n\n@mcp.resource(\"data://status\")\ndef get_status() -&gt; dict:\n    \"\"\"Current server status.\"\"\"\n    return {\n        \"status\": \"running\",\n        \"uptime\": \"2 hours\",\n        \"memory_usage\": \"45MB\"\n    }\n</code></pre>"},{"location":"development/fastmcp/resources/#file-resources","title":"File Resources","text":"<pre><code>from pathlib import Path\n\n@mcp.resource(\"file://app.log\")\ndef get_log_file() -&gt; str:\n    \"\"\"Read the application log file.\"\"\"\n    log_path = Path(\"app.log\")\n    if log_path.exists():\n        return log_path.read_text()\n    return \"Log file not found\"\n\n@mcp.resource(\"file://data.json\")\ndef get_data_file() -&gt; bytes:\n    \"\"\"Read binary data file.\"\"\"\n    return Path(\"data.json\").read_bytes()\n</code></pre>"},{"location":"development/fastmcp/resources/#database-resources","title":"Database Resources","text":"<pre><code>import sqlite3\n\n@mcp.resource(\"db://users/count\")\ndef get_user_count() -&gt; dict:\n    \"\"\"Get total number of users.\"\"\"\n    conn = sqlite3.connect(\"app.db\")\n    cursor = conn.execute(\"SELECT COUNT(*) FROM users\")\n    count = cursor.fetchone()[0]\n    conn.close()\n\n    return {\"total_users\": count}\n\n@mcp.resource(\"db://recent_orders\")\ndef get_recent_orders() -&gt; list[dict]:\n    \"\"\"Get recent orders from database.\"\"\"\n    conn = sqlite3.connect(\"app.db\")\n    conn.row_factory = sqlite3.Row\n\n    cursor = conn.execute(\"\"\"\n        SELECT id, customer_name, total, created_at\n        FROM orders\n        ORDER BY created_at DESC\n        LIMIT 10\n    \"\"\")\n\n    orders = [dict(row) for row in cursor.fetchall()]\n    conn.close()\n\n    return orders\n</code></pre>"},{"location":"development/fastmcp/resources/#api-resources","title":"API Resources","text":"<pre><code>import httpx\n\n@mcp.resource(\"api://github/status\")\nasync def get_github_status() -&gt; dict:\n    \"\"\"Get GitHub API status.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(\"https://api.github.com/status\")\n        return response.json()\n\n@mcp.resource(\"api://weather/current\")\nasync def get_current_weather() -&gt; str:\n    \"\"\"Get current weather data.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(\"https://wttr.in/?format=j1\")\n        data = response.json()\n        current = data[\"current_condition\"][0]\n        return f\"Temperature: {current['temp_C']}\u00b0C, {current['weatherDesc'][0]['value']}\"\n</code></pre>"},{"location":"development/fastmcp/resources/#resource-templates","title":"Resource Templates","text":"<p>Resource templates use URI patterns with parameters to create dynamic resources. They're perfect for user profiles, file browsers, or any parameterized data.</p>"},{"location":"development/fastmcp/resources/#basic-templates","title":"Basic Templates","text":"<pre><code>@mcp.resource(\"user://{user_id}/profile\")\ndef get_user_profile(user_id: str) -&gt; dict:\n    \"\"\"Get user profile by ID.\"\"\"\n    # This function will be called with user_id parameter\n    # when client requests \"user://123/profile\"\n\n    users = {\n        \"123\": {\"name\": \"Alice\", \"email\": \"alice@example.com\"},\n        \"456\": {\"name\": \"Bob\", \"email\": \"bob@example.com\"}\n    }\n\n    if user_id not in users:\n        raise ValueError(f\"User {user_id} not found\")\n\n    return users[user_id]\n\n@mcp.resource(\"file://{path}\")\ndef read_file(path: str) -&gt; str:\n    \"\"\"Read any file by path.\"\"\"\n    try:\n        return Path(path).read_text()\n    except FileNotFoundError:\n        raise ValueError(f\"File not found: {path}\")\n    except PermissionError:\n        raise ValueError(f\"Permission denied: {path}\")\n</code></pre>"},{"location":"development/fastmcp/resources/#multiple-parameters","title":"Multiple Parameters","text":"<pre><code>@mcp.resource(\"weather://{city}/{type}\")\ndef get_weather(city: str, type: str) -&gt; dict:\n    \"\"\"Get weather data for a city and type.\n\n    Examples:\n    - weather://london/current\n    - weather://paris/forecast\n    - weather://tokyo/historical\n    \"\"\"\n\n    if type not in [\"current\", \"forecast\", \"historical\"]:\n        raise ValueError(f\"Invalid weather type: {type}\")\n\n    # Mock weather data\n    weather_data = {\n        \"current\": {\"temp\": 20, \"condition\": \"sunny\"},\n        \"forecast\": {\"tomorrow\": {\"temp\": 22, \"condition\": \"cloudy\"}},\n        \"historical\": {\"yesterday\": {\"temp\": 18, \"condition\": \"rainy\"}}\n    }\n\n    return {\n        \"city\": city,\n        \"type\": type,\n        \"data\": weather_data[type]\n    }\n\n@mcp.resource(\"repo://{owner}/{repo}/issues/{state}\")\ndef get_repo_issues(owner: str, repo: str, state: str) -&gt; list[dict]:\n    \"\"\"Get GitHub repository issues.\n\n    Examples:\n    - repo://microsoft/vscode/issues/open\n    - repo://python/cpython/issues/closed\n    \"\"\"\n    # Implementation would call GitHub API\n    return [\n        {\"id\": 1, \"title\": \"Bug report\", \"state\": state},\n        {\"id\": 2, \"title\": \"Feature request\", \"state\": state}\n    ]\n</code></pre>"},{"location":"development/fastmcp/resources/#typed-parameters","title":"Typed Parameters","text":"<p>Use type hints and Pydantic Field for parameter validation:</p> <pre><code>from pydantic import Field\nfrom typing import Annotated\n\n@mcp.resource(\"data://{category}/items/{page}\")\ndef get_paginated_data(\n    category: Annotated[str, Field(pattern=r\"^[a-z]+$\")],\n    page: Annotated[int, Field(ge=1, le=100)]\n) -&gt; dict:\n    \"\"\"Get paginated data with validation.\n\n    Category must be lowercase letters only.\n    Page must be between 1 and 100.\n    \"\"\"\n\n    items_per_page = 10\n    start_idx = (page - 1) * items_per_page\n\n    # Mock data\n    all_items = [f\"{category}_item_{i}\" for i in range(1, 101)]\n    page_items = all_items[start_idx:start_idx + items_per_page]\n\n    return {\n        \"category\": category,\n        \"page\": page,\n        \"items\": page_items,\n        \"total_pages\": len(all_items) // items_per_page\n    }\n</code></pre>"},{"location":"development/fastmcp/resources/#resource-metadata","title":"Resource Metadata","text":""},{"location":"development/fastmcp/resources/#custom-names-and-descriptions","title":"Custom Names and Descriptions","text":"<pre><code>@mcp.resource(\n    \"data://config\",\n    name=\"Server Configuration\",\n    description=\"Application configuration settings\"\n)\ndef get_config() -&gt; str:\n    return \"config data here\"\n\n@mcp.resource(\n    \"user://{id}\",\n    name=\"User Profile\",\n    description=\"Get user profile information by ID\"\n)\ndef get_user(id: str) -&gt; dict:\n    return {\"id\": id, \"name\": \"User\"}\n</code></pre>"},{"location":"development/fastmcp/resources/#mime-types","title":"MIME Types","text":"<p>Specify MIME types for non-text content:</p> <pre><code>@mcp.resource(\n    \"image://logo\",\n    mime_type=\"image/png\"\n)\ndef get_logo() -&gt; bytes:\n    \"\"\"Return PNG logo image.\"\"\"\n    return Path(\"logo.png\").read_bytes()\n\n@mcp.resource(\n    \"data://metrics\",\n    mime_type=\"application/json\"\n)\ndef get_metrics() -&gt; dict:\n    \"\"\"Return JSON metrics data.\"\"\"\n    return {\"cpu\": 45, \"memory\": 67, \"disk\": 23}\n\n@mcp.resource(\n    \"file://{path}\",\n    mime_type=\"text/plain\"\n)\ndef read_text_file(path: str) -&gt; str:\n    \"\"\"Read text file.\"\"\"\n    return Path(path).read_text()\n</code></pre>"},{"location":"development/fastmcp/resources/#async-resources","title":"Async Resources","text":"<p>Resources can be async for I/O operations:</p> <pre><code>import asyncio\nimport aiofiles\nimport httpx\n\n@mcp.resource(\"file://async/{filename}\")\nasync def read_file_async(filename: str) -&gt; str:\n    \"\"\"Read file asynchronously.\"\"\"\n    async with aiofiles.open(filename, 'r') as f:\n        return await f.read()\n\n@mcp.resource(\"api://posts/{post_id}\")\nasync def get_post(post_id: str) -&gt; dict:\n    \"\"\"Fetch blog post from API.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.example.com/posts/{post_id}\")\n        return response.json()\n\n@mcp.resource(\"db://async/users\")\nasync def get_users_async() -&gt; list[dict]:\n    \"\"\"Get users from async database.\"\"\"\n    # Simulate async database call\n    await asyncio.sleep(0.1)\n    return [\n        {\"id\": 1, \"name\": \"Alice\"},\n        {\"id\": 2, \"name\": \"Bob\"}\n    ]\n</code></pre>"},{"location":"development/fastmcp/resources/#programmatic-resource-creation","title":"Programmatic Resource Creation","text":""},{"location":"development/fastmcp/resources/#using-resource-classes","title":"Using Resource Classes","text":"<pre><code>from mcp.server.fastmcp.resources import TextResource, BinaryResource\n\n# Text resource\ntext_resource = TextResource(\n    uri=\"data://readme\",\n    text=\"# Welcome\\nThis is the README\",\n    name=\"README File\",\n    description=\"Project README content\"\n)\nmcp.add_resource(text_resource)\n\n# Binary resource\nbinary_data = b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR...\"\nbinary_resource = BinaryResource(\n    uri=\"image://icon\",\n    data=binary_data,\n    mime_type=\"image/png\",\n    name=\"Application Icon\"\n)\nmcp.add_resource(binary_resource)\n</code></pre>"},{"location":"development/fastmcp/resources/#function-resources","title":"Function Resources","text":"<pre><code>from mcp.server.fastmcp.resources import FunctionResource\n\ndef get_timestamp():\n    from datetime import datetime\n    return datetime.now().isoformat()\n\n# Create function resource\ntimestamp_resource = FunctionResource.from_function(\n    fn=get_timestamp,\n    uri=\"time://current\",\n    name=\"Current Timestamp\",\n    description=\"Current server timestamp\"\n)\nmcp.add_resource(timestamp_resource)\n</code></pre>"},{"location":"development/fastmcp/resources/#error-handling","title":"Error Handling","text":""},{"location":"development/fastmcp/resources/#graceful-error-handling","title":"Graceful Error Handling","text":"<pre><code>@mcp.resource(\"file://{path}\")\ndef safe_file_read(path: str) -&gt; str:\n    \"\"\"Safely read a file with error handling.\"\"\"\n    try:\n        file_path = Path(path)\n\n        # Security check\n        if not file_path.is_file():\n            raise ValueError(f\"Not a file: {path}\")\n\n        # Size check (max 1MB)\n        if file_path.stat().st_size &gt; 1024 * 1024:\n            raise ValueError(f\"File too large: {path}\")\n\n        return file_path.read_text()\n\n    except FileNotFoundError:\n        raise ValueError(f\"File not found: {path}\")\n    except PermissionError:\n        raise ValueError(f\"Permission denied: {path}\")\n    except UnicodeDecodeError:\n        raise ValueError(f\"File is not text: {path}\")\n</code></pre>"},{"location":"development/fastmcp/resources/#resource-validation","title":"Resource Validation","text":"<pre><code>from pathlib import Path\n\n@mcp.resource(\"directory://{path}/list\")\ndef list_directory(path: str) -&gt; list[str]:\n    \"\"\"List directory contents with validation.\"\"\"\n\n    # Validate path\n    if not path or \"..\" in path:\n        raise ValueError(\"Invalid path\")\n\n    dir_path = Path(path)\n\n    if not dir_path.exists():\n        raise ValueError(f\"Directory does not exist: {path}\")\n\n    if not dir_path.is_dir():\n        raise ValueError(f\"Not a directory: {path}\")\n\n    try:\n        return [item.name for item in dir_path.iterdir()]\n    except PermissionError:\n        raise ValueError(f\"Permission denied: {path}\")\n</code></pre>"},{"location":"development/fastmcp/resources/#advanced-examples","title":"Advanced Examples","text":""},{"location":"development/fastmcp/resources/#database-resource-with-connection-pooling","title":"Database Resource with Connection Pooling","text":"<pre><code>import sqlite3\nfrom contextlib import contextmanager\n\nclass DatabaseManager:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n\n    @contextmanager\n    def get_connection(self):\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row\n        try:\n            yield conn\n        finally:\n            conn.close()\n\ndb_manager = DatabaseManager(\"app.db\")\n\n@mcp.resource(\"db://table/{table_name}\")\ndef query_table(table_name: str) -&gt; list[dict]:\n    \"\"\"Query any table in the database.\"\"\"\n\n    # Validate table name (prevent SQL injection)\n    if not table_name.isalnum():\n        raise ValueError(\"Invalid table name\")\n\n    with db_manager.get_connection() as conn:\n        try:\n            cursor = conn.execute(f\"SELECT * FROM {table_name} LIMIT 100\")\n            return [dict(row) for row in cursor.fetchall()]\n        except sqlite3.OperationalError as e:\n            raise ValueError(f\"Database error: {e}\")\n</code></pre>"},{"location":"development/fastmcp/resources/#cached-resource","title":"Cached Resource","text":"<pre><code>import time\nfrom functools import lru_cache\n\n@mcp.resource(\"api://cached/{endpoint}\")\ndef cached_api_call(endpoint: str) -&gt; dict:\n    \"\"\"API call with caching.\"\"\"\n\n    @lru_cache(maxsize=128)\n    def _fetch_data(endpoint: str, cache_key: int) -&gt; dict:\n        # Cache key based on 5-minute intervals\n        import httpx\n        response = httpx.get(f\"https://api.example.com/{endpoint}\")\n        return response.json()\n\n    # Create cache key (refreshes every 5 minutes)\n    cache_key = int(time.time() // 300)\n\n    return _fetch_data(endpoint, cache_key)\n</code></pre>"},{"location":"development/fastmcp/resources/#file-browser-resource","title":"File Browser Resource","text":"<pre><code>import mimetypes\nfrom pathlib import Path\n\n@mcp.resource(\"browse://{path}\")\ndef browse_filesystem(path: str) -&gt; dict:\n    \"\"\"Browse filesystem with metadata.\"\"\"\n\n    # Security: restrict to specific directories\n    allowed_roots = [\"/home/user/documents\", \"/home/user/projects\"]\n    abs_path = Path(path).resolve()\n\n    if not any(str(abs_path).startswith(root) for root in allowed_roots):\n        raise ValueError(\"Access denied\")\n\n    if not abs_path.exists():\n        raise ValueError(f\"Path does not exist: {path}\")\n\n    if abs_path.is_file():\n        stat = abs_path.stat()\n        mime_type, _ = mimetypes.guess_type(str(abs_path))\n\n        return {\n            \"type\": \"file\",\n            \"name\": abs_path.name,\n            \"size\": stat.st_size,\n            \"modified\": stat.st_mtime,\n            \"mime_type\": mime_type\n        }\n\n    elif abs_path.is_dir():\n        items = []\n        for item in abs_path.iterdir():\n            stat = item.stat()\n            items.append({\n                \"name\": item.name,\n                \"type\": \"directory\" if item.is_dir() else \"file\",\n                \"size\": stat.st_size if item.is_file() else None,\n                \"modified\": stat.st_mtime\n            })\n\n        return {\n            \"type\": \"directory\",\n            \"path\": str(abs_path),\n            \"items\": sorted(items, key=lambda x: (x[\"type\"], x[\"name\"]))\n        }\n</code></pre>"},{"location":"development/fastmcp/resources/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/resources/#1-use-meaningful-uris","title":"1. Use Meaningful URIs","text":"<pre><code># Good: Clear, hierarchical URIs\n@mcp.resource(\"user://{id}/profile\")\n@mcp.resource(\"project://{id}/files/{path}\")\n@mcp.resource(\"weather://{city}/current\")\n\n# Avoid: Unclear or flat URIs\n@mcp.resource(\"data://{x}\")\n@mcp.resource(\"resource1\")\n</code></pre>"},{"location":"development/fastmcp/resources/#2-validate-parameters","title":"2. Validate Parameters","text":"<pre><code>@mcp.resource(\"user://{user_id}/posts/{page}\")\ndef get_user_posts(\n    user_id: Annotated[str, Field(pattern=r\"^\\d+$\")],\n    page: Annotated[int, Field(ge=1, le=1000)]\n) -&gt; list[dict]:\n    \"\"\"Get user posts with validation.\"\"\"\n    # user_id must be numeric string\n    # page must be 1-1000\n    pass\n</code></pre>"},{"location":"development/fastmcp/resources/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<pre><code>@mcp.resource(\"file://{path}\")\ndef read_file_safe(path: str) -&gt; str:\n    \"\"\"Read file with comprehensive error handling.\"\"\"\n    try:\n        return Path(path).read_text()\n    except FileNotFoundError:\n        raise ValueError(f\"File not found: {path}\")\n    except PermissionError:\n        raise ValueError(f\"Access denied: {path}\")\n    except IsADirectoryError:\n        raise ValueError(f\"Path is a directory: {path}\")\n    except UnicodeDecodeError:\n        raise ValueError(f\"File is not text: {path}\")\n</code></pre>"},{"location":"development/fastmcp/resources/#4-use-appropriate-mime-types","title":"4. Use Appropriate MIME Types","text":"<pre><code>@mcp.resource(\"image://{name}\", mime_type=\"image/png\")\ndef get_image(name: str) -&gt; bytes:\n    return Path(f\"{name}.png\").read_bytes()\n\n@mcp.resource(\"data://{name}\", mime_type=\"application/json\")\ndef get_json_data(name: str) -&gt; dict:\n    return {\"data\": name}\n\n@mcp.resource(\"doc://{name}\", mime_type=\"text/markdown\")\ndef get_document(name: str) -&gt; str:\n    return f\"# {name}\\n\\nDocument content...\"\n</code></pre>"},{"location":"development/fastmcp/resources/#5-implement-security","title":"5. Implement Security","text":"<pre><code>from pathlib import Path\n\nALLOWED_PATHS = [\"/safe/directory\", \"/another/safe/path\"]\n\n@mcp.resource(\"secure-file://{path}\")\ndef secure_file_access(path: str) -&gt; str:\n    \"\"\"Secure file access with path validation.\"\"\"\n\n    # Resolve path and check it's within allowed directories\n    abs_path = Path(path).resolve()\n\n    if not any(str(abs_path).startswith(allowed) for allowed in ALLOWED_PATHS):\n        raise ValueError(\"Access denied: path outside allowed directories\")\n\n    if not abs_path.exists():\n        raise ValueError(f\"File not found: {path}\")\n\n    return abs_path.read_text()\n</code></pre>"},{"location":"development/fastmcp/security-guide/","title":"Security Guide","text":"<p>Comprehensive security guide for FastMCP servers, covering authentication, authorization, input validation, secret management, and security best practices.</p>"},{"location":"development/fastmcp/security-guide/#security-overview","title":"Security Overview","text":"<p>FastMCP servers can handle sensitive data and provide access to system resources. This guide covers essential security practices to protect your servers and users.</p>"},{"location":"development/fastmcp/security-guide/#security-principles","title":"Security Principles","text":"<ol> <li>Defense in Depth: Multiple layers of security controls</li> <li>Least Privilege: Grant minimal necessary permissions</li> <li>Input Validation: Validate and sanitize all inputs</li> <li>Secure Defaults: Security-first configuration</li> <li>Regular Updates: Keep dependencies updated</li> </ol>"},{"location":"development/fastmcp/security-guide/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"development/fastmcp/security-guide/#1-oauth2-authentication","title":"1. OAuth2 Authentication","text":"<p>Secure HTTP transport setup: <pre><code>import secrets\nimport time\nfrom typing import Optional\nfrom mcp.server.auth.provider import (\n    OAuthAuthorizationServerProvider,\n    AccessToken,\n    RefreshToken,\n    AuthorizationCode\n)\nfrom mcp.server.auth.settings import AuthSettings, ClientRegistrationOptions\nfrom mcp.server.fastmcp import FastMCP\n\nclass SecureOAuthProvider(OAuthAuthorizationServerProvider):\n    \"\"\"Production-ready OAuth provider with security best practices.\"\"\"\n\n    def __init__(self):\n        # Use secure storage in production (database with encryption)\n        self.clients = {}\n        self.access_tokens = {}\n        self.refresh_tokens = {}\n        self.auth_codes = {}\n        self.rate_limits = {}  # Track rate limiting\n\n    async def get_client(self, client_id: str) -&gt; Optional[dict]:\n        \"\"\"Get client with security checks.\"\"\"\n        # Rate limiting check\n        if await self._is_rate_limited(client_id):\n            raise ValueError(\"Rate limit exceeded\")\n\n        return self.clients.get(client_id)\n\n    async def create_authorization_code(self, params) -&gt; str:\n        \"\"\"Create secure authorization code.\"\"\"\n        # Generate cryptographically secure code\n        code = secrets.token_urlsafe(64)\n\n        # Store with expiration (short-lived)\n        self.auth_codes[code] = AuthorizationCode(\n            code=code,\n            scopes=params.scopes or [],\n            expires_at=time.time() + 300,  # 5 minutes only\n            client_id=params.client_id,\n            code_challenge=params.code_challenge,\n            redirect_uri=params.redirect_uri,\n            redirect_uri_provided_explicitly=params.redirect_uri_provided_explicitly\n        )\n\n        return code\n\n    async def exchange_code_for_tokens(self, code: str, client_id: str) -&gt; Optional[dict]:\n        \"\"\"Exchange code for tokens with security validation.\"\"\"\n        auth_code = self.auth_codes.get(code)\n        if not auth_code:\n            return None\n\n        # Validate code hasn't expired\n        if auth_code.expires_at &lt; time.time():\n            del self.auth_codes[code]  # Clean up expired code\n            return None\n\n        # Validate client_id matches\n        if auth_code.client_id != client_id:\n            return None\n\n        # Generate secure tokens\n        access_token = secrets.token_urlsafe(64)\n        refresh_token = secrets.token_urlsafe(64)\n\n        # Store tokens with expiration\n        self.access_tokens[access_token] = AccessToken(\n            token=access_token,\n            client_id=client_id,\n            scopes=auth_code.scopes,\n            expires_at=int(time.time() + 3600)  # 1 hour\n        )\n\n        self.refresh_tokens[refresh_token] = RefreshToken(\n            token=refresh_token,\n            client_id=client_id,\n            scopes=auth_code.scopes,\n            expires_at=int(time.time() + 86400 * 30)  # 30 days\n        )\n\n        # Clean up used authorization code\n        del self.auth_codes[code]\n\n        return {\n            \"access_token\": access_token,\n            \"refresh_token\": refresh_token,\n            \"token_type\": \"Bearer\",\n            \"expires_in\": 3600,\n            \"scope\": \" \".join(auth_code.scopes)\n        }\n\n    async def load_access_token(self, token: str) -&gt; Optional[AccessToken]:\n        \"\"\"Load and validate access token.\"\"\"\n        access_token = self.access_tokens.get(token)\n        if not access_token:\n            return None\n\n        # Check expiration\n        if access_token.expires_at and access_token.expires_at &lt; time.time():\n            del self.access_tokens[token]  # Clean up expired token\n            return None\n\n        return access_token\n\n    async def _is_rate_limited(self, client_id: str) -&gt; bool:\n        \"\"\"Simple rate limiting implementation.\"\"\"\n        current_time = time.time()\n        window = 60  # 1 minute window\n        max_requests = 100\n\n        if client_id not in self.rate_limits:\n            self.rate_limits[client_id] = []\n\n        # Clean old requests\n        self.rate_limits[client_id] = [\n            req_time for req_time in self.rate_limits[client_id]\n            if current_time - req_time &lt; window\n        ]\n\n        # Check limit\n        if len(self.rate_limits[client_id]) &gt;= max_requests:\n            return True\n\n        # Add current request\n        self.rate_limits[client_id].append(current_time)\n        return False\n\n# Configure secure authentication\nauth_provider = SecureOAuthProvider()\nauth_settings = AuthSettings(\n    issuer_url=\"https://your-domain.com\",\n    required_scopes=[\"read\", \"write\"],\n    client_registration_options=ClientRegistrationOptions(\n        enabled=True,\n        valid_scopes=[\"read\", \"write\", \"admin\"],\n        default_scopes=[\"read\"]\n    )\n)\n\nmcp = FastMCP(\n    \"Secure Server\",\n    auth_server_provider=auth_provider,\n    auth=auth_settings\n)\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#2-scope-based-authorization","title":"2. Scope-based Authorization","text":"<p>Implement granular permissions: <pre><code>from mcp.server.fastmcp import Context\n\ndef require_scope(required_scope: str):\n    \"\"\"Decorator to require specific OAuth scope.\"\"\"\n    def decorator(func):\n        async def wrapper(*args, **kwargs):\n            # Find context in arguments\n            ctx = None\n            for arg in args:\n                if isinstance(arg, Context):\n                    ctx = arg\n                    break\n\n            if not ctx:\n                raise ValueError(\"Context required for authorization\")\n\n            # Check if user has required scope\n            # This would be implemented based on your auth system\n            user_scopes = getattr(ctx, 'user_scopes', [])\n            if required_scope not in user_scopes:\n                raise PermissionError(f\"Scope '{required_scope}' required\")\n\n            return await func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n@mcp.tool()\n@require_scope(\"admin\")\nasync def admin_operation(data: str, ctx: Context) -&gt; str:\n    \"\"\"Operation requiring admin privileges.\"\"\"\n    ctx.info(\"Admin operation executed\")\n    return f\"Admin processed: {data}\"\n\n@mcp.tool()\n@require_scope(\"read\")\nasync def read_operation(ctx: Context) -&gt; str:\n    \"\"\"Operation requiring read privileges.\"\"\"\n    return \"Public data\"\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#input-validation-sanitization","title":"Input Validation &amp; Sanitization","text":""},{"location":"development/fastmcp/security-guide/#1-pydantic-validation","title":"1. Pydantic Validation","text":"<p>Comprehensive input validation: <pre><code>from pydantic import BaseModel, Field, validator, EmailStr\nfrom typing import List, Optional\nimport re\nimport html\n\nclass SecureUserInput(BaseModel):\n    \"\"\"Secure user input model with validation.\"\"\"\n\n    name: str = Field(\n        min_length=1,\n        max_length=100,\n        regex=r\"^[a-zA-Z0-9\\s\\-_]+$\"  # Only alphanumeric, spaces, hyphens, underscores\n    )\n    email: EmailStr\n    age: int = Field(ge=13, le=120)  # Age restrictions\n    phone: Optional[str] = Field(\n        None,\n        regex=r\"^\\+?1?\\d{9,15}$\"  # International phone format\n    )\n    bio: Optional[str] = Field(None, max_length=500)\n    tags: List[str] = Field(default_factory=list, max_items=10)\n\n    @validator('name')\n    def validate_name(cls, v):\n        \"\"\"Additional name validation.\"\"\"\n        # Remove any potential HTML/script content\n        cleaned = html.escape(v.strip())\n        if cleaned != v.strip():\n            raise ValueError(\"Invalid characters in name\")\n        return cleaned\n\n    @validator('bio')\n    def validate_bio(cls, v):\n        \"\"\"Sanitize bio content.\"\"\"\n        if v:\n            # Remove HTML tags and escape content\n            cleaned = html.escape(v.strip())\n            # Check for suspicious patterns\n            suspicious_patterns = [\n                r'&lt;script', r'javascript:', r'data:', r'vbscript:',\n                r'onload=', r'onerror=', r'onclick='\n            ]\n            v_lower = v.lower()\n            for pattern in suspicious_patterns:\n                if re.search(pattern, v_lower):\n                    raise ValueError(\"Potentially unsafe content in bio\")\n            return cleaned\n        return v\n\n    @validator('tags')\n    def validate_tags(cls, v):\n        \"\"\"Validate tag list.\"\"\"\n        if v:\n            for tag in v:\n                if not isinstance(tag, str) or len(tag) &gt; 50:\n                    raise ValueError(\"Invalid tag format\")\n                # Sanitize each tag\n                if html.escape(tag) != tag:\n                    raise ValueError(\"Invalid characters in tag\")\n        return v\n\n@mcp.tool()\nasync def create_user(user_data: SecureUserInput, ctx: Context) -&gt; str:\n    \"\"\"Create user with validated input.\"\"\"\n    ctx.info(f\"Creating user: {user_data.name}\")\n\n    # Additional business logic validation\n    if user_data.name.lower() in ['admin', 'root', 'system']:\n        raise ValueError(\"Reserved username\")\n\n    # Process validated data\n    return f\"User {user_data.name} created successfully\"\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#2-file-upload-security","title":"2. File Upload Security","text":"<p>Secure file handling: <pre><code>import os\nimport mimetypes\nfrom pathlib import Path\nimport magic  # python-magic for file type detection\n\nclass SecureFileHandler:\n    \"\"\"Secure file upload and processing.\"\"\"\n\n    ALLOWED_TYPES = {\n        'image/jpeg', 'image/png', 'image/gif', 'image/webp',\n        'text/plain', 'text/csv', 'application/json',\n        'application/pdf'\n    }\n\n    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n    UPLOAD_DIR = Path(\"/secure/uploads\")\n\n    def __init__(self):\n        self.UPLOAD_DIR.mkdir(parents=True, exist_ok=True)\n\n    def validate_file(self, file_data: bytes, filename: str) -&gt; bool:\n        \"\"\"Comprehensive file validation.\"\"\"\n        # Check file size\n        if len(file_data) &gt; self.MAX_FILE_SIZE:\n            raise ValueError(f\"File too large: {len(file_data)} bytes\")\n\n        # Validate filename\n        if not self._is_safe_filename(filename):\n            raise ValueError(\"Unsafe filename\")\n\n        # Check MIME type by content (not extension)\n        detected_type = magic.from_buffer(file_data, mime=True)\n        if detected_type not in self.ALLOWED_TYPES:\n            raise ValueError(f\"File type not allowed: {detected_type}\")\n\n        # Additional checks for images\n        if detected_type.startswith('image/'):\n            return self._validate_image(file_data)\n\n        return True\n\n    def _is_safe_filename(self, filename: str) -&gt; bool:\n        \"\"\"Check filename safety.\"\"\"\n        if not filename or len(filename) &gt; 255:\n            return False\n\n        # Check for path traversal\n        if '..' in filename or '/' in filename or '\\\\' in filename:\n            return False\n\n        # Check for null bytes or control characters\n        if any(ord(c) &lt; 32 for c in filename):\n            return False\n\n        # Check for executable extensions\n        dangerous_extensions = {\n            '.exe', '.bat', '.cmd', '.com', '.scr', '.vbs', '.js',\n            '.jar', '.php', '.py', '.sh', '.bash', '.ps1'\n        }\n        file_ext = Path(filename).suffix.lower()\n        if file_ext in dangerous_extensions:\n            return False\n\n        return True\n\n    def _validate_image(self, file_data: bytes) -&gt; bool:\n        \"\"\"Additional validation for images.\"\"\"\n        try:\n            from PIL import Image\n            import io\n\n            # Try to open and verify image\n            img = Image.open(io.BytesIO(file_data))\n            img.verify()  # Verify it's a valid image\n\n            # Check dimensions (prevent decompression bombs)\n            if img.size[0] * img.size[1] &gt; 100_000_000:  # 100MP limit\n                raise ValueError(\"Image too large\")\n\n            return True\n        except Exception:\n            raise ValueError(\"Invalid image file\")\n\n    def save_file(self, file_data: bytes, filename: str) -&gt; str:\n        \"\"\"Securely save file.\"\"\"\n        self.validate_file(file_data, filename)\n\n        # Generate secure filename\n        safe_filename = secrets.token_hex(16) + \"_\" + filename\n        file_path = self.UPLOAD_DIR / safe_filename\n\n        # Write file with restricted permissions\n        with open(file_path, 'wb') as f:\n            f.write(file_data)\n\n        # Set secure permissions\n        os.chmod(file_path, 0o644)\n\n        return str(file_path)\n\nfile_handler = SecureFileHandler()\n\n@mcp.tool()\nasync def upload_file(file_data: str, filename: str, ctx: Context) -&gt; str:\n    \"\"\"Secure file upload tool.\"\"\"\n    try:\n        # Decode base64 file data\n        import base64\n        decoded_data = base64.b64decode(file_data)\n\n        # Validate and save file\n        file_path = file_handler.save_file(decoded_data, filename)\n\n        ctx.info(f\"File uploaded securely: {filename}\")\n        return f\"File saved: {file_path}\"\n\n    except Exception as e:\n        ctx.error(f\"File upload failed: {e}\")\n        raise ValueError(f\"Upload failed: {e}\")\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#secret-management","title":"Secret Management","text":""},{"location":"development/fastmcp/security-guide/#1-environment-variables","title":"1. Environment Variables","text":"<p>Secure secret handling: <pre><code>import os\nfrom pydantic import BaseModel, Field, validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom typing import Optional\n\nclass SecuritySettings(BaseSettings):\n    \"\"\"Secure settings management.\"\"\"\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive=True,\n        extra=\"ignore\"\n    )\n\n    # Database credentials\n    DB_PASSWORD: str = Field(..., description=\"Database password\")\n    DB_HOST: str = Field(default=\"localhost\")\n    DB_USER: str = Field(default=\"user\")\n\n    # API keys\n    OPENAI_API_KEY: Optional[str] = Field(None, description=\"OpenAI API key\")\n    STRIPE_SECRET_KEY: Optional[str] = Field(None, description=\"Stripe secret key\")\n\n    # JWT secrets\n    JWT_SECRET_KEY: str = Field(..., min_length=32, description=\"JWT signing key\")\n\n    # Encryption keys\n    ENCRYPTION_KEY: str = Field(..., min_length=32, description=\"Data encryption key\")\n\n    @validator('JWT_SECRET_KEY', 'ENCRYPTION_KEY')\n    def validate_keys(cls, v):\n        \"\"\"Ensure keys are sufficiently random.\"\"\"\n        if len(v) &lt; 32:\n            raise ValueError(\"Key must be at least 32 characters\")\n        # Check for common weak patterns\n        if v in ['password', '123456', 'secret', 'key']:\n            raise ValueError(\"Key is too weak\")\n        return v\n\n    def get_database_url(self) -&gt; str:\n        \"\"\"Construct database URL securely.\"\"\"\n        return f\"postgresql://{self.DB_USER}:{self.DB_PASSWORD}@{self.DB_HOST}/mydb\"\n\n# Load settings with validation\nsettings = SecuritySettings()\n\n@mcp.tool()\nasync def secure_api_call(query: str, ctx: Context) -&gt; str:\n    \"\"\"Make API call with secure key management.\"\"\"\n    if not settings.OPENAI_API_KEY:\n        raise ValueError(\"API key not configured\")\n\n    # Use the key securely\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"https://api.openai.com/v1/completions\",\n            headers={\n                \"Authorization\": f\"Bearer {settings.OPENAI_API_KEY}\",\n                \"Content-Type\": \"application/json\"\n            },\n            json={\"prompt\": query, \"max_tokens\": 100}\n        )\n\n    # Don't log the response content (might contain sensitive data)\n    ctx.info(\"API call completed successfully\")\n    return \"Response received\"\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#2-encryption-at-rest","title":"2. Encryption at Rest","text":"<p>Encrypt sensitive data: <pre><code>from cryptography.fernet import Fernet\nimport base64\nimport json\n\nclass DataEncryption:\n    \"\"\"Handle data encryption and decryption.\"\"\"\n\n    def __init__(self, key: str):\n        # Derive key from settings\n        key_bytes = key.encode()[:32].ljust(32, b'0')  # Ensure 32 bytes\n        self.fernet = Fernet(base64.urlsafe_b64encode(key_bytes))\n\n    def encrypt(self, data: str) -&gt; str:\n        \"\"\"Encrypt sensitive data.\"\"\"\n        encrypted_bytes = self.fernet.encrypt(data.encode())\n        return base64.urlsafe_b64encode(encrypted_bytes).decode()\n\n    def decrypt(self, encrypted_data: str) -&gt; str:\n        \"\"\"Decrypt sensitive data.\"\"\"\n        encrypted_bytes = base64.urlsafe_b64decode(encrypted_data.encode())\n        decrypted_bytes = self.fernet.decrypt(encrypted_bytes)\n        return decrypted_bytes.decode()\n\nencryption = DataEncryption(settings.ENCRYPTION_KEY)\n\n@mcp.tool()\nasync def store_sensitive_data(user_id: int, sensitive_info: str, ctx: Context) -&gt; str:\n    \"\"\"Store sensitive data with encryption.\"\"\"\n    try:\n        # Encrypt before storing\n        encrypted_info = encryption.encrypt(sensitive_info)\n\n        # Store in database (encrypted)\n        # await db.execute(\n        #     \"INSERT INTO user_data (user_id, encrypted_data) VALUES ($1, $2)\",\n        #     user_id, encrypted_info\n        # )\n\n        ctx.info(f\"Sensitive data stored for user {user_id}\")\n        return \"Data stored securely\"\n\n    except Exception as e:\n        ctx.error(f\"Failed to store data: {e}\")\n        raise\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#network-security","title":"Network Security","text":""},{"location":"development/fastmcp/security-guide/#1-https-configuration","title":"1. HTTPS Configuration","text":"<p>Secure transport configuration: <pre><code>import ssl\nfrom pathlib import Path\n\ndef create_ssl_context() -&gt; ssl.SSLContext:\n    \"\"\"Create secure SSL context.\"\"\"\n    context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n\n    # Load certificates\n    cert_file = Path(\"/path/to/cert.pem\")\n    key_file = Path(\"/path/to/key.pem\")\n\n    if cert_file.exists() and key_file.exists():\n        context.load_cert_chain(cert_file, key_file)\n\n    # Security settings\n    context.minimum_version = ssl.TLSVersion.TLSv1_2\n    context.set_ciphers('ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:!aNULL:!MD5:!DSS')\n\n    return context\n\n# Use with uvicorn in production\n\"\"\"\nuvicorn main:app \\\\\n    --host 0.0.0.0 \\\\\n    --port 443 \\\\\n    --ssl-keyfile /path/to/key.pem \\\\\n    --ssl-certfile /path/to/cert.pem \\\\\n    --ssl-version 3 \\\\\n    --ssl-ciphers TLSv1.2\n\"\"\"\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#2-request-rate-limiting","title":"2. Request Rate Limiting","text":"<p>Implement rate limiting: <pre><code>import time\nfrom collections import defaultdict, deque\nfrom typing import Dict\nimport asyncio\n\nclass RateLimiter:\n    \"\"\"Token bucket rate limiter.\"\"\"\n\n    def __init__(self, max_requests: int = 100, window_seconds: int = 60):\n        self.max_requests = max_requests\n        self.window_seconds = window_seconds\n        self.clients: Dict[str, deque] = defaultdict(deque)\n        self.lock = asyncio.Lock()\n\n    async def is_allowed(self, client_id: str) -&gt; bool:\n        \"\"\"Check if request is allowed.\"\"\"\n        async with self.lock:\n            current_time = time.time()\n            client_requests = self.clients[client_id]\n\n            # Remove old requests outside window\n            while client_requests and client_requests[0] &lt; current_time - self.window_seconds:\n                client_requests.popleft()\n\n            # Check if under limit\n            if len(client_requests) &gt;= self.max_requests:\n                return False\n\n            # Add current request\n            client_requests.append(current_time)\n            return True\n\nrate_limiter = RateLimiter(max_requests=100, window_seconds=60)\n\ndef rate_limited(func):\n    \"\"\"Decorator to enforce rate limiting.\"\"\"\n    async def wrapper(*args, **kwargs):\n        # Extract client ID from context\n        ctx = None\n        for arg in args:\n            if isinstance(arg, Context):\n                ctx = arg\n                break\n\n        client_id = ctx.client_id if ctx else \"unknown\"\n\n        if not await rate_limiter.is_allowed(client_id):\n            raise ValueError(\"Rate limit exceeded\")\n\n        return await func(*args, **kwargs)\n    return wrapper\n\n@mcp.tool()\n@rate_limited\nasync def rate_limited_operation(data: str, ctx: Context) -&gt; str:\n    \"\"\"Operation with rate limiting.\"\"\"\n    return f\"Processed: {data}\"\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#security-monitoring","title":"Security Monitoring","text":""},{"location":"development/fastmcp/security-guide/#1-security-logging","title":"1. Security Logging","text":"<p>Comprehensive security logging: <pre><code>import logging\nimport json\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nclass SecurityLogger:\n    \"\"\"Security-focused logging.\"\"\"\n\n    def __init__(self):\n        self.logger = logging.getLogger(\"security\")\n        self.logger.setLevel(logging.INFO)\n\n        # Create file handler for security logs\n        handler = logging.FileHandler(\"/var/log/fastmcp/security.log\")\n        formatter = logging.Formatter(\n            '%(asctime)s - %(levelname)s - %(message)s'\n        )\n        handler.setFormatter(formatter)\n        self.logger.addHandler(handler)\n\n    def log_auth_attempt(self, client_id: str, success: bool, ip_address: str = None):\n        \"\"\"Log authentication attempts.\"\"\"\n        event = {\n            \"event_type\": \"auth_attempt\",\n            \"client_id\": client_id,\n            \"success\": success,\n            \"ip_address\": ip_address,\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n\n        level = logging.INFO if success else logging.WARNING\n        self.logger.log(level, json.dumps(event))\n\n    def log_permission_denied(self, client_id: str, resource: str, action: str):\n        \"\"\"Log permission denied events.\"\"\"\n        event = {\n            \"event_type\": \"permission_denied\",\n            \"client_id\": client_id,\n            \"resource\": resource,\n            \"action\": action,\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n        self.logger.warning(json.dumps(event))\n\n    def log_suspicious_activity(self, client_id: str, activity: str, details: Dict[str, Any]):\n        \"\"\"Log suspicious activities.\"\"\"\n        event = {\n            \"event_type\": \"suspicious_activity\",\n            \"client_id\": client_id,\n            \"activity\": activity,\n            \"details\": details,\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n        self.logger.error(json.dumps(event))\n\nsecurity_logger = SecurityLogger()\n\n@mcp.tool()\nasync def monitored_operation(data: str, ctx: Context) -&gt; str:\n    \"\"\"Operation with security monitoring.\"\"\"\n    client_id = ctx.client_id or \"unknown\"\n\n    # Check for suspicious patterns\n    if any(pattern in data.lower() for pattern in ['&lt;script', 'javascript:', 'eval(']):\n        security_logger.log_suspicious_activity(\n            client_id,\n            \"potential_xss_attempt\",\n            {\"input_data\": data[:100]}  # Log partial data only\n        )\n        raise ValueError(\"Suspicious input detected\")\n\n    # Log successful operation\n    security_logger.log_auth_attempt(client_id, True)\n\n    return f\"Safely processed: {html.escape(data)}\"\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#2-anomaly-detection","title":"2. Anomaly Detection","text":"<p>Basic anomaly detection: <pre><code>from collections import defaultdict\nimport statistics\n\nclass AnomalyDetector:\n    \"\"\"Detect unusual patterns in requests.\"\"\"\n\n    def __init__(self):\n        self.request_patterns = defaultdict(list)\n        self.alert_threshold = 3.0  # Standard deviations\n\n    def record_request(self, client_id: str, endpoint: str, response_time: float):\n        \"\"\"Record request metrics.\"\"\"\n        key = f\"{client_id}:{endpoint}\"\n        self.request_patterns[key].append({\n            \"response_time\": response_time,\n            \"timestamp\": time.time()\n        })\n\n        # Keep only recent data (last 1000 requests)\n        if len(self.request_patterns[key]) &gt; 1000:\n            self.request_patterns[key] = self.request_patterns[key][-1000:]\n\n    def detect_anomalies(self, client_id: str, endpoint: str, response_time: float) -&gt; bool:\n        \"\"\"Detect if current request is anomalous.\"\"\"\n        key = f\"{client_id}:{endpoint}\"\n        pattern = self.request_patterns[key]\n\n        if len(pattern) &lt; 10:  # Need baseline data\n            return False\n\n        # Calculate statistics\n        times = [r[\"response_time\"] for r in pattern[-100:]]  # Last 100 requests\n        mean_time = statistics.mean(times)\n        std_dev = statistics.stdev(times) if len(times) &gt; 1 else 0\n\n        # Check if current request is anomalous\n        if std_dev &gt; 0:\n            z_score = abs(response_time - mean_time) / std_dev\n            return z_score &gt; self.alert_threshold\n\n        return False\n\nanomaly_detector = AnomalyDetector()\n\ndef monitor_performance(func):\n    \"\"\"Decorator to monitor and detect anomalies.\"\"\"\n    async def wrapper(*args, **kwargs):\n        start_time = time.time()\n        ctx = None\n\n        # Find context\n        for arg in args:\n            if isinstance(arg, Context):\n                ctx = arg\n                break\n\n        try:\n            result = await func(*args, **kwargs)\n            response_time = time.time() - start_time\n\n            # Record and check for anomalies\n            if ctx:\n                client_id = ctx.client_id or \"unknown\"\n                endpoint = func.__name__\n\n                anomaly_detector.record_request(client_id, endpoint, response_time)\n\n                if anomaly_detector.detect_anomalies(client_id, endpoint, response_time):\n                    security_logger.log_suspicious_activity(\n                        client_id,\n                        \"performance_anomaly\",\n                        {\n                            \"endpoint\": endpoint,\n                            \"response_time\": response_time,\n                            \"anomaly_type\": \"slow_response\"\n                        }\n                    )\n\n            return result\n\n        except Exception as e:\n            response_time = time.time() - start_time\n            # Log failed requests\n            if ctx:\n                security_logger.log_suspicious_activity(\n                    ctx.client_id or \"unknown\",\n                    \"request_failure\",\n                    {\n                        \"endpoint\": func.__name__,\n                        \"error\": str(e),\n                        \"response_time\": response_time\n                    }\n                )\n            raise\n\n    return wrapper\n\n@mcp.tool()\n@monitor_performance\nasync def monitored_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with performance monitoring.\"\"\"\n    await asyncio.sleep(0.1)  # Simulate work\n    return f\"Processed: {data}\"\n</code></pre></p>"},{"location":"development/fastmcp/security-guide/#security-best-practices","title":"Security Best Practices","text":""},{"location":"development/fastmcp/security-guide/#1-secure-coding-practices","title":"1. Secure Coding Practices","text":"<pre><code># \u2705 Good: Input validation and sanitization\n@mcp.tool()\nasync def secure_search(query: str, ctx: Context) -&gt; list[str]:\n    \"\"\"Secure search with input validation.\"\"\"\n    # Validate input\n    if not query or len(query) &gt; 1000:\n        raise ValueError(\"Invalid query length\")\n\n    # Sanitize input\n    safe_query = html.escape(query.strip())\n\n    # Use parameterized queries\n    results = await db.fetch(\n        \"SELECT title FROM articles WHERE title ILIKE $1 LIMIT 10\",\n        f\"%{safe_query}%\"\n    )\n\n    return [row[\"title\"] for row in results]\n\n# \u2705 Good: Proper error handling without information disclosure\n@mcp.tool()\nasync def secure_user_lookup(user_id: int, ctx: Context) -&gt; dict:\n    \"\"\"Secure user lookup.\"\"\"\n    try:\n        if user_id &lt;= 0:\n            raise ValueError(\"Invalid user ID\")\n\n        user = await db.fetchrow(\"SELECT id, name FROM users WHERE id = $1\", user_id)\n        if not user:\n            return {\"error\": \"User not found\"}  # Generic message\n\n        return {\"id\": user[\"id\"], \"name\": user[\"name\"]}\n\n    except Exception as e:\n        # Log detailed error for debugging\n        ctx.error(f\"User lookup failed for ID {user_id}: {e}\")\n        # Return generic error to client\n        return {\"error\": \"Lookup failed\"}\n\n# \u2705 Good: Secure file operations\n@mcp.tool()\nasync def secure_file_read(filename: str, ctx: Context) -&gt; str:\n    \"\"\"Secure file reading with path validation.\"\"\"\n    # Validate filename\n    if not filename or '..' in filename or '/' in filename:\n        raise ValueError(\"Invalid filename\")\n\n    # Restrict to safe directory\n    safe_dir = Path(\"/app/data\")\n    file_path = safe_dir / filename\n\n    # Ensure path is within safe directory\n    if not str(file_path.resolve()).startswith(str(safe_dir.resolve())):\n        raise ValueError(\"Path traversal attempt\")\n\n    # Check file exists and is readable\n    if not file_path.exists() or not file_path.is_file():\n        raise ValueError(\"File not found\")\n\n    # Read with size limit\n    if file_path.stat().st_size &gt; 1024 * 1024:  # 1MB limit\n        raise ValueError(\"File too large\")\n\n    return file_path.read_text(encoding='utf-8')\n</code></pre>"},{"location":"development/fastmcp/security-guide/#2-security-donts","title":"2. Security Don'ts","text":"<pre><code># \u274c Bad: No input validation\n@mcp.tool()\nasync def insecure_search(query: str) -&gt; list[str]:\n    # SQL injection vulnerability\n    results = await db.fetch(f\"SELECT * FROM articles WHERE title LIKE '%{query}%'\")\n    return [row[\"title\"] for row in results]\n\n# \u274c Bad: Information disclosure in errors\n@mcp.tool()\nasync def insecure_database_operation(user_id: int) -&gt; dict:\n    try:\n        result = await db.fetchrow(\"SELECT * FROM sensitive_table WHERE id = $1\", user_id)\n        return dict(result)\n    except Exception as e:\n        # Exposes internal information\n        return {\"error\": f\"Database error: {e}\"}\n\n# \u274c Bad: Hardcoded secrets\n@mcp.tool()\nasync def insecure_api_call() -&gt; str:\n    api_key = \"sk-1234567890abcdef\"  # Never hardcode secrets\n    # Use the API key...\n\n# \u274c Bad: No path validation\n@mcp.tool()\nasync def insecure_file_read(filepath: str) -&gt; str:\n    # Path traversal vulnerability\n    with open(filepath, 'r') as f:\n        return f.read()\n\n# \u274c Bad: Logging sensitive data\n@mcp.tool()\nasync def insecure_logging(password: str, ctx: Context) -&gt; str:\n    ctx.info(f\"User password: {password}\")  # Never log secrets\n    return \"Password updated\"\n</code></pre>"},{"location":"development/fastmcp/security-guide/#security-checklist","title":"Security Checklist","text":""},{"location":"development/fastmcp/security-guide/#development-phase","title":"Development Phase","text":"<ul> <li> Input validation on all user inputs</li> <li> Output encoding/escaping</li> <li> Parameterized database queries</li> <li> Secure secret management</li> <li> Error handling without information disclosure</li> <li> Path traversal protection</li> <li> File upload restrictions</li> <li> Rate limiting implementation</li> </ul>"},{"location":"development/fastmcp/security-guide/#deployment-phase","title":"Deployment Phase","text":"<ul> <li> HTTPS/TLS configuration</li> <li> Security headers configuration</li> <li> OAuth2 authentication setup</li> <li> Database connection encryption</li> <li> Log security configuration</li> <li> Monitoring and alerting setup</li> <li> Regular security updates</li> <li> Backup encryption</li> </ul>"},{"location":"development/fastmcp/security-guide/#operations-phase","title":"Operations Phase","text":"<ul> <li> Regular security audits</li> <li> Dependency vulnerability scanning</li> <li> Log monitoring for suspicious activity</li> <li> Access review and cleanup</li> <li> Incident response procedures</li> <li> Security training for team</li> <li> Penetration testing</li> <li> Compliance validation</li> </ul> <p>This security guide provides comprehensive protection strategies for FastMCP servers in production environments.</p>"},{"location":"development/fastmcp/session-management/","title":"Session Management","text":"<p>Advanced guide to session handling in FastMCP servers, covering stateful vs stateless patterns, event store integration, session lifecycle management, and custom session patterns.</p>"},{"location":"development/fastmcp/session-management/#overview","title":"Overview","text":"<p>FastMCP provides sophisticated session management capabilities that vary by transport type. This guide covers advanced session patterns, particularly for StreamableHTTP transport which supports both stateful and stateless session modes.</p>"},{"location":"development/fastmcp/session-management/#session-architecture","title":"Session Architecture","text":"<p>FastMCP's session system has three key components:</p> <ol> <li>ServerSession: Core session implementation with initialization state management</li> <li>Transport Layer: Protocol-specific session handling (stdio, SSE, StreamableHTTP)</li> <li>Session Manager: High-level session lifecycle and coordination</li> </ol>"},{"location":"development/fastmcp/session-management/#session-states","title":"Session States","text":"<pre><code>from enum import Enum\n\nclass InitializationState(Enum):\n    NotInitialized = \"not_initialized\"\n    Initializing = \"initializing\"\n    Initialized = \"initialized\"\n\n# Session progresses through these states during handshake\n</code></pre>"},{"location":"development/fastmcp/session-management/#transport-specific-session-behavior","title":"Transport-Specific Session Behavior","text":""},{"location":"development/fastmcp/session-management/#stdio-transport","title":"Stdio Transport","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\n# Stdio: Single persistent session\nmcp = FastMCP(\"stdio-server\")\n\n# Session lifecycle tied to process lifetime\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")  # One session until process terminates\n</code></pre> <p>Characteristics: - Single session per process - Direct stdin/stdout communication - Session ends when process terminates - No session resumability</p>"},{"location":"development/fastmcp/session-management/#sse-transport","title":"SSE Transport","text":"<pre><code># SSE: Session per browser connection\nmcp = FastMCP(\"sse-server\")\n\n# Each browser connection creates new session\nif __name__ == \"__main__\":\n    mcp.run(\"sse\")  # Session per HTTP connection\n</code></pre> <p>Characteristics: - Session per HTTP connection - Automatic cleanup on disconnect - No built-in resumability - Browser-friendly with automatic reconnection</p>"},{"location":"development/fastmcp/session-management/#streamablehttp-transport","title":"StreamableHTTP Transport","text":"<p>StreamableHTTP offers the most advanced session management with configurable behavior.</p>"},{"location":"development/fastmcp/session-management/#stateful-vs-stateless-sessions","title":"Stateful vs Stateless Sessions","text":""},{"location":"development/fastmcp/session-management/#stateful-sessions-default","title":"Stateful Sessions (Default)","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\n# Stateful mode - sessions persist between requests\nmcp = FastMCP(\"stateful-server\")\n\n# Configure for persistent sessions\nmcp.settings.stateless_http = False  # Default\n\n@mcp.tool()\nasync def stateful_tool(message: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with session-aware behavior.\"\"\"\n    session_id = ctx.session_id if hasattr(ctx, 'session_id') else 'unknown'\n\n    # Session state persists across requests\n    await ctx.info(f\"Processing in session {session_id}\")\n\n    return f\"Processed '{message}' in persistent session\"\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre> <p>Characteristics: - Sessions persist between HTTP requests - Client maintains session ID via headers - Better for long-running conversations - Supports event store integration for resumability</p>"},{"location":"development/fastmcp/session-management/#stateless-sessions","title":"Stateless Sessions","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\n# Stateless mode - new session per request\nmcp = FastMCP(\"stateless-server\")\nmcp.settings.stateless_http = True\n\n@mcp.tool()\nasync def stateless_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool optimized for stateless operation.\"\"\"\n    # Each request creates fresh session\n    # No persistent state between requests\n    return f\"Processed '{data}' in fresh session\"\n\nif __name__ == \"__main__\":\n    mcp.run(\"streamable-http\")\n</code></pre> <p>Characteristics: - Fresh session created per request - No session ID tracking required - Lower memory footprint - Simpler deployment and scaling</p>"},{"location":"development/fastmcp/session-management/#event-store-integration","title":"Event Store Integration","text":"<p>Event stores enable session resumability by persisting session events that can be replayed after disconnection.</p>"},{"location":"development/fastmcp/session-management/#built-in-in-memory-event-store","title":"Built-in In-Memory Event Store","text":"<pre><code>from mcp.server.fastmcp import FastMCP\nfrom mcp.server.streamable_http import InMemoryEventStore\n\n# Create event store for resumability\nevent_store = InMemoryEventStore(max_events_per_stream=1000)\n\n# Configure server with event store\nmcp = FastMCP(\"resumable-server\")\n\n# Event store enables automatic resumability\n@mcp.tool()\nasync def logged_operation(action: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with automatic event logging.\"\"\"\n    # All interactions automatically logged to event store\n    await ctx.info(f\"Executing action: {action}\")\n\n    # Simulate work\n    import asyncio\n    await asyncio.sleep(1)\n\n    await ctx.info(f\"Completed action: {action}\")\n    return f\"Action '{action}' completed successfully\"\n\nif __name__ == \"__main__\":\n    # Events automatically stored and available for replay\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/session-management/#custom-event-store-implementation","title":"Custom Event Store Implementation","text":"<pre><code>from mcp.server.streamable_http import EventStore, EventId, StreamId\nfrom mcp.types import JSONRPCMessage\nimport asyncio\nimport json\n\nclass DatabaseEventStore(EventStore):\n    \"\"\"Custom event store using database backend.\"\"\"\n\n    def __init__(self, db_connection):\n        self.db = db_connection\n        self._next_event_id = 1\n\n    async def store_event(\n        self,\n        stream_id: StreamId,\n        message: JSONRPCMessage\n    ) -&gt; EventId:\n        \"\"\"Store event in database.\"\"\"\n        event_id = EventId(self._next_event_id)\n        self._next_event_id += 1\n\n        # Store in database\n        await self.db.execute(\n            \"INSERT INTO events (event_id, stream_id, message, timestamp) VALUES (?, ?, ?, ?)\",\n            (event_id, stream_id, json.dumps(message.model_dump()), time.time())\n        )\n\n        return event_id\n\n    async def replay_events_after(\n        self,\n        last_event_id: EventId,\n        send_callback\n    ) -&gt; StreamId | None:\n        \"\"\"Replay events from database.\"\"\"\n        cursor = await self.db.execute(\n            \"SELECT stream_id, message FROM events WHERE event_id &gt; ? ORDER BY event_id\",\n            (last_event_id,)\n        )\n\n        stream_id = None\n        async for row in cursor:\n            stream_id = StreamId(row[0])\n            message_data = json.loads(row[1])\n\n            # Reconstruct message and send\n            await send_callback(message_data)\n\n        return stream_id\n\n# Usage with custom event store\nasync def create_db_event_store():\n    # Initialize database connection\n    db = await aiosqlite.connect(\"events.db\")\n    await db.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS events (\n            event_id INTEGER PRIMARY KEY,\n            stream_id TEXT,\n            message TEXT,\n            timestamp REAL\n        )\n    \"\"\")\n    return DatabaseEventStore(db)\n\n# Configure FastMCP with custom event store\nevent_store = await create_db_event_store()\nmcp = FastMCP(\"db-resumable-server\")\n</code></pre>"},{"location":"development/fastmcp/session-management/#advanced-session-patterns","title":"Advanced Session Patterns","text":""},{"location":"development/fastmcp/session-management/#session-aware-tools","title":"Session-Aware Tools","text":"<pre><code>from mcp.server.fastmcp import FastMCP, Context\nfrom collections import defaultdict\nimport time\n\n# Session state storage\nsession_state = defaultdict(dict)\n\nmcp = FastMCP(\"session-aware-server\")\n\n@mcp.tool()\nasync def set_session_data(key: str, value: str, ctx: Context) -&gt; str:\n    \"\"\"Store data in session scope.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n    session_state[session_id][key] = {\n        'value': value,\n        'timestamp': time.time()\n    }\n\n    await ctx.info(f\"Stored {key} in session {session_id}\")\n    return f\"Stored {key} = {value}\"\n\n@mcp.tool()\nasync def get_session_data(key: str, ctx: Context) -&gt; str:\n    \"\"\"Retrieve data from session scope.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    if session_id in session_state and key in session_state[session_id]:\n        data = session_state[session_id][key]\n        await ctx.info(f\"Retrieved {key} from session {session_id}\")\n        return f\"{key} = {data['value']} (stored at {data['timestamp']})\"\n    else:\n        return f\"No data found for key '{key}' in current session\"\n\n@mcp.tool()\nasync def list_session_keys(ctx: Context) -&gt; str:\n    \"\"\"List all keys in current session.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    if session_id in session_state:\n        keys = list(session_state[session_id].keys())\n        return f\"Session {session_id} keys: {keys}\"\n    else:\n        return f\"No data in session {session_id}\"\n</code></pre>"},{"location":"development/fastmcp/session-management/#session-lifecycle-management","title":"Session Lifecycle Management","text":"<pre><code>from contextlib import asynccontextmanager\nfrom typing import Dict, Any\nimport asyncio\n\nclass SessionManager:\n    \"\"\"Custom session lifecycle manager.\"\"\"\n\n    def __init__(self):\n        self.sessions: Dict[str, Dict[str, Any]] = {}\n        self.cleanup_tasks: Dict[str, asyncio.Task] = {}\n\n    async def create_session(self, session_id: str) -&gt; Dict[str, Any]:\n        \"\"\"Initialize new session with cleanup timer.\"\"\"\n        session_data = {\n            'created_at': time.time(),\n            'last_activity': time.time(),\n            'request_count': 0,\n            'data': {}\n        }\n\n        self.sessions[session_id] = session_data\n\n        # Schedule cleanup after inactivity\n        self.cleanup_tasks[session_id] = asyncio.create_task(\n            self._cleanup_after_timeout(session_id, timeout=3600)  # 1 hour\n        )\n\n        return session_data\n\n    async def _cleanup_after_timeout(self, session_id: str, timeout: int):\n        \"\"\"Clean up session after timeout.\"\"\"\n        await asyncio.sleep(timeout)\n\n        if session_id in self.sessions:\n            last_activity = self.sessions[session_id]['last_activity']\n            if time.time() - last_activity &gt;= timeout:\n                await self.cleanup_session(session_id)\n\n    async def cleanup_session(self, session_id: str):\n        \"\"\"Remove session and cancel cleanup task.\"\"\"\n        if session_id in self.sessions:\n            del self.sessions[session_id]\n\n        if session_id in self.cleanup_tasks:\n            self.cleanup_tasks[session_id].cancel()\n            del self.cleanup_tasks[session_id]\n\n    def update_activity(self, session_id: str):\n        \"\"\"Update last activity timestamp.\"\"\"\n        if session_id in self.sessions:\n            self.sessions[session_id]['last_activity'] = time.time()\n            self.sessions[session_id]['request_count'] += 1\n\n# Global session manager\nsession_manager = SessionManager()\n\n@mcp.tool()\nasync def managed_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with managed session lifecycle.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    # Ensure session exists\n    if session_id not in session_manager.sessions:\n        await session_manager.create_session(session_id)\n\n    # Update activity\n    session_manager.update_activity(session_id)\n\n    # Access session data\n    session_data = session_manager.sessions[session_id]\n\n    return f\"Processed '{data}' (request #{session_data['request_count']})\"\n</code></pre>"},{"location":"development/fastmcp/session-management/#multi-session-coordination","title":"Multi-Session Coordination","text":"<pre><code>from typing import Set\nimport asyncio\n\nclass MultiSessionCoordinator:\n    \"\"\"Coordinate actions across multiple sessions.\"\"\"\n\n    def __init__(self):\n        self.active_sessions: Set[str] = set()\n        self.broadcast_queue = asyncio.Queue()\n        self.session_contexts: Dict[str, Context] = {}\n\n    async def register_session(self, session_id: str, ctx: Context):\n        \"\"\"Register session for coordination.\"\"\"\n        self.active_sessions.add(session_id)\n        self.session_contexts[session_id] = ctx\n\n    async def unregister_session(self, session_id: str):\n        \"\"\"Unregister session.\"\"\"\n        self.active_sessions.discard(session_id)\n        self.session_contexts.pop(session_id, None)\n\n    async def broadcast_message(self, message: str, exclude_session: str = None):\n        \"\"\"Broadcast message to all active sessions.\"\"\"\n        for session_id in self.active_sessions:\n            if session_id != exclude_session:\n                ctx = self.session_contexts.get(session_id)\n                if ctx:\n                    try:\n                        await ctx.info(f\"Broadcast: {message}\")\n                    except Exception as e:\n                        # Handle failed delivery\n                        await self.unregister_session(session_id)\n\ncoordinator = MultiSessionCoordinator()\n\n@mcp.tool()\nasync def join_coordination(ctx: Context) -&gt; str:\n    \"\"\"Join multi-session coordination.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n    await coordinator.register_session(session_id, ctx)\n\n    await coordinator.broadcast_message(\n        f\"Session {session_id} joined\",\n        exclude_session=session_id\n    )\n\n    return f\"Joined coordination as session {session_id}\"\n\n@mcp.tool()\nasync def broadcast_to_sessions(message: str, ctx: Context) -&gt; str:\n    \"\"\"Broadcast message to all other sessions.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    await coordinator.broadcast_message(message, exclude_session=session_id)\n\n    return f\"Broadcasted '{message}' to {len(coordinator.active_sessions) - 1} other sessions\"\n</code></pre>"},{"location":"development/fastmcp/session-management/#session-security-patterns","title":"Session Security Patterns","text":""},{"location":"development/fastmcp/session-management/#session-based-authentication","title":"Session-Based Authentication","text":"<pre><code>from mcp.server.auth.middleware import get_access_token\nfrom mcp.server.fastmcp import Context\n\n# Session authentication state\nauthenticated_sessions: Set[str] = set()\n\n@mcp.tool()\nasync def secure_login(username: str, password: str, ctx: Context) -&gt; str:\n    \"\"\"Authenticate current session.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    # Validate credentials (implement your auth logic)\n    if await validate_credentials(username, password):\n        authenticated_sessions.add(session_id)\n        await ctx.info(f\"Session {session_id} authenticated as {username}\")\n        return f\"Authenticated as {username}\"\n    else:\n        await ctx.error(\"Authentication failed\")\n        raise ValueError(\"Invalid credentials\")\n\n@mcp.tool()\nasync def secure_operation(action: str, ctx: Context) -&gt; str:\n    \"\"\"Operation requiring session authentication.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    # Check session authentication\n    if session_id not in authenticated_sessions:\n        raise ValueError(\"Session not authenticated. Call secure_login first.\")\n\n    # Also check OAuth token if available\n    access_token = get_access_token()\n    if access_token and \"admin\" not in access_token.scopes:\n        raise ValueError(\"Insufficient permissions\")\n\n    await ctx.info(f\"Executing secure action: {action}\")\n    return f\"Executed secure action: {action}\"\n\n@mcp.tool()\nasync def logout(ctx: Context) -&gt; str:\n    \"\"\"Logout current session.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n    authenticated_sessions.discard(session_id)\n\n    await ctx.info(f\"Session {session_id} logged out\")\n    return \"Logged out successfully\"\n</code></pre>"},{"location":"development/fastmcp/session-management/#session-data-isolation","title":"Session Data Isolation","text":"<pre><code>import hashlib\nfrom typing import Dict, Any\n\nclass SecureSessionStorage:\n    \"\"\"Secure session data storage with encryption.\"\"\"\n\n    def __init__(self, encryption_key: str):\n        self.encryption_key = encryption_key.encode()\n        self._storage: Dict[str, Dict[str, str]] = {}\n\n    def _encrypt_data(self, data: str) -&gt; str:\n        \"\"\"Simple encryption (use proper crypto in production).\"\"\"\n        import base64\n        return base64.b64encode(data.encode()).decode()\n\n    def _decrypt_data(self, encrypted_data: str) -&gt; str:\n        \"\"\"Simple decryption (use proper crypto in production).\"\"\"\n        import base64\n        return base64.b64decode(encrypted_data.encode()).decode()\n\n    async def store_secure_data(\n        self,\n        session_id: str,\n        key: str,\n        value: str\n    ) -&gt; None:\n        \"\"\"Store encrypted data for session.\"\"\"\n        if session_id not in self._storage:\n            self._storage[session_id] = {}\n\n        encrypted_value = self._encrypt_data(value)\n        self._storage[session_id][key] = encrypted_value\n\n    async def get_secure_data(\n        self,\n        session_id: str,\n        key: str\n    ) -&gt; str | None:\n        \"\"\"Retrieve and decrypt data for session.\"\"\"\n        if session_id in self._storage and key in self._storage[session_id]:\n            encrypted_value = self._storage[session_id][key]\n            return self._decrypt_data(encrypted_value)\n        return None\n\nsecure_storage = SecureSessionStorage(\"your-encryption-key\")\n\n@mcp.tool()\nasync def store_sensitive_data(\n    key: str,\n    value: str,\n    ctx: Context\n) -&gt; str:\n    \"\"\"Store sensitive data securely in session.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    await secure_storage.store_secure_data(session_id, key, value)\n    await ctx.info(f\"Stored sensitive data for key: {key}\")\n\n    return f\"Securely stored data for key: {key}\"\n\n@mcp.tool()\nasync def retrieve_sensitive_data(key: str, ctx: Context) -&gt; str:\n    \"\"\"Retrieve sensitive data from session.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    value = await secure_storage.get_secure_data(session_id, key)\n\n    if value:\n        await ctx.info(f\"Retrieved sensitive data for key: {key}\")\n        return f\"Retrieved: {value}\"\n    else:\n        return f\"No data found for key: {key}\"\n</code></pre>"},{"location":"development/fastmcp/session-management/#configuration-and-environment-variables","title":"Configuration and Environment Variables","text":""},{"location":"development/fastmcp/session-management/#session-configuration","title":"Session Configuration","text":"<pre><code>from mcp.server.fastmcp import FastMCP\nimport os\n\n# Configure via environment variables\nos.environ.update({\n    'FASTMCP_STATELESS_HTTP': 'false',  # Enable stateful sessions\n    'FASTMCP_JSON_RESPONSE': 'true',    # JSON responses\n    'FASTMCP_HOST': '0.0.0.0',\n    'FASTMCP_PORT': '8080'\n})\n\nmcp = FastMCP(\"configured-server\")\n\n# Or configure programmatically\nmcp.settings.stateless_http = False\nmcp.settings.json_response = True\nmcp.settings.host = \"0.0.0.0\"\nmcp.settings.port = 8080\n\n@mcp.tool()\nasync def get_server_config(ctx: Context) -&gt; dict:\n    \"\"\"Return current server configuration.\"\"\"\n    return {\n        \"stateless_http\": mcp.settings.stateless_http,\n        \"json_response\": mcp.settings.json_response,\n        \"host\": mcp.settings.host,\n        \"port\": mcp.settings.port,\n        \"mount_path\": mcp.settings.mount_path,\n        \"streamable_http_path\": mcp.settings.streamable_http_path\n    }\n</code></pre>"},{"location":"development/fastmcp/session-management/#performance-considerations","title":"Performance Considerations","text":""},{"location":"development/fastmcp/session-management/#session-memory-management","title":"Session Memory Management","text":"<pre><code>import asyncio\nimport weakref\nfrom typing import WeakSet\n\nclass SessionResourceManager:\n    \"\"\"Manage session resources and prevent memory leaks.\"\"\"\n\n    def __init__(self):\n        self._session_resources: Dict[str, WeakSet] = defaultdict(weakref.WeakSet)\n        self._cleanup_interval = 300  # 5 minutes\n        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())\n\n    async def register_resource(self, session_id: str, resource):\n        \"\"\"Register resource for cleanup when session ends.\"\"\"\n        self._session_resources[session_id].add(resource)\n\n    async def cleanup_session_resources(self, session_id: str):\n        \"\"\"Clean up all resources for a session.\"\"\"\n        if session_id in self._session_resources:\n            resources = list(self._session_resources[session_id])\n            for resource in resources:\n                try:\n                    if hasattr(resource, 'close'):\n                        await resource.close()\n                    elif hasattr(resource, '__aexit__'):\n                        await resource.__aexit__(None, None, None)\n                except Exception as e:\n                    # Log cleanup errors\n                    print(f\"Error cleaning up resource: {e}\")\n\n            del self._session_resources[session_id]\n\n    async def _periodic_cleanup(self):\n        \"\"\"Periodically clean up dead references.\"\"\"\n        while True:\n            await asyncio.sleep(self._cleanup_interval)\n\n            # Clean up empty weak sets\n            empty_sessions = [\n                session_id for session_id, resources in self._session_resources.items()\n                if len(resources) == 0\n            ]\n\n            for session_id in empty_sessions:\n                del self._session_resources[session_id]\n\nresource_manager = SessionResourceManager()\n\n@mcp.tool()\nasync def create_managed_resource(resource_type: str, ctx: Context) -&gt; str:\n    \"\"\"Create resource with automatic cleanup.\"\"\"\n    session_id = getattr(ctx, 'session_id', 'default')\n\n    # Create resource (example: file handle)\n    if resource_type == \"file\":\n        resource = open(f\"/tmp/session_{session_id}.txt\", \"w\")\n        await resource_manager.register_resource(session_id, resource)\n        return f\"Created file resource for session {session_id}\"\n\n    return f\"Unknown resource type: {resource_type}\"\n</code></pre>"},{"location":"development/fastmcp/session-management/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/session-management/#session-design-guidelines","title":"Session Design Guidelines","text":"<ol> <li>Choose Appropriate Session Mode:</li> <li>Use stateless for simple, independent requests</li> <li> <p>Use stateful for conversational or multi-step workflows</p> </li> <li> <p>Implement Proper Cleanup:</p> </li> <li>Always clean up session resources</li> <li>Use weak references to prevent memory leaks</li> <li> <p>Implement session timeouts for inactive sessions</p> </li> <li> <p>Security Considerations:</p> </li> <li>Validate session IDs and implement proper authentication</li> <li>Encrypt sensitive session data</li> <li> <p>Implement session isolation between users</p> </li> <li> <p>Performance Optimization:</p> </li> <li>Monitor session memory usage</li> <li>Implement efficient session storage</li> <li>Use connection pooling for external resources</li> </ol>"},{"location":"development/fastmcp/session-management/#error-handling","title":"Error Handling","text":"<pre><code>@mcp.tool()\nasync def robust_session_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with comprehensive session error handling.\"\"\"\n    try:\n        session_id = getattr(ctx, 'session_id', 'unknown')\n\n        # Validate session state\n        if not session_id or session_id == 'unknown':\n            await ctx.error(\"Invalid session state\")\n            raise ValueError(\"Session ID not available\")\n\n        # Process with proper error handling\n        result = await process_session_data(session_id, data)\n\n        await ctx.info(f\"Successfully processed data in session {session_id}\")\n        return result\n\n    except Exception as e:\n        await ctx.error(f\"Error in session tool: {e}\")\n\n        # Attempt session recovery\n        try:\n            await recover_session_state(session_id)\n            await ctx.info(\"Session state recovered\")\n        except Exception as recovery_error:\n            await ctx.error(f\"Session recovery failed: {recovery_error}\")\n\n        raise  # Re-raise original exception\n</code></pre>"},{"location":"development/fastmcp/session-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/fastmcp/session-management/#common-session-issues","title":"Common Session Issues","text":"<ol> <li>Session ID Not Available:</li> <li>Check transport type (stdio doesn't have session IDs)</li> <li>Verify client is sending proper session headers</li> <li> <p>Ensure stateful mode is enabled for StreamableHTTP</p> </li> <li> <p>Session State Loss:</p> </li> <li>Implement event store for resumability</li> <li>Check session timeout settings</li> <li> <p>Verify session storage implementation</p> </li> <li> <p>Memory Leaks:</p> </li> <li>Implement proper session cleanup</li> <li>Use weak references for session data</li> <li> <p>Monitor session count and memory usage</p> </li> <li> <p>Authentication Issues:</p> </li> <li>Verify auth middleware configuration</li> <li>Check token validation logic</li> <li>Ensure proper scope checking</li> </ol> <p>This guide provides comprehensive patterns for implementing advanced session management in FastMCP servers. Choose the appropriate patterns based on your application's requirements for state persistence, security, and performance.</p>"},{"location":"development/fastmcp/testing-guide/","title":"Testing Guide","text":"<p>Comprehensive guide for testing FastMCP servers, including unit tests, integration tests, mocking strategies, and testing best practices.</p>"},{"location":"development/fastmcp/testing-guide/#quick-start","title":"Quick Start","text":""},{"location":"development/fastmcp/testing-guide/#basic-test-setup","title":"Basic Test Setup","text":"<p>Install testing dependencies: <pre><code>uv add --dev pytest pytest-anyio pytest-examples\nuv add --dev pytest-mock pytest-cov  # Optional: mocking and coverage\n</code></pre></p> <p>Configure pytest in <code>pyproject.toml</code>: <pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\naddopts = [\n    \"--anyio-backends=asyncio\",\n    \"--strict-markers\",\n    \"--strict-config\",\n    \"-ra\"\n]\nmarkers = [\n    \"anyio: mark test as async\",\n    \"integration: mark test as integration test\"\n]\n</code></pre></p> <p>Basic test structure: <pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 conftest.py              # Shared fixtures\n\u251c\u2500\u2500 test_tools.py           # Tool tests\n\u251c\u2500\u2500 test_resources.py       # Resource tests\n\u251c\u2500\u2500 test_prompts.py         # Prompt tests\n\u251c\u2500\u2500 test_integration.py     # Integration tests\n\u2514\u2500\u2500 test_performance.py     # Performance tests\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#essential-conftestpy","title":"Essential conftest.py","text":"<p>Create <code>tests/conftest.py</code> with shared fixtures:</p> <pre><code>import pytest\nfrom mcp.server.fastmcp import FastMCP\n\n@pytest.fixture\ndef anyio_backend():\n    \"\"\"Configure anyio backend for async tests.\"\"\"\n    return \"asyncio\"\n\n@pytest.fixture\ndef test_server():\n    \"\"\"Create a test FastMCP server.\"\"\"\n    return FastMCP(\"Test Server\", debug=True)\n\n@pytest.fixture\ndef sample_data():\n    \"\"\"Sample test data.\"\"\"\n    return {\n        \"users\": [\n            {\"id\": 1, \"name\": \"Alice\", \"email\": \"alice@example.com\"},\n            {\"id\": 2, \"name\": \"Bob\", \"email\": \"bob@example.com\"},\n        ]\n    }\n</code></pre>"},{"location":"development/fastmcp/testing-guide/#unit-testing","title":"Unit Testing","text":""},{"location":"development/fastmcp/testing-guide/#1-testing-tools","title":"1. Testing Tools","text":"<p>Basic tool testing: <pre><code>import pytest\nfrom mcp.server.fastmcp import FastMCP, Context\n\nclass TestTools:\n    @pytest.mark.anyio\n    async def test_simple_tool(self, test_server):\n        \"\"\"Test a simple synchronous tool.\"\"\"\n\n        @test_server.tool()\n        def add_numbers(a: int, b: int) -&gt; int:\n            \"\"\"Add two numbers.\"\"\"\n            return a + b\n\n        # Test tool registration\n        tools = await test_server.list_tools()\n        assert len(tools) == 1\n        assert tools[0].name == \"add_numbers\"\n        assert tools[0].description == \"Add two numbers.\"\n\n        # Test tool execution\n        result = await test_server.call_tool(\"add_numbers\", {\"a\": 5, \"b\": 3})\n        assert result[0].text == \"8\"\n\n    @pytest.mark.anyio\n    async def test_async_tool(self, test_server):\n        \"\"\"Test an async tool.\"\"\"\n\n        @test_server.tool()\n        async def fetch_data(url: str) -&gt; str:\n            \"\"\"Fetch data from URL.\"\"\"\n            # Simulate async operation\n            import asyncio\n            await asyncio.sleep(0.01)\n            return f\"Data from {url}\"\n\n        result = await test_server.call_tool(\"fetch_data\", {\"url\": \"https://api.example.com\"})\n        assert result[0].text == \"Data from https://api.example.com\"\n\n    @pytest.mark.anyio\n    async def test_tool_with_context(self, test_server):\n        \"\"\"Test tool that uses context.\"\"\"\n\n        context_messages = []\n\n        @test_server.tool()\n        async def logging_tool(message: str, ctx: Context) -&gt; str:\n            \"\"\"Tool that uses context for logging.\"\"\"\n            await ctx.info(f\"Processing: {message}\")\n            context_messages.append(f\"Processing: {message}\")\n            return f\"Processed: {message}\"\n\n        # Create a mock context to capture logging\n        from unittest.mock import AsyncMock\n        mock_context = AsyncMock(spec=Context)\n\n        # Get the tool and run it directly with mock context\n        tool = test_server._tool_manager.get_tool(\"logging_tool\")\n        result = await tool.run({\"message\": \"test\"}, context=mock_context)\n\n        assert result == \"Processed: test\"\n        mock_context.info.assert_called_once_with(\"Processing: test\")\n\n    @pytest.mark.anyio\n    async def test_tool_error_handling(self, test_server):\n        \"\"\"Test tool error handling.\"\"\"\n\n        @test_server.tool()\n        def error_tool(should_fail: bool) -&gt; str:\n            \"\"\"Tool that can fail.\"\"\"\n            if should_fail:\n                raise ValueError(\"Tool failed!\")\n            return \"success\"\n\n        # Test successful execution\n        result = await test_server.call_tool(\"error_tool\", {\"should_fail\": False})\n        assert result[0].text == \"success\"\n\n        # Test error handling\n        with pytest.raises(Exception):  # FastMCP wraps errors\n            await test_server.call_tool(\"error_tool\", {\"should_fail\": True})\n\n    def test_tool_validation(self, test_server):\n        \"\"\"Test tool parameter validation.\"\"\"\n        from pydantic import Field\n\n        @test_server.tool()\n        def validated_tool(\n            name: str = Field(min_length=2, max_length=50),\n            age: int = Field(ge=0, le=150)\n        ) -&gt; str:\n            \"\"\"Tool with validation.\"\"\"\n            return f\"{name} is {age} years old\"\n\n        # Test schema generation\n        tools = test_server._tool_manager.list_tools()\n        tool_info = tools[0]\n\n        assert \"name\" in tool_info.parameters[\"properties\"]\n        assert \"age\" in tool_info.parameters[\"properties\"]\n        assert tool_info.parameters[\"properties\"][\"name\"][\"minLength\"] == 2\n        assert tool_info.parameters[\"properties\"][\"age\"][\"minimum\"] == 0\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#2-testing-resources","title":"2. Testing Resources","text":"<p>Resource testing patterns: <pre><code>import pytest\nfrom mcp.server.fastmcp import FastMCP\n\nclass TestResources:\n    @pytest.mark.anyio\n    async def test_static_resource(self, test_server):\n        \"\"\"Test static resource.\"\"\"\n\n        @test_server.resource(\"data://config\")\n        def get_config() -&gt; str:\n            \"\"\"Server configuration.\"\"\"\n            return \"debug=true\\nport=8000\"\n\n        # Test resource listing\n        resources = await test_server.list_resources()\n        assert len(resources) == 1\n        assert str(resources[0].uri) == \"data://config\"\n        assert resources[0].mime_type == \"text/plain\"\n\n        # Test resource reading\n        content = await test_server.read_resource(\"data://config\")\n        assert content[0].content == \"debug=true\\nport=8000\"\n\n    @pytest.mark.anyio\n    async def test_template_resource(self, test_server, sample_data):\n        \"\"\"Test template resource with parameters.\"\"\"\n\n        @test_server.resource(\"data://users/{user_id}\")\n        def get_user(user_id: int) -&gt; dict:\n            \"\"\"Get user by ID.\"\"\"\n            users = sample_data[\"users\"]\n            user = next((u for u in users if u[\"id\"] == user_id), None)\n            if not user:\n                raise ValueError(f\"User {user_id} not found\")\n            return user\n\n        # Test template listing\n        templates = await test_server.list_resource_templates()\n        assert len(templates) == 1\n        assert templates[0].uriTemplate == \"data://users/{user_id}\"\n\n        # Test resource access\n        content = await test_server.read_resource(\"data://users/1\")\n        import json\n        data = json.loads(content[0].content)\n        assert data[\"name\"] == \"Alice\"\n        assert data[\"email\"] == \"alice@example.com\"\n\n    @pytest.mark.anyio\n    async def test_binary_resource(self, test_server):\n        \"\"\"Test binary resource.\"\"\"\n\n        @test_server.resource(\"data://image\", mime_type=\"image/png\")\n        def get_image() -&gt; bytes:\n            \"\"\"Get a test image.\"\"\"\n            # Create a minimal PNG header (simplified)\n            return b'\\x89PNG\\r\\n\\x1a\\n' + b'\\x00' * 100\n\n        content = await test_server.read_resource(\"data://image\")\n        assert isinstance(content[0].content, bytes)\n        assert content[0].mime_type == \"image/png\"\n\n    @pytest.mark.anyio\n    async def test_async_resource(self, test_server):\n        \"\"\"Test async resource.\"\"\"\n\n        @test_server.resource(\"data://async\")\n        async def get_async_data() -&gt; str:\n            \"\"\"Get data asynchronously.\"\"\"\n            import asyncio\n            await asyncio.sleep(0.01)  # Simulate async operation\n            return \"async data\"\n\n        content = await test_server.read_resource(\"data://async\")\n        assert content[0].content == \"async data\"\n\n    @pytest.mark.anyio\n    async def test_resource_error_handling(self, test_server):\n        \"\"\"Test resource error handling.\"\"\"\n\n        @test_server.resource(\"data://error\")\n        def error_resource() -&gt; str:\n            \"\"\"Resource that fails.\"\"\"\n            raise RuntimeError(\"Resource error!\")\n\n        with pytest.raises(Exception):  # FastMCP wraps resource errors\n            await test_server.read_resource(\"data://error\")\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#3-testing-prompts","title":"3. Testing Prompts","text":"<p>Prompt testing patterns: <pre><code>import pytest\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.prompts.base import UserMessage, AssistantMessage\n\nclass TestPrompts:\n    @pytest.mark.anyio\n    async def test_simple_prompt(self, test_server):\n        \"\"\"Test simple prompt.\"\"\"\n\n        @test_server.prompt()\n        def greeting_prompt(name: str) -&gt; str:\n            \"\"\"Generate a greeting prompt.\"\"\"\n            return f\"Hello, {name}! How are you doing today?\"\n\n        # Test prompt listing\n        prompts = await test_server.list_prompts()\n        assert len(prompts) == 1\n        assert prompts[0].name == \"greeting_prompt\"\n\n        # Test prompt rendering\n        result = await test_server.get_prompt(\"greeting_prompt\", {\"name\": \"Alice\"})\n        messages = result.messages\n        assert len(messages) == 1\n        assert messages[0][\"content\"][\"text\"] == \"Hello, Alice! How are you doing today?\"\n\n    @pytest.mark.anyio\n    async def test_prompt_with_messages(self, test_server):\n        \"\"\"Test prompt returning Message objects.\"\"\"\n\n        @test_server.prompt()\n        def conversation_prompt(topic: str) -&gt; list:\n            \"\"\"Generate a conversation prompt.\"\"\"\n            return [\n                UserMessage(f\"Let's discuss {topic}\"),\n                AssistantMessage(\"I'd be happy to discuss that with you!\")\n            ]\n\n        result = await test_server.get_prompt(\"conversation_prompt\", {\"topic\": \"Python\"})\n        messages = result.messages\n        assert len(messages) == 2\n        assert messages[0][\"role\"] == \"user\"\n        assert messages[1][\"role\"] == \"assistant\"\n\n    @pytest.mark.anyio\n    async def test_async_prompt(self, test_server):\n        \"\"\"Test async prompt.\"\"\"\n\n        @test_server.prompt()\n        async def async_prompt(query: str) -&gt; str:\n            \"\"\"Generate prompt asynchronously.\"\"\"\n            import asyncio\n            await asyncio.sleep(0.01)  # Simulate async operation\n            return f\"Please analyze: {query}\"\n\n        result = await test_server.get_prompt(\"async_prompt\", {\"query\": \"data trends\"})\n        assert \"Please analyze: data trends\" in result.messages[0][\"content\"][\"text\"]\n\n    @pytest.mark.anyio\n    async def test_prompt_validation(self, test_server):\n        \"\"\"Test prompt parameter validation.\"\"\"\n\n        @test_server.prompt()\n        def validated_prompt(required_param: str, optional_param: str = \"default\") -&gt; str:\n            \"\"\"Prompt with required and optional parameters.\"\"\"\n            return f\"{required_param} - {optional_param}\"\n\n        # Test with required parameter\n        result = await test_server.get_prompt(\"validated_prompt\", {\"required_param\": \"test\"})\n        assert \"test - default\" in result.messages[0][\"content\"][\"text\"]\n\n        # Test missing required parameter\n        with pytest.raises(Exception):\n            await test_server.get_prompt(\"validated_prompt\", {})\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#integration-testing","title":"Integration Testing","text":""},{"location":"development/fastmcp/testing-guide/#1-end-to-end-testing","title":"1. End-to-End Testing","text":"<p>Test complete client-server interaction: <pre><code>import pytest\nfrom mcp.shared.memory import create_connected_server_and_client_session\n\nclass TestIntegration:\n    @pytest.mark.anyio\n    async def test_full_server_interaction(self, test_server, sample_data):\n        \"\"\"Test complete server interaction.\"\"\"\n\n        # Set up server\n        @test_server.tool()\n        def get_user_count() -&gt; int:\n            \"\"\"Get total user count.\"\"\"\n            return len(sample_data[\"users\"])\n\n        @test_server.resource(\"data://summary\")\n        def get_summary() -&gt; str:\n            \"\"\"Get data summary.\"\"\"\n            return f\"Total users: {len(sample_data['users'])}\"\n\n        @test_server.prompt()\n        def analysis_prompt() -&gt; str:\n            \"\"\"Generate analysis prompt.\"\"\"\n            return \"Please analyze the user data.\"\n\n        # Test with connected session\n        async with create_connected_server_and_client_session(test_server._mcp_server) as (\n            server_session,\n            client_session,\n        ):\n            # Test initialization\n            await client_session.initialize()\n\n            # Test tool listing and execution\n            tools_result = await client_session.list_tools()\n            assert len(tools_result.tools) == 1\n\n            tool_result = await client_session.call_tool(\"get_user_count\")\n            assert tool_result.content[0].text == \"2\"\n\n            # Test resource listing and reading\n            resources_result = await client_session.list_resources()\n            assert len(resources_result.resources) == 1\n\n            resource_result = await client_session.read_resource(\"data://summary\")\n            assert resource_result.contents[0].text == \"Total users: 2\"\n\n            # Test prompt listing and rendering\n            prompts_result = await client_session.list_prompts()\n            assert len(prompts_result.prompts) == 1\n\n            prompt_result = await client_session.get_prompt(\"analysis_prompt\")\n            assert \"analyze the user data\" in prompt_result.messages[0][\"content\"][\"text\"]\n\n    @pytest.mark.anyio\n    async def test_error_propagation(self, test_server):\n        \"\"\"Test that errors propagate correctly through the protocol.\"\"\"\n\n        @test_server.tool()\n        def failing_tool() -&gt; str:\n            \"\"\"Tool that always fails.\"\"\"\n            raise ValueError(\"This tool always fails\")\n\n        async with create_connected_server_and_client_session(test_server._mcp_server) as (\n            server_session,\n            client_session,\n        ):\n            await client_session.initialize()\n\n            with pytest.raises(Exception):\n                await client_session.call_tool(\"failing_tool\")\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#2-http-transport-testing","title":"2. HTTP Transport Testing","text":"<p>Test HTTP transports: <pre><code>import pytest\nimport asyncio\nimport socket\nfrom mcp.client.sse import sse_client\nfrom mcp.client.session import ClientSession\n\nclass TestHTTPTransport:\n    @pytest.fixture\n    def server_port(self):\n        \"\"\"Get a free port for testing.\"\"\"\n        with socket.socket() as s:\n            s.bind((\"127.0.0.1\", 0))\n            return s.getsockname()[1]\n\n    @pytest.mark.anyio\n    async def test_sse_transport(self, test_server, server_port):\n        \"\"\"Test SSE transport.\"\"\"\n\n        @test_server.tool()\n        def ping() -&gt; str:\n            \"\"\"Simple ping tool.\"\"\"\n            return \"pong\"\n\n        # Configure server for testing\n        test_server.settings.port = server_port\n\n        # Start server in background\n        server_task = asyncio.create_task(test_server.run_sse_async())\n\n        try:\n            # Wait for server to start\n            await asyncio.sleep(0.1)\n\n            # Test client connection\n            async with sse_client(f\"http://127.0.0.1:{server_port}\") as (read, write):\n                async with ClientSession(read, write) as session:\n                    await session.initialize()\n\n                    result = await session.call_tool(\"ping\")\n                    assert result.content[0].text == \"pong\"\n        finally:\n            server_task.cancel()\n            try:\n                await server_task\n            except asyncio.CancelledError:\n                pass\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#mocking-and-fixtures","title":"Mocking and Fixtures","text":""},{"location":"development/fastmcp/testing-guide/#1-mocking-external-dependencies","title":"1. Mocking External Dependencies","text":"<p>Mock external API calls: <pre><code>import pytest\nfrom unittest.mock import AsyncMock, patch\nimport httpx\n\nclass TestExternalAPIs:\n    @pytest.mark.anyio\n    async def test_tool_with_api_call(self, test_server):\n        \"\"\"Test tool that makes external API calls.\"\"\"\n\n        @test_server.tool()\n        async def fetch_weather(city: str) -&gt; str:\n            \"\"\"Fetch weather for a city.\"\"\"\n            async with httpx.AsyncClient() as client:\n                response = await client.get(f\"https://api.weather.com/{city}\")\n                data = response.json()\n                return f\"Weather in {city}: {data['temperature']}\u00b0C\"\n\n        # Mock the HTTP client\n        with patch('httpx.AsyncClient') as mock_client:\n            mock_response = AsyncMock()\n            mock_response.json.return_value = {\"temperature\": 22}\n            mock_client.return_value.__aenter__.return_value.get.return_value = mock_response\n\n            result = await test_server.call_tool(\"fetch_weather\", {\"city\": \"London\"})\n            assert result[0].text == \"Weather in London: 22\u00b0C\"\n\n    @pytest.mark.anyio\n    async def test_tool_with_database(self, test_server):\n        \"\"\"Test tool that uses database.\"\"\"\n\n        @test_server.tool()\n        async def get_user_from_db(user_id: int) -&gt; str:\n            \"\"\"Get user from database.\"\"\"\n            # This would normally connect to a real database\n            import asyncpg\n            conn = await asyncpg.connect(\"postgresql://...\")\n            user = await conn.fetchrow(\"SELECT name FROM users WHERE id = $1\", user_id)\n            return user[\"name\"]\n\n        # Mock the database connection\n        with patch('asyncpg.connect') as mock_connect:\n            mock_conn = AsyncMock()\n            mock_conn.fetchrow.return_value = {\"name\": \"Alice\"}\n            mock_connect.return_value = mock_conn\n\n            result = await test_server.call_tool(\"get_user_from_db\", {\"user_id\": 1})\n            assert result[0].text == \"Alice\"\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#2-test-fixtures","title":"2. Test Fixtures","text":"<p>Create reusable test fixtures: <pre><code>import pytest\nimport tempfile\nfrom pathlib import Path\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for tests.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        yield Path(temp_dir)\n\n@pytest.fixture\ndef sample_files(temp_dir):\n    \"\"\"Create sample files for testing.\"\"\"\n    (temp_dir / \"data.txt\").write_text(\"sample data\")\n    (temp_dir / \"config.json\").write_text('{\"setting\": \"value\"}')\n    return temp_dir\n\n@pytest.fixture\ndef mock_database():\n    \"\"\"Mock database with sample data.\"\"\"\n    return {\n        \"users\": [\n            {\"id\": 1, \"name\": \"Alice\", \"active\": True},\n            {\"id\": 2, \"name\": \"Bob\", \"active\": False},\n        ]\n    }\n\nclass TestWithFixtures:\n    @pytest.mark.anyio\n    async def test_file_operations(self, test_server, sample_files):\n        \"\"\"Test file operations with fixtures.\"\"\"\n\n        @test_server.tool()\n        def read_file(filename: str) -&gt; str:\n            \"\"\"Read a file.\"\"\"\n            return (sample_files / filename).read_text()\n\n        result = await test_server.call_tool(\"read_file\", {\"filename\": \"data.txt\"})\n        assert result[0].text == \"sample data\"\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#3-context-mocking","title":"3. Context Mocking","text":"<p>Mock FastMCP Context: <pre><code>import pytest\nfrom unittest.mock import AsyncMock, Mock\nfrom mcp.server.fastmcp import Context\n\n@pytest.fixture\ndef mock_context():\n    \"\"\"Create a mock Context for testing.\"\"\"\n    ctx = AsyncMock(spec=Context)\n    ctx.info = AsyncMock()\n    ctx.debug = AsyncMock()\n    ctx.warning = AsyncMock()\n    ctx.error = AsyncMock()\n    ctx.report_progress = AsyncMock()\n    ctx.request_id = \"test-request-123\"\n    ctx.client_id = \"test-client\"\n    return ctx\n\nclass TestContextUsage:\n    @pytest.mark.anyio\n    async def test_tool_logging(self, test_server, mock_context):\n        \"\"\"Test tool that uses context for logging.\"\"\"\n\n        @test_server.tool()\n        async def logged_operation(data: str, ctx: Context) -&gt; str:\n            \"\"\"Operation with comprehensive logging.\"\"\"\n            await ctx.info(f\"Starting operation with: {data}\")\n            await ctx.report_progress(0, 100, \"Starting\")\n\n            # Simulate work\n            result = data.upper()\n\n            await ctx.report_progress(100, 100, \"Complete\")\n            await ctx.info(f\"Operation completed: {result}\")\n            return result\n\n        # Test the tool directly with mock context\n        tool = test_server._tool_manager.get_tool(\"logged_operation\")\n        result = await tool.run({\"data\": \"test\"}, context=mock_context)\n\n        assert result == \"TEST\"\n\n        # Verify logging calls\n        mock_context.info.assert_any_call(\"Starting operation with: test\")\n        mock_context.info.assert_any_call(\"Operation completed: TEST\")\n        mock_context.report_progress.assert_any_call(0, 100, \"Starting\")\n        mock_context.report_progress.assert_any_call(100, 100, \"Complete\")\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#performance-testing","title":"Performance Testing","text":""},{"location":"development/fastmcp/testing-guide/#1-load-testing","title":"1. Load Testing","text":"<p>Test server under load: <pre><code>import pytest\nimport asyncio\nimport time\n\nclass TestPerformance:\n    @pytest.mark.anyio\n    async def test_concurrent_tool_calls(self, test_server):\n        \"\"\"Test handling multiple concurrent tool calls.\"\"\"\n\n        @test_server.tool()\n        async def slow_operation(duration: float) -&gt; str:\n            \"\"\"Simulate a slow operation.\"\"\"\n            await asyncio.sleep(duration)\n            return f\"Completed after {duration}s\"\n\n        async with create_connected_server_and_client_session(test_server._mcp_server) as (\n            server_session,\n            client_session,\n        ):\n            await client_session.initialize()\n\n            # Run multiple operations concurrently\n            start_time = time.time()\n            tasks = [\n                client_session.call_tool(\"slow_operation\", {\"duration\": 0.1})\n                for _ in range(10)\n            ]\n            results = await asyncio.gather(*tasks)\n            end_time = time.time()\n\n            # Should complete in roughly 0.1s (concurrent), not 1.0s (sequential)\n            assert end_time - start_time &lt; 0.5\n            assert len(results) == 10\n\n    @pytest.mark.anyio\n    async def test_memory_usage(self, test_server):\n        \"\"\"Test memory usage doesn't grow excessively.\"\"\"\n        import psutil\n        import os\n\n        @test_server.tool()\n        def memory_test(size: int) -&gt; str:\n            \"\"\"Tool that creates temporary data.\"\"\"\n            # Create temporary data\n            data = \"x\" * size\n            return f\"Created {len(data)} characters\"\n\n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss\n\n        # Run tool multiple times\n        async with create_connected_server_and_client_session(test_server._mcp_server) as (\n            server_session,\n            client_session,\n        ):\n            await client_session.initialize()\n\n            for _ in range(100):\n                await client_session.call_tool(\"memory_test\", {\"size\": 1000})\n\n        final_memory = process.memory_info().rss\n        memory_growth = final_memory - initial_memory\n\n        # Memory growth should be reasonable (less than 10MB for this test)\n        assert memory_growth &lt; 10 * 1024 * 1024\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#2-benchmarking","title":"2. Benchmarking","text":"<p>Benchmark critical operations: <pre><code>import pytest\nimport time\n\nclass TestBenchmarks:\n    @pytest.mark.anyio\n    async def test_tool_registration_performance(self):\n        \"\"\"Benchmark tool registration performance.\"\"\"\n\n        start_time = time.time()\n        test_server = FastMCP(\"Benchmark Server\")\n\n        # Register many tools\n        for i in range(1000):\n            def make_tool(index):\n                def tool_func(x: int) -&gt; int:\n                    return x + index\n                tool_func.__name__ = f\"tool_{index}\"\n                return tool_func\n\n            test_server.add_tool(make_tool(i))\n\n        end_time = time.time()\n        registration_time = end_time - start_time\n\n        # Should register 1000 tools in reasonable time\n        assert registration_time &lt; 5.0  # 5 seconds max\n        assert len(test_server._tool_manager._tools) == 1000\n\n    @pytest.mark.anyio\n    async def test_large_payload_handling(self, test_server):\n        \"\"\"Test handling large payloads.\"\"\"\n\n        @test_server.tool()\n        def process_large_data(data: str) -&gt; str:\n            \"\"\"Process large data payload.\"\"\"\n            return f\"Processed {len(data)} characters\"\n\n        # Test with large payload (1MB)\n        large_data = \"x\" * (1024 * 1024)\n\n        start_time = time.time()\n        result = await test_server.call_tool(\"process_large_data\", {\"data\": large_data})\n        end_time = time.time()\n\n        assert \"Processed 1048576 characters\" in result[0].text\n        assert end_time - start_time &lt; 1.0  # Should process quickly\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"development/fastmcp/testing-guide/#1-test-organization","title":"1. Test Organization","text":"<p>Organize tests logically: <pre><code># tests/test_user_management.py\nclass TestUserManagement:\n    \"\"\"Tests for user-related functionality.\"\"\"\n\n    class TestUserCreation:\n        \"\"\"Tests for creating users.\"\"\"\n        pass\n\n    class TestUserRetrieval:\n        \"\"\"Tests for retrieving users.\"\"\"\n        pass\n\n    class TestUserValidation:\n        \"\"\"Tests for user data validation.\"\"\"\n        pass\n\n# tests/test_data_processing.py\nclass TestDataProcessing:\n    \"\"\"Tests for data processing tools.\"\"\"\n    pass\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#2-test-data-management","title":"2. Test Data Management","text":"<p>Use factories for test data: <pre><code>import pytest\nfrom typing import Dict, Any\n\nclass UserFactory:\n    \"\"\"Factory for creating test users.\"\"\"\n\n    @staticmethod\n    def create(overrides: Dict[str, Any] = None) -&gt; Dict[str, Any]:\n        user = {\n            \"id\": 1,\n            \"name\": \"Test User\",\n            \"email\": \"test@example.com\",\n            \"active\": True,\n            \"created_at\": \"2024-01-01T00:00:00Z\",\n        }\n        if overrides:\n            user.update(overrides)\n        return user\n\n@pytest.fixture\ndef users():\n    \"\"\"Create test users.\"\"\"\n    return [\n        UserFactory.create({\"id\": 1, \"name\": \"Alice\"}),\n        UserFactory.create({\"id\": 2, \"name\": \"Bob\", \"active\": False}),\n    ]\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#3-error-testing","title":"3. Error Testing","text":"<p>Test error conditions systematically: <pre><code>class TestErrorHandling:\n    @pytest.mark.anyio\n    async def test_tool_validation_errors(self, test_server):\n        \"\"\"Test various validation errors.\"\"\"\n\n        @test_server.tool()\n        def validated_tool(\n            positive_int: int,\n            non_empty_string: str,\n            email: str\n        ) -&gt; str:\n            if positive_int &lt;= 0:\n                raise ValueError(\"Must be positive\")\n            if not non_empty_string.strip():\n                raise ValueError(\"Cannot be empty\")\n            if \"@\" not in email:\n                raise ValueError(\"Invalid email\")\n            return \"valid\"\n\n        # Test each validation error\n        with pytest.raises(Exception):\n            await test_server.call_tool(\"validated_tool\", {\n                \"positive_int\": -1,\n                \"non_empty_string\": \"test\",\n                \"email\": \"test@example.com\"\n            })\n\n        with pytest.raises(Exception):\n            await test_server.call_tool(\"validated_tool\", {\n                \"positive_int\": 1,\n                \"non_empty_string\": \"\",\n                \"email\": \"test@example.com\"\n            })\n\n        with pytest.raises(Exception):\n            await test_server.call_tool(\"validated_tool\", {\n                \"positive_int\": 1,\n                \"non_empty_string\": \"test\",\n                \"email\": \"invalid-email\"\n            })\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#4-test-coverage","title":"4. Test Coverage","text":"<p>Run tests with coverage: <pre><code># Install coverage\nuv add --dev pytest-cov\n\n# Run tests with coverage\nuv run pytest --cov=src --cov-report=html --cov-report=term\n\n# View coverage report\nopen htmlcov/index.html\n</code></pre></p> <p>Configuration in <code>pyproject.toml</code>: <pre><code>[tool.coverage.run]\nsource = [\"src\"]\nomit = [\"tests/*\", \"*/conftest.py\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"if TYPE_CHECKING:\",\n    \"raise NotImplementedError\",\n]\n</code></pre></p>"},{"location":"development/fastmcp/testing-guide/#running-tests","title":"Running Tests","text":""},{"location":"development/fastmcp/testing-guide/#1-basic-test-execution","title":"1. Basic Test Execution","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest tests/test_tools.py\n\n# Run specific test class\nuv run pytest tests/test_tools.py::TestTools\n\n# Run specific test method\nuv run pytest tests/test_tools.py::TestTools::test_simple_tool\n\n# Run with verbose output\nuv run pytest -v\n\n# Run tests in parallel\nuv run pytest -n auto\n</code></pre>"},{"location":"development/fastmcp/testing-guide/#2-test-filtering","title":"2. Test Filtering","text":"<pre><code># Run only integration tests\nuv run pytest -m integration\n\n# Run everything except integration tests\nuv run pytest -m \"not integration\"\n\n# Run tests matching pattern\nuv run pytest -k \"test_tool\"\n\n# Run failed tests from last run\nuv run pytest --lf\n</code></pre>"},{"location":"development/fastmcp/testing-guide/#3-debugging-tests","title":"3. Debugging Tests","text":"<pre><code># Drop into debugger on failure\nuv run pytest --pdb\n\n# Stop on first failure\nuv run pytest -x\n\n# Show local variables in tracebacks\nuv run pytest -l\n\n# Capture print statements\nuv run pytest -s\n</code></pre> <p>This comprehensive testing guide provides everything needed to thoroughly test FastMCP servers, from simple unit tests to complex integration scenarios and performance benchmarks.</p>"},{"location":"development/fastmcp/tools/","title":"Tools","text":"<p>Tools are functions that MCP clients can call to perform actions. FastMCP makes it easy to register Python functions as tools using simple decorators.</p>"},{"location":"development/fastmcp/tools/#quick-start","title":"Quick Start","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator\")\n\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\n@mcp.tool()\ndef greet(name: str) -&gt; str:\n    \"\"\"Greet someone by name.\"\"\"\n    return f\"Hello, {name}!\"\n</code></pre>"},{"location":"development/fastmcp/tools/#basic-tool-registration","title":"Basic Tool Registration","text":""},{"location":"development/fastmcp/tools/#using-the-decorator","title":"Using the Decorator","text":"<p>The <code>@mcp.tool()</code> decorator is the recommended way to register tools:</p> <pre><code>@mcp.tool()\ndef simple_tool(x: int) -&gt; str:\n    \"\"\"Convert a number to string.\"\"\"\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/tools/#using-add_tool-method","title":"Using add_tool Method","text":"<p>You can also register tools programmatically:</p> <pre><code>def my_function(x: int) -&gt; str:\n    return str(x)\n\nmcp.add_tool(my_function, name=\"convert\", description=\"Convert number to string\")\n</code></pre>"},{"location":"development/fastmcp/tools/#custom-names-and-descriptions","title":"Custom Names and Descriptions","text":"<pre><code>@mcp.tool(\n    name=\"custom_name\",\n    description=\"This overrides the docstring description\"\n)\ndef my_tool(x: int) -&gt; str:\n    \"\"\"This docstring will be ignored.\"\"\"\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/tools/#type-validation","title":"Type Validation","text":"<p>FastMCP automatically validates tool inputs and outputs using Python type hints and Pydantic.</p>"},{"location":"development/fastmcp/tools/#basic-types","title":"Basic Types","text":"<pre><code>@mcp.tool()\ndef basic_types(\n    text: str,\n    number: int,\n    decimal: float,\n    flag: bool,\n    items: list[str]\n) -&gt; dict[str, any]:\n    \"\"\"Tool with various basic types.\"\"\"\n    return {\n        \"text\": text,\n        \"number\": number,\n        \"decimal\": decimal,\n        \"flag\": flag,\n        \"items\": items\n    }\n</code></pre>"},{"location":"development/fastmcp/tools/#optional-parameters","title":"Optional Parameters","text":"<pre><code>@mcp.tool()\ndef optional_params(\n    required: str,\n    optional: str = \"default value\",\n    maybe_none: str | None = None\n) -&gt; str:\n    \"\"\"Tool with optional parameters.\"\"\"\n    result = f\"Required: {required}\"\n    if optional != \"default value\":\n        result += f\", Optional: {optional}\"\n    if maybe_none:\n        result += f\", Maybe: {maybe_none}\"\n    return result\n</code></pre>"},{"location":"development/fastmcp/tools/#parameter-descriptions","title":"Parameter Descriptions","text":"<p>Use Pydantic <code>Field</code> for detailed parameter documentation:</p> <pre><code>from pydantic import Field\n\n@mcp.tool()\ndef documented_tool(\n    name: str = Field(description=\"The person's name to greet\"),\n    title: str = Field(description=\"Optional title like Mr/Ms/Dr\", default=\"\"),\n    times: int = Field(description=\"Number of times to repeat\", default=1, ge=1, le=10)\n) -&gt; str:\n    \"\"\"Greet someone with optional title and repetition.\"\"\"\n    greeting = f\"Hello {title + ' ' if title else ''}{name}!\"\n    return \"\\n\".join([greeting] * times)\n</code></pre>"},{"location":"development/fastmcp/tools/#complex-types-with-pydantic-models","title":"Complex Types with Pydantic Models","text":"<pre><code>from pydantic import BaseModel\nfrom typing import Annotated\n\nclass User(BaseModel):\n    name: Annotated[str, Field(max_length=50)]\n    age: Annotated[int, Field(ge=0, le=150)]\n    email: str\n\n@mcp.tool()\ndef create_user(user: User) -&gt; str:\n    \"\"\"Create a new user account.\"\"\"\n    return f\"Created user {user.name} ({user.age}) with email {user.email}\"\n</code></pre>"},{"location":"development/fastmcp/tools/#validation-with-constraints","title":"Validation with Constraints","text":"<pre><code>from typing import Annotated\nfrom pydantic import Field\n\n@mcp.tool()\ndef constrained_inputs(\n    # String constraints\n    username: Annotated[str, Field(min_length=3, max_length=20, pattern=r\"^[a-zA-Z0-9_]+$\")],\n\n    # Numeric constraints\n    age: Annotated[int, Field(ge=0, le=150)],\n\n    # List constraints\n    tags: Annotated[list[str], Field(max_length=5)],\n\n    # Custom validation\n    email: Annotated[str, Field(pattern=r\"^[^@]+@[^@]+\\.[^@]+$\")]\n) -&gt; dict:\n    \"\"\"Tool with various input constraints.\"\"\"\n    return {\n        \"username\": username,\n        \"age\": age,\n        \"tags\": tags,\n        \"email\": email\n    }\n</code></pre>"},{"location":"development/fastmcp/tools/#async-tools","title":"Async Tools","text":"<p>FastMCP supports both synchronous and asynchronous tools:</p> <pre><code>import asyncio\nimport httpx\n\n@mcp.tool()\nasync def fetch_data(url: str) -&gt; str:\n    \"\"\"Fetch data from a URL asynchronously.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text\n\n@mcp.tool()\nasync def slow_operation(duration: int) -&gt; str:\n    \"\"\"Simulate a slow operation.\"\"\"\n    await asyncio.sleep(duration)\n    return f\"Completed after {duration} seconds\"\n</code></pre>"},{"location":"development/fastmcp/tools/#context-injection","title":"Context Injection","text":"<p>Tools can access MCP capabilities through context injection:</p> <pre><code>from mcp.server.fastmcp import Context\n\n@mcp.tool()\nasync def tool_with_context(text: str, ctx: Context) -&gt; str:\n    \"\"\"Tool that uses MCP context capabilities.\"\"\"\n\n    # Log messages to the client\n    await ctx.info(f\"Processing: {text}\")\n    await ctx.debug(\"Debug information\")\n    await ctx.warning(\"Warning message\")\n\n    # Report progress\n    await ctx.report_progress(25, 100, \"Started processing\")\n\n    # Simulate work\n    import asyncio\n    await asyncio.sleep(1)\n\n    await ctx.report_progress(75, 100, \"Almost done\")\n\n    # Read resources\n    try:\n        resource_data = await ctx.read_resource(\"data://config\")\n        await ctx.info(\"Found configuration data\")\n    except Exception as e:\n        await ctx.error(f\"Could not read config: {e}\")\n\n    await ctx.report_progress(100, 100, \"Complete\")\n\n    return f\"Processed: {text}\"\n</code></pre>"},{"location":"development/fastmcp/tools/#context-parameter-names","title":"Context Parameter Names","text":"<p>The context parameter can have any name as long as it's typed as <code>Context</code>:</p> <pre><code>@mcp.tool()\nasync def my_tool(x: int, context: Context) -&gt; str:\n    await context.info(\"Using 'context' parameter name\")\n    return str(x)\n\n@mcp.tool()\nasync def another_tool(x: int, ctx: Context) -&gt; str:\n    await ctx.info(\"Using 'ctx' parameter name\")\n    return str(x)\n\n@mcp.tool()\nasync def third_tool(x: int, mcp_context: Context) -&gt; str:\n    await mcp_context.info(\"Using 'mcp_context' parameter name\")\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/tools/#return-types","title":"Return Types","text":"<p>Tools can return various types of data:</p>"},{"location":"development/fastmcp/tools/#simple-returns","title":"Simple Returns","text":"<pre><code>@mcp.tool()\ndef return_string() -&gt; str:\n    return \"Hello, world!\"\n\n@mcp.tool()\ndef return_number() -&gt; int:\n    return 42\n\n@mcp.tool()\ndef return_dict() -&gt; dict:\n    return {\"status\": \"success\", \"data\": [1, 2, 3]}\n</code></pre>"},{"location":"development/fastmcp/tools/#rich-content","title":"Rich Content","text":"<pre><code>from mcp.types import TextContent, ImageContent\nfrom mcp.server.fastmcp import Image\n\n@mcp.tool()\ndef return_rich_content() -&gt; list[TextContent]:\n    \"\"\"Return rich text content.\"\"\"\n    return [\n        TextContent(type=\"text\", text=\"Here's the result:\"),\n        TextContent(type=\"text\", text=\"Some formatted data\")\n    ]\n\n@mcp.tool()\ndef return_image() -&gt; Image:\n    \"\"\"Return an image.\"\"\"\n    # Read image data\n    with open(\"chart.png\", \"rb\") as f:\n        image_data = f.read()\n\n    return Image(data=image_data, mime_type=\"image/png\")\n</code></pre>"},{"location":"development/fastmcp/tools/#multiple-return-types","title":"Multiple Return Types","text":"<pre><code>from typing import Union\n\n@mcp.tool()\ndef flexible_return(format: str) -&gt; Union[str, dict, list]:\n    \"\"\"Return different types based on format parameter.\"\"\"\n    data = {\"name\": \"John\", \"age\": 30}\n\n    if format == \"json\":\n        return data\n    elif format == \"list\":\n        return [data[\"name\"], data[\"age\"]]\n    else:\n        return f\"Name: {data['name']}, Age: {data['age']}\"\n</code></pre>"},{"location":"development/fastmcp/tools/#error-handling","title":"Error Handling","text":""},{"location":"development/fastmcp/tools/#automatic-error-handling","title":"Automatic Error Handling","text":"<p>FastMCP automatically catches and reports tool errors:</p> <pre><code>@mcp.tool()\ndef might_fail(x: int) -&gt; str:\n    \"\"\"Tool that might raise an exception.\"\"\"\n    if x &lt; 0:\n        raise ValueError(\"x must be positive\")\n    return f\"Result: {x * 2}\"\n</code></pre>"},{"location":"development/fastmcp/tools/#custom-error-types","title":"Custom Error Types","text":"<pre><code>from mcp.server.fastmcp.exceptions import ToolError\n\n@mcp.tool()\ndef custom_error(x: int) -&gt; str:\n    \"\"\"Tool with custom error handling.\"\"\"\n    if x &lt; 0:\n        raise ToolError(\"Custom error: x must be positive\")\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/tools/#error-context","title":"Error Context","text":"<pre><code>@mcp.tool()\nasync def error_with_context(x: int, ctx: Context) -&gt; str:\n    \"\"\"Tool that reports errors through context.\"\"\"\n    try:\n        if x &lt; 0:\n            raise ValueError(\"Negative values not allowed\")\n        return str(x)\n    except ValueError as e:\n        await ctx.error(f\"Validation error: {e}\")\n        raise  # Re-raise so client knows it failed\n</code></pre>"},{"location":"development/fastmcp/tools/#tool-annotations","title":"Tool Annotations","text":"<p>Add metadata to tools using annotations:</p> <pre><code>from mcp.types import ToolAnnotations\n\n@mcp.tool(annotations=ToolAnnotations(\n    audience=[\"human\", \"ai\"],\n    priority=\"high\"\n))\ndef important_tool(x: int) -&gt; str:\n    \"\"\"A high-priority tool.\"\"\"\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/tools/#advanced-examples","title":"Advanced Examples","text":""},{"location":"development/fastmcp/tools/#file-processing-tool","title":"File Processing Tool","text":"<pre><code>import json\nfrom pathlib import Path\n\n@mcp.tool()\nasync def process_file(\n    file_path: str,\n    operation: str = Field(description=\"Operation: 'read', 'size', or 'type'\"),\n    ctx: Context\n) -&gt; str:\n    \"\"\"Process a file and return information about it.\"\"\"\n\n    await ctx.info(f\"Processing file: {file_path}\")\n\n    try:\n        path = Path(file_path)\n\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n\n        if operation == \"read\":\n            await ctx.info(\"Reading file contents\")\n            return path.read_text()\n\n        elif operation == \"size\":\n            size = path.stat().st_size\n            await ctx.info(f\"File size: {size} bytes\")\n            return f\"{size} bytes\"\n\n        elif operation == \"type\":\n            suffix = path.suffix\n            await ctx.info(f\"File type: {suffix}\")\n            return suffix or \"no extension\"\n\n        else:\n            raise ValueError(f\"Unknown operation: {operation}\")\n\n    except Exception as e:\n        await ctx.error(f\"Error processing file: {e}\")\n        raise ToolError(str(e))\n</code></pre>"},{"location":"development/fastmcp/tools/#database-query-tool","title":"Database Query Tool","text":"<pre><code>import sqlite3\nfrom typing import List, Dict, Any\n\n@mcp.tool()\nasync def query_database(\n    query: str = Field(description=\"SQL query to execute\"),\n    params: List[Any] = Field(description=\"Query parameters\", default=[]),\n    ctx: Context\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Execute a SQL query and return results.\"\"\"\n\n    await ctx.info(f\"Executing query: {query}\")\n\n    try:\n        conn = sqlite3.connect(\"database.db\")\n        conn.row_factory = sqlite3.Row  # Return rows as dictionaries\n\n        cursor = conn.execute(query, params)\n        rows = cursor.fetchall()\n\n        results = [dict(row) for row in rows]\n\n        await ctx.info(f\"Query returned {len(results)} rows\")\n        return results\n\n    except sqlite3.Error as e:\n        await ctx.error(f\"Database error: {e}\")\n        raise ToolError(f\"Database error: {e}\")\n    finally:\n        conn.close()\n</code></pre>"},{"location":"development/fastmcp/tools/#http-api-tool","title":"HTTP API Tool","text":"<pre><code>import httpx\nfrom typing import Optional, Dict, Any\n\n@mcp.tool()\nasync def api_request(\n    url: str,\n    method: str = Field(description=\"HTTP method\", default=\"GET\"),\n    headers: Optional[Dict[str, str]] = Field(description=\"Request headers\", default=None),\n    data: Optional[Dict[str, Any]] = Field(description=\"Request body\", default=None),\n    ctx: Context\n) -&gt; Dict[str, Any]:\n    \"\"\"Make an HTTP API request.\"\"\"\n\n    await ctx.info(f\"Making {method} request to {url}\")\n\n    try:\n        async with httpx.AsyncClient() as client:\n            response = await client.request(\n                method=method,\n                url=url,\n                headers=headers or {},\n                json=data\n            )\n\n            result = {\n                \"status_code\": response.status_code,\n                \"headers\": dict(response.headers),\n                \"data\": response.json() if response.headers.get(\"content-type\", \"\").startswith(\"application/json\") else response.text\n            }\n\n            await ctx.info(f\"Request completed with status {response.status_code}\")\n            return result\n\n    except httpx.RequestError as e:\n        await ctx.error(f\"Request failed: {e}\")\n        raise ToolError(f\"Request failed: {e}\")\n</code></pre>"},{"location":"development/fastmcp/tools/#best-practices","title":"Best Practices","text":""},{"location":"development/fastmcp/tools/#1-use-descriptive-names-and-docstrings","title":"1. Use Descriptive Names and Docstrings","text":"<pre><code>@mcp.tool()\ndef calculate_compound_interest(\n    principal: float = Field(description=\"Initial investment amount\"),\n    rate: float = Field(description=\"Annual interest rate (as decimal, e.g., 0.05 for 5%)\"),\n    time: int = Field(description=\"Number of years\"),\n    compound_freq: int = Field(description=\"Compounding frequency per year\", default=1)\n) -&gt; float:\n    \"\"\"\n    Calculate compound interest on an investment.\n\n    Uses the formula: A = P(1 + r/n)^(nt)\n    Where A is final amount, P is principal, r is rate, n is compound frequency, t is time.\n    \"\"\"\n    return principal * (1 + rate / compound_freq) ** (compound_freq * time)\n</code></pre>"},{"location":"development/fastmcp/tools/#2-validate-inputs-early","title":"2. Validate Inputs Early","text":"<pre><code>@mcp.tool()\ndef divide_numbers(a: float, b: float) -&gt; float:\n    \"\"\"Divide two numbers.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n</code></pre>"},{"location":"development/fastmcp/tools/#3-use-context-for-user-feedback","title":"3. Use Context for User Feedback","text":"<pre><code>@mcp.tool()\nasync def long_running_task(items: list[str], ctx: Context) -&gt; str:\n    \"\"\"Process a list of items with progress reporting.\"\"\"\n    total = len(items)\n    results = []\n\n    for i, item in enumerate(items):\n        await ctx.report_progress(i, total, f\"Processing {item}\")\n\n        # Process item...\n        result = f\"processed_{item}\"\n        results.append(result)\n\n        await ctx.info(f\"Completed {item}\")\n\n    await ctx.report_progress(total, total, \"Complete\")\n    return \", \".join(results)\n</code></pre>"},{"location":"development/fastmcp/tools/#4-handle-errors-gracefully","title":"4. Handle Errors Gracefully","text":"<pre><code>@mcp.tool()\nasync def robust_tool(data: dict, ctx: Context) -&gt; str:\n    \"\"\"A robust tool with comprehensive error handling.\"\"\"\n    try:\n        # Validate required fields\n        if \"name\" not in data:\n            raise ValueError(\"Missing required field: name\")\n\n        # Process data\n        result = process_data(data)\n\n        await ctx.info(\"Processing completed successfully\")\n        return result\n\n    except ValueError as e:\n        await ctx.error(f\"Validation error: {e}\")\n        raise ToolError(f\"Invalid input: {e}\")\n    except Exception as e:\n        await ctx.error(f\"Unexpected error: {e}\")\n        raise ToolError(f\"Processing failed: {e}\")\n</code></pre>"},{"location":"development/fastmcp/tools/#5-return-structured-data","title":"5. Return Structured Data","text":"<pre><code>from pydantic import BaseModel\n\nclass AnalysisResult(BaseModel):\n    summary: str\n    score: float\n    recommendations: list[str]\n    metadata: dict[str, Any]\n\n@mcp.tool()\ndef analyze_data(data: list[float]) -&gt; AnalysisResult:\n    \"\"\"Analyze numerical data and return structured results.\"\"\"\n    avg = sum(data) / len(data)\n    score = min(avg / 100, 1.0)\n\n    return AnalysisResult(\n        summary=f\"Analyzed {len(data)} data points\",\n        score=score,\n        recommendations=[\"Consider more data points\"] if len(data) &lt; 10 else [],\n        metadata={\"count\": len(data), \"average\": avg}\n    )\n</code></pre>"},{"location":"development/fastmcp/transports/","title":"Transport Protocols","text":"<p>FastMCP supports three transport protocols for client-server communication. Each protocol has different characteristics and is suitable for different deployment scenarios.</p>"},{"location":"development/fastmcp/transports/#overview","title":"Overview","text":"Transport Use Case Pros Cons stdio Local processes Simple, efficient, reliable Process-based only streamable-http Web deployment HTTP compatible, resumable More complex setup sse Legacy HTTP Broad compatibility Limited feature set"},{"location":"development/fastmcp/transports/#stdio-transport","title":"stdio Transport","text":"<p>Recommended for: Local development, command-line tools, desktop applications</p> <p>The stdio transport uses standard input/output streams for communication. This is the most common and recommended transport for MCP servers.</p>"},{"location":"development/fastmcp/transports/#characteristics","title":"Characteristics","text":"<ul> <li>Process-based: Client and server run as separate processes</li> <li>Efficient: Direct stdin/stdout communication with minimal overhead</li> <li>Simple: No network configuration required</li> <li>Reliable: Built-in process lifecycle management</li> </ul>"},{"location":"development/fastmcp/transports/#usage","title":"Usage","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My Server\")\n\n# Add your tools, resources, prompts...\n\nif __name__ == \"__main__\":\n    # Run with stdio (most common)\n    mcp.run(\"stdio\")\n</code></pre>"},{"location":"development/fastmcp/transports/#client-connection","title":"Client Connection","text":"<p>Clients typically launch your server as a subprocess:</p> <pre><code># Client runs your server directly\npython my_server.py\n</code></pre>"},{"location":"development/fastmcp/transports/#configuration","title":"Configuration","text":"<p>Stdio transport uses minimal configuration:</p> <pre><code># No additional settings needed for stdio\nmcp = FastMCP(\"My Server\")\nmcp.run(\"stdio\")\n</code></pre>"},{"location":"development/fastmcp/transports/#streamablehttp-transport","title":"StreamableHTTP Transport","text":"<p>Recommended for: Web deployment, cloud services, containerized environments</p> <p>StreamableHTTP is a modern HTTP-based protocol that supports resumable connections and stateful sessions.</p>"},{"location":"development/fastmcp/transports/#characteristics_1","title":"Characteristics","text":"<ul> <li>HTTP-based: Works with standard web infrastructure</li> <li>Resumable: Clients can reconnect and resume sessions</li> <li>Stateful: Maintains session state between requests</li> <li>Scalable: Supports multiple concurrent clients</li> </ul>"},{"location":"development/fastmcp/transports/#usage_1","title":"Usage","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My Server\")\n\n# Add your tools, resources, prompts...\n\nif __name__ == \"__main__\":\n    # Run with StreamableHTTP\n    mcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/transports/#configuration_1","title":"Configuration","text":"<p>StreamableHTTP supports extensive configuration:</p> <pre><code>mcp = FastMCP(\n    \"My Server\",\n    # HTTP settings\n    host=\"0.0.0.0\",\n    port=8000,\n    streamable_http_path=\"/mcp\",\n\n    # StreamableHTTP specific\n    json_response=True,  # Return JSON responses\n    stateless_http=False,  # Enable session state\n)\n\nmcp.run(\"streamable-http\")\n</code></pre>"},{"location":"development/fastmcp/transports/#environment-variables","title":"Environment Variables","text":"<pre><code># Configure via environment\nFASTMCP_HOST=0.0.0.0\nFASTMCP_PORT=8000\nFASTMCP_STREAMABLE_HTTP_PATH=/mcp\nFASTMCP_JSON_RESPONSE=true\nFASTMCP_STATELESS_HTTP=false\n</code></pre>"},{"location":"development/fastmcp/transports/#client-connection_1","title":"Client Connection","text":"<p>Clients connect via HTTP:</p> <pre><code># Server endpoint\nhttp://localhost:8000/mcp\n</code></pre>"},{"location":"development/fastmcp/transports/#session-management","title":"Session Management","text":"<p>StreamableHTTP maintains sessions for each client:</p> <ul> <li>Session ID: Unique identifier per client connection</li> <li>Event Store: Optional storage for message history and resumability</li> <li>Lifecycle: Automatic cleanup when clients disconnect</li> </ul> <pre><code>from mcp.server.streamable_http import EventStore\n\n# Optional: Add event store for resumability\nevent_store = EventStore()\n\nmcp = FastMCP(\n    \"My Server\",\n    event_store=event_store\n)\n</code></pre>"},{"location":"development/fastmcp/transports/#sse-transport","title":"SSE Transport","text":"<p>Recommended for: Legacy systems, HTTP-only environments</p> <p>Server-Sent Events (SSE) provides a simpler HTTP-based transport with broader compatibility.</p>"},{"location":"development/fastmcp/transports/#characteristics_2","title":"Characteristics","text":"<ul> <li>HTTP-compatible: Works with standard HTTP infrastructure</li> <li>One-way streaming: Server-to-client via SSE, client-to-server via POST</li> <li>Simple: Easier to understand and debug than StreamableHTTP</li> <li>Limited: No built-in resumability or advanced features</li> </ul>"},{"location":"development/fastmcp/transports/#usage_2","title":"Usage","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My Server\")\n\n# Add your tools, resources, prompts...\n\nif __name__ == \"__main__\":\n    # Run with SSE\n    mcp.run(\"sse\")\n</code></pre>"},{"location":"development/fastmcp/transports/#configuration_2","title":"Configuration","text":"<pre><code>mcp = FastMCP(\n    \"My Server\",\n    # HTTP settings\n    host=\"127.0.0.1\",\n    port=8000,\n    mount_path=\"/\",\n\n    # SSE specific endpoints\n    sse_path=\"/sse\",\n    message_path=\"/messages/\",\n)\n\nmcp.run(\"sse\")\n</code></pre>"},{"location":"development/fastmcp/transports/#endpoints","title":"Endpoints","text":"<p>SSE creates two HTTP endpoints:</p> <ul> <li>GET /sse: Server-Sent Events stream for server-to-client messages</li> <li>POST /messages/: HTTP endpoint for client-to-server messages</li> </ul>"},{"location":"development/fastmcp/transports/#choosing-a-transport","title":"Choosing a Transport","text":""},{"location":"development/fastmcp/transports/#use-stdio-when","title":"Use stdio when:","text":"<ul> <li>Building command-line tools or desktop applications</li> <li>Developing locally or for single-user scenarios</li> <li>You want the simplest possible setup</li> <li>Your client can launch processes directly</li> </ul>"},{"location":"development/fastmcp/transports/#use-streamable-http-when","title":"Use streamable-http when:","text":"<ul> <li>Deploying to web servers or cloud platforms</li> <li>You need to support multiple concurrent clients</li> <li>You want resumable connections</li> <li>You're building web-based or distributed applications</li> <li>You need advanced features like authentication</li> </ul>"},{"location":"development/fastmcp/transports/#use-sse-when","title":"Use sse when:","text":"<ul> <li>You need HTTP compatibility but StreamableHTTP is too complex</li> <li>Working with legacy systems or strict HTTP requirements</li> <li>You don't need advanced features like resumability</li> <li>Building simple web integrations</li> </ul>"},{"location":"development/fastmcp/transports/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"development/fastmcp/transports/#multiple-transports","title":"Multiple Transports","text":"<p>You can run different transports in different environments:</p> <pre><code>import os\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My Server\")\n\nif __name__ == \"__main__\":\n    # Choose transport based on environment\n    transport = os.getenv(\"MCP_TRANSPORT\", \"stdio\")\n    mcp.run(transport)\n</code></pre>"},{"location":"development/fastmcp/transports/#custom-http-apps","title":"Custom HTTP Apps","text":"<p>For advanced use cases, you can access the underlying HTTP applications:</p> <pre><code># Get the Starlette app for StreamableHTTP\napp = mcp.streamable_http_app()\n\n# Get the Starlette app for SSE\napp = mcp.sse_app()\n\n# Then mount in your own FastAPI/Starlette application\n</code></pre>"},{"location":"development/fastmcp/transports/#authentication","title":"Authentication","text":"<p>HTTP transports support OAuth2 authentication:</p> <pre><code>from mcp.server.auth.provider import OAuthAuthorizationServerProvider\nfrom mcp.server.auth.settings import AuthSettings\n\n# Configure authentication (HTTP transports only)\nauth_provider = OAuthAuthorizationServerProvider(...)\nauth_settings = AuthSettings(...)\n\nmcp = FastMCP(\n    \"My Server\",\n    auth_server_provider=auth_provider,\n    auth=auth_settings\n)\n</code></pre>"},{"location":"development/fastmcp/transports/#performance-considerations","title":"Performance Considerations","text":""},{"location":"development/fastmcp/transports/#stdio","title":"stdio","text":"<ul> <li>Latency: Lowest latency, direct process communication</li> <li>Throughput: High throughput for single client</li> <li>Memory: Minimal memory overhead</li> <li>CPU: Lowest CPU usage</li> </ul>"},{"location":"development/fastmcp/transports/#streamable-http","title":"streamable-http","text":"<ul> <li>Latency: Higher latency due to HTTP overhead</li> <li>Throughput: Good throughput, supports multiple clients</li> <li>Memory: Higher memory usage for session management</li> <li>CPU: Moderate CPU usage for HTTP processing</li> </ul>"},{"location":"development/fastmcp/transports/#sse","title":"sse","text":"<ul> <li>Latency: Moderate latency, HTTP-based</li> <li>Throughput: Good for simple workloads</li> <li>Memory: Moderate memory usage</li> <li>CPU: Lower CPU usage than StreamableHTTP</li> </ul>"},{"location":"development/fastmcp/transports/#debugging-and-monitoring","title":"Debugging and Monitoring","text":""},{"location":"development/fastmcp/transports/#logging","title":"Logging","text":"<p>All transports support comprehensive logging:</p> <pre><code>mcp = FastMCP(\n    \"My Server\",\n    log_level=\"DEBUG\",  # Enable debug logging\n    debug=True          # Enable debug mode\n)\n</code></pre>"},{"location":"development/fastmcp/transports/#health-checks","title":"Health Checks","text":"<p>HTTP transports can expose health check endpoints:</p> <pre><code>@mcp.custom_route(\"/health\", methods=[\"GET\"])\nasync def health_check(request):\n    return JSONResponse({\"status\": \"healthy\"})\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/","title":"Troubleshooting","text":"<p>Complete guide for diagnosing and resolving common FastMCP issues, including error messages, debugging techniques, and performance problems.</p>"},{"location":"development/fastmcp/troubleshooting/#quick-diagnostics","title":"Quick Diagnostics","text":""},{"location":"development/fastmcp/troubleshooting/#health-check","title":"Health Check","text":"<p>First, verify your server is working with a basic health check:</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Health Check\", debug=True, log_level=\"DEBUG\")\n\n@mcp.tool()\ndef ping() -&gt; str:\n    \"\"\"Basic connectivity test.\"\"\"\n    return \"pong\"\n\n@mcp.resource(\"health://status\")\ndef health_status() -&gt; str:\n    \"\"\"Server health status.\"\"\"\n    return \"OK\"\n\nif __name__ == \"__main__\":\n    # Test with stdio\n    mcp.run(\"stdio\")\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#enable-debug-mode","title":"Enable Debug Mode","text":"<p>Always start troubleshooting with debug mode enabled:</p> <pre><code># In code\nmcp = FastMCP(\"My Server\", debug=True, log_level=\"DEBUG\")\n\n# Via environment\nexport FASTMCP_DEBUG=true\nexport FASTMCP_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#common-errors","title":"Common Errors","text":""},{"location":"development/fastmcp/troubleshooting/#1-decorator-errors","title":"1. Decorator Errors","text":""},{"location":"development/fastmcp/troubleshooting/#error-the-tool-decorator-was-used-incorrectly","title":"Error: \"The @tool decorator was used incorrectly\"","text":"<p>Problem: Using decorators without parentheses</p> <pre><code># \u274c WRONG - Missing parentheses\n@mcp.tool\ndef my_tool() -&gt; str:\n    return \"hello\"\n\n# \u274c WRONG - Called incorrectly\n@mcp.tool(my_function)\ndef my_tool() -&gt; str:\n    return \"hello\"\n</code></pre> <p>Solution: Always use parentheses when calling decorators</p> <pre><code># \u2705 CORRECT\n@mcp.tool()\ndef my_tool() -&gt; str:\n    return \"hello\"\n\n@mcp.resource(\"data://example\")\ndef my_resource() -&gt; str:\n    return \"data\"\n\n@mcp.prompt()\ndef my_prompt() -&gt; str:\n    return \"prompt text\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#error-you-must-provide-a-name-for-lambda-functions","title":"Error: \"You must provide a name for lambda functions\"","text":"<p>Problem: Using lambda functions without explicit names</p> <pre><code># \u274c WRONG - Lambda without name\nmcp.add_tool(lambda x: x * 2)\n</code></pre> <p>Solution: Use named functions or provide explicit names</p> <pre><code># \u2705 CORRECT - Named function\ndef double(x: int) -&gt; int:\n    return x * 2\n\nmcp.add_tool(double)\n\n# \u2705 CORRECT - Lambda with explicit name\nmcp.add_tool(lambda x: x * 2, name=\"double\")\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#2-type-annotation-errors","title":"2. Type Annotation Errors","text":""},{"location":"development/fastmcp/troubleshooting/#error-invalid-signature-for-use-with-fastmcp","title":"Error: \"Invalid signature for use with FastMCP\"","text":"<p>Problem: Missing or incorrect type annotations</p> <pre><code># \u274c WRONG - No type annotations\n@mcp.tool()\ndef bad_tool(x):\n    return x\n\n# \u274c WRONG - Incomplete annotations\n@mcp.tool()\ndef bad_tool(x: int):\n    return x  # Missing return type\n</code></pre> <p>Solution: Provide complete type annotations</p> <pre><code># \u2705 CORRECT\n@mcp.tool()\ndef good_tool(x: int) -&gt; str:\n    return str(x)\n\n# \u2705 CORRECT - Optional parameters\nfrom typing import Optional\n\n@mcp.tool()\ndef optional_tool(x: int, y: Optional[str] = None) -&gt; str:\n    return f\"{x}: {y or 'default'}\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#error-context-type-not-recognized","title":"Error: Context type not recognized","text":"<p>Problem: Context parameter not properly typed</p> <pre><code># \u274c WRONG - Missing type annotation\n@mcp.tool()\ndef bad_context_tool(x: int, ctx) -&gt; str:\n    return str(x)\n\n# \u274c WRONG - Wrong import or type\n@mcp.tool()\ndef bad_context_tool(x: int, ctx: object) -&gt; str:\n    return str(x)\n</code></pre> <p>Solution: Import and use Context correctly</p> <pre><code># \u2705 CORRECT\nfrom mcp.server.fastmcp import Context\n\n@mcp.tool()\ndef good_context_tool(x: int, ctx: Context) -&gt; str:\n    ctx.info(f\"Processing {x}\")\n    return str(x)\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#3-resource-errors","title":"3. Resource Errors","text":""},{"location":"development/fastmcp/troubleshooting/#error-mismatch-between-uri-parameters-and-function-parameters","title":"Error: \"Mismatch between URI parameters and function parameters\"","text":"<p>Problem: URI template parameters don't match function parameters</p> <pre><code># \u274c WRONG - Parameter name mismatch\n@mcp.resource(\"data://{user_id}\")\ndef get_user(id: str) -&gt; str:  # 'id' != 'user_id'\n    return f\"User {id}\"\n\n# \u274c WRONG - Missing parameters\n@mcp.resource(\"data://{user_id}/{post_id}\")\ndef get_post(user_id: str) -&gt; str:  # Missing 'post_id'\n    return f\"Post by {user_id}\"\n</code></pre> <p>Solution: Ensure parameter names match exactly</p> <pre><code># \u2705 CORRECT\n@mcp.resource(\"data://{user_id}\")\ndef get_user(user_id: str) -&gt; str:\n    return f\"User {user_id}\"\n\n@mcp.resource(\"data://{user_id}/{post_id}\")\ndef get_post(user_id: str, post_id: str) -&gt; str:\n    return f\"Post {post_id} by {user_id}\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#error-unknown-resource","title":"Error: \"Unknown resource\"","text":"<p>Problem: Accessing non-existent or incorrectly named resources</p> <pre><code># Resource registered as:\n@mcp.resource(\"data://users\")\ndef users() -&gt; str:\n    return \"user list\"\n\n# \u274c WRONG - Incorrect URI\ncontent = await ctx.read_resource(\"data://user\")  # Missing 's'\n</code></pre> <p>Solution: Use exact URI when accessing resources</p> <pre><code># \u2705 CORRECT\ncontent = await ctx.read_resource(\"data://users\")\n\n# For debugging, list all resources\n@mcp.tool()\nasync def list_resources(ctx: Context) -&gt; list[str]:\n    \"\"\"Debug tool to list all available resources.\"\"\"\n    server = ctx.fastmcp\n    resources = await server.list_resources()\n    return [str(r.uri) for r in resources]\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#4-authentication-errors","title":"4. Authentication Errors","text":""},{"location":"development/fastmcp/troubleshooting/#error-settingsauth-must-be-specified-if-and-only-if-auth_server_provider-is-specified","title":"Error: \"settings.auth must be specified if and only if auth_server_provider is specified\"","text":"<p>Problem: Mismatched authentication configuration</p> <pre><code># \u274c WRONG - Auth provider without settings\nfrom mcp.server.auth.provider import OAuthAuthorizationServerProvider\n\nprovider = OAuthAuthorizationServerProvider(...)\nmcp = FastMCP(\"Server\", auth_server_provider=provider)  # Missing auth settings\n\n# \u274c WRONG - Auth settings without provider\nfrom mcp.server.auth.settings import AuthSettings\n\nauth_settings = AuthSettings(...)\nmcp = FastMCP(\"Server\", auth=auth_settings)  # Missing provider\n</code></pre> <p>Solution: Configure both auth provider and settings together</p> <pre><code># \u2705 CORRECT - Both together\nfrom mcp.server.auth.provider import OAuthAuthorizationServerProvider\nfrom mcp.server.auth.settings import AuthSettings\n\nprovider = OAuthAuthorizationServerProvider(...)\nauth_settings = AuthSettings(\n    issuer_url=\"https://your-domain.com\",\n    required_scopes=[\"read\", \"write\"]\n)\n\nmcp = FastMCP(\n    \"Secure Server\",\n    auth_server_provider=provider,\n    auth=auth_settings\n)\n\n# \u2705 CORRECT - Neither (no auth)\nmcp = FastMCP(\"Public Server\")  # No auth required\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#5-transport-errors","title":"5. Transport Errors","text":""},{"location":"development/fastmcp/troubleshooting/#error-session-manager-can-only-be-accessed-after-calling-streamable_http_app","title":"Error: \"Session manager can only be accessed after calling streamable_http_app()\"","text":"<p>Problem: Accessing session manager before initialization</p> <pre><code># \u274c WRONG\nmcp = FastMCP(\"My Server\")\nmanager = mcp.session_manager  # Not initialized yet\n</code></pre> <p>Solution: Initialize the app first</p> <pre><code># \u2705 CORRECT\nmcp = FastMCP(\"My Server\")\napp = mcp.streamable_http_app()  # Initialize first\nmanager = mcp.session_manager   # Now available\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#error-port-already-in-use","title":"Error: Port already in use","text":"<p>Problem: Multiple servers using the same port</p> <pre><code># Error message\nOSError: [Errno 48] Address already in use\n</code></pre> <p>Solution: Use different ports or find/kill existing processes</p> <pre><code># Option 1: Use different port\nmcp = FastMCP(\"Server\", port=8001)\n\n# Option 2: Find and kill process using port\n# On macOS/Linux:\n# lsof -ti:8000 | xargs kill -9\n\n# Option 3: Let the system choose a free port\nimport socket\n\ndef get_free_port() -&gt; int:\n    with socket.socket() as s:\n        s.bind((\"127.0.0.1\", 0))\n        return s.getsockname()[1]\n\nmcp = FastMCP(\"Server\", port=get_free_port())\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#6-context-errors","title":"6. Context Errors","text":""},{"location":"development/fastmcp/troubleshooting/#error-context-is-not-available-outside-of-a-request","title":"Error: \"Context is not available outside of a request\"","text":"<p>Problem: Using context outside of tool/resource execution</p> <pre><code># \u274c WRONG - Context used outside tool\nmcp = FastMCP(\"Server\")\n\n@mcp.tool()\ndef setup_tool() -&gt; str:\n    return \"setup complete\"\n\n# This fails - no active request context\nctx = mcp.get_context()\nctx.info(\"Server starting\")  # Error!\n</code></pre> <p>Solution: Only use context within tools/resources</p> <pre><code># \u2705 CORRECT - Context in tool\n@mcp.tool()\ndef good_tool(ctx: Context) -&gt; str:\n    ctx.info(\"Tool executing\")  # Works!\n    return \"success\"\n\n# \u2705 CORRECT - Alternative logging\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nmcp = FastMCP(\"Server\", log_level=\"INFO\")\nlogger.info(\"Server starting\")  # Use regular logging\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"development/fastmcp/troubleshooting/#1-slow-tool-execution","title":"1. Slow Tool Execution","text":""},{"location":"development/fastmcp/troubleshooting/#problem-tools-taking-too-long-to-respond","title":"Problem: Tools taking too long to respond","text":"<pre><code># \u274c PROBLEMATIC - Blocking operation\n@mcp.tool()\ndef slow_tool(data: str) -&gt; str:\n    import time\n    time.sleep(10)  # Blocks the entire server\n    return \"done\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#solution-use-async-operations-and-timeouts","title":"Solution: Use async operations and timeouts","text":"<pre><code># \u2705 BETTER - Async with timeout\nimport asyncio\nfrom asyncio import timeout\n\n@mcp.tool()\nasync def fast_tool(data: str, ctx: Context) -&gt; str:\n    try:\n        async with timeout(5.0):  # 5 second timeout\n            await asyncio.sleep(0.1)  # Non-blocking\n            return \"done\"\n    except asyncio.TimeoutError:\n        ctx.warning(\"Operation timed out\")\n        return \"timeout\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#2-memory-issues","title":"2. Memory Issues","text":""},{"location":"development/fastmcp/troubleshooting/#problem-memory-usage-growing-over-time","title":"Problem: Memory usage growing over time","text":"<pre><code># \u274c PROBLEMATIC - Memory leak\n_cache = {}  # Global cache that grows forever\n\n@mcp.tool()\ndef leaky_tool(key: str) -&gt; str:\n    if key not in _cache:\n        _cache[key] = expensive_computation(key)\n    return _cache[key]\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#solution-use-proper-caching-with-limits","title":"Solution: Use proper caching with limits","text":"<pre><code># \u2705 BETTER - LRU cache with size limit\nfrom functools import lru_cache\n\n@lru_cache(maxsize=100)  # Limit cache size\ndef cached_computation(key: str) -&gt; str:\n    return expensive_computation(key)\n\n@mcp.tool()\ndef efficient_tool(key: str) -&gt; str:\n    return cached_computation(key)\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#3-concurrent-request-issues","title":"3. Concurrent Request Issues","text":""},{"location":"development/fastmcp/troubleshooting/#problem-tools-interfering-with-each-other","title":"Problem: Tools interfering with each other","text":"<pre><code># \u274c PROBLEMATIC - Shared mutable state\ncounter = 0\n\n@mcp.tool()\ndef unsafe_counter() -&gt; int:\n    global counter\n    counter += 1  # Race condition!\n    return counter\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#solution-use-thread-safe-operations","title":"Solution: Use thread-safe operations","text":"<pre><code># \u2705 BETTER - Thread-safe counter\nimport threading\n\n_lock = threading.Lock()\n_counter = 0\n\n@mcp.tool()\ndef safe_counter() -&gt; int:\n    global _counter\n    with _lock:\n        _counter += 1\n        return _counter\n\n# \u2705 EVEN BETTER - Stateless operations\nfrom datetime import datetime\n\n@mcp.tool()\ndef stateless_id() -&gt; str:\n    \"\"\"Generate unique ID without shared state.\"\"\"\n    return f\"id_{datetime.now().isoformat()}\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"development/fastmcp/troubleshooting/#1-enhanced-logging","title":"1. Enhanced Logging","text":"<pre><code>from mcp.server.fastmcp import FastMCP, Context\nimport logging\nimport traceback\n\n# Configure detailed logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\nmcp = FastMCP(\"Debug Server\", debug=True, log_level=\"DEBUG\")\n\n@mcp.tool()\nasync def debug_tool(data: dict, ctx: Context) -&gt; str:\n    \"\"\"Tool with comprehensive debugging.\"\"\"\n    try:\n        ctx.debug(f\"Input received: {data}\")\n        ctx.info(\"Processing started\")\n\n        # Add progress reporting\n        await ctx.report_progress(0, 100, \"Starting\")\n\n        # Your logic here\n        result = process_data(data)\n\n        await ctx.report_progress(100, 100, \"Complete\")\n        ctx.info(f\"Processing completed: {result}\")\n        return result\n\n    except Exception as e:\n        # Log full traceback\n        error_trace = traceback.format_exc()\n        ctx.error(f\"Tool failed: {e}\")\n        ctx.debug(f\"Full traceback: {error_trace}\")\n\n        # Return user-friendly error\n        return f\"Error: {str(e)}\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#2-request-tracing","title":"2. Request Tracing","text":"<pre><code>import uuid\nfrom contextvars import ContextVar\n\nrequest_id_var: ContextVar[str] = ContextVar('request_id')\n\n@mcp.tool()\nasync def traced_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with request tracing.\"\"\"\n    # Generate unique request ID\n    req_id = str(uuid.uuid4())[:8]\n    request_id_var.set(req_id)\n\n    ctx.info(f\"[{req_id}] Request started\")\n\n    try:\n        result = await process_with_trace(data, req_id)\n        ctx.info(f\"[{req_id}] Request completed\")\n        return result\n    except Exception as e:\n        ctx.error(f\"[{req_id}] Request failed: {e}\")\n        raise\n\nasync def process_with_trace(data: str, req_id: str) -&gt; str:\n    \"\"\"Helper function with tracing.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info(f\"[{req_id}] Processing: {data}\")\n    # Your logic here\n    return \"processed\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#3-health-and-status-monitoring","title":"3. Health and Status Monitoring","text":"<pre><code>import time\nimport psutil\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nmcp = FastMCP(\"Monitored Server\")\n\n# Store server metrics\n_server_stats = {\n    \"start_time\": time.time(),\n    \"request_count\": 0,\n    \"error_count\": 0,\n}\n\n@mcp.tool()\ndef get_server_stats() -&gt; Dict[str, Any]:\n    \"\"\"Get server health and statistics.\"\"\"\n    uptime = time.time() - _server_stats[\"start_time\"]\n\n    return {\n        \"uptime_seconds\": uptime,\n        \"requests_total\": _server_stats[\"request_count\"],\n        \"errors_total\": _server_stats[\"error_count\"],\n        \"memory_usage_mb\": psutil.Process().memory_info().rss / 1024 / 1024,\n        \"cpu_percent\": psutil.cpu_percent(interval=1),\n        \"timestamp\": datetime.now().isoformat(),\n    }\n\n@mcp.tool()\nasync def monitored_tool(data: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with built-in monitoring.\"\"\"\n    _server_stats[\"request_count\"] += 1\n\n    try:\n        result = await process_data(data)\n        return result\n    except Exception as e:\n        _server_stats[\"error_count\"] += 1\n        ctx.error(f\"Tool error: {e}\")\n        raise\n\n@mcp.resource(\"health://metrics\")\ndef health_metrics() -&gt; str:\n    \"\"\"Health check endpoint.\"\"\"\n    stats = get_server_stats()\n    if stats[\"memory_usage_mb\"] &gt; 1000:  # 1GB limit\n        return \"WARNING: High memory usage\"\n    return \"OK\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#common-patterns-and-solutions","title":"Common Patterns and Solutions","text":""},{"location":"development/fastmcp/troubleshooting/#1-graceful-error-handling","title":"1. Graceful Error Handling","text":"<pre><code>from typing import Union, Dict, Any\n\n@mcp.tool()\nasync def robust_api_call(url: str, ctx: Context) -&gt; Union[str, Dict[str, Any]]:\n    \"\"\"Make API call with comprehensive error handling.\"\"\"\n    import httpx\n\n    try:\n        ctx.info(f\"Making request to: {url}\")\n\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            response = await client.get(url)\n            response.raise_for_status()\n\n            data = response.json()\n            ctx.info(\"Request successful\")\n            return data\n\n    except httpx.TimeoutException:\n        error_msg = \"Request timed out\"\n        ctx.warning(error_msg)\n        return {\"error\": error_msg, \"type\": \"timeout\"}\n\n    except httpx.HTTPStatusError as e:\n        error_msg = f\"HTTP {e.response.status_code}: {e.response.text}\"\n        ctx.warning(error_msg)\n        return {\"error\": error_msg, \"type\": \"http_error\"}\n\n    except httpx.RequestError as e:\n        error_msg = f\"Network error: {str(e)}\"\n        ctx.error(error_msg)\n        return {\"error\": error_msg, \"type\": \"network_error\"}\n\n    except Exception as e:\n        error_msg = f\"Unexpected error: {str(e)}\"\n        ctx.error(error_msg)\n        # Re-raise for unexpected errors\n        raise\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#2-input-validation","title":"2. Input Validation","text":"<pre><code>from pydantic import BaseModel, Field, validator\nfrom typing import List\n\nclass ProcessingRequest(BaseModel):\n    items: List[str] = Field(min_items=1, max_items=100)\n    mode: str = Field(regex=r'^(fast|slow|batch)$')\n\n    @validator('items')\n    def validate_items(cls, v):\n        for item in v:\n            if not item.strip():\n                raise ValueError(\"Items cannot be empty\")\n        return v\n\n@mcp.tool()\nasync def validated_tool(request: ProcessingRequest, ctx: Context) -&gt; str:\n    \"\"\"Tool with automatic input validation.\"\"\"\n    ctx.info(f\"Processing {len(request.items)} items in {request.mode} mode\")\n\n    # Request is automatically validated by Pydantic\n    results = []\n    for item in request.items:\n        processed = await process_item(item, request.mode)\n        results.append(processed)\n\n    return f\"Processed {len(results)} items\"\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#3-resource-cleanup","title":"3. Resource Cleanup","text":"<pre><code>from contextlib import asynccontextmanager\nfrom typing import AsyncIterator\n\n@asynccontextmanager\nasync def database_connection():\n    \"\"\"Context manager for database connections.\"\"\"\n    conn = await connect_to_database()\n    try:\n        yield conn\n    finally:\n        await conn.close()\n\n@mcp.tool()\nasync def database_tool(query: str, ctx: Context) -&gt; str:\n    \"\"\"Tool with proper resource cleanup.\"\"\"\n    try:\n        async with database_connection() as conn:\n            ctx.info(\"Database connected\")\n            result = await conn.execute(query)\n            ctx.info(\"Query executed successfully\")\n            return str(result)\n    except Exception as e:\n        ctx.error(f\"Database error: {e}\")\n        raise\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#when-to-get-help","title":"When to Get Help","text":""},{"location":"development/fastmcp/troubleshooting/#1-check-documentation-first","title":"1. Check Documentation First","text":"<ul> <li>Review the API Reference for method signatures</li> <li>Check Examples for working code patterns</li> <li>Read Development Workflow for setup issues</li> </ul>"},{"location":"development/fastmcp/troubleshooting/#2-gather-debug-information","title":"2. Gather Debug Information","text":"<p>Before reporting issues, collect:</p> <pre><code>@mcp.tool()\ndef debug_info() -&gt; Dict[str, Any]:\n    \"\"\"Collect debug information for issue reports.\"\"\"\n    import platform\n    import sys\n    from mcp.shared.version import __version__\n\n    return {\n        \"mcp_version\": __version__,\n        \"python_version\": sys.version,\n        \"platform\": platform.platform(),\n        \"server_name\": mcp.name,\n        \"debug_mode\": mcp.settings.debug,\n        \"log_level\": mcp.settings.log_level,\n        \"tools_count\": len(mcp._tool_manager._tools),\n        \"resources_count\": len(mcp._resource_manager._resources),\n    }\n</code></pre>"},{"location":"development/fastmcp/troubleshooting/#3-create-minimal-reproduction","title":"3. Create Minimal Reproduction","text":"<p>Create the smallest possible example that reproduces your issue:</p> <pre><code>\"\"\"Minimal reproduction case for [issue description]\"\"\"\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Issue Reproduction\", debug=True)\n\n@mcp.tool()\ndef problem_tool() -&gt; str:\n    # Minimal code that demonstrates the issue\n    return \"issue reproduction\"\n\nif __name__ == \"__main__\":\n    mcp.run(\"stdio\")\n</code></pre> <p>This troubleshooting guide should help you resolve most common FastMCP issues. For complex problems, use the debugging techniques to gather detailed information before seeking help.</p>"},{"location":"development/github_actions/claude_workflow/","title":"Claude GitHub Actions Workflow Documentation","text":""},{"location":"development/github_actions/claude_workflow/#overview","title":"Overview","text":"<p>The Claude GitHub Actions workflow (<code>claude.yml</code>) integrates Anthropic's Claude AI assistant directly into our GitHub repository workflow. This automation enables intelligent code assistance, issue management, and pull request reviews through natural language interactions.</p>"},{"location":"development/github_actions/claude_workflow/#workflow-location","title":"Workflow Location","text":"<ul> <li>File: <code>.github/workflows/claude.yml</code></li> <li>Action Source: <code>anthropics/claude-code-action@beta</code></li> <li>Documentation: Claude Code Actions</li> </ul>"},{"location":"development/github_actions/claude_workflow/#how-it-works","title":"How It Works","text":""},{"location":"development/github_actions/claude_workflow/#trigger-mechanism","title":"Trigger Mechanism","text":"<p>The workflow activates when <code>@claude</code> is mentioned in:</p> <ol> <li>Issue Comments: When someone comments <code>@claude</code> on any issue</li> <li>Pull Request Review Comments: When <code>@claude</code> is mentioned in PR review comments</li> <li>Pull Request Reviews: When submitting a PR review containing <code>@claude</code></li> <li>New Issues: When creating issues with <code>@claude</code> in the title or body</li> </ol>"},{"location":"development/github_actions/claude_workflow/#workflow-execution","title":"Workflow Execution","text":"<ol> <li>Event Detection: GitHub detects the trigger events listed above</li> <li>Conditional Check: Workflow only runs if <code>@claude</code> is present in the triggering content</li> <li>Environment Setup:</li> <li>Checks out repository code</li> <li>Configures Git with bot credentials</li> <li>Sets up Ubuntu runner environment</li> <li>Claude Execution: Runs the Claude Code action with configured parameters</li> </ol>"},{"location":"development/github_actions/claude_workflow/#configuration-details","title":"Configuration Details","text":""},{"location":"development/github_actions/claude_workflow/#required-secrets","title":"Required Secrets","text":"<p>The workflow requires these GitHub secrets to be configured:</p> <ul> <li><code>ANTHROPIC_API_KEY</code>: Your Anthropic API key for Claude access</li> <li><code>GITHUB_TOKEN</code>: Automatically provided by GitHub Actions</li> </ul>"},{"location":"development/github_actions/claude_workflow/#permissions","title":"Permissions","text":"<p>The workflow requires these permissions:</p> <ul> <li><code>contents: write</code> - Create branches, commit files</li> <li><code>pull-requests: write</code> - Create/update PRs and comments</li> <li><code>issues: write</code> - Create/update issues and comments</li> <li><code>id-token: write</code> - OIDC authentication</li> </ul>"},{"location":"development/github_actions/claude_workflow/#security-allowed-tools","title":"Security: Allowed Tools","text":"<p>Claude is restricted to a whitelist of tools for security:</p>"},{"location":"development/github_actions/claude_workflow/#shell-commands","title":"Shell Commands","text":"<ul> <li>Basic operations: <code>cd</code>, <code>cp</code>, <code>find</code>, <code>ls</code>, <code>mv</code>, <code>tree</code></li> <li>Git operations: <code>git status</code>, <code>git diff</code>, <code>git log</code>, etc.</li> <li>Project tools: <code>just</code>, <code>uv</code>, <code>pytest</code>, <code>ruff</code>, <code>python</code></li> </ul>"},{"location":"development/github_actions/claude_workflow/#code-operations","title":"Code Operations","text":"<ul> <li>File operations: <code>Read</code>, <code>Write</code>, <code>Edit</code>, <code>MultiEdit</code></li> <li>Search tools: <code>Grep</code>, <code>Glob</code>, <code>RipgrepSearch</code></li> <li>Code analysis: <code>View</code>, <code>LS</code></li> </ul>"},{"location":"development/github_actions/claude_workflow/#github-integration-via-mcp","title":"GitHub Integration (via MCP)","text":"<ul> <li>Issue management: Create, update, comment on issues</li> <li>PR management: Create, review, merge pull requests</li> <li>Repository operations: Branch creation, file operations</li> <li>Security: Access to code scanning and secret scanning alerts</li> </ul>"},{"location":"development/github_actions/claude_workflow/#documentation-access","title":"Documentation Access","text":"<p>Restricted web access to trusted domains: - <code>docs.anthropic.com</code> - Anthropic documentation - <code>discordpy.readthedocs.io</code> - Discord.py documentation - <code>python.langchain.com</code> - LangChain documentation - <code>docs.astral.sh</code> - UV/Ruff documentation - And other project-relevant documentation sites</p>"},{"location":"development/github_actions/claude_workflow/#usage-examples","title":"Usage Examples","text":""},{"location":"development/github_actions/claude_workflow/#basic-issue-assistance","title":"Basic Issue Assistance","text":"<pre><code>@claude Can you help debug this error in the Discord bot startup?\n</code></pre>"},{"location":"development/github_actions/claude_workflow/#code-review-request","title":"Code Review Request","text":"<pre><code>@claude Please review this pull request and check for potential issues\n</code></pre>"},{"location":"development/github_actions/claude_workflow/#feature-development","title":"Feature Development","text":"<pre><code>@claude Implement a new command for downloading media from Twitter\n</code></pre>"},{"location":"development/github_actions/claude_workflow/#testing-assistance","title":"Testing Assistance","text":"<pre><code>@claude Run the test suite and fix any failing tests\n</code></pre>"},{"location":"development/github_actions/claude_workflow/#advanced-configuration-options","title":"Advanced Configuration Options","text":""},{"location":"development/github_actions/claude_workflow/#custom-instructions-optional","title":"Custom Instructions (Optional)","text":"<pre><code>custom_instructions: |\n  This is the boss-bot project. Always read CLAUDE.md first.\n  Focus on the immediate success criteria.\n  Follow conventional commits and create focused, working code.\n  Always read the codebase and the documentation first.\n</code></pre>"},{"location":"development/github_actions/claude_workflow/#model-selection-optional","title":"Model Selection (Optional)","text":"<pre><code># Use Claude Opus instead of default Sonnet\nmodel: \"claude-opus-4-20250514\"\n</code></pre>"},{"location":"development/github_actions/claude_workflow/#custom-trigger-phrase-optional","title":"Custom Trigger Phrase (Optional)","text":"<pre><code># Use /claude instead of @claude\ntrigger_phrase: \"/claude\"\n</code></pre>"},{"location":"development/github_actions/claude_workflow/#environment-variables-optional","title":"Environment Variables (Optional)","text":"<pre><code>claude_env: |\n  NODE_ENV: test\n  DEBUG: true\n</code></pre>"},{"location":"development/github_actions/claude_workflow/#best-practices","title":"Best Practices","text":""},{"location":"development/github_actions/claude_workflow/#for-users","title":"For Users","text":"<ol> <li>Be Specific: Provide clear, detailed requests to Claude</li> <li>Reference Context: Mention relevant files, issues, or PRs</li> <li>Follow Up: Engage in conversation to refine solutions</li> <li>Review Changes: Always review Claude's suggestions before merging</li> </ol>"},{"location":"development/github_actions/claude_workflow/#for-maintainers","title":"For Maintainers","text":"<ol> <li>Monitor Usage: Track Claude interactions for effectiveness</li> <li>Update Permissions: Regularly review and update allowed tools</li> <li>Secure Secrets: Ensure API keys are properly stored as secrets</li> <li>Test Changes: Validate workflow changes in a safe environment</li> </ol>"},{"location":"development/github_actions/claude_workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/github_actions/claude_workflow/#common-issues","title":"Common Issues","text":"<ol> <li>No Response from Claude</li> <li>Check if <code>@claude</code> is properly mentioned</li> <li>Verify <code>ANTHROPIC_API_KEY</code> secret is set</li> <li> <p>Ensure workflow has proper permissions</p> </li> <li> <p>Permission Errors</p> </li> <li>Review repository permissions settings</li> <li>Check if bot has necessary access rights</li> <li> <p>Verify token permissions are sufficient</p> </li> <li> <p>Tool Restrictions</p> </li> <li>Claude may be blocked from certain operations</li> <li>Review <code>allowed_tools</code> configuration</li> <li>Add necessary tools to the whitelist</li> </ol>"},{"location":"development/github_actions/claude_workflow/#debugging-steps","title":"Debugging Steps","text":"<ol> <li>Check workflow run logs in GitHub Actions tab</li> <li>Verify trigger conditions are met</li> <li>Review Claude's error messages in comments</li> <li>Test with simple requests first</li> </ol>"},{"location":"development/github_actions/claude_workflow/#security-considerations","title":"Security Considerations","text":""},{"location":"development/github_actions/claude_workflow/#tool-restrictions","title":"Tool Restrictions","text":"<ul> <li>Claude is limited to whitelisted tools only</li> <li>No access to sensitive operations like deployment</li> <li>Web access restricted to trusted documentation domains</li> </ul>"},{"location":"development/github_actions/claude_workflow/#secrets-management","title":"Secrets Management","text":"<ul> <li>API keys stored as encrypted GitHub secrets</li> <li>No sensitive data exposed in workflow files</li> <li>Bot identity clearly marked in commits</li> </ul>"},{"location":"development/github_actions/claude_workflow/#access-control","title":"Access Control","text":"<ul> <li>Workflow only responds to explicit <code>@claude</code> mentions</li> <li>No automatic execution without user trigger</li> <li>All actions are logged and auditable</li> </ul>"},{"location":"development/github_actions/claude_workflow/#integration-with-boss-bot","title":"Integration with Boss-Bot","text":""},{"location":"development/github_actions/claude_workflow/#project-specific-features","title":"Project-Specific Features","text":"<ul> <li>CLAUDE.md Awareness: Claude reads project instructions automatically</li> <li>Just Task Runner: Integration with project build system</li> <li>UV Package Manager: Python dependency management support</li> <li>Discord.py Patterns: Understanding of Discord bot architecture</li> <li>AI Architecture: Knowledge of LangChain/LangGraph components</li> </ul>"},{"location":"development/github_actions/claude_workflow/#development-workflow","title":"Development Workflow","text":"<ol> <li>Issue Creation: Claude can help create detailed issues</li> <li>Code Development: Assists with implementation and debugging</li> <li>Testing: Runs test suites and fixes failures</li> <li>Code Review: Provides automated PR reviews</li> <li>Documentation: Updates documentation as needed</li> </ol>"},{"location":"development/github_actions/claude_workflow/#future-enhancements","title":"Future Enhancements","text":""},{"location":"development/github_actions/claude_workflow/#planned-improvements","title":"Planned Improvements","text":"<ul> <li>Custom Prompts: Project-specific behavioral instructions</li> <li>Advanced Triggers: More sophisticated activation conditions</li> <li>Integration Webhooks: Better integration with external tools</li> <li>Performance Monitoring: Usage analytics and optimization</li> </ul>"},{"location":"development/github_actions/claude_workflow/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<ul> <li>Track Claude usage patterns</li> <li>Measure resolution effectiveness</li> <li>Monitor response times and accuracy</li> <li>Collect user feedback for improvements</li> </ul> <p>This documentation covers the Claude GitHub Actions workflow as of the current implementation. For the latest updates and features, refer to the official Anthropic Claude Code documentation.</p>"},{"location":"development/testing/pytest_recording/","title":"VCR Cassettes","text":"<p>This directory contains VCR cassettes for testing API interactions with external services like Twitter, Reddit, YouTube, etc.</p>"},{"location":"development/testing/pytest_recording/#security-considerations","title":"Security Considerations","text":"<p>All cassettes in this directory are safe to commit to the repository because:</p> <ol> <li>Request Filtering: Sensitive headers like <code>Authorization</code>, <code>Cookie</code>, <code>X-API-Key</code> are replaced with dummy values</li> <li>Response Sanitization: Rate limiting headers, request IDs, and cookies are normalized</li> <li>Body Filtering: API keys, tokens, usernames, and passwords in request bodies are replaced with dummy values</li> <li>URL Matching: Similar requests (e.g., Twitter API calls) are matched by pattern rather than exact URL</li> </ol>"},{"location":"development/testing/pytest_recording/#cassette-structure","title":"Cassette Structure","text":"<pre><code>interactions:\n- request:\n    body: null\n    headers: {}  # All sensitive headers removed\n    method: GET\n    uri: https://x.com/i/api/graphql/...\n  response:\n    body:\n      string: !!binary |\n        # Base64 encoded response (gzipped)\n    headers:\n      # Sanitized headers with fake values\n      x-rate-limit-remaining: \"49\"\n      set-cookie:\n      - guest_id=v1%3FAKEBROTHER; Path=/; Domain=.x.com\n    status:\n      code: 200\n      message: OK\n</code></pre>"},{"location":"development/testing/pytest_recording/#usage-in-tests","title":"Usage in Tests","text":"<pre><code>import pytest\n\n@pytest.mark.vcr\nasync def test_twitter_download():\n    \"\"\"Test Twitter download with VCR recording.\"\"\"\n    # This will use/create a cassette file automatically\n    async with AsyncGalleryDL() as client:\n        async for item in client.download(\"https://twitter.com/example\"):\n            assert item[\"extractor\"] == \"twitter\"\n</code></pre>"},{"location":"development/testing/pytest_recording/#cassette-files","title":"Cassette Files","text":"<ul> <li><code>test_twitter_api_download.yaml</code> - Twitter API interactions</li> <li><code>test_reddit_api_download.yaml</code> - Reddit API interactions</li> <li><code>test_youtube_api_download.yaml</code> - YouTube API interactions</li> <li><code>test_strategy_fallback.yaml</code> - Strategy fallback scenarios</li> </ul>"},{"location":"development/testing/pytest_recording/#regenerating-cassettes","title":"Regenerating Cassettes","text":"<p>To regenerate cassettes with new API responses:</p> <ol> <li>Delete the existing cassette file</li> <li>Run the test with real API access</li> <li>The new cassette will be recorded with sanitized data</li> <li>Verify the cassette contains no sensitive information before committing</li> </ol>"},{"location":"development/testing/pytest_recording/#verification-checklist","title":"Verification Checklist","text":"<p>Before committing cassettes, ensure:</p> <ul> <li> No real API keys or tokens in headers</li> <li> No real usernames/passwords in request bodies</li> <li> Cookies are replaced with fake values</li> <li> Rate limiting headers use dummy values</li> <li> Request IDs are anonymized</li> </ul>"},{"location":"development/typing/monkeytype/","title":"PYTEST.md","text":"<p>A comprehensive guide to using MonkeyType with pytest to automatically generate type annotations for any Python repository.</p>"},{"location":"development/typing/monkeytype/#overview","title":"Overview","text":"<p>MonkeyType can trace your pytest test runs to collect runtime type information and automatically generate type annotations. This is particularly useful for adding types to existing codebases or discovering type issues.</p>"},{"location":"development/typing/monkeytype/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have MonkeyType and pytest installed:</p> <pre><code>uv add monkeytype pytest\n</code></pre>"},{"location":"development/typing/monkeytype/#basic-workflow","title":"Basic Workflow","text":""},{"location":"development/typing/monkeytype/#1-run-tests-with-monkeytype-tracing","title":"1. Run Tests with MonkeyType Tracing","text":"<p>Trace your entire test suite to collect type information:</p> <pre><code># Basic tracing\nuv run monkeytype run pytest\n\n# Trace with specific pytest options\nuv run monkeytype run pytest tests/ -v\n\n# Trace specific test files\nuv run monkeytype run pytest tests/test_mymodule.py\n\n# Trace with pytest markers\nuv run monkeytype run pytest -m \"not slow\"\n</code></pre> <p>This creates a <code>monkeytype.sqlite3</code> database file containing type traces.</p>"},{"location":"development/typing/monkeytype/#2-generate-type-stubs","title":"2. Generate Type Stubs","text":"<p>View the collected type information:</p> <pre><code># Generate stub for a specific module\nuv run monkeytype stub mymodule\n\n# Generate stub for a package\nuv run monkeytype stub mypackage.submodule\n</code></pre>"},{"location":"development/typing/monkeytype/#3-apply-type-annotations","title":"3. Apply Type Annotations","text":"<p>Apply the discovered types directly to your source code:</p> <pre><code># Apply types to a module\nuv run monkeytype apply mymodule\n\n# Apply types to multiple modules\nuv run monkeytype apply mypackage.module1 mypackage.module2\n</code></pre>"},{"location":"development/typing/monkeytype/#4-verify-with-type-checker","title":"4. Verify with Type Checker","text":"<p>Check your newly annotated code:</p> <pre><code># Type check with mypy\nuv run mypy .\n\n# Type check specific files\nuv run mypy src/mymodule.py\n</code></pre>"},{"location":"development/typing/monkeytype/#advanced-usage","title":"Advanced Usage","text":""},{"location":"development/typing/monkeytype/#custom-configuration","title":"Custom Configuration","text":"<p>Create a <code>monkeytype_config.py</code> file in your project root:</p> <pre><code>from monkeytype.config import DefaultConfig\nfrom monkeytype.db.sqlite import SQLiteStore\n\nclass MyConfig(DefaultConfig):\n    def trace_store(self):\n        return SQLiteStore.make_store(\"custom_traces.sqlite3\")\n\n    def code_filter(self):\n        # Only trace your own code, not dependencies\n        def filter_fn(code):\n            return code.co_filename.startswith('/path/to/your/project')\n        return filter_fn\n</code></pre> <p>Use the custom config:</p> <pre><code>uv run monkeytype -c monkeytype_config run pytest\n</code></pre>"},{"location":"development/typing/monkeytype/#targeting-specific-code","title":"Targeting Specific Code","text":""},{"location":"development/typing/monkeytype/#filter-by-module-path","title":"Filter by Module Path","text":"<pre><code># Only trace specific modules during test runs\nuv run monkeytype run pytest --tb=short\nuv run monkeytype stub myapp.models\nuv run monkeytype apply myapp.models\n</code></pre>"},{"location":"development/typing/monkeytype/#trace-specific-test-categories","title":"Trace Specific Test Categories","text":"<pre><code># Trace only integration tests\nuv run monkeytype run pytest tests/integration/\n\n# Trace only unit tests for a component\nuv run monkeytype run pytest tests/unit/test_database.py\n</code></pre>"},{"location":"development/typing/monkeytype/#working-with-test-data","title":"Working with Test Data","text":""},{"location":"development/typing/monkeytype/#clean-slate-approach","title":"Clean Slate Approach","text":"<pre><code># Remove old traces\nrm monkeytype.sqlite3\n\n# Run comprehensive test suite\nuv run monkeytype run pytest --maxfail=1\n\n# Apply all discovered types\nuv run monkeytype list-modules | xargs -I {} uv run monkeytype apply {}\n</code></pre>"},{"location":"development/typing/monkeytype/#incremental-approach","title":"Incremental Approach","text":"<pre><code># Add more traces to existing data\nuv run monkeytype run pytest tests/new_feature/\n\n# Check what modules have traces\nuv run monkeytype list-modules\n\n# Apply types to newly traced modules\nuv run monkeytype apply new_feature.handlers\n</code></pre>"},{"location":"development/typing/monkeytype/#integration-patterns","title":"Integration Patterns","text":""},{"location":"development/typing/monkeytype/#with-cicd","title":"With CI/CD","text":"<p>Create a script to check type coverage:</p> <pre><code>#!/bin/bash\n# scripts/check_types.sh\n\n# Run tests with tracing\nuv run monkeytype run pytest\n\n# Generate stubs for review\nfor module in $(uv run monkeytype list-modules); do\n    echo \"=== $module ===\"\n    uv run monkeytype stub \"$module\"\ndone\n\n# Type check current code\nuv run mypy . --ignore-missing-imports\n</code></pre>"},{"location":"development/typing/monkeytype/#with-pre-commit-hooks","title":"With Pre-commit Hooks","text":"<p>Add to <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: local\n    hooks:\n      - id: monkeytype-check\n        name: MonkeyType type consistency\n        entry: bash -c 'uv run monkeytype run pytest &amp;&amp; uv run mypy .'\n        language: system\n        pass_filenames: false\n</code></pre>"},{"location":"development/typing/monkeytype/#with-tox","title":"With tox","text":"<p>Add to <code>tox.ini</code>:</p> <pre><code>[testenv:types]\ndeps =\n    monkeytype\n    mypy\n    pytest\ncommands =\n    monkeytype run pytest\n    mypy src/\n</code></pre>"},{"location":"development/typing/monkeytype/#best-practices","title":"Best Practices","text":""},{"location":"development/typing/monkeytype/#1-comprehensive-test-coverage","title":"1. Comprehensive Test Coverage","text":"<ul> <li>Ensure your tests exercise all code paths</li> <li>Include edge cases and error conditions</li> <li>Test with realistic data types</li> </ul>"},{"location":"development/typing/monkeytype/#2-iterative-refinement","title":"2. Iterative Refinement","text":"<pre><code># Initial pass\nuv run monkeytype run pytest\nuv run monkeytype apply mymodule\n\n# Review and fix issues\nuv run mypy mymodule.py\n\n# Re-trace after fixes\nrm monkeytype.sqlite3\nuv run monkeytype run pytest\nuv run monkeytype apply mymodule\n</code></pre>"},{"location":"development/typing/monkeytype/#3-module-by-module-approach","title":"3. Module-by-Module Approach","text":"<pre><code># Focus on one module at a time\nuv run monkeytype run pytest tests/test_auth.py\nuv run monkeytype stub auth\n# Review output, then apply\nuv run monkeytype apply auth\n</code></pre>"},{"location":"development/typing/monkeytype/#4-handling-complex-types","title":"4. Handling Complex Types","text":"<ul> <li>Use parametrized tests to capture type variations</li> <li>Include tests with different data shapes</li> <li>Test both success and failure cases</li> </ul>"},{"location":"development/typing/monkeytype/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"development/typing/monkeytype/#issue-too-many-generic-types","title":"Issue: Too Many Generic Types","text":"<p>Problem: MonkeyType generates <code>Any</code> or overly generic types.</p> <p>Solution: Add more specific test cases: <pre><code>def test_user_creation_with_various_types():\n    # This helps MonkeyType see concrete types\n    user1 = create_user(name=\"John\", age=30, active=True)\n    user2 = create_user(name=\"Jane\", age=25, active=False)\n    assert isinstance(user1.age, int)\n</code></pre></p>"},{"location":"development/typing/monkeytype/#issue-missing-type-information","title":"Issue: Missing Type Information","text":"<p>Problem: Some functions aren't getting traced.</p> <p>Solution: Ensure they're called during tests: <pre><code># Check what was traced\nuv run monkeytype list-modules\n\n# Add tests for missing functions\n# Re-run tracing\n</code></pre></p>"},{"location":"development/typing/monkeytype/#issue-conflicting-type-information","title":"Issue: Conflicting Type Information","text":"<p>Problem: Different test runs produce different types.</p> <p>Solution: Clean slate approach: <pre><code>rm monkeytype.sqlite3\nuv run monkeytype run pytest -x  # Stop on first failure\n</code></pre></p>"},{"location":"development/typing/monkeytype/#example-workflow-for-new-project","title":"Example Workflow for New Project","text":"<pre><code># 1. Set up environment\nuv add monkeytype pytest mypy\n\n# 2. Run existing tests with tracing\nuv run monkeytype run pytest\n\n# 3. See what modules were traced\nuv run monkeytype list-modules\n\n# 4. Start with core modules\nuv run monkeytype stub myapp.models\nuv run monkeytype apply myapp.models\n\n# 5. Check for type errors\nuv run mypy myapp/models.py\n\n# 6. Fix any issues and repeat\n# 7. Continue with other modules\nuv run monkeytype apply myapp.handlers\nuv run mypy myapp/handlers.py\n\n# 8. Full project type check\nuv run mypy .\n</code></pre> <p>This workflow helps you systematically add type annotations to any Python project using pytest as the execution vehicle for MonkeyType's runtime type collection.</p>"},{"location":"til/","title":"Today I Learned (TIL)","text":"<p>A collection of concise write-ups on small things I(you/us/one adobe) learn day to day across various languages and technologies involving AI.</p>"},{"location":"til/#latest-posts","title":"Latest Posts","text":""},{"location":"coverage/","title":"Coverage","text":""}]}